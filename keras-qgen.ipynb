{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Lambda\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "from qgen.data import training_data, test_data, collapse_documents, expand_answers, _read_data\n",
    "\n",
    "import qgen.utils as utils\n",
    "from qgen.embedding import glove\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMETERS ####\n",
    "\n",
    "VOCAB_SIZE = glove.shape[0]\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "GRU_UNITS = 100\n",
    "MAX_DOC_SIZE = None\n",
    "MAX_ANSWER_SIZE = None\n",
    "MAX_Q_SIZE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAINING MODEL ####\n",
    "\n",
    "document_tokens = Input(shape=(MAX_DOC_SIZE,), name=\"document_tokens\")\n",
    "\n",
    "embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIMENS, weights=[glove], mask_zero = True, name = 'embedding')\n",
    "document_emb = embedding(document_tokens)\n",
    "\n",
    "answer_outputs = Bidirectional(GRU(GRU_UNITS, return_sequences=True), name = 'answer_outputs')(document_emb)\n",
    "answer_tags = Dense(1, activation = 'sigmoid', name = 'answer_tags')(answer_outputs)\n",
    "\n",
    "encoder_input_mask = Input(shape=(MAX_ANSWER_SIZE, MAX_DOC_SIZE), name=\"encoder_input_mask\")\n",
    "encoder_inputs = Lambda(lambda x: K.batch_dot(x[0], x[1]), name=\"encoder_inputs\")([encoder_input_mask, answer_outputs])\n",
    "encoder_cell = GRU(2 * GRU_UNITS, name = 'encoder_cell')(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(MAX_Q_SIZE,), name=\"decoder_inputs\")\n",
    "decoder_emb = embedding(decoder_inputs)\n",
    "decoder_emb.trainable = False\n",
    "decoder_cell = GRU(2 * GRU_UNITS, return_sequences = True, name = 'decoder_cell')\n",
    "decoder_states = decoder_cell(decoder_emb, initial_state = [encoder_cell])\n",
    "\n",
    "decoder_projection = Dense(VOCAB_SIZE, name = 'decoder_projection', activation = 'softmax')\n",
    "decoder_outputs = decoder_projection(decoder_states)\n",
    "\n",
    "total_model = Model([document_tokens, decoder_inputs, encoder_input_mask], [answer_tags, decoder_outputs])\n",
    "plot_model(total_model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INFERENCE MODEL ####\n",
    "\n",
    "decoder_inputs_dynamic = Input(shape=(1,), name=\"decoder_inputs_dynamic\")\n",
    "decoder_emb_dynamic = embedding(decoder_inputs_dynamic)\n",
    "decoder_init_state_dynamic = Input(shape=(2 * GRU_UNITS,), name = 'decoder_init_state_dynamic') #the embedding of the previous word\n",
    "decoder_states_dynamic = decoder_cell(decoder_emb_dynamic, initial_state = [decoder_init_state_dynamic])\n",
    "decoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)\n",
    "\n",
    "answer_model = Model(document_tokens, [answer_tags])\n",
    "decoder_initial_state_model = Model([document_tokens, encoder_input_mask], [encoder_cell])\n",
    "question_model = Model([decoder_inputs_dynamic, decoder_init_state_dynamic], [decoder_outputs_dynamic, decoder_states_dynamic])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMPILE TRAINING MODEL ####\n",
    "\n",
    "opti = Adam(lr=0.01)\n",
    "total_model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "                    , optimizer=opti\n",
    "                    , loss_weights = [0.5, 0.5]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "\n",
    "training_data_gen = training_data()\n",
    "# training_data_gen = [next(training_data_gen)]\n",
    "test_data_gen = test_data()\n",
    "\n",
    "training_loss_history = []\n",
    "test_loss_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0: Train Loss: [5.055319, 0.9013131, 9.209325] | Test Loss: [4.6524873, 0.6337282, 8.671247]\n",
      "1: Train Loss: [4.645729, 0.57349265, 8.717965] | Test Loss: [4.212451, 0.91654813, 7.5083537]\n",
      "2: Train Loss: [3.9394507, 0.4959245, 7.382977] | Test Loss: [3.5608807, 0.60691094, 6.51485]\n",
      "3: Train Loss: [3.644256, 0.50674725, 6.781765] | Test Loss: [3.3598495, 0.5141988, 6.2055]\n",
      "4: Train Loss: [3.2938123, 0.42791322, 6.1597114] | Test Loss: [3.341103, 0.45441782, 6.2277884]\n",
      "5: Train Loss: [3.3498712, 0.4937119, 6.2060304] | Test Loss: [3.3614786, 0.42073113, 6.302226]\n",
      "6: Train Loss: [3.4325159, 0.40416187, 6.46087] | Test Loss: [3.3437803, 0.33606195, 6.3514986]\n",
      "7: Train Loss: [3.2320545, 0.41837296, 6.045736] | Test Loss: [3.176444, 0.40054023, 5.9523478]\n",
      "8: Train Loss: [3.176085, 0.36128604, 5.990884] | Test Loss: [3.278519, 0.44428986, 6.112748]\n",
      "9: Train Loss: [3.0794191, 0.34687468, 5.8119636] | Test Loss: [3.0889208, 0.4403595, 5.737482]\n",
      "10: Train Loss: [3.1363404, 0.38268307, 5.8899975] | Test Loss: [3.0226357, 0.351091, 5.6941805]\n",
      "11: Train Loss: [3.0401506, 0.2891977, 5.7911034] | Test Loss: [2.8871636, 0.3592088, 5.4151187]\n",
      "12: Train Loss: [2.971475, 0.27316016, 5.66979] | Test Loss: [3.1114662, 0.34463352, 5.8782988]\n",
      "13: Train Loss: [3.064057, 0.2954075, 5.832707] | Test Loss: [3.001037, 0.37780026, 5.6242733]\n",
      "14: Train Loss: [2.9466848, 0.31882864, 5.574541] | Test Loss: [3.102725, 0.31725585, 5.888194]\n",
      "15: Train Loss: [2.9043217, 0.36362293, 5.44502] | Test Loss: [3.0750399, 0.34319174, 5.806888]\n",
      "16: Train Loss: [3.0468478, 0.44006371, 5.653632] | Test Loss: [2.9492493, 0.33636934, 5.562129]\n",
      "17: Train Loss: [3.0463495, 0.3917637, 5.7009354] | Test Loss: [2.817973, 0.36835048, 5.2675953]\n",
      "18: Train Loss: [2.9913938, 0.37134805, 5.6114397] | Test Loss: [2.8954167, 0.37198377, 5.41885]\n",
      "19: Train Loss: [2.8612466, 0.39191705, 5.330576] | Test Loss: [2.850089, 0.3269485, 5.3732295]\n",
      "20: Train Loss: [2.7839131, 0.34563494, 5.2221913] | Test Loss: [2.8305094, 0.36804277, 5.292976]\n",
      "21: Train Loss: [2.9885352, 0.34299698, 5.6340733] | Test Loss: [2.9237819, 0.37946165, 5.468102]\n",
      "22: Train Loss: [2.7427917, 0.35889328, 5.12669] | Test Loss: [2.9709525, 0.411497, 5.530408]\n",
      "23: Train Loss: [2.8765774, 0.33962542, 5.4135294] | Test Loss: [2.850723, 0.33430746, 5.3671384]\n",
      "24: Train Loss: [2.9079812, 0.334434, 5.4815283] | Test Loss: [2.8270285, 0.44454795, 5.209509]\n",
      "25: Train Loss: [2.853786, 0.3626023, 5.3449697] | Test Loss: [2.7361186, 0.2830036, 5.1892333]\n",
      "26: Train Loss: [2.8584666, 0.35983986, 5.3570933] | Test Loss: [2.8778102, 0.33667707, 5.4189434]\n",
      "27: Train Loss: [2.6824884, 0.35361683, 5.01136] | Test Loss: [2.941562, 0.43225244, 5.4508715]\n",
      "28: Train Loss: [2.914936, 0.35013384, 5.479738] | Test Loss: [2.8043823, 0.34980512, 5.25896]\n",
      "29: Train Loss: [2.786461, 0.3171793, 5.255743] | Test Loss: [2.7293305, 0.3528761, 5.105785]\n",
      "30: Train Loss: [2.8930976, 0.35189703, 5.434298] | Test Loss: [2.7858758, 0.33555683, 5.2361946]\n",
      "31: Train Loss: [2.7718751, 0.3285998, 5.2151504] | Test Loss: [2.820708, 0.3520071, 5.289409]\n",
      "32: Train Loss: [2.882407, 0.28677008, 5.478044] | Test Loss: [2.785096, 0.35708264, 5.213109]\n",
      "33: Train Loss: [2.9414039, 0.32605645, 5.5567513] | Test Loss: [2.8056993, 0.35177842, 5.25962]\n",
      "34: Train Loss: [2.8738387, 0.37179315, 5.375884] | Test Loss: [2.8178055, 0.39470965, 5.2409015]\n",
      "35: Train Loss: [2.7200823, 0.2883863, 5.151778] | Test Loss: [2.9615815, 0.3635426, 5.5596204]\n",
      "36: Train Loss: [2.877049, 0.36003906, 5.3940587] | Test Loss: [2.7644083, 0.31107765, 5.217739]\n",
      "37: Train Loss: [2.931854, 0.34154043, 5.5221677] | Test Loss: [2.807605, 0.3098891, 5.3053207]\n",
      "38: Train Loss: [2.7001653, 0.322736, 5.0775948] | Test Loss: [2.804936, 0.3127039, 5.297168]\n",
      "39: Train Loss: [2.8217854, 0.3345815, 5.3089895] | Test Loss: [2.8460152, 0.3958435, 5.296187]\n",
      "40: Train Loss: [2.8115509, 0.32943007, 5.2936716] | Test Loss: [2.7892745, 0.32345623, 5.2550926]\n",
      "41: Train Loss: [2.830147, 0.3389024, 5.3213916] | Test Loss: [2.7259285, 0.3007179, 5.1511393]\n",
      "42: Train Loss: [2.6126864, 0.28723213, 4.938141] | Test Loss: [2.818056, 0.45263806, 5.183474]\n",
      "43: Train Loss: [2.756913, 0.29185972, 5.2219663] | Test Loss: [2.750745, 0.38730562, 5.1141844]\n",
      "44: Train Loss: [2.6841428, 0.4141572, 4.9541283] | Test Loss: [2.8080664, 0.37430558, 5.241827]\n",
      "45: Train Loss: [2.8570163, 0.33624658, 5.377786] | Test Loss: [2.8384273, 0.35856664, 5.318288]\n",
      "46: Train Loss: [2.6329982, 0.32087332, 4.945123] | Test Loss: [2.910669, 0.34470156, 5.4766364]\n",
      "47: Train Loss: [2.743395, 0.29403687, 5.1927533] | Test Loss: [2.777232, 0.3382022, 5.216262]\n",
      "48: Train Loss: [2.6775653, 0.3554366, 4.999694] | Test Loss: [2.764833, 0.32661557, 5.2030506]\n",
      "49: Train Loss: [2.8211615, 0.39787188, 5.244451] | Test Loss: [2.7896955, 0.38129038, 5.1981006]\n",
      "50: Train Loss: [2.7945368, 0.33146283, 5.257611] | Test Loss: [2.7608736, 0.31831115, 5.203436]\n",
      "51: Train Loss: [2.8292682, 0.30186194, 5.3566747] | Test Loss: [2.742745, 0.29934588, 5.186144]\n",
      "52: Train Loss: [2.9215307, 0.42498198, 5.4180794] | Test Loss: [2.7526956, 0.33495563, 5.1704354]\n",
      "53: Train Loss: [2.8399441, 0.36437738, 5.3155107] | Test Loss: [2.7675805, 0.3814888, 5.153672]\n",
      "54: Train Loss: [2.9308207, 0.28659356, 5.575048] | Test Loss: [2.9294918, 0.33770704, 5.5212765]\n",
      "55: Train Loss: [2.7525935, 0.35929418, 5.145893] | Test Loss: [2.7002633, 0.33071423, 5.0698123]\n",
      "56: Train Loss: [2.66556, 0.35503227, 4.9760876] | Test Loss: [2.7251606, 0.37216774, 5.0781536]\n",
      "57: Train Loss: [2.6384907, 0.33023185, 4.9467497] | Test Loss: [2.737003, 0.31490913, 5.159097]\n",
      "58: Train Loss: [2.7072191, 0.30166683, 5.1127715] | Test Loss: [2.810272, 0.34813663, 5.2724075]\n",
      "59: Train Loss: [2.6536055, 0.3247497, 4.982461] | Test Loss: [2.8692954, 0.38830268, 5.350288]\n",
      "60: Train Loss: [2.731306, 0.34963298, 5.1129794] | Test Loss: [2.8104107, 0.29413602, 5.3266854]\n",
      "61: Train Loss: [2.7757, 0.33087972, 5.2205205] | Test Loss: [2.707902, 0.3186974, 5.0971065]\n",
      "62: Train Loss: [2.821235, 0.31346926, 5.3290005] | Test Loss: [2.599429, 0.27949056, 4.9193673]\n",
      "63: Train Loss: [2.8709552, 0.36241388, 5.3794966] | Test Loss: [2.7880087, 0.3323021, 5.2437153]\n",
      "64: Train Loss: [2.7387497, 0.33077288, 5.1467266] | Test Loss: [2.8121703, 0.32381824, 5.3005223]\n",
      "65: Train Loss: [2.7314866, 0.32944822, 5.133525] | Test Loss: [2.7329478, 0.2997792, 5.166116]\n",
      "66: Train Loss: [2.567245, 0.38494322, 4.749547] | Test Loss: [2.7795253, 0.34062585, 5.218425]\n",
      "67: Train Loss: [2.7248626, 0.33420292, 5.1155224] | Test Loss: [2.732999, 0.39584395, 5.070154]\n",
      "68: Train Loss: [2.6628067, 0.36059046, 4.965023] | Test Loss: [2.5665123, 0.33275163, 4.800273]\n",
      "69: Train Loss: [2.8222551, 0.28019103, 5.3643193] | Test Loss: [2.7190306, 0.40445966, 5.0336018]\n",
      "70: Train Loss: [2.6447191, 0.3570435, 4.9323945] | Test Loss: [2.849088, 0.27309057, 5.4250855]\n",
      "71: Train Loss: [2.7472627, 0.30899188, 5.1855335] | Test Loss: [2.792635, 0.35929847, 5.2259717]\n",
      "72: Train Loss: [2.780969, 0.3489605, 5.2129774] | Test Loss: [2.9017856, 0.4221257, 5.3814454]\n",
      "73: Train Loss: [2.6418602, 0.27609527, 5.007625] | Test Loss: [2.6883223, 0.33540437, 5.04124]\n",
      "74: Train Loss: [2.9157717, 0.30890718, 5.5226364] | Test Loss: [2.8364522, 0.341467, 5.3314376]\n",
      "75: Train Loss: [2.7020009, 0.30221784, 5.1017838] | Test Loss: [2.8076787, 0.39703408, 5.218323]\n",
      "76: Train Loss: [2.7377477, 0.25945622, 5.216039] | Test Loss: [2.7245798, 0.45772862, 4.991431]\n",
      "77: Train Loss: [2.6582427, 0.32537812, 4.9911075] | Test Loss: [2.8983912, 0.3138419, 5.4829407]\n",
      "78: Train Loss: [2.8077025, 0.44157356, 5.1738315] | Test Loss: [2.8152041, 0.37112147, 5.259287]\n",
      "79: Train Loss: [2.7365134, 0.38917896, 5.083848] | Test Loss: [2.8491879, 0.32269076, 5.3756847]\n",
      "80: Train Loss: [2.6716907, 0.33532986, 5.0080514] | Test Loss: [2.7731938, 0.37239662, 5.173991]\n",
      "81: Train Loss: [2.6447604, 0.35959798, 4.9299226] | Test Loss: [2.6640227, 0.33847493, 4.9895706]\n",
      "82: Train Loss: [2.731138, 0.3905919, 5.071684] | Test Loss: [2.6615767, 0.33808437, 4.9850693]\n",
      "83: Train Loss: [2.842403, 0.35288018, 5.331926] | Test Loss: [2.6888895, 0.26991948, 5.1078596]\n",
      "84: Train Loss: [2.719747, 0.29160717, 5.1478868] | Test Loss: [2.8105772, 0.3351312, 5.286023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85: Train Loss: [2.6526804, 0.2948496, 5.0105114] | Test Loss: [2.660247, 0.49524742, 4.825247]\n",
      "86: Train Loss: [2.5958748, 0.31106848, 4.880681] | Test Loss: [2.584488, 0.37291935, 4.7960563]\n",
      "87: Train Loss: [2.769708, 0.47719997, 5.062216] | Test Loss: [2.8525686, 0.38720262, 5.3179345]\n",
      "88: Train Loss: [2.6464107, 0.3200336, 4.972788] | Test Loss: [2.6819835, 0.30170405, 5.062263]\n",
      "89: Train Loss: [2.5803497, 0.32862324, 4.832076] | Test Loss: [2.731052, 0.33953255, 5.1225715]\n",
      "90: Train Loss: [2.645426, 0.33985677, 4.9509954] | Test Loss: [2.814649, 0.36767998, 5.261618]\n",
      "91: Train Loss: [2.9283485, 0.33476737, 5.5219297] | Test Loss: [2.5859823, 0.29519323, 4.8767715]\n",
      "92: Train Loss: [2.8007555, 0.34852093, 5.2529902] | Test Loss: [2.7390277, 0.32921946, 5.148836]\n",
      "93: Train Loss: [2.5624309, 0.3102621, 4.8145995] | Test Loss: [2.7416704, 0.31216538, 5.1711755]\n",
      "94: Train Loss: [2.7050676, 0.33830258, 5.0718327] | Test Loss: [2.5525413, 0.34352258, 4.76156]\n",
      "95: Train Loss: [2.635787, 0.3820291, 4.889545] | Test Loss: [2.6370275, 0.3234313, 4.9506235]\n",
      "96: Train Loss: [2.698818, 0.3282949, 5.069341] | Test Loss: [2.7180724, 0.3908541, 5.045291]\n",
      "97: Train Loss: [2.6406574, 0.3433621, 4.937953] | Test Loss: [2.6973526, 0.3345464, 5.0601587]\n",
      "98: Train Loss: [2.5821466, 0.31131503, 4.852978] | Test Loss: [2.7945862, 0.40042204, 5.1887503]\n",
      "99: Train Loss: [2.7984424, 0.28415164, 5.312733] | Test Loss: [2.7735944, 0.30778974, 5.239399]\n",
      "100: Train Loss: [2.6953712, 0.39212012, 4.9986224] | Test Loss: [2.7257771, 0.29575092, 5.155803]\n",
      "101: Train Loss: [2.719686, 0.34702802, 5.092344] | Test Loss: [2.6232407, 0.37825283, 4.8682284]\n",
      "102: Train Loss: [2.703235, 0.32224435, 5.0842257] | Test Loss: [2.6777842, 0.3536618, 5.0019064]\n",
      "103: Train Loss: [2.8481827, 0.35742122, 5.338944] | Test Loss: [2.7495532, 0.33159292, 5.1675134]\n",
      "104: Train Loss: [2.6974502, 0.30763823, 5.087262] | Test Loss: [2.8028815, 0.36644834, 5.2393146]\n",
      "105: Train Loss: [2.444561, 0.3394664, 4.5496554] | Test Loss: [2.8424494, 0.3184181, 5.366481]\n",
      "106: Train Loss: [2.7267287, 0.29881802, 5.1546392] | Test Loss: [2.6226463, 0.35841602, 4.8868766]\n",
      "107: Train Loss: [2.6361918, 0.34389037, 4.9284935] | Test Loss: [2.789836, 0.34421632, 5.2354555]\n",
      "108: Train Loss: [2.5555637, 0.3406207, 4.770507] | Test Loss: [2.6218295, 0.30337787, 4.9402814]\n",
      "109: Train Loss: [2.733694, 0.32930902, 5.138079] | Test Loss: [2.680351, 0.32615647, 5.0345454]\n",
      "110: Train Loss: [2.5968292, 0.25793377, 4.9357247] | Test Loss: [2.6849372, 0.33883393, 5.0310407]\n",
      "111: Train Loss: [2.6297078, 0.30121696, 4.9581985] | Test Loss: [2.6404731, 0.3183194, 4.962627]\n",
      "112: Train Loss: [2.8040402, 0.3405565, 5.267524] | Test Loss: [2.530962, 0.32034996, 4.741574]\n",
      "113: Train Loss: [2.7414985, 0.38151252, 5.1014843] | Test Loss: [2.7394333, 0.34424186, 5.134625]\n",
      "114: Train Loss: [2.7570632, 0.29894015, 5.215186] | Test Loss: [2.6147244, 0.3199743, 4.9094744]\n",
      "115: Train Loss: [2.666438, 0.3292666, 5.0036097] | Test Loss: [2.7598596, 0.35331106, 5.166408]\n",
      "116: Train Loss: [2.8655567, 0.2582769, 5.4728365] | Test Loss: [2.6171865, 0.37192184, 4.862451]\n",
      "117: Train Loss: [2.6664863, 0.2939737, 5.0389986] | Test Loss: [2.7350123, 0.38741675, 5.0826077]\n",
      "118: Train Loss: [2.865716, 0.36377734, 5.367655] | Test Loss: [2.6667397, 0.32648593, 5.0069933]\n",
      "119: Train Loss: [2.7836635, 0.38037267, 5.1869545] | Test Loss: [2.833763, 0.27187696, 5.395649]\n",
      "120: Train Loss: [2.4966688, 0.3063848, 4.6869526] | Test Loss: [2.6006172, 0.35990566, 4.8413286]\n",
      "121: Train Loss: [2.5346158, 0.3741735, 4.695058] | Test Loss: [2.5952477, 0.31788725, 4.872608]\n",
      "122: Train Loss: [2.637658, 0.3214559, 4.9538603] | Test Loss: [2.5210133, 0.30780253, 4.734224]\n",
      "123: Train Loss: [2.6079843, 0.326742, 4.8892264] | Test Loss: [2.653074, 0.3125025, 4.9936457]\n",
      "124: Train Loss: [2.7933655, 0.35485098, 5.23188] | Test Loss: [2.7490766, 0.37982696, 5.118326]\n",
      "125: Train Loss: [2.7388752, 0.323793, 5.1539574] | Test Loss: [2.5362794, 0.33304742, 4.7395115]\n",
      "126: Train Loss: [2.5511153, 0.3780484, 4.724182] | Test Loss: [2.7688763, 0.34683657, 5.190916]\n",
      "127: Train Loss: [2.562192, 0.29898298, 4.825401] | Test Loss: [2.7965212, 0.3301782, 5.262864]\n",
      "128: Train Loss: [2.5539205, 0.42639938, 4.681442] | Test Loss: [2.6423538, 0.3302892, 4.954418]\n",
      "129: Train Loss: [2.6426778, 0.31718665, 4.9681687] | Test Loss: [2.632349, 0.3163383, 4.94836]\n",
      "130: Train Loss: [2.7126856, 0.33408585, 5.091285] | Test Loss: [2.7835784, 0.39077908, 5.176378]\n",
      "131: Train Loss: [2.6802697, 0.34291416, 5.0176253] | Test Loss: [2.6838596, 0.32701105, 5.040708]\n",
      "132: Train Loss: [2.7606602, 0.32301462, 5.1983056] | Test Loss: [2.6452699, 0.3099382, 4.9806013]\n",
      "133: Train Loss: [2.594637, 0.34507027, 4.8442035] | Test Loss: [2.5228329, 0.3344557, 4.7112103]\n",
      "134: Train Loss: [2.5275862, 0.3695704, 4.685602] | Test Loss: [2.4814837, 0.33777508, 4.625192]\n",
      "135: Train Loss: [2.4811525, 0.34247655, 4.6198287] | Test Loss: [2.5566013, 0.28084618, 4.8323565]\n",
      "136: Train Loss: [2.4812381, 0.32441857, 4.6380577] | Test Loss: [2.6649404, 0.30373248, 5.0261483]\n",
      "137: Train Loss: [2.6392102, 0.39704776, 4.881373] | Test Loss: [2.7728176, 0.33263803, 5.212997]\n",
      "138: Train Loss: [2.662481, 0.42453486, 4.9004273] | Test Loss: [2.6873567, 0.35355836, 5.021155]\n",
      "139: Train Loss: [2.461707, 0.3315138, 4.5919003] | Test Loss: [2.7674382, 0.36585376, 5.1690226]\n",
      "140: Train Loss: [2.8148065, 0.37828076, 5.2513323] | Test Loss: [2.5950482, 0.3393055, 4.850791]\n",
      "141: Train Loss: [2.599855, 0.3756939, 4.824016] | Test Loss: [2.869006, 0.3361344, 5.4018774]\n",
      "142: Train Loss: [2.5051608, 0.34221393, 4.6681075] | Test Loss: [2.6791298, 0.38675442, 4.971505]\n",
      "143: Train Loss: [2.6381333, 0.44785774, 4.8284087] | Test Loss: [2.6082382, 0.33550647, 4.88097]\n",
      "144: Train Loss: [2.6145864, 0.31466472, 4.914508] | Test Loss: [2.6151946, 0.33555633, 4.8948326]\n",
      "145: Train Loss: [2.6285446, 0.29034722, 4.966742] | Test Loss: [2.8622673, 0.3085707, 5.4159636]\n",
      "146: Train Loss: [2.5213854, 0.32487783, 4.717893] | Test Loss: [2.7048712, 0.3911172, 5.0186253]\n",
      "147: Train Loss: [2.6248887, 0.34879506, 4.9009824] | Test Loss: [2.6292388, 0.37634304, 4.8821344]\n",
      "148: Train Loss: [2.6448429, 0.34343338, 4.9462523] | Test Loss: [2.65231, 0.34919283, 4.955427]\n",
      "149: Train Loss: [2.6813211, 0.31192568, 5.0507164] | Test Loss: [2.3727853, 0.35967967, 4.385891]\n",
      "150: Train Loss: [2.604469, 0.30929616, 4.899642] | Test Loss: [2.5000994, 0.39437458, 4.6058245]\n",
      "151: Train Loss: [2.5248969, 0.31955677, 4.730237] | Test Loss: [2.7309515, 0.33792552, 5.1239777]\n",
      "152: Train Loss: [2.6701646, 0.36886805, 4.9714613] | Test Loss: [2.7057943, 0.31340998, 5.098179]\n",
      "153: Train Loss: [2.5877934, 0.26988843, 4.9056983] | Test Loss: [2.657637, 0.363642, 4.9516315]\n",
      "154: Train Loss: [2.6645265, 0.28951433, 5.0395384] | Test Loss: [2.6800535, 0.3164644, 5.0436425]\n",
      "155: Train Loss: [2.7135124, 0.30360305, 5.1234217] | Test Loss: [2.5977798, 0.32489118, 4.8706684]\n",
      "156: Train Loss: [2.7173948, 0.3007949, 5.1339946] | Test Loss: [2.7293632, 0.2980525, 5.160674]\n",
      "157: Train Loss: [2.6747127, 0.3197338, 5.0296917] | Test Loss: [2.5557249, 0.3766496, 4.7348003]\n",
      "158: Train Loss: [2.5290103, 0.26693615, 4.7910843] | Test Loss: [2.664841, 0.30671823, 5.0229635]\n",
      "159: Train Loss: [2.7624378, 0.34320664, 5.181669] | Test Loss: [2.5711625, 0.30532607, 4.836999]\n",
      "160: Train Loss: [2.5472622, 0.2747722, 4.819752] | Test Loss: [2.5793297, 0.35385832, 4.804801]\n",
      "161: Train Loss: [2.6203544, 0.3737117, 4.8669972] | Test Loss: [2.686715, 0.27492294, 5.098507]\n",
      "162: Train Loss: [2.7688203, 0.330766, 5.2068744] | Test Loss: [2.356696, 0.2855682, 4.4278235]\n",
      "163: Train Loss: [2.6201165, 0.32504055, 4.9151926] | Test Loss: [2.57489, 0.32649687, 4.8232827]\n",
      "164: Train Loss: [2.7157264, 0.29379025, 5.1376624] | Test Loss: [2.4809575, 0.30808902, 4.653826]\n",
      "165: Train Loss: [2.6648655, 0.29198068, 5.0377502] | Test Loss: [2.6236632, 0.38930705, 4.8580194]\n",
      "166: Train Loss: [2.5225146, 0.30793583, 4.7370934] | Test Loss: [2.579292, 0.3038476, 4.8547363]\n",
      "167: Train Loss: [2.7210884, 0.33937478, 5.102802] | Test Loss: [2.597978, 0.3036104, 4.892346]\n",
      "168: Train Loss: [2.7131805, 0.33133867, 5.095022] | Test Loss: [2.6404428, 0.45038372, 4.830502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169: Train Loss: [2.508101, 0.34908375, 4.667118] | Test Loss: [2.673772, 0.35317004, 4.9943743]\n",
      "170: Train Loss: [2.6697993, 0.29691604, 5.0426826] | Test Loss: [2.7152987, 0.4119308, 5.0186663]\n",
      "171: Train Loss: [2.7387981, 0.31426993, 5.1633263] | Test Loss: [2.6118965, 0.3036591, 4.920134]\n",
      "172: Train Loss: [2.4990873, 0.32007435, 4.6781] | Test Loss: [2.646851, 0.31062755, 4.9830747]\n",
      "173: Train Loss: [2.6097302, 0.2944859, 4.9249744] | Test Loss: [2.703979, 0.36612782, 5.04183]\n",
      "174: Train Loss: [2.567686, 0.3002893, 4.835083] | Test Loss: [2.6783412, 0.3508335, 5.005849]\n",
      "175: Train Loss: [2.5706463, 0.352751, 4.788542] | Test Loss: [2.6011753, 0.3554986, 4.846852]\n",
      "176: Train Loss: [2.4475102, 0.40745613, 4.4875646] | Test Loss: [2.7717779, 0.3841856, 5.15937]\n",
      "177: Train Loss: [2.6920216, 0.35200536, 5.0320377] | Test Loss: [2.6315525, 0.37767202, 4.8854327]\n",
      "178: Train Loss: [2.4624074, 0.30931598, 4.6154985] | Test Loss: [2.708766, 0.33253777, 5.0849943]\n",
      "179: Train Loss: [2.697846, 0.36066225, 5.0350294] | Test Loss: [2.1456587, 0.3168501, 3.9744675]\n",
      "180: Train Loss: [2.540866, 0.36976767, 4.711964] | Test Loss: [2.8742042, 0.37859735, 5.369811]\n",
      "181: Train Loss: [2.7953367, 0.33065608, 5.2600174] | Test Loss: [2.4163241, 0.2892592, 4.5433893]\n",
      "182: Train Loss: [2.591876, 0.41922754, 4.7645245] | Test Loss: [2.526512, 0.31357333, 4.7394505]\n",
      "183: Train Loss: [2.4921453, 0.301814, 4.6824765] | Test Loss: [2.608242, 0.29686645, 4.9196177]\n",
      "184: Train Loss: [2.5365305, 0.27314028, 4.7999206] | Test Loss: [2.6888285, 0.35878444, 5.0188727]\n",
      "185: Train Loss: [2.6720846, 0.31857023, 5.025599] | Test Loss: [2.672747, 0.35547474, 4.990019]\n",
      "186: Train Loss: [2.5845957, 0.33158672, 4.8376045] | Test Loss: [2.4562922, 0.27518812, 4.6373963]\n",
      "187: Train Loss: [2.4302921, 0.30151373, 4.5590706] | Test Loss: [2.7018082, 0.35102788, 5.0525885]\n",
      "188: Train Loss: [2.5029178, 0.31859097, 4.6872444] | Test Loss: [2.4400744, 0.3882407, 4.491908]\n",
      "189: Train Loss: [2.689592, 0.30213282, 5.077051] | Test Loss: [2.606801, 0.36007842, 4.8535237]\n",
      "190: Train Loss: [2.805874, 0.40343526, 5.208313] | Test Loss: [2.6224072, 0.35362226, 4.891192]\n",
      "191: Train Loss: [2.6684563, 0.35222656, 4.984686] | Test Loss: [2.4383636, 0.34473246, 4.531995]\n",
      "192: Train Loss: [2.7010698, 0.31176952, 5.09037] | Test Loss: [2.6282215, 0.32018, 4.936263]\n",
      "193: Train Loss: [2.6951501, 0.40441078, 4.9858894] | Test Loss: [2.4734914, 0.34117442, 4.6058083]\n",
      "194: Train Loss: [2.6214325, 0.32984388, 4.913021] | Test Loss: [2.7400367, 0.38690177, 5.0931716]\n",
      "195: Train Loss: [2.4862895, 0.3856245, 4.5869546] | Test Loss: [2.6539545, 0.35983124, 4.9480777]\n",
      "196: Train Loss: [2.722362, 0.3213926, 5.1233315] | Test Loss: [2.5684335, 0.3718497, 4.7650175]\n",
      "197: Train Loss: [2.445892, 0.3497421, 4.5420423] | Test Loss: [2.6100783, 0.33643892, 4.8837175]\n",
      "198: Train Loss: [2.5446422, 0.35453832, 4.734746] | Test Loss: [2.552894, 0.33443472, 4.7713537]\n",
      "199: Train Loss: [2.5914266, 0.3735932, 4.80926] | Test Loss: [2.5444407, 0.3534801, 4.7354016]\n",
      "200: Train Loss: [2.5741189, 0.30541328, 4.8428245] | Test Loss: [2.6373117, 0.37312517, 4.9014983]\n",
      "201: Train Loss: [2.5312276, 0.3518386, 4.7106166] | Test Loss: [2.5936399, 0.30348706, 4.883793]\n",
      "202: Train Loss: [2.6730068, 0.43260857, 4.913405] | Test Loss: [2.5885916, 0.31665874, 4.8605247]\n",
      "203: Train Loss: [2.7657006, 0.34043664, 5.1909647] | Test Loss: [2.5564446, 0.33111715, 4.781772]\n",
      "204: Train Loss: [2.7619247, 0.354814, 5.1690354] | Test Loss: [2.3764858, 0.3091724, 4.4437995]\n",
      "205: Train Loss: [2.5668664, 0.28747773, 4.8462553] | Test Loss: [2.6777565, 0.37121242, 4.9843006]\n",
      "206: Train Loss: [2.4219587, 0.3031734, 4.540744] | Test Loss: [2.5086398, 0.33687496, 4.6804047]\n",
      "207: Train Loss: [2.4232523, 0.33154833, 4.5149565] | Test Loss: [2.5001013, 0.32103834, 4.6791644]\n",
      "208: Train Loss: [2.6214147, 0.43197, 4.810859] | Test Loss: [2.4054325, 0.32246655, 4.4883986]\n",
      "209: Train Loss: [2.4843903, 0.37851322, 4.590267] | Test Loss: [2.3153496, 0.28991163, 4.3407874]\n",
      "210: Train Loss: [2.5822997, 0.3481722, 4.816427] | Test Loss: [2.4507334, 0.3521789, 4.549288]\n",
      "211: Train Loss: [2.5289083, 0.28198516, 4.775831] | Test Loss: [2.5195777, 0.35779655, 4.681359]\n",
      "212: Train Loss: [2.5599508, 0.36534745, 4.7545543] | Test Loss: [2.4130235, 0.3833427, 4.442704]\n",
      "213: Train Loss: [2.484039, 0.35000318, 4.618075] | Test Loss: [2.3203554, 0.27002463, 4.370686]\n",
      "214: Train Loss: [2.3415213, 0.31839442, 4.3646483] | Test Loss: [2.3394332, 0.3138759, 4.36499]\n",
      "215: Train Loss: [2.5283296, 0.28222194, 4.7744374] | Test Loss: [2.5214026, 0.33215395, 4.7106514]\n",
      "216: Train Loss: [2.4364228, 0.31326756, 4.559578] | Test Loss: [2.5464554, 0.38367897, 4.709232]\n",
      "217: Train Loss: [2.761954, 0.3588756, 5.1650324] | Test Loss: [2.460149, 0.36777166, 4.5525265]\n",
      "218: Train Loss: [2.5118268, 0.31444466, 4.709209] | Test Loss: [2.4820614, 0.314999, 4.6491237]\n",
      "219: Train Loss: [2.496191, 0.34511438, 4.647268] | Test Loss: [2.391814, 0.29536816, 4.48826]\n",
      "220: Train Loss: [2.4883091, 0.3470284, 4.62959] | Test Loss: [2.4705038, 0.3209992, 4.6200085]\n",
      "221: Train Loss: [2.5420625, 0.30130973, 4.7828155] | Test Loss: [2.3647556, 0.4271454, 4.302366]\n",
      "222: Train Loss: [2.4600213, 0.40399522, 4.5160475] | Test Loss: [2.4605443, 0.3522147, 4.568874]\n",
      "223: Train Loss: [2.3981302, 0.30369177, 4.4925685] | Test Loss: [2.5288336, 0.3602763, 4.697391]\n",
      "224: Train Loss: [2.5047476, 0.3044328, 4.7050624] | Test Loss: [2.4842672, 0.34104106, 4.6274934]\n",
      "225: Train Loss: [2.4421186, 0.31445774, 4.5697794] | Test Loss: [2.5263195, 0.2873118, 4.7653275]\n",
      "226: Train Loss: [2.3348093, 0.3345891, 4.3350296] | Test Loss: [2.3895185, 0.36910275, 4.409934]\n",
      "227: Train Loss: [2.371356, 0.33272853, 4.4099836] | Test Loss: [2.7296238, 0.31443766, 5.1448097]\n",
      "228: Train Loss: [2.4410539, 0.27316317, 4.6089444] | Test Loss: [2.4431884, 0.35102317, 4.5353537]\n",
      "229: Train Loss: [2.520517, 0.34433925, 4.696695] | Test Loss: [2.5328915, 0.29812238, 4.7676606]\n",
      "230: Train Loss: [2.3516092, 0.30443668, 4.398782] | Test Loss: [2.4162092, 0.33516702, 4.4972515]\n",
      "231: Train Loss: [2.5574481, 0.37285817, 4.7420382] | Test Loss: [2.5523276, 0.38158923, 4.723066]\n",
      "232: Train Loss: [2.5761974, 0.32610148, 4.8262935] | Test Loss: [2.4471598, 0.33411592, 4.5602036]\n",
      "233: Train Loss: [2.4444315, 0.33471882, 4.5541444] | Test Loss: [2.437348, 0.3311495, 4.543546]\n",
      "234: Train Loss: [2.4624848, 0.2986567, 4.626313] | Test Loss: [2.5211086, 0.33504343, 4.707174]\n",
      "235: Train Loss: [2.4111016, 0.27428284, 4.54792] | Test Loss: [2.557683, 0.31745753, 4.7979083]\n",
      "236: Train Loss: [2.3821516, 0.384557, 4.379746] | Test Loss: [2.3985445, 0.30804366, 4.4890456]\n",
      "237: Train Loss: [2.293048, 0.3892929, 4.196803] | Test Loss: [2.6064312, 0.350038, 4.8628244]\n",
      "238: Train Loss: [2.5371673, 0.309039, 4.7652955] | Test Loss: [2.430408, 0.29209235, 4.5687237]\n",
      "239: Train Loss: [2.5247338, 0.27763814, 4.7718296] | Test Loss: [2.7064822, 0.4021924, 5.0107718]\n",
      "240: Train Loss: [2.4506283, 0.26221713, 4.6390395] | Test Loss: [2.4953327, 0.33930245, 4.651363]\n",
      "241: Train Loss: [2.3932312, 0.4186064, 4.367856] | Test Loss: [2.4762437, 0.28910705, 4.6633806]\n",
      "242: Train Loss: [2.7345874, 0.25943097, 5.209744] | Test Loss: [2.5831826, 0.3552185, 4.8111467]\n",
      "243: Train Loss: [2.5027468, 0.30764255, 4.697851] | Test Loss: [2.4505432, 0.35727733, 4.543809]\n",
      "244: Train Loss: [2.3365052, 0.3525874, 4.320423] | Test Loss: [2.6218283, 0.2685579, 4.9750986]\n",
      "245: Train Loss: [2.369747, 0.2944656, 4.4450283] | Test Loss: [2.4778302, 0.380076, 4.5755844]\n",
      "246: Train Loss: [2.4245105, 0.34347278, 4.505548] | Test Loss: [2.5184915, 0.31260124, 4.724382]\n",
      "247: Train Loss: [2.5640588, 0.40312225, 4.724995] | Test Loss: [2.509645, 0.33743864, 4.6818514]\n",
      "248: Train Loss: [2.6230583, 0.4208236, 4.825293] | Test Loss: [2.5215251, 0.3279518, 4.7150984]\n",
      "249: Train Loss: [2.543693, 0.3545054, 4.7328806] | Test Loss: [2.3332894, 0.32239306, 4.344186]\n",
      "250: Train Loss: [2.49024, 0.4382366, 4.5422435] | Test Loss: [2.320255, 0.3111939, 4.329316]\n",
      "251: Train Loss: [2.6287458, 0.5097928, 4.747699] | Test Loss: [2.39686, 0.33491057, 4.4588094]\n",
      "252: Train Loss: [2.4097712, 0.31052363, 4.509019] | Test Loss: [2.4724486, 0.38322484, 4.561672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253: Train Loss: [2.34475, 0.33213338, 4.3573666] | Test Loss: [2.4903283, 0.340457, 4.6401997]\n",
      "254: Train Loss: [2.3680434, 0.35405302, 4.382034] | Test Loss: [2.4544168, 0.45352268, 4.455311]\n",
      "255: Train Loss: [2.4240885, 0.4303476, 4.4178295] | Test Loss: [2.4911647, 0.26460043, 4.717729]\n",
      "256: Train Loss: [2.3941095, 0.30286512, 4.485354] | Test Loss: [2.5898638, 0.3939936, 4.785734]\n",
      "257: Train Loss: [2.2339492, 0.29505423, 4.172844] | Test Loss: [2.449703, 0.45945886, 4.439947]\n",
      "258: Train Loss: [2.4673, 0.32269877, 4.6119013] | Test Loss: [2.4129095, 0.3736065, 4.4522123]\n",
      "259: Train Loss: [2.6065426, 0.34719497, 4.86589] | Test Loss: [2.4511094, 0.35379532, 4.5484233]\n",
      "260: Train Loss: [2.337016, 0.27221477, 4.4018173] | Test Loss: [2.3199697, 0.3244065, 4.3155327]\n",
      "261: Train Loss: [2.4235518, 0.34849885, 4.498605] | Test Loss: [2.5649385, 0.37058014, 4.759297]\n",
      "262: Train Loss: [2.4054549, 0.33079463, 4.480115] | Test Loss: [2.3985288, 0.28080076, 4.516257]\n",
      "263: Train Loss: [2.327477, 0.34996894, 4.304985] | Test Loss: [2.5075326, 0.353009, 4.6620564]\n",
      "264: Train Loss: [2.394682, 0.37587398, 4.41349] | Test Loss: [2.4388502, 0.291977, 4.5857234]\n",
      "265: Train Loss: [2.4452975, 0.3142891, 4.576306] | Test Loss: [2.3410237, 0.3315587, 4.3504887]\n",
      "266: Train Loss: [2.4940376, 0.34305772, 4.6450176] | Test Loss: [2.2841578, 0.32182017, 4.2464952]\n",
      "267: Train Loss: [2.4093602, 0.37562868, 4.443092] | Test Loss: [2.6011994, 0.379352, 4.8230467]\n",
      "268: Train Loss: [2.403346, 0.3476451, 4.459047] | Test Loss: [2.2317505, 0.29836708, 4.165134]\n",
      "269: Train Loss: [2.4810128, 0.261905, 4.7001204] | Test Loss: [2.621762, 0.33506918, 4.908455]\n",
      "270: Train Loss: [2.3783035, 0.33630937, 4.4202976] | Test Loss: [2.4563408, 0.41902202, 4.4936595]\n",
      "271: Train Loss: [2.2260923, 0.3106485, 4.141536] | Test Loss: [2.454131, 0.3070761, 4.601186]\n",
      "272: Train Loss: [2.3479943, 0.3058381, 4.3901505] | Test Loss: [2.4736419, 0.343933, 4.6033506]\n",
      "273: Train Loss: [2.388578, 0.35221902, 4.424937] | Test Loss: [2.426582, 0.27086535, 4.5822988]\n",
      "274: Train Loss: [2.381929, 0.32037705, 4.443481] | Test Loss: [2.1140053, 0.3529602, 3.8750505]\n",
      "275: Train Loss: [2.283627, 0.32728744, 4.2399664] | Test Loss: [2.4453793, 0.29795843, 4.5928]\n",
      "276: Train Loss: [2.4893613, 0.37212157, 4.606601] | Test Loss: [2.6525903, 0.39991897, 4.9052615]\n",
      "277: Train Loss: [2.3952026, 0.26204875, 4.5283566] | Test Loss: [2.265354, 0.29973134, 4.2309766]\n",
      "278: Train Loss: [2.3062093, 0.27857322, 4.3338456] | Test Loss: [2.3684592, 0.35906813, 4.3778505]\n",
      "279: Train Loss: [2.3881905, 0.29502636, 4.4813547] | Test Loss: [2.484366, 0.35109124, 4.6176405]\n",
      "280: Train Loss: [2.3517878, 0.32411537, 4.3794603] | Test Loss: [2.198331, 0.3059141, 4.0907483]\n",
      "281: Train Loss: [2.3694057, 0.29458886, 4.4442225] | Test Loss: [2.3971057, 0.3156858, 4.4785256]\n",
      "282: Train Loss: [2.3034282, 0.34860462, 4.2582517] | Test Loss: [2.5296655, 0.33564734, 4.723684]\n",
      "283: Train Loss: [2.4789886, 0.3035986, 4.654379] | Test Loss: [2.4829426, 0.31008762, 4.6557975]\n",
      "284: Train Loss: [2.53689, 0.38484237, 4.6889377] | Test Loss: [2.36465, 0.30255145, 4.4267488]\n",
      "285: Train Loss: [2.3843226, 0.26879767, 4.4998474] | Test Loss: [2.3141708, 0.26468453, 4.363657]\n",
      "286: Train Loss: [2.4164252, 0.3834805, 4.44937] | Test Loss: [2.4749377, 0.3239113, 4.625964]\n",
      "287: Train Loss: [2.2703063, 0.34602132, 4.1945915] | Test Loss: [2.3645039, 0.34672913, 4.3822784]\n",
      "288: Train Loss: [2.2577565, 0.33072102, 4.184792] | Test Loss: [2.2727919, 0.31331304, 4.2322707]\n",
      "289: Train Loss: [2.38181, 0.37741187, 4.386208] | Test Loss: [2.4198153, 0.34955028, 4.4900804]\n",
      "290: Train Loss: [2.4342656, 0.35439473, 4.5141363] | Test Loss: [2.4131637, 0.32427865, 4.5020485]\n",
      "291: Train Loss: [2.222793, 0.30341983, 4.1421666] | Test Loss: [2.4165163, 0.28804672, 4.544986]\n",
      "292: Train Loss: [2.383084, 0.3041883, 4.46198] | Test Loss: [2.4243913, 0.42215937, 4.4266233]\n",
      "293: Train Loss: [2.4734442, 0.34978577, 4.5971026] | Test Loss: [2.3897824, 0.33900613, 4.440559]\n",
      "294: Train Loss: [2.4064054, 0.31408337, 4.4987273] | Test Loss: [2.2737541, 0.36104158, 4.1864667]\n",
      "295: Train Loss: [2.2681682, 0.28716776, 4.249169] | Test Loss: [2.3600793, 0.43370923, 4.2864494]\n",
      "296: Train Loss: [2.3967865, 0.33077738, 4.4627957] | Test Loss: [2.4724905, 0.37813476, 4.5668464]\n",
      "297: Train Loss: [2.3294234, 0.3384011, 4.3204455] | Test Loss: [2.5078676, 0.37640566, 4.6393294]\n",
      "298: Train Loss: [2.4584155, 0.3316682, 4.5851626] | Test Loss: [2.253061, 0.37546554, 4.1306567]\n",
      "299: Train Loss: [2.3458872, 0.29865292, 4.3931212] | Test Loss: [2.5115469, 0.28335437, 4.7397394]\n",
      "300: Train Loss: [2.6922333, 0.31465003, 5.0698166] | Test Loss: [2.4308856, 0.28666022, 4.575111]\n",
      "301: Train Loss: [2.3957617, 0.360766, 4.4307575] | Test Loss: [2.349308, 0.29847336, 4.4001427]\n",
      "302: Train Loss: [2.3056555, 0.3292525, 4.2820582] | Test Loss: [2.2985134, 0.3255554, 4.2714715]\n",
      "303: Train Loss: [2.096994, 0.32587737, 3.8681104] | Test Loss: [2.3857043, 0.28138548, 4.490023]\n",
      "304: Train Loss: [2.3107297, 0.336186, 4.2852736] | Test Loss: [2.2528915, 0.29696736, 4.2088156]\n",
      "305: Train Loss: [2.3623743, 0.35539818, 4.3693504] | Test Loss: [2.3589597, 0.3304894, 4.3874297]\n",
      "306: Train Loss: [2.328452, 0.3034017, 4.3535023] | Test Loss: [2.437768, 0.3513493, 4.5241866]\n",
      "307: Train Loss: [2.4910545, 0.3523907, 4.6297183] | Test Loss: [2.3516326, 0.38237384, 4.3208914]\n",
      "308: Train Loss: [2.4206686, 0.3069541, 4.5343833] | Test Loss: [2.2316332, 0.35581073, 4.1074557]\n",
      "309: Train Loss: [2.2388105, 0.30544192, 4.172179] | Test Loss: [2.4589124, 0.38041618, 4.5374084]\n",
      "310: Train Loss: [2.2474246, 0.26514286, 4.2297063] | Test Loss: [2.2415879, 0.29924092, 4.1839347]\n",
      "311: Train Loss: [2.2132256, 0.33269826, 4.093753] | Test Loss: [2.3885458, 0.36836538, 4.408726]\n",
      "312: Train Loss: [2.5119662, 0.36394155, 4.659991] | Test Loss: [2.3557868, 0.32539764, 4.386176]\n",
      "313: Train Loss: [2.3568087, 0.32998976, 4.3836274] | Test Loss: [2.3717568, 0.28572762, 4.457786]\n",
      "314: Train Loss: [2.379166, 0.28405875, 4.474273] | Test Loss: [2.2513719, 0.29188395, 4.21086]\n",
      "315: Train Loss: [2.5234203, 0.35718462, 4.6896563] | Test Loss: [2.550783, 0.3207257, 4.78084]\n",
      "316: Train Loss: [2.3536832, 0.2866348, 4.4207315] | Test Loss: [2.3399608, 0.32818124, 4.3517404]\n",
      "317: Train Loss: [2.4575021, 0.21702915, 4.697975] | Test Loss: [2.2751913, 0.3509334, 4.199449]\n",
      "318: Train Loss: [2.295648, 0.28699327, 4.3043027] | Test Loss: [2.3519175, 0.38389316, 4.319942]\n",
      "319: Train Loss: [2.402406, 0.2962381, 4.508574] | Test Loss: [2.4676778, 0.38532487, 4.5500307]\n",
      "320: Train Loss: [2.3153749, 0.33953407, 4.2912154] | Test Loss: [2.4027603, 0.38869524, 4.4168253]\n",
      "321: Train Loss: [2.3335383, 0.3358146, 4.331262] | Test Loss: [2.3760254, 0.359948, 4.3921027]\n",
      "322: Train Loss: [2.234617, 0.35752934, 4.111705] | Test Loss: [2.3316712, 0.26964086, 4.3937016]\n",
      "323: Train Loss: [2.2706597, 0.33860686, 4.2027125] | Test Loss: [2.4386768, 0.32418567, 4.553168]\n",
      "324: Train Loss: [2.312886, 0.36371195, 4.26206] | Test Loss: [2.464537, 0.37683746, 4.5522366]\n",
      "325: Train Loss: [2.2881398, 0.3269775, 4.2493024] | Test Loss: [2.3085735, 0.35575306, 4.261394]\n",
      "326: Train Loss: [2.374675, 0.32238463, 4.426965] | Test Loss: [2.3903227, 0.30774236, 4.4729033]\n",
      "327: Train Loss: [2.4581306, 0.36492345, 4.5513377] | Test Loss: [2.2397227, 0.42434585, 4.0550995]\n",
      "328: Train Loss: [2.2159052, 0.3620058, 4.0698047] | Test Loss: [2.4145956, 0.35378012, 4.475411]\n",
      "329: Train Loss: [2.260745, 0.39006537, 4.131425] | Test Loss: [2.5955331, 0.41167328, 4.779393]\n",
      "330: Train Loss: [2.2877965, 0.34051824, 4.2350745] | Test Loss: [2.2668664, 0.34291375, 4.1908193]\n",
      "331: Train Loss: [2.307568, 0.3400909, 4.2750454] | Test Loss: [2.3298447, 0.36582044, 4.293869]\n",
      "332: Train Loss: [2.4636579, 0.31559995, 4.611716] | Test Loss: [2.3848891, 0.3435525, 4.4262257]\n",
      "333: Train Loss: [2.2848103, 0.35802525, 4.2115955] | Test Loss: [2.399061, 0.32383537, 4.4742866]\n",
      "334: Train Loss: [2.3347244, 0.2721638, 4.397285] | Test Loss: [2.395554, 0.3329489, 4.4581594]\n",
      "335: Train Loss: [2.360778, 0.34520158, 4.3763547] | Test Loss: [2.2770047, 0.3627547, 4.1912546]\n",
      "336: Train Loss: [2.3919458, 0.3056636, 4.478228] | Test Loss: [2.3042672, 0.32267123, 4.285863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337: Train Loss: [2.5148623, 0.39101228, 4.6387124] | Test Loss: [2.2829206, 0.4545037, 4.1113377]\n",
      "338: Train Loss: [2.3310208, 0.3243872, 4.3376546] | Test Loss: [2.2912347, 0.3474532, 4.2350163]\n",
      "339: Train Loss: [2.2094767, 0.33343962, 4.0855136] | Test Loss: [2.2737958, 0.40674734, 4.1408443]\n",
      "340: Train Loss: [2.4527984, 0.50140214, 4.404195] | Test Loss: [2.3342957, 0.2940451, 4.3745465]\n",
      "341: Train Loss: [2.2033916, 0.30782413, 4.098959] | Test Loss: [2.3073678, 0.32720342, 4.2875323]\n",
      "342: Train Loss: [2.3818429, 0.36833185, 4.395354] | Test Loss: [2.2890506, 0.3646199, 4.2134814]\n",
      "343: Train Loss: [2.2671547, 0.33171862, 4.202591] | Test Loss: [2.370292, 0.37292942, 4.3676543]\n",
      "344: Train Loss: [2.2246704, 0.31633234, 4.1330085] | Test Loss: [2.2450786, 0.28792432, 4.202233]\n",
      "345: Train Loss: [2.3452013, 0.29273525, 4.3976674] | Test Loss: [2.2696724, 0.34152192, 4.197823]\n",
      "346: Train Loss: [2.3724842, 0.3216529, 4.4233155] | Test Loss: [2.4694827, 0.32349443, 4.615471]\n",
      "347: Train Loss: [2.5404363, 0.47125852, 4.609614] | Test Loss: [2.4865255, 0.33448842, 4.6385627]\n",
      "348: Train Loss: [2.1451895, 0.36373398, 3.926645] | Test Loss: [2.2274113, 0.28125483, 4.173568]\n",
      "349: Train Loss: [2.450094, 0.40076444, 4.4994235] | Test Loss: [2.3588648, 0.31460282, 4.4031267]\n",
      "350: Train Loss: [2.3604512, 0.37120795, 4.3496947] | Test Loss: [2.4729726, 0.34291613, 4.6030293]\n",
      "351: Train Loss: [2.3220303, 0.32808462, 4.315976] | Test Loss: [2.2933955, 0.3132556, 4.2735353]\n",
      "352: Train Loss: [2.297789, 0.30926034, 4.286318] | Test Loss: [2.4371035, 0.38013563, 4.4940715]\n",
      "353: Train Loss: [2.4194977, 0.36436623, 4.4746294] | Test Loss: [2.416474, 0.30515677, 4.5277915]\n",
      "354: Train Loss: [2.3490455, 0.33866102, 4.35943] | Test Loss: [2.4368567, 0.33860767, 4.5351057]\n",
      "355: Train Loss: [2.231893, 0.33723745, 4.126549] | Test Loss: [2.4270546, 0.2933922, 4.560717]\n",
      "356: Train Loss: [2.390452, 0.39159894, 4.3893046] | Test Loss: [2.4444473, 0.4400221, 4.4488726]\n",
      "357: Train Loss: [2.370622, 0.34555426, 4.3956895] | Test Loss: [2.4110534, 0.2780138, 4.544093]\n",
      "358: Train Loss: [2.1471684, 0.27867916, 4.0156574] | Test Loss: [2.4072618, 0.33434469, 4.480179]\n",
      "359: Train Loss: [2.5117972, 0.2923065, 4.731288] | Test Loss: [2.356859, 0.29154328, 4.4221745]\n",
      "360: Train Loss: [2.3991327, 0.32783693, 4.4704285] | Test Loss: [2.394131, 0.2526174, 4.5356445]\n",
      "361: Train Loss: [2.5146894, 0.3764503, 4.6529284] | Test Loss: [2.3484142, 0.40013966, 4.2966886]\n",
      "362: Train Loss: [2.2612991, 0.28441072, 4.238188] | Test Loss: [2.311776, 0.28520536, 4.3383465]\n",
      "363: Train Loss: [2.1461978, 0.3023926, 3.990003] | Test Loss: [2.3456802, 0.35941112, 4.331949]\n",
      "364: Train Loss: [2.4416025, 0.31810135, 4.5651035] | Test Loss: [2.2259133, 0.3067307, 4.145096]\n",
      "365: Train Loss: [2.3966174, 0.37868032, 4.4145546] | Test Loss: [2.2298064, 0.27989364, 4.1797194]\n",
      "366: Train Loss: [2.1833107, 0.32493195, 4.0416894] | Test Loss: [2.1658826, 0.2937594, 4.038006]\n",
      "367: Train Loss: [2.2062345, 0.3386474, 4.0738215] | Test Loss: [2.3479972, 0.35164055, 4.3443537]\n",
      "368: Train Loss: [2.1628354, 0.29441562, 4.0312552] | Test Loss: [2.3691933, 0.31636858, 4.422018]\n",
      "369: Train Loss: [2.1610627, 0.36007947, 3.962046] | Test Loss: [2.3015232, 0.33338672, 4.2696595]\n",
      "370: Train Loss: [2.4506721, 0.34273297, 4.5586114] | Test Loss: [2.4120553, 0.42857036, 4.39554]\n",
      "371: Train Loss: [2.4078178, 0.31262597, 4.50301] | Test Loss: [2.177917, 0.26352245, 4.0923114]\n",
      "372: Train Loss: [2.251558, 0.2899331, 4.213183] | Test Loss: [2.3323176, 0.3209438, 4.3436913]\n",
      "373: Train Loss: [2.3420548, 0.34574312, 4.3383665] | Test Loss: [2.448457, 0.33543006, 4.561484]\n",
      "374: Train Loss: [2.2883742, 0.29771325, 4.279035] | Test Loss: [2.4274237, 0.30218282, 4.5526648]\n",
      "375: Train Loss: [2.27477, 0.28313196, 4.266408] | Test Loss: [2.3116279, 0.44665483, 4.176601]\n",
      "376: Train Loss: [2.276458, 0.3053567, 4.2475595] | Test Loss: [2.2574735, 0.30799234, 4.2069545]\n",
      "377: Train Loss: [2.3497925, 0.2626756, 4.436909] | Test Loss: [2.2431, 0.2982712, 4.1879287]\n",
      "378: Train Loss: [2.3316128, 0.36850232, 4.2947235] | Test Loss: [2.2140026, 0.38797498, 4.04003]\n",
      "379: Train Loss: [2.2992063, 0.36358428, 4.2348285] | Test Loss: [2.3269722, 0.35467795, 4.2992663]\n",
      "380: Train Loss: [2.3612309, 0.26426336, 4.4581985] | Test Loss: [2.2623382, 0.37220117, 4.1524754]\n",
      "381: Train Loss: [2.166587, 0.27246106, 4.0607133] | Test Loss: [2.3100903, 0.27839383, 4.341787]\n",
      "382: Train Loss: [2.5467045, 0.34948874, 4.7439203] | Test Loss: [2.3046997, 0.36445662, 4.2449427]\n",
      "383: Train Loss: [2.3235319, 0.29652163, 4.350542] | Test Loss: [2.1994243, 0.36409166, 4.0347567]\n",
      "384: Train Loss: [2.32494, 0.3643614, 4.2855186] | Test Loss: [2.3451426, 0.2922922, 4.397993]\n",
      "385: Train Loss: [2.346635, 0.3210004, 4.3722696] | Test Loss: [2.4857974, 0.3110299, 4.660565]\n",
      "386: Train Loss: [2.2414482, 0.34764737, 4.135249] | Test Loss: [2.3558285, 0.36924112, 4.342416]\n",
      "387: Train Loss: [2.3043332, 0.33984876, 4.2688174] | Test Loss: [2.4256175, 0.36763775, 4.4835973]\n",
      "388: Train Loss: [2.240891, 0.3380673, 4.1437144] | Test Loss: [2.2814744, 0.34944847, 4.2135]\n",
      "389: Train Loss: [2.291429, 0.3283744, 4.2544837] | Test Loss: [2.4314227, 0.36591694, 4.4969287]\n",
      "390: Train Loss: [2.3023272, 0.2970922, 4.307562] | Test Loss: [2.2976024, 0.35802764, 4.2371774]\n",
      "391: Train Loss: [2.3619525, 0.3474527, 4.3764524] | Test Loss: [2.4081712, 0.37365845, 4.4426837]\n",
      "392: Train Loss: [2.3792577, 0.29717702, 4.4613385] | Test Loss: [2.2154655, 0.30425298, 4.126678]\n",
      "393: Train Loss: [2.2749186, 0.36365193, 4.1861854] | Test Loss: [2.342618, 0.3120358, 4.3732]\n",
      "394: Train Loss: [2.1521795, 0.35164052, 3.9527183] | Test Loss: [2.255278, 0.3910642, 4.119492]\n",
      "395: Train Loss: [2.2272713, 0.42078987, 4.033753] | Test Loss: [2.3664153, 0.33418453, 4.398646]\n",
      "396: Train Loss: [2.4710047, 0.5025533, 4.439456] | Test Loss: [2.2551358, 0.35299063, 4.157281]\n",
      "397: Train Loss: [2.4777226, 0.34839052, 4.6070547] | Test Loss: [2.2964246, 0.33096755, 4.261882]\n",
      "398: Train Loss: [2.4039729, 0.3626761, 4.4452696] | Test Loss: [2.3467577, 0.3252265, 4.368289]\n",
      "399: Train Loss: [2.355308, 0.35036796, 4.360248] | Test Loss: [2.3398628, 0.2983481, 4.3813777]\n",
      "400: Train Loss: [2.4318442, 0.31332114, 4.5503674] | Test Loss: [2.3242676, 0.31172884, 4.3368063]\n",
      "401: Train Loss: [2.266344, 0.3450792, 4.1876087] | Test Loss: [2.1929154, 0.37583137, 4.0099993]\n",
      "402: Train Loss: [2.407519, 0.3359426, 4.4790955] | Test Loss: [2.3522966, 0.2917071, 4.412886]\n",
      "403: Train Loss: [2.2486548, 0.31003943, 4.18727] | Test Loss: [2.2606359, 0.2946437, 4.226628]\n",
      "404: Train Loss: [2.2454398, 0.36902073, 4.1218586] | Test Loss: [2.253216, 0.2918435, 4.2145886]\n",
      "405: Train Loss: [2.5169444, 0.34799024, 4.685899] | Test Loss: [2.1760259, 0.31517428, 4.0368776]\n",
      "406: Train Loss: [2.2324085, 0.37071326, 4.094104] | Test Loss: [2.3865788, 0.39934042, 4.373817]\n",
      "407: Train Loss: [2.1510084, 0.37386447, 3.9281523] | Test Loss: [2.2938142, 0.38566837, 4.20196]\n",
      "408: Train Loss: [2.3160684, 0.39926198, 4.232875] | Test Loss: [2.3490043, 0.35999507, 4.3380136]\n",
      "409: Train Loss: [2.2797077, 0.36375064, 4.195665] | Test Loss: [2.2460449, 0.35855597, 4.133534]\n",
      "410: Train Loss: [2.0462677, 0.34079558, 3.7517397] | Test Loss: [2.4306865, 0.36816266, 4.4932103]\n",
      "411: Train Loss: [2.1792912, 0.3232617, 4.0353208] | Test Loss: [2.4113977, 0.32554865, 4.4972467]\n",
      "412: Train Loss: [2.3593104, 0.34398013, 4.3746405] | Test Loss: [2.3981884, 0.29920882, 4.497168]\n",
      "413: Train Loss: [2.2816036, 0.42000237, 4.1432047] | Test Loss: [2.2053964, 0.36082175, 4.049971]\n",
      "414: Train Loss: [2.2601256, 0.4230376, 4.0972137] | Test Loss: [2.207458, 0.32146436, 4.0934515]\n",
      "415: Train Loss: [2.3674552, 0.39897928, 4.3359313] | Test Loss: [2.4311764, 0.41464022, 4.4477124]\n",
      "416: Train Loss: [2.1880186, 0.28504616, 4.090991] | Test Loss: [2.3191142, 0.32614458, 4.3120837]\n",
      "417: Train Loss: [2.45794, 0.33716598, 4.5787144] | Test Loss: [2.2615535, 0.32119435, 4.201913]\n",
      "418: Train Loss: [2.0117054, 0.35952163, 3.663889] | Test Loss: [2.1982808, 0.3529334, 4.043628]\n",
      "419: Train Loss: [2.197366, 0.33843687, 4.056295] | Test Loss: [2.056488, 0.43854123, 3.6744347]\n",
      "420: Train Loss: [2.3833818, 0.31150442, 4.4552593] | Test Loss: [2.452054, 0.43458056, 4.4695272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421: Train Loss: [2.3881717, 0.3104867, 4.4658566] | Test Loss: [2.3225002, 0.35710064, 4.2879]\n",
      "422: Train Loss: [2.233052, 0.33384806, 4.132256] | Test Loss: [2.2620344, 0.3390409, 4.185028]\n",
      "423: Train Loss: [2.2849429, 0.34174076, 4.228145] | Test Loss: [2.2431018, 0.38045815, 4.1057453]\n",
      "424: Train Loss: [2.1968741, 0.23242506, 4.161323] | Test Loss: [2.3352742, 0.36024404, 4.310304]\n",
      "425: Train Loss: [2.1373067, 0.34982482, 3.9247887] | Test Loss: [2.3132818, 0.35034287, 4.276221]\n",
      "426: Train Loss: [2.3068998, 0.36007735, 4.253722] | Test Loss: [2.3149748, 0.36787048, 4.2620792]\n",
      "427: Train Loss: [2.4659863, 0.40668553, 4.525287] | Test Loss: [2.2851782, 0.30446506, 4.2658916]\n",
      "428: Train Loss: [2.2713454, 0.30849457, 4.234196] | Test Loss: [2.194476, 0.35585362, 4.033098]\n",
      "429: Train Loss: [2.473399, 0.35306033, 4.5937376] | Test Loss: [2.2648926, 0.3224879, 4.2072973]\n",
      "430: Train Loss: [2.1043403, 0.31966412, 3.8890166] | Test Loss: [2.170065, 0.35668725, 3.9834428]\n",
      "431: Train Loss: [2.4059734, 0.32429686, 4.48765] | Test Loss: [2.4210494, 0.30861008, 4.5334888]\n",
      "432: Train Loss: [2.2245555, 0.357436, 4.091675] | Test Loss: [2.2569242, 0.3827094, 4.131139]\n",
      "433: Train Loss: [2.4580963, 0.3146252, 4.6015673] | Test Loss: [2.2834864, 0.35732067, 4.209652]\n",
      "434: Train Loss: [2.351956, 0.39431491, 4.309597] | Test Loss: [2.1861691, 0.31978238, 4.052556]\n",
      "435: Train Loss: [2.3248372, 0.29543027, 4.354244] | Test Loss: [2.1588404, 0.3945854, 3.9230952]\n",
      "436: Train Loss: [2.173982, 0.283462, 4.064502] | Test Loss: [2.3381717, 0.3964663, 4.279877]\n",
      "437: Train Loss: [2.350542, 0.34049046, 4.360594] | Test Loss: [2.3493204, 0.30252585, 4.396115]\n",
      "438: Train Loss: [2.163931, 0.32006693, 4.007795] | Test Loss: [2.2279742, 0.30371028, 4.152238]\n",
      "439: Train Loss: [2.1844947, 0.31816974, 4.05082] | Test Loss: [2.1362686, 0.32446232, 3.948075]\n",
      "440: Train Loss: [2.2167177, 0.2985323, 4.134903] | Test Loss: [2.1834357, 0.2993513, 4.06752]\n",
      "441: Train Loss: [2.304709, 0.33197913, 4.2774386] | Test Loss: [2.378383, 0.32208884, 4.434677]\n",
      "442: Train Loss: [2.2704105, 0.2844775, 4.2563434] | Test Loss: [2.4050796, 0.37696514, 4.433194]\n",
      "443: Train Loss: [2.1571527, 0.3485682, 3.965737] | Test Loss: [2.3595104, 0.32588816, 4.3931327]\n",
      "444: Train Loss: [2.3448484, 0.5464355, 4.1432614] | Test Loss: [2.2974355, 0.29314718, 4.301724]\n",
      "445: Train Loss: [2.204779, 0.28839064, 4.121167] | Test Loss: [2.1738625, 0.374898, 3.9728267]\n",
      "446: Train Loss: [2.3503242, 0.35174286, 4.3489056] | Test Loss: [2.3048763, 0.31778875, 4.291964]\n",
      "447: Train Loss: [2.320938, 0.31987944, 4.3219967] | Test Loss: [2.3097088, 0.2791778, 4.34024]\n",
      "448: Train Loss: [2.3578262, 0.3473083, 4.3683443] | Test Loss: [2.3336258, 0.31823745, 4.3490143]\n",
      "449: Train Loss: [2.1836162, 0.30482855, 4.0624037] | Test Loss: [2.1915019, 0.39243254, 3.990571]\n",
      "450: Train Loss: [2.2006762, 0.3297622, 4.0715904] | Test Loss: [2.2044435, 0.386173, 4.0227137]\n",
      "451: Train Loss: [2.2228827, 0.28406042, 4.161705] | Test Loss: [2.20229, 0.32935068, 4.0752296]\n",
      "452: Train Loss: [2.4426885, 0.2964228, 4.588954] | Test Loss: [2.186753, 0.29929975, 4.0742064]\n",
      "453: Train Loss: [2.1152914, 0.38899538, 3.8415875] | Test Loss: [2.1417122, 0.3116626, 3.971762]\n",
      "454: Train Loss: [2.3382895, 0.3019292, 4.37465] | Test Loss: [2.2243557, 0.296602, 4.152109]\n",
      "455: Train Loss: [2.3674092, 0.3798006, 4.3550177] | Test Loss: [2.1862016, 0.32453588, 4.0478673]\n",
      "456: Train Loss: [2.131015, 0.3262502, 3.9357798] | Test Loss: [2.2422438, 0.34677225, 4.1377153]\n",
      "457: Train Loss: [2.3656423, 0.32622516, 4.4050593] | Test Loss: [2.295961, 0.31862196, 4.2732997]\n",
      "458: Train Loss: [2.135674, 0.38615704, 3.8851912] | Test Loss: [2.2917066, 0.32072842, 4.262685]\n",
      "459: Train Loss: [2.2212365, 0.31124654, 4.1312265] | Test Loss: [2.4266171, 0.32747766, 4.525757]\n",
      "460: Train Loss: [2.3277092, 0.35433912, 4.3010793] | Test Loss: [2.2102993, 0.29806507, 4.1225333]\n",
      "461: Train Loss: [2.290592, 0.33456886, 4.246615] | Test Loss: [2.150325, 0.32418168, 3.9764686]\n",
      "462: Train Loss: [2.230484, 0.36701947, 4.0939484] | Test Loss: [2.2253335, 0.28364336, 4.1670237]\n",
      "463: Train Loss: [2.4283047, 0.37510207, 4.4815073] | Test Loss: [2.2628121, 0.35003474, 4.1755896]\n",
      "464: Train Loss: [2.334351, 0.28390512, 4.384797] | Test Loss: [2.2517555, 0.42038253, 4.0831285]\n",
      "465: Train Loss: [2.1020498, 0.29100603, 3.9130936] | Test Loss: [2.2077491, 0.40617073, 4.0093274]\n",
      "466: Train Loss: [2.1821985, 0.29836097, 4.066036] | Test Loss: [2.2872617, 0.3189648, 4.2555585]\n",
      "467: Train Loss: [2.1430364, 0.26001832, 4.0260544] | Test Loss: [2.274837, 0.32490548, 4.2247686]\n",
      "468: Train Loss: [2.2745452, 0.34167427, 4.207416] | Test Loss: [2.237275, 0.27216285, 4.202387]\n",
      "469: Train Loss: [2.2698984, 0.3542546, 4.185542] | Test Loss: [2.2336004, 0.31777123, 4.1494293]\n",
      "470: Train Loss: [2.0689, 0.3605482, 3.777252] | Test Loss: [2.3668761, 0.31469116, 4.419061]\n",
      "471: Train Loss: [2.1172996, 0.26833314, 3.966266] | Test Loss: [2.4592562, 0.41188446, 4.506628]\n",
      "472: Train Loss: [2.190437, 0.29454798, 4.086326] | Test Loss: [2.2712944, 0.33423725, 4.2083516]\n",
      "473: Train Loss: [2.2898948, 0.32677004, 4.25302] | Test Loss: [2.32603, 0.34938437, 4.3026757]\n",
      "474: Train Loss: [2.3428802, 0.31398588, 4.3717747] | Test Loss: [2.228801, 0.34155396, 4.116048]\n",
      "475: Train Loss: [2.2259984, 0.2733435, 4.1786532] | Test Loss: [2.395535, 0.4035943, 4.3874755]\n",
      "476: Train Loss: [2.2533045, 0.29212838, 4.2144804] | Test Loss: [2.4144435, 0.3002645, 4.5286226]\n",
      "477: Train Loss: [2.2617967, 0.30632174, 4.217272] | Test Loss: [2.1707873, 0.35235405, 3.9892209]\n",
      "478: Train Loss: [2.1999755, 0.32915592, 4.070795] | Test Loss: [2.3370426, 0.4332279, 4.240857]\n",
      "479: Train Loss: [2.0660026, 0.3003762, 3.8316288] | Test Loss: [2.471699, 0.3139534, 4.6294446]\n",
      "480: Train Loss: [2.255386, 0.323931, 4.186841] | Test Loss: [2.2177455, 0.37572223, 4.0597687]\n",
      "481: Train Loss: [2.1912, 0.33304834, 4.0493517] | Test Loss: [2.175852, 0.30051148, 4.0511928]\n",
      "482: Train Loss: [2.2882152, 0.27977595, 4.296654] | Test Loss: [2.1226814, 0.3538887, 3.891474]\n",
      "483: Train Loss: [2.2150123, 0.38245663, 4.047568] | Test Loss: [2.1252115, 0.30277008, 3.947653]\n",
      "484: Train Loss: [2.2633195, 0.28730342, 4.2393355] | Test Loss: [2.215455, 0.33521432, 4.095696]\n",
      "485: Train Loss: [2.3351986, 0.33118904, 4.339208] | Test Loss: [2.257253, 0.28022295, 4.234283]\n",
      "486: Train Loss: [2.2606375, 0.33326814, 4.188007] | Test Loss: [2.2827702, 0.35232195, 4.213218]\n",
      "487: Train Loss: [2.255327, 0.33447713, 4.176177] | Test Loss: [2.2250366, 0.36107436, 4.088999]\n",
      "488: Train Loss: [2.067182, 0.330231, 3.804133] | Test Loss: [2.3066316, 0.31907287, 4.2941904]\n",
      "489: Train Loss: [2.222405, 0.28874227, 4.156068] | Test Loss: [2.2560909, 0.31196445, 4.2002172]\n",
      "490: Train Loss: [2.1987188, 0.3522094, 4.045228] | Test Loss: [2.2377903, 0.33206493, 4.1435156]\n",
      "491: Train Loss: [2.1450577, 0.31839022, 3.9717252] | Test Loss: [2.2706034, 0.3037854, 4.2374215]\n",
      "492: Train Loss: [2.159535, 0.32158408, 3.9974856] | Test Loss: [2.4357522, 0.40374252, 4.467762]\n",
      "493: Train Loss: [2.4236808, 0.358108, 4.4892535] | Test Loss: [2.2187603, 0.31977838, 4.117742]\n",
      "494: Train Loss: [2.3740163, 0.45434758, 4.293685] | Test Loss: [2.299089, 0.38079682, 4.217381]\n",
      "495: Train Loss: [2.2172582, 0.3367592, 4.0977573] | Test Loss: [2.380406, 0.40924257, 4.351569]\n",
      "496: Train Loss: [2.3606565, 0.3507005, 4.3706126] | Test Loss: [2.299537, 0.34194225, 4.2571316]\n",
      "497: Train Loss: [2.2267752, 0.32891476, 4.1246357] | Test Loss: [2.3329432, 0.3402058, 4.3256807]\n",
      "498: Train Loss: [2.3196063, 0.323557, 4.3156557] | Test Loss: [2.3456087, 0.31557956, 4.375638]\n",
      "499: Train Loss: [2.2374592, 0.3706663, 4.104252] | Test Loss: [2.3338094, 0.33307788, 4.334541]\n",
      "500: Train Loss: [2.2422233, 0.29746228, 4.186984] | Test Loss: [2.2914, 0.3714539, 4.211346]\n",
      "501: Train Loss: [2.15691, 0.29187942, 4.0219407] | Test Loss: [2.3645267, 0.35434124, 4.3747125]\n",
      "502: Train Loss: [2.3422227, 0.25248712, 4.431958] | Test Loss: [2.2062724, 0.37247312, 4.0400715]\n",
      "503: Train Loss: [2.292277, 0.3364257, 4.2481284] | Test Loss: [2.2586846, 0.3445332, 4.172836]\n",
      "504: Train Loss: [2.329225, 0.40705818, 4.251392] | Test Loss: [2.2493174, 0.30962804, 4.189007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505: Train Loss: [2.2572124, 0.3221608, 4.192264] | Test Loss: [2.1395195, 0.28802568, 3.9910133]\n",
      "506: Train Loss: [2.2037778, 0.32957363, 4.077982] | Test Loss: [2.245999, 0.3169309, 4.1750674]\n",
      "507: Train Loss: [2.2517517, 0.35897243, 4.144531] | Test Loss: [2.2266693, 0.3424825, 4.110856]\n",
      "508: Train Loss: [2.3292267, 0.3502007, 4.308253] | Test Loss: [2.32828, 0.36989453, 4.2866654]\n",
      "509: Train Loss: [2.1521046, 0.33638197, 3.9678273] | Test Loss: [2.2192605, 0.28077707, 4.157744]\n",
      "510: Train Loss: [2.2454815, 0.36328545, 4.1276774] | Test Loss: [2.2896342, 0.39897668, 4.1802917]\n",
      "511: Train Loss: [2.3437276, 0.35027683, 4.337178] | Test Loss: [2.092229, 0.31951806, 3.86494]\n",
      "512: Train Loss: [2.1759932, 0.3186757, 4.033311] | Test Loss: [2.1953964, 0.3786872, 4.0121055]\n",
      "513: Train Loss: [2.336413, 0.37200266, 4.300823] | Test Loss: [2.2929313, 0.32436913, 4.2614937]\n",
      "514: Train Loss: [2.3381195, 0.39557174, 4.2806673] | Test Loss: [2.357184, 0.35480452, 4.3595634]\n",
      "515: Train Loss: [2.250597, 0.3201738, 4.1810203] | Test Loss: [2.3941545, 0.36964893, 4.41866]\n",
      "516: Train Loss: [2.2539842, 0.3074528, 4.2005157] | Test Loss: [2.2137759, 0.2918351, 4.1357164]\n",
      "517: Train Loss: [2.2592585, 0.3903863, 4.128131] | Test Loss: [2.3670852, 0.3078825, 4.426288]\n",
      "518: Train Loss: [2.2227013, 0.2960338, 4.149369] | Test Loss: [2.4081948, 0.36795205, 4.4484377]\n",
      "519: Train Loss: [2.175042, 0.3942946, 3.955789] | Test Loss: [2.3032372, 0.31800207, 4.288472]\n",
      "520: Train Loss: [2.4088378, 0.355172, 4.4625034] | Test Loss: [2.1369176, 0.30123436, 3.9726007]\n",
      "521: Train Loss: [2.2717576, 0.318088, 4.225427] | Test Loss: [2.3106751, 0.3830006, 4.2383494]\n",
      "522: Train Loss: [2.2479813, 0.33862793, 4.157335] | Test Loss: [2.3825183, 0.31703338, 4.4480033]\n",
      "523: Train Loss: [2.1630764, 0.43474212, 3.8914108] | Test Loss: [2.1305478, 0.29647774, 3.9646177]\n",
      "Epoch 2\n",
      "0: Train Loss: [2.1045287, 0.30780086, 3.9012563] | Test Loss: [2.1019955, 0.28773615, 3.916255]\n",
      "1: Train Loss: [2.0444708, 0.2974299, 3.7915118] | Test Loss: [2.287076, 0.31947893, 4.254673]\n",
      "2: Train Loss: [2.0305347, 0.30271047, 3.758359] | Test Loss: [2.3039944, 0.38083583, 4.227153]\n",
      "3: Train Loss: [2.0832355, 0.3150977, 3.8513732] | Test Loss: [2.252884, 0.39351404, 4.1122537]\n",
      "4: Train Loss: [2.041256, 0.26260924, 3.819903] | Test Loss: [2.212941, 0.41180938, 4.0140724]\n",
      "5: Train Loss: [2.1876807, 0.33817872, 4.037183] | Test Loss: [2.153286, 0.26575297, 4.040819]\n",
      "6: Train Loss: [2.1210337, 0.2879374, 3.9541297] | Test Loss: [2.3346028, 0.32730794, 4.3418975]\n",
      "7: Train Loss: [2.1024032, 0.3604263, 3.84438] | Test Loss: [2.253115, 0.31308907, 4.193141]\n",
      "8: Train Loss: [1.9115927, 0.3464725, 3.476713] | Test Loss: [2.200591, 0.29777488, 4.1034074]\n",
      "9: Train Loss: [2.1334393, 0.28725725, 3.9796212] | Test Loss: [2.227017, 0.384765, 4.0692687]\n",
      "10: Train Loss: [2.2725017, 0.32548544, 4.219518] | Test Loss: [2.3247013, 0.33898526, 4.310417]\n",
      "11: Train Loss: [2.0749786, 0.3372924, 3.812665] | Test Loss: [2.217057, 0.36209175, 4.0720224]\n",
      "12: Train Loss: [2.0136714, 0.30488044, 3.7224622] | Test Loss: [2.2634132, 0.3396203, 4.1872063]\n",
      "13: Train Loss: [2.1551447, 0.35352507, 3.9567645] | Test Loss: [2.3563797, 0.36449432, 4.348265]\n",
      "14: Train Loss: [2.2309384, 0.37823188, 4.083645] | Test Loss: [2.2216547, 0.31671423, 4.126595]\n",
      "15: Train Loss: [2.1548028, 0.25958088, 4.0500245] | Test Loss: [2.0322416, 0.33090097, 3.7335823]\n",
      "16: Train Loss: [1.9483905, 0.26610476, 3.6306763] | Test Loss: [2.2851026, 0.31090885, 4.2592964]\n",
      "17: Train Loss: [2.0668213, 0.38105687, 3.752586] | Test Loss: [2.3394332, 0.32478848, 4.354078]\n",
      "18: Train Loss: [2.1103952, 0.34445354, 3.876337] | Test Loss: [2.3069265, 0.28885943, 4.3249936]\n",
      "19: Train Loss: [2.2918093, 0.3218288, 4.26179] | Test Loss: [2.1712654, 0.30653474, 4.035996]\n",
      "20: Train Loss: [2.1307566, 0.29784507, 3.9636683] | Test Loss: [2.3654857, 0.31304526, 4.4179263]\n",
      "21: Train Loss: [2.2441185, 0.30322504, 4.185012] | Test Loss: [2.166313, 0.2782121, 4.054414]\n",
      "22: Train Loss: [2.0770614, 0.36155716, 3.7925658] | Test Loss: [2.2659426, 0.3467004, 4.185185]\n",
      "23: Train Loss: [2.2337315, 0.3522629, 4.1152] | Test Loss: [2.16076, 0.325517, 3.996003]\n",
      "24: Train Loss: [2.1248672, 0.37132356, 3.878411] | Test Loss: [2.213654, 0.27427316, 4.153035]\n",
      "25: Train Loss: [2.0864995, 0.29729062, 3.875708] | Test Loss: [2.1166387, 0.31747055, 3.9158065]\n",
      "26: Train Loss: [2.137117, 0.27290615, 4.0013275] | Test Loss: [2.1621702, 0.34960288, 3.9747376]\n",
      "27: Train Loss: [2.0206122, 0.2724023, 3.768822] | Test Loss: [2.3055942, 0.2769702, 4.334218]\n",
      "28: Train Loss: [2.068149, 0.3589558, 3.7773426] | Test Loss: [2.3109407, 0.47431916, 4.1475625]\n",
      "29: Train Loss: [2.3045392, 0.27464294, 4.3344355] | Test Loss: [2.3251412, 0.35245878, 4.2978234]\n",
      "30: Train Loss: [2.249536, 0.44758993, 4.051482] | Test Loss: [2.5059476, 0.48725277, 4.5246425]\n",
      "31: Train Loss: [2.2170026, 0.29783353, 4.136172] | Test Loss: [2.156789, 0.31673366, 3.9968443]\n",
      "32: Train Loss: [2.1811519, 0.2891051, 4.073199] | Test Loss: [2.2485151, 0.33696654, 4.1600637]\n",
      "33: Train Loss: [2.0474763, 0.28717458, 3.8077781] | Test Loss: [2.1917162, 0.3981201, 3.9853125]\n",
      "34: Train Loss: [2.1512861, 0.39866492, 3.9039075] | Test Loss: [2.1605983, 0.38242683, 3.9387696]\n",
      "35: Train Loss: [2.0515063, 0.34044603, 3.7625666] | Test Loss: [2.332781, 0.3227602, 4.342802]\n",
      "36: Train Loss: [2.2561479, 0.3471167, 4.1651793] | Test Loss: [2.3657863, 0.36108953, 4.370483]\n",
      "37: Train Loss: [2.1017253, 0.28814727, 3.9153035] | Test Loss: [2.1350584, 0.3032259, 3.966891]\n",
      "38: Train Loss: [2.2088928, 0.37115535, 4.0466304] | Test Loss: [2.3174567, 0.28199992, 4.3529134]\n",
      "39: Train Loss: [2.0987892, 0.34605008, 3.8515282] | Test Loss: [2.3258955, 0.44940072, 4.20239]\n",
      "40: Train Loss: [2.280047, 0.42093426, 4.1391597] | Test Loss: [2.3362114, 0.41156328, 4.2608595]\n",
      "41: Train Loss: [2.390897, 0.30946222, 4.472332] | Test Loss: [2.321698, 0.32983115, 4.313565]\n",
      "42: Train Loss: [2.218529, 0.3586506, 4.0784073] | Test Loss: [2.3324525, 0.32769176, 4.3372135]\n",
      "43: Train Loss: [2.1084526, 0.32631162, 3.8905933] | Test Loss: [2.2713575, 0.3802973, 4.162418]\n",
      "44: Train Loss: [2.1484053, 0.30706066, 3.9897501] | Test Loss: [2.2140079, 0.3193344, 4.108681]\n",
      "45: Train Loss: [2.0781078, 0.30531806, 3.8508976] | Test Loss: [2.0381327, 0.5320656, 3.5442]\n",
      "46: Train Loss: [1.9553385, 0.2928546, 3.6178224] | Test Loss: [2.2849538, 0.35823366, 4.211674]\n",
      "47: Train Loss: [2.2396238, 0.35156217, 4.1276855] | Test Loss: [2.2502847, 0.37204394, 4.1285253]\n",
      "48: Train Loss: [2.2437143, 0.34657565, 4.140853] | Test Loss: [2.1455312, 0.37360534, 3.917457]\n",
      "49: Train Loss: [2.247786, 0.32336855, 4.1722035] | Test Loss: [2.2586906, 0.32930264, 4.1880784]\n",
      "50: Train Loss: [2.2342234, 0.28279468, 4.1856523] | Test Loss: [2.3400836, 0.34699112, 4.333176]\n",
      "51: Train Loss: [2.3342838, 0.33552217, 4.3330455] | Test Loss: [2.3164408, 0.33426896, 4.2986126]\n",
      "52: Train Loss: [1.9678373, 0.32045972, 3.6152148] | Test Loss: [2.204475, 0.3068423, 4.1021075]\n",
      "53: Train Loss: [2.3256807, 0.3550364, 4.296325] | Test Loss: [2.416562, 0.34566322, 4.487461]\n",
      "54: Train Loss: [2.031506, 0.32764977, 3.7353623] | Test Loss: [2.3253658, 0.34958455, 4.301147]\n",
      "55: Train Loss: [2.05394, 0.33124083, 3.7766395] | Test Loss: [2.4412074, 0.35110188, 4.531313]\n",
      "56: Train Loss: [2.2609506, 0.33019912, 4.191702] | Test Loss: [2.1874604, 0.38125443, 3.9936666]\n",
      "57: Train Loss: [2.2160594, 0.34760854, 4.0845103] | Test Loss: [2.2765527, 0.35522225, 4.197883]\n",
      "58: Train Loss: [2.0156295, 0.26663482, 3.764624] | Test Loss: [2.281589, 0.37428, 4.188898]\n",
      "59: Train Loss: [2.0462759, 0.36593086, 3.726621] | Test Loss: [2.1837835, 0.28709584, 4.080471]\n",
      "60: Train Loss: [2.1817696, 0.30354747, 4.059992] | Test Loss: [2.3231673, 0.34150118, 4.3048334]\n",
      "61: Train Loss: [2.1700082, 0.26436535, 4.075651] | Test Loss: [2.132352, 0.32552752, 3.9391766]\n",
      "62: Train Loss: [2.1362627, 0.293039, 3.9794865] | Test Loss: [2.0981205, 0.3561806, 3.8400602]\n",
      "63: Train Loss: [2.204859, 0.31705502, 4.092663] | Test Loss: [2.187175, 0.3669472, 4.007403]\n",
      "64: Train Loss: [2.1932333, 0.39362457, 3.9928417] | Test Loss: [2.2197053, 0.28125212, 4.158159]\n",
      "65: Train Loss: [2.1274948, 0.34509704, 3.9098926] | Test Loss: [2.3001304, 0.3294267, 4.270834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66: Train Loss: [2.1470296, 0.26353565, 4.030524] | Test Loss: [2.2006748, 0.4325229, 3.9688265]\n",
      "67: Train Loss: [2.0752041, 0.2907945, 3.8596137] | Test Loss: [2.3671439, 0.2866346, 4.4476533]\n",
      "68: Train Loss: [2.23671, 0.31121686, 4.1622033] | Test Loss: [2.2703562, 0.3114215, 4.229291]\n",
      "69: Train Loss: [2.1535962, 0.32429352, 3.982899] | Test Loss: [2.2596612, 0.3989119, 4.1204104]\n",
      "70: Train Loss: [2.108574, 0.2915375, 3.9256103] | Test Loss: [2.1287112, 0.26601002, 3.9914124]\n",
      "71: Train Loss: [2.166471, 0.43794093, 3.8950012] | Test Loss: [2.156555, 0.30039173, 4.012718]\n",
      "72: Train Loss: [2.2276835, 0.36363044, 4.091737] | Test Loss: [2.1892004, 0.37303466, 4.0053663]\n",
      "73: Train Loss: [2.1882966, 0.37861302, 3.99798] | Test Loss: [2.3486784, 0.35808316, 4.3392735]\n",
      "74: Train Loss: [2.164232, 0.3336829, 3.9947813] | Test Loss: [2.2113492, 0.3183631, 4.1043353]\n",
      "75: Train Loss: [2.068724, 0.31079584, 3.8266518] | Test Loss: [2.2953594, 0.429185, 4.161534]\n",
      "76: Train Loss: [2.1430385, 0.31646627, 3.9696107] | Test Loss: [2.3590488, 0.36361194, 4.354486]\n",
      "77: Train Loss: [1.9949752, 0.346798, 3.6431525] | Test Loss: [2.422933, 0.37364918, 4.472217]\n",
      "78: Train Loss: [2.1956785, 0.35281906, 4.038538] | Test Loss: [2.2971425, 0.30503663, 4.2892485]\n",
      "79: Train Loss: [2.166174, 0.28306258, 4.0492854] | Test Loss: [2.0030713, 0.2940322, 3.7121103]\n",
      "80: Train Loss: [2.2284408, 0.2819196, 4.174962] | Test Loss: [2.2642055, 0.3431437, 4.185267]\n",
      "81: Train Loss: [1.9991423, 0.3754596, 3.622825] | Test Loss: [2.3621116, 0.35232067, 4.3719025]\n",
      "82: Train Loss: [2.0646067, 0.37583372, 3.7533798] | Test Loss: [2.2748947, 0.3756309, 4.1741586]\n",
      "83: Train Loss: [2.117943, 0.2911443, 3.9447417] | Test Loss: [2.1042855, 0.3625323, 3.8460388]\n",
      "84: Train Loss: [2.214812, 0.34506404, 4.08456] | Test Loss: [2.1220376, 0.31817064, 3.9259045]\n",
      "85: Train Loss: [2.1527247, 0.44730517, 3.8581443] | Test Loss: [2.2416248, 0.3682239, 4.115026]\n",
      "86: Train Loss: [2.0856514, 0.34388676, 3.8274162] | Test Loss: [2.17008, 0.3241787, 4.015981]\n",
      "87: Train Loss: [2.0428662, 0.31422383, 3.7715087] | Test Loss: [2.4006886, 0.37390447, 4.427473]\n",
      "88: Train Loss: [2.0906496, 0.31272894, 3.8685703] | Test Loss: [2.2545443, 0.3428389, 4.1662498]\n",
      "89: Train Loss: [2.0789566, 0.29844874, 3.8594646] | Test Loss: [2.161563, 0.30535328, 4.0177727]\n",
      "90: Train Loss: [2.251402, 0.30142638, 4.2013774] | Test Loss: [2.3136165, 0.33837745, 4.2888556]\n",
      "91: Train Loss: [2.2713177, 0.33910793, 4.2035275] | Test Loss: [2.3043659, 0.41487625, 4.1938553]\n",
      "92: Train Loss: [1.9631212, 0.29304016, 3.6332023] | Test Loss: [2.3248575, 0.35493752, 4.2947774]\n",
      "93: Train Loss: [2.267861, 0.41076916, 4.124953] | Test Loss: [2.259195, 0.3456706, 4.1727195]\n",
      "94: Train Loss: [2.1436684, 0.29473832, 3.9925985] | Test Loss: [2.3161776, 0.32484156, 4.3075137]\n",
      "95: Train Loss: [2.1708632, 0.37342176, 3.9683044] | Test Loss: [2.2476206, 0.30267975, 4.1925616]\n",
      "96: Train Loss: [2.1083403, 0.35403877, 3.8626418] | Test Loss: [2.2269065, 0.34275076, 4.1110625]\n",
      "97: Train Loss: [2.150837, 0.31425852, 3.9874153] | Test Loss: [2.2879236, 0.35705623, 4.218791]\n",
      "98: Train Loss: [2.1359515, 0.2991616, 3.9727414] | Test Loss: [2.2135062, 0.39620322, 4.0308094]\n",
      "99: Train Loss: [2.2150834, 0.3886148, 4.041552] | Test Loss: [2.1523056, 0.3814656, 3.9231458]\n",
      "100: Train Loss: [2.085879, 0.3234647, 3.8482935] | Test Loss: [2.1926932, 0.30764762, 4.077739]\n",
      "101: Train Loss: [2.1746016, 0.35102084, 3.9981823] | Test Loss: [2.2723994, 0.3692475, 4.1755514]\n",
      "102: Train Loss: [2.0998895, 0.38901386, 3.8107653] | Test Loss: [2.2440574, 0.3038812, 4.1842337]\n",
      "103: Train Loss: [2.2067447, 0.30570936, 4.10778] | Test Loss: [2.238818, 0.3349909, 4.142645]\n",
      "104: Train Loss: [2.2994857, 0.32767075, 4.271301] | Test Loss: [2.287484, 0.36201593, 4.212952]\n",
      "105: Train Loss: [2.156608, 0.2640705, 4.0491457] | Test Loss: [2.3297334, 0.33875594, 4.3207107]\n",
      "106: Train Loss: [2.1349325, 0.30550236, 3.9643629] | Test Loss: [2.0641162, 0.27285028, 3.8553822]\n",
      "107: Train Loss: [2.209976, 0.2946737, 4.125278] | Test Loss: [2.6155322, 0.4859156, 4.7451487]\n",
      "108: Train Loss: [2.1978557, 0.32866162, 4.06705] | Test Loss: [2.1755397, 0.34122947, 4.00985]\n",
      "109: Train Loss: [2.0636487, 0.3537928, 3.7735045] | Test Loss: [2.267973, 0.30012998, 4.235816]\n",
      "110: Train Loss: [2.301101, 0.39826295, 4.203939] | Test Loss: [2.2049127, 0.33497816, 4.074847]\n",
      "111: Train Loss: [2.0646875, 0.27718797, 3.852187] | Test Loss: [2.224784, 0.30725777, 4.14231]\n",
      "112: Train Loss: [2.0697694, 0.33810592, 3.801433] | Test Loss: [2.223907, 0.36113816, 4.0866756]\n",
      "113: Train Loss: [1.9866711, 0.32040924, 3.652933] | Test Loss: [2.149223, 0.3630894, 3.9353569]\n",
      "114: Train Loss: [2.2012074, 0.32035086, 4.082064] | Test Loss: [2.2292585, 0.3164149, 4.1421022]\n",
      "115: Train Loss: [2.3080816, 0.3018342, 4.314329] | Test Loss: [2.2030077, 0.35877904, 4.0472364]\n",
      "116: Train Loss: [2.2018242, 0.34807867, 4.0555696] | Test Loss: [2.2275395, 0.29090524, 4.1641736]\n",
      "117: Train Loss: [2.270957, 0.28219244, 4.2597218] | Test Loss: [2.3241978, 0.4024423, 4.245953]\n",
      "118: Train Loss: [2.14186, 0.290541, 3.9931788] | Test Loss: [2.36876, 0.36812037, 4.3694]\n",
      "119: Train Loss: [2.130344, 0.2857583, 3.9749296] | Test Loss: [2.3608687, 0.37460238, 4.347135]\n",
      "120: Train Loss: [2.0758848, 0.36477256, 3.7869973] | Test Loss: [2.3742352, 0.30479273, 4.4436774]\n",
      "121: Train Loss: [2.032474, 0.26844856, 3.7964997] | Test Loss: [2.172971, 0.35865095, 3.9872909]\n",
      "122: Train Loss: [2.1236482, 0.27755886, 3.9697373] | Test Loss: [2.3465586, 0.39535522, 4.297762]\n",
      "123: Train Loss: [2.1286998, 0.28903872, 3.9683607] | Test Loss: [2.1369274, 0.3214217, 3.952433]\n",
      "124: Train Loss: [2.2886014, 0.5298826, 4.0473204] | Test Loss: [2.2629185, 0.28337023, 4.242467]\n",
      "125: Train Loss: [2.1395235, 0.28858152, 3.9904654] | Test Loss: [2.2595716, 0.38625324, 4.1328897]\n",
      "126: Train Loss: [1.9923333, 0.33321458, 3.651452] | Test Loss: [2.1790128, 0.36588427, 3.9921412]\n",
      "127: Train Loss: [2.1388843, 0.30006656, 3.9777021] | Test Loss: [2.3090355, 0.33810177, 4.279969]\n",
      "128: Train Loss: [2.1511207, 0.3191497, 3.9830916] | Test Loss: [2.1146789, 0.35129142, 3.8780663]\n",
      "129: Train Loss: [2.2112062, 0.31259516, 4.109817] | Test Loss: [2.251121, 0.35347036, 4.148772]\n",
      "130: Train Loss: [2.133694, 0.33837968, 3.929008] | Test Loss: [2.1139398, 0.34795767, 3.879922]\n",
      "131: Train Loss: [2.2342641, 0.37677708, 4.091751] | Test Loss: [2.26475, 0.31029344, 4.219207]\n",
      "132: Train Loss: [2.1313415, 0.317623, 3.94506] | Test Loss: [2.2824545, 0.32779598, 4.237113]\n",
      "133: Train Loss: [2.1293895, 0.3161148, 3.9426641] | Test Loss: [2.3924804, 0.3658735, 4.4190874]\n",
      "134: Train Loss: [2.0896337, 0.2723256, 3.906942] | Test Loss: [2.2181644, 0.33938295, 4.096946]\n",
      "135: Train Loss: [2.20776, 0.52024835, 3.8952718] | Test Loss: [2.3306613, 0.50886923, 4.1524534]\n",
      "136: Train Loss: [2.1999872, 0.2989666, 4.101008] | Test Loss: [2.0925107, 0.2961498, 3.8888717]\n",
      "137: Train Loss: [2.2361999, 0.26868755, 4.203712] | Test Loss: [2.37579, 0.29403445, 4.4575458]\n",
      "138: Train Loss: [2.1422472, 0.39021784, 3.8942764] | Test Loss: [2.4196355, 0.38189733, 4.4573736]\n",
      "139: Train Loss: [2.0244844, 0.3710249, 3.6779437] | Test Loss: [2.353091, 0.3525956, 4.353586]\n",
      "140: Train Loss: [2.2059417, 0.28835952, 4.1235237] | Test Loss: [2.17933, 0.3468793, 4.0117807]\n",
      "141: Train Loss: [2.131521, 0.30032608, 3.962716] | Test Loss: [2.2016096, 0.38287684, 4.0203424]\n",
      "142: Train Loss: [2.0724967, 0.37127978, 3.7737136] | Test Loss: [2.3661442, 0.396703, 4.3355856]\n",
      "143: Train Loss: [2.240218, 0.36539045, 4.1150455] | Test Loss: [2.3579116, 0.3133569, 4.4024663]\n",
      "144: Train Loss: [2.2218823, 0.27480394, 4.1689606] | Test Loss: [2.2428777, 0.4277233, 4.058032]\n",
      "145: Train Loss: [2.3195176, 0.3252944, 4.3137407] | Test Loss: [2.2785993, 0.3859985, 4.1712]\n",
      "146: Train Loss: [2.2957208, 0.47755182, 4.1138897] | Test Loss: [2.3909762, 0.3513352, 4.4306173]\n",
      "147: Train Loss: [2.3343575, 0.3107038, 4.3580112] | Test Loss: [2.2543423, 0.40552607, 4.1031585]\n",
      "148: Train Loss: [2.016167, 0.2518002, 3.7805336] | Test Loss: [2.164422, 0.2991122, 4.0297318]\n",
      "149: Train Loss: [2.1237295, 0.24905205, 3.998407] | Test Loss: [2.207365, 0.32958567, 4.0851445]\n",
      "150: Train Loss: [2.0996373, 0.32034767, 3.8789268] | Test Loss: [2.208651, 0.3216494, 4.0956526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151: Train Loss: [2.2085426, 0.26674587, 4.150339] | Test Loss: [2.1972218, 0.3381278, 4.056316]\n",
      "152: Train Loss: [2.164415, 0.37177554, 3.9570544] | Test Loss: [2.2796626, 0.3393875, 4.219938]\n",
      "153: Train Loss: [2.0773075, 0.3042247, 3.8503904] | Test Loss: [2.3332305, 0.33267325, 4.333788]\n",
      "154: Train Loss: [2.2814684, 0.31149104, 4.251446] | Test Loss: [2.1682124, 0.27998316, 4.056442]\n",
      "155: Train Loss: [2.1777859, 0.36602902, 3.9895427] | Test Loss: [2.211525, 0.37027425, 4.052776]\n",
      "156: Train Loss: [2.178342, 0.30954054, 4.0471435] | Test Loss: [2.2296154, 0.2833287, 4.1759024]\n",
      "157: Train Loss: [2.2490144, 0.32285872, 4.17517] | Test Loss: [2.116263, 0.4133084, 3.8192172]\n",
      "158: Train Loss: [2.1239777, 0.31447124, 3.9334843] | Test Loss: [2.1690278, 0.35891548, 3.9791403]\n",
      "159: Train Loss: [2.187044, 0.2898513, 4.0842366] | Test Loss: [2.3594499, 0.3444171, 4.3744826]\n",
      "160: Train Loss: [2.0151365, 0.32813567, 3.7021375] | Test Loss: [2.2526345, 0.3291712, 4.176098]\n",
      "161: Train Loss: [2.1883452, 0.32116634, 4.055524] | Test Loss: [2.0785303, 0.32572424, 3.8313363]\n",
      "162: Train Loss: [2.2358015, 0.3156996, 4.1559033] | Test Loss: [2.3567197, 0.32213354, 4.391306]\n",
      "163: Train Loss: [2.2688437, 0.32813162, 4.2095556] | Test Loss: [2.25394, 0.34132457, 4.166556]\n",
      "164: Train Loss: [2.2115319, 0.30722666, 4.115837] | Test Loss: [2.2006977, 0.3522636, 4.049132]\n",
      "165: Train Loss: [1.9577701, 0.26124203, 3.654298] | Test Loss: [2.1009831, 0.4075144, 3.794452]\n",
      "166: Train Loss: [2.2009232, 0.3440736, 4.0577726] | Test Loss: [2.3226514, 0.35594243, 4.2893605]\n",
      "167: Train Loss: [2.259152, 0.31778568, 4.200518] | Test Loss: [2.112458, 0.3755062, 3.8494098]\n",
      "168: Train Loss: [2.1970527, 0.36445048, 4.029655] | Test Loss: [2.2284877, 0.3071541, 4.1498213]\n",
      "169: Train Loss: [2.2410097, 0.2981634, 4.183856] | Test Loss: [2.198926, 0.33055347, 4.0672984]\n",
      "170: Train Loss: [2.1488943, 0.33562043, 3.9621682] | Test Loss: [2.084641, 0.29780155, 3.8714805]\n",
      "171: Train Loss: [2.045744, 0.3448262, 3.7466617] | Test Loss: [2.2056746, 0.40941274, 4.0019364]\n",
      "172: Train Loss: [2.0749795, 0.3230603, 3.8268988] | Test Loss: [2.383753, 0.34261146, 4.424895]\n",
      "173: Train Loss: [2.0380318, 0.3555097, 3.7205539] | Test Loss: [2.1490111, 0.32003, 3.9779923]\n",
      "174: Train Loss: [2.1953838, 0.3378685, 4.052899] | Test Loss: [2.1063495, 0.39770254, 3.8149962]\n",
      "175: Train Loss: [2.0804818, 0.32095745, 3.840006] | Test Loss: [2.1986415, 0.29171604, 4.105567]\n",
      "176: Train Loss: [2.2156126, 0.32115898, 4.1100664] | Test Loss: [2.3716269, 0.3557837, 4.3874702]\n",
      "177: Train Loss: [2.2136188, 0.3150957, 4.1121416] | Test Loss: [2.3290408, 0.3704036, 4.287678]\n",
      "178: Train Loss: [2.1262238, 0.3221031, 3.9303443] | Test Loss: [2.2067842, 0.3725237, 4.0410447]\n",
      "179: Train Loss: [2.1742432, 0.3103104, 4.038176] | Test Loss: [2.6202734, 0.40739974, 4.833147]\n",
      "180: Train Loss: [2.1237419, 0.362094, 3.8853896] | Test Loss: [2.1930058, 0.33814514, 4.0478663]\n",
      "181: Train Loss: [2.2139761, 0.35128018, 4.076672] | Test Loss: [2.2517846, 0.34777918, 4.15579]\n",
      "182: Train Loss: [2.3142986, 0.37488055, 4.253717] | Test Loss: [2.1235979, 0.32917288, 3.9180229]\n",
      "183: Train Loss: [2.2312891, 0.38961914, 4.072959] | Test Loss: [2.3488817, 0.36443126, 4.333332]\n",
      "184: Train Loss: [2.1492147, 0.27360734, 4.024822] | Test Loss: [2.2891116, 0.36210144, 4.2161217]\n",
      "185: Train Loss: [2.1674426, 0.29568914, 4.039196] | Test Loss: [2.0895896, 0.35630852, 3.8228705]\n",
      "186: Train Loss: [2.189923, 0.3400515, 4.0397944] | Test Loss: [2.3766313, 0.3219232, 4.4313393]\n",
      "187: Train Loss: [2.2240114, 0.30108264, 4.14694] | Test Loss: [2.4409661, 0.34976268, 4.53217]\n",
      "188: Train Loss: [2.2915661, 0.31164205, 4.27149] | Test Loss: [2.1371841, 0.3382934, 3.9360747]\n",
      "189: Train Loss: [2.137989, 0.35420182, 3.9217763] | Test Loss: [2.3077638, 0.31725854, 4.2982693]\n",
      "190: Train Loss: [2.061361, 0.34505466, 3.7776673] | Test Loss: [2.3605173, 0.35635754, 4.364677]\n",
      "191: Train Loss: [2.168524, 0.32774597, 4.009302] | Test Loss: [2.1848094, 0.28255385, 4.087065]\n",
      "192: Train Loss: [2.1432762, 0.31729326, 3.9692593] | Test Loss: [2.1405046, 0.284164, 3.9968452]\n",
      "193: Train Loss: [1.9591165, 0.3266881, 3.5915449] | Test Loss: [2.198644, 0.35009524, 4.0471926]\n",
      "194: Train Loss: [2.2226956, 0.34000716, 4.105384] | Test Loss: [2.24914, 0.31596166, 4.182318]\n",
      "195: Train Loss: [2.1240442, 0.34049264, 3.9075959] | Test Loss: [2.2096503, 0.39528504, 4.0240154]\n",
      "196: Train Loss: [2.2778277, 0.31121492, 4.2444406] | Test Loss: [2.346162, 0.35733283, 4.3349915]\n",
      "197: Train Loss: [2.0886438, 0.29159677, 3.8856907] | Test Loss: [2.2159445, 0.2985163, 4.133373]\n",
      "198: Train Loss: [2.157405, 0.33091998, 3.98389] | Test Loss: [2.4094386, 0.37993866, 4.4389386]\n",
      "199: Train Loss: [2.2208173, 0.34420028, 4.0974345] | Test Loss: [2.2979448, 0.3384939, 4.2573957]\n",
      "200: Train Loss: [2.077381, 0.32744366, 3.8273182] | Test Loss: [1.9511327, 0.38853684, 3.5137284]\n",
      "201: Train Loss: [2.078144, 0.35068902, 3.8055992] | Test Loss: [2.2999778, 0.32861465, 4.271341]\n",
      "202: Train Loss: [2.2663233, 0.34098548, 4.1916614] | Test Loss: [2.1499934, 0.32796073, 3.9720263]\n",
      "203: Train Loss: [2.1531942, 0.2962065, 4.010182] | Test Loss: [2.142731, 0.32800752, 3.9574544]\n",
      "204: Train Loss: [2.1069615, 0.30360705, 3.9103158] | Test Loss: [2.1314046, 0.3161051, 3.9467041]\n",
      "205: Train Loss: [2.1902695, 0.37410128, 4.006438] | Test Loss: [2.287086, 0.30191746, 4.2722545]\n",
      "206: Train Loss: [2.180139, 0.318338, 4.04194] | Test Loss: [2.1176393, 0.42491236, 3.8103662]\n",
      "207: Train Loss: [2.2846768, 0.34549406, 4.2238593] | Test Loss: [2.1863358, 0.31338608, 4.0592856]\n",
      "208: Train Loss: [2.1617124, 0.30008045, 4.0233445] | Test Loss: [2.4110055, 0.34723243, 4.4747787]\n",
      "209: Train Loss: [2.143455, 0.31622958, 3.9706807] | Test Loss: [2.2060251, 0.31590134, 4.096149]\n",
      "210: Train Loss: [2.1030831, 0.29682404, 3.909342] | Test Loss: [2.1176484, 0.32804224, 3.9072545]\n",
      "211: Train Loss: [2.101657, 0.30755934, 3.8957546] | Test Loss: [2.2488778, 0.32562912, 4.1721263]\n",
      "212: Train Loss: [2.230482, 0.3000972, 4.160867] | Test Loss: [2.1653287, 0.33484113, 3.9958165]\n",
      "213: Train Loss: [2.1957526, 0.33317763, 4.0583277] | Test Loss: [2.2665224, 0.30429596, 4.228749]\n",
      "214: Train Loss: [2.1150131, 0.26988545, 3.960141] | Test Loss: [2.251174, 0.3756052, 4.126743]\n",
      "215: Train Loss: [2.1517494, 0.3913554, 3.9121435] | Test Loss: [2.3819635, 0.38581407, 4.378113]\n",
      "216: Train Loss: [2.0854852, 0.39535508, 3.7756155] | Test Loss: [2.3298714, 0.39141616, 4.2683268]\n",
      "217: Train Loss: [2.0734992, 0.26336634, 3.883632] | Test Loss: [2.3068483, 0.32170063, 4.291996]\n",
      "218: Train Loss: [2.1962187, 0.3498657, 4.0425715] | Test Loss: [2.2349694, 0.35488555, 4.115053]\n",
      "219: Train Loss: [2.2452922, 0.40715268, 4.0834317] | Test Loss: [2.1724768, 0.3301256, 4.0148277]\n",
      "220: Train Loss: [2.1793458, 0.3499208, 4.008771] | Test Loss: [2.3653872, 0.36294115, 4.367833]\n",
      "221: Train Loss: [2.0811508, 0.36040866, 3.801893] | Test Loss: [2.2489405, 0.39769143, 4.1001897]\n",
      "222: Train Loss: [2.1171205, 0.28651628, 3.9477248] | Test Loss: [2.2415426, 0.34338108, 4.139704]\n",
      "223: Train Loss: [2.2051642, 0.3453216, 4.0650067] | Test Loss: [2.122902, 0.2897, 3.956104]\n",
      "224: Train Loss: [2.0436244, 0.2758479, 3.8114011] | Test Loss: [2.3623862, 0.3449053, 4.379867]\n",
      "225: Train Loss: [2.1908288, 0.37358695, 4.0080705] | Test Loss: [2.1317255, 0.37126216, 3.892189]\n",
      "226: Train Loss: [2.115618, 0.26645732, 3.9647787] | Test Loss: [2.1886702, 0.2891965, 4.088144]\n",
      "227: Train Loss: [2.179172, 0.30300134, 4.0553427] | Test Loss: [2.1139956, 0.3235615, 3.9044294]\n",
      "228: Train Loss: [2.187034, 0.3188861, 4.0551815] | Test Loss: [2.269709, 0.365059, 4.1743593]\n",
      "229: Train Loss: [2.0930643, 0.408849, 3.7772794] | Test Loss: [2.3746634, 0.3504725, 4.3988543]\n",
      "230: Train Loss: [2.0893292, 0.27502644, 3.903632] | Test Loss: [2.2456408, 0.3195408, 4.1717405]\n",
      "231: Train Loss: [2.0736134, 0.31071174, 3.836515] | Test Loss: [2.1898985, 0.3063468, 4.07345]\n",
      "232: Train Loss: [2.259419, 0.3343289, 4.184509] | Test Loss: [2.2857773, 0.40589133, 4.1656632]\n",
      "233: Train Loss: [2.116118, 0.37458077, 3.8576553] | Test Loss: [2.2772946, 0.32098177, 4.2336073]\n",
      "234: Train Loss: [1.9506856, 0.3071634, 3.5942078] | Test Loss: [2.2245774, 0.3103831, 4.1387715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235: Train Loss: [2.240617, 0.3587879, 4.122446] | Test Loss: [2.1342804, 0.35021594, 3.918345]\n",
      "236: Train Loss: [2.173322, 0.3373726, 4.009271] | Test Loss: [2.2238464, 0.4049013, 4.0427914]\n",
      "237: Train Loss: [2.1096094, 0.38518175, 3.8340368] | Test Loss: [2.3583627, 0.29733965, 4.419386]\n",
      "238: Train Loss: [2.1119132, 0.35476905, 3.8690574] | Test Loss: [2.220284, 0.38992232, 4.050646]\n",
      "239: Train Loss: [2.1983125, 0.30458337, 4.0920415] | Test Loss: [2.2530687, 0.30675027, 4.199387]\n",
      "240: Train Loss: [2.1113377, 0.29978356, 3.9228916] | Test Loss: [2.1688397, 0.32237345, 4.015306]\n",
      "241: Train Loss: [2.2646546, 0.36677542, 4.1625338] | Test Loss: [2.224484, 0.33807507, 4.110893]\n",
      "242: Train Loss: [2.0984163, 0.3861726, 3.8106601] | Test Loss: [2.2056987, 0.33478206, 4.0766153]\n",
      "243: Train Loss: [2.1377294, 0.25788686, 4.017572] | Test Loss: [2.201402, 0.31748256, 4.0853214]\n",
      "244: Train Loss: [2.0465078, 0.31343764, 3.7795782] | Test Loss: [2.2646906, 0.36362013, 4.165761]\n",
      "245: Train Loss: [2.2655618, 0.3097491, 4.2213745] | Test Loss: [2.156259, 0.3966547, 3.9158635]\n",
      "246: Train Loss: [2.179132, 0.39088878, 3.9673753] | Test Loss: [2.2414112, 0.36610168, 4.1167207]\n",
      "247: Train Loss: [2.1032212, 0.35510737, 3.851335] | Test Loss: [2.2930892, 0.3300664, 4.256112]\n",
      "248: Train Loss: [2.2528255, 0.31408578, 4.191565] | Test Loss: [2.2379625, 0.30027658, 4.175648]\n",
      "249: Train Loss: [2.2238977, 0.3225688, 4.1252265] | Test Loss: [2.2768407, 0.33384645, 4.219835]\n",
      "250: Train Loss: [2.2310987, 0.29047322, 4.171724] | Test Loss: [2.2969944, 0.40904444, 4.1849446]\n",
      "251: Train Loss: [2.2034614, 0.3604933, 4.0464296] | Test Loss: [2.201598, 0.34462562, 4.0585704]\n",
      "252: Train Loss: [2.120876, 0.33070022, 3.911052] | Test Loss: [2.237056, 0.31251723, 4.161595]\n",
      "253: Train Loss: [2.1309836, 0.32320797, 3.9387593] | Test Loss: [2.2206721, 0.33597094, 4.1053734]\n",
      "254: Train Loss: [2.2634299, 0.32002795, 4.206832] | Test Loss: [2.2634346, 0.27403724, 4.252832]\n",
      "255: Train Loss: [2.2186627, 0.34751287, 4.0898128] | Test Loss: [2.1225576, 0.5310871, 3.7140281]\n",
      "256: Train Loss: [2.2474177, 0.37071696, 4.1241183] | Test Loss: [2.1928089, 0.3452279, 4.04039]\n",
      "257: Train Loss: [2.0630875, 0.3156975, 3.8104773] | Test Loss: [2.136477, 0.28969693, 3.9832573]\n",
      "258: Train Loss: [2.1455526, 0.30581823, 3.985287] | Test Loss: [2.2532265, 0.32970178, 4.176751]\n",
      "259: Train Loss: [2.018756, 0.3639359, 3.673576] | Test Loss: [2.1925864, 0.3224445, 4.0627284]\n",
      "260: Train Loss: [2.2715187, 0.3414939, 4.2015433] | Test Loss: [2.299473, 0.4564305, 4.1425157]\n",
      "261: Train Loss: [2.1773026, 0.34776735, 4.006838] | Test Loss: [2.177569, 0.37997773, 3.9751601]\n",
      "262: Train Loss: [2.0443468, 0.33015823, 3.7585356] | Test Loss: [2.1433349, 0.34416914, 3.9425006]\n",
      "263: Train Loss: [2.324839, 0.33439672, 4.3152814] | Test Loss: [2.2845168, 0.314671, 4.2543626]\n",
      "264: Train Loss: [2.032386, 0.3246155, 3.7401564] | Test Loss: [2.3013735, 0.33206952, 4.2706776]\n",
      "265: Train Loss: [2.2318277, 0.30330455, 4.160351] | Test Loss: [2.2726848, 0.3076739, 4.2376957]\n",
      "266: Train Loss: [1.9243463, 0.29094508, 3.5577476] | Test Loss: [2.2671075, 0.3579472, 4.1762676]\n",
      "267: Train Loss: [2.2924232, 0.29891288, 4.2859335] | Test Loss: [2.1470547, 0.32923707, 3.9648721]\n",
      "268: Train Loss: [2.0770714, 0.32946506, 3.8246777] | Test Loss: [2.1072953, 0.30481032, 3.9097803]\n",
      "269: Train Loss: [2.2334976, 0.35056838, 4.116427] | Test Loss: [2.3628185, 0.35310516, 4.372532]\n",
      "270: Train Loss: [2.3340456, 0.36955822, 4.298533] | Test Loss: [2.26005, 0.29174346, 4.228357]\n",
      "271: Train Loss: [2.2819235, 0.36261863, 4.2012286] | Test Loss: [2.3113148, 0.31102166, 4.311608]\n",
      "272: Train Loss: [2.1762257, 0.32654262, 4.0259085] | Test Loss: [2.2193303, 0.32354662, 4.115114]\n",
      "273: Train Loss: [2.0319204, 0.29883537, 3.7650054] | Test Loss: [2.132989, 0.34149522, 3.9244828]\n",
      "274: Train Loss: [2.2144089, 0.34643224, 4.0823855] | Test Loss: [2.3281538, 0.36422983, 4.292078]\n",
      "275: Train Loss: [2.0936303, 0.32706112, 3.8601995] | Test Loss: [2.233703, 0.36960718, 4.097799]\n",
      "276: Train Loss: [2.2045581, 0.34518084, 4.0639353] | Test Loss: [2.224681, 0.33431187, 4.11505]\n",
      "277: Train Loss: [2.1799622, 0.34531882, 4.0146055] | Test Loss: [2.2801242, 0.3660048, 4.1942434]\n",
      "278: Train Loss: [2.1207066, 0.3480609, 3.893352] | Test Loss: [2.2477176, 0.3526327, 4.1428027]\n",
      "279: Train Loss: [2.0355544, 0.27549314, 3.795616] | Test Loss: [2.209483, 0.3329588, 4.086007]\n",
      "280: Train Loss: [2.1330643, 0.3320502, 3.9340782] | Test Loss: [2.3492734, 0.32037294, 4.378174]\n",
      "281: Train Loss: [2.1293406, 0.32872537, 3.9299557] | Test Loss: [2.2395203, 0.357465, 4.121576]\n",
      "282: Train Loss: [1.9885799, 0.29905266, 3.678107] | Test Loss: [2.2241046, 0.31972373, 4.1284857]\n",
      "283: Train Loss: [2.157196, 0.3780142, 3.9363778] | Test Loss: [2.2758029, 0.41444305, 4.1371627]\n",
      "284: Train Loss: [2.0489044, 0.33742043, 3.7603886] | Test Loss: [2.1744769, 0.3351659, 4.0137877]\n",
      "285: Train Loss: [2.1715276, 0.33359724, 4.009458] | Test Loss: [2.0164542, 0.42422882, 3.6086795]\n",
      "286: Train Loss: [2.1829576, 0.40537003, 3.9605453] | Test Loss: [2.3005254, 0.34133548, 4.2597156]\n",
      "287: Train Loss: [2.241177, 0.33682653, 4.145528] | Test Loss: [2.2901938, 0.29894537, 4.281442]\n",
      "288: Train Loss: [2.1535583, 0.29180837, 4.0153084] | Test Loss: [2.2207792, 0.30250713, 4.1390514]\n",
      "289: Train Loss: [2.164177, 0.34034294, 3.9880111] | Test Loss: [2.0894477, 0.35171664, 3.827179]\n",
      "290: Train Loss: [2.2409236, 0.32454342, 4.157304] | Test Loss: [2.3669887, 0.37074438, 4.363233]\n",
      "291: Train Loss: [2.1913247, 0.32566705, 4.0569825] | Test Loss: [2.255128, 0.33296233, 4.1772933]\n",
      "292: Train Loss: [2.3505917, 0.3866902, 4.314493] | Test Loss: [2.2147117, 0.35865688, 4.0707664]\n",
      "293: Train Loss: [2.1197257, 0.35865933, 3.880792] | Test Loss: [2.229384, 0.35279784, 4.10597]\n",
      "294: Train Loss: [2.0372787, 0.33981088, 3.7347462] | Test Loss: [2.2089443, 0.3177569, 4.100132]\n",
      "295: Train Loss: [2.0924897, 0.28376392, 3.9012153] | Test Loss: [2.2095346, 0.37922275, 4.0398464]\n",
      "296: Train Loss: [2.225638, 0.2844745, 4.1668015] | Test Loss: [2.2573395, 0.31723532, 4.1974435]\n",
      "297: Train Loss: [2.103062, 0.34079102, 3.8653326] | Test Loss: [2.253314, 0.3293726, 4.1772556]\n",
      "298: Train Loss: [2.163102, 0.38497636, 3.9412277] | Test Loss: [2.1085932, 0.41306278, 3.8041236]\n",
      "299: Train Loss: [2.0728226, 0.31744358, 3.8282015] | Test Loss: [2.2652164, 0.37051487, 4.159918]\n",
      "300: Train Loss: [2.1327767, 0.31857744, 3.9469762] | Test Loss: [2.1517174, 0.3165433, 3.9868915]\n",
      "301: Train Loss: [2.232894, 0.35421157, 4.111576] | Test Loss: [2.2474256, 0.2952198, 4.199631]\n",
      "302: Train Loss: [2.1300924, 0.3608557, 3.8993292] | Test Loss: [2.253336, 0.3343581, 4.1723137]\n",
      "303: Train Loss: [2.2501836, 0.3819601, 4.1184072] | Test Loss: [2.2735045, 0.3469425, 4.2000666]\n",
      "304: Train Loss: [2.083198, 0.37349522, 3.792901] | Test Loss: [2.3730507, 0.32893074, 4.4171705]\n",
      "305: Train Loss: [2.1821256, 0.3392284, 4.025023] | Test Loss: [2.2812376, 0.3729302, 4.189545]\n",
      "306: Train Loss: [2.0985045, 0.33737278, 3.8596363] | Test Loss: [2.0511632, 0.37123385, 3.7310925]\n",
      "307: Train Loss: [2.111953, 0.32210574, 3.9018004] | Test Loss: [2.3303766, 0.34037706, 4.3203764]\n",
      "308: Train Loss: [2.0233188, 0.2995615, 3.747076] | Test Loss: [2.3046613, 0.4062979, 4.203025]\n",
      "309: Train Loss: [2.2819476, 0.31170404, 4.252191] | Test Loss: [2.1978016, 0.41809052, 3.9775126]\n",
      "310: Train Loss: [2.1290467, 0.35337794, 3.9047153] | Test Loss: [2.1935525, 0.31381106, 4.0732937]\n",
      "311: Train Loss: [2.3007271, 0.3478395, 4.253615] | Test Loss: [2.1570723, 0.3093272, 4.0048175]\n",
      "312: Train Loss: [2.1462073, 0.38524088, 3.9071739] | Test Loss: [2.4116423, 0.43019667, 4.393088]\n",
      "313: Train Loss: [2.1392465, 0.26752308, 4.0109696] | Test Loss: [2.3035445, 0.3266605, 4.2804284]\n",
      "314: Train Loss: [2.2133296, 0.3078846, 4.1187744] | Test Loss: [2.1325884, 0.36502367, 3.9001532]\n",
      "315: Train Loss: [2.233541, 0.3337064, 4.1333756] | Test Loss: [1.9705288, 0.29835153, 3.6427062]\n",
      "316: Train Loss: [2.141909, 0.24870352, 4.0351143] | Test Loss: [2.260153, 0.32877296, 4.191533]\n",
      "317: Train Loss: [2.2204015, 0.34993413, 4.090869] | Test Loss: [2.2111773, 0.33645844, 4.0858965]\n",
      "318: Train Loss: [2.0977724, 0.28283903, 3.9127057] | Test Loss: [2.128696, 0.32675242, 3.9306393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319: Train Loss: [2.1936336, 0.31530562, 4.0719614] | Test Loss: [2.2423613, 0.35882628, 4.1258965]\n",
      "320: Train Loss: [2.212452, 0.3399877, 4.084916] | Test Loss: [2.3449259, 0.31785962, 4.371992]\n",
      "321: Train Loss: [2.068736, 0.323907, 3.813565] | Test Loss: [2.2576346, 0.3480336, 4.167236]\n",
      "322: Train Loss: [2.1760907, 0.32246435, 4.029717] | Test Loss: [2.3208344, 0.3858021, 4.2558665]\n",
      "323: Train Loss: [2.1130738, 0.36258352, 3.8635643] | Test Loss: [2.1284761, 0.3242088, 3.9327433]\n",
      "324: Train Loss: [2.216496, 0.30414432, 4.1288476] | Test Loss: [2.215031, 0.35277072, 4.077291]\n",
      "325: Train Loss: [2.2491817, 0.34231257, 4.156051] | Test Loss: [2.2186913, 0.40581173, 4.031571]\n",
      "326: Train Loss: [2.2569373, 0.35391062, 4.159964] | Test Loss: [2.2582672, 0.34009114, 4.176443]\n",
      "327: Train Loss: [2.1341503, 0.27886152, 3.989439] | Test Loss: [2.3546746, 0.32030228, 4.3890467]\n",
      "328: Train Loss: [2.152347, 0.3412652, 3.9634287] | Test Loss: [2.0331676, 0.2759158, 3.7904193]\n",
      "329: Train Loss: [2.2128615, 0.31862712, 4.107096] | Test Loss: [2.25998, 0.3319313, 4.188029]\n",
      "330: Train Loss: [2.177281, 0.26046285, 4.094099] | Test Loss: [2.2651, 0.39957199, 4.130628]\n",
      "331: Train Loss: [2.134171, 0.32904175, 3.9393003] | Test Loss: [2.0921795, 0.2680951, 3.9162638]\n",
      "332: Train Loss: [2.16915, 0.26258856, 4.0757117] | Test Loss: [2.3173633, 0.4319217, 4.202805]\n",
      "333: Train Loss: [2.0847702, 0.3261401, 3.8434002] | Test Loss: [2.2507977, 0.40443587, 4.09716]\n",
      "334: Train Loss: [2.1494524, 0.347647, 3.9512577] | Test Loss: [2.2210891, 0.32362527, 4.118553]\n",
      "335: Train Loss: [2.2648535, 0.36762595, 4.162081] | Test Loss: [2.3201108, 0.409529, 4.2306924]\n",
      "336: Train Loss: [2.0796034, 0.2999973, 3.8592095] | Test Loss: [2.2269218, 0.3200732, 4.1337705]\n",
      "337: Train Loss: [2.1659932, 0.34494427, 3.9870422] | Test Loss: [2.2200658, 0.312263, 4.1278687]\n",
      "338: Train Loss: [2.2241476, 0.2901035, 4.1581917] | Test Loss: [2.1911154, 0.3421363, 4.0400944]\n",
      "339: Train Loss: [2.0659761, 0.28217584, 3.8497765] | Test Loss: [2.2637649, 0.32239303, 4.205137]\n",
      "340: Train Loss: [2.0846405, 0.2807253, 3.8885558] | Test Loss: [2.278279, 0.30330446, 4.2532535]\n",
      "341: Train Loss: [2.1821146, 0.3607471, 4.003482] | Test Loss: [2.3174772, 0.36333725, 4.2716174]\n",
      "342: Train Loss: [2.1626742, 0.3179657, 4.007383] | Test Loss: [2.1803818, 0.3233881, 4.0373755]\n",
      "343: Train Loss: [2.2547624, 0.30487266, 4.2046523] | Test Loss: [2.176852, 0.34978235, 4.0039215]\n",
      "344: Train Loss: [2.1960287, 0.3050414, 4.087016] | Test Loss: [2.2002828, 0.37141603, 4.0291495]\n",
      "345: Train Loss: [2.226249, 0.4585588, 3.9939392] | Test Loss: [2.2057865, 0.32788908, 4.083684]\n",
      "346: Train Loss: [2.0396712, 0.36166075, 3.7176816] | Test Loss: [2.2710023, 0.4511169, 4.0908875]\n",
      "347: Train Loss: [2.1683757, 0.33678374, 3.9999678] | Test Loss: [2.2844672, 0.3191591, 4.2497754]\n",
      "348: Train Loss: [2.0932152, 0.35837847, 3.8280518] | Test Loss: [2.164037, 0.31185123, 4.016223]\n",
      "349: Train Loss: [2.087156, 0.32062662, 3.8536856] | Test Loss: [2.1961572, 0.32846773, 4.0638466]\n",
      "350: Train Loss: [2.129545, 0.34111735, 3.9179728] | Test Loss: [2.2548199, 0.42759687, 4.0820427]\n",
      "351: Train Loss: [2.268038, 0.31759626, 4.2184796] | Test Loss: [2.2211936, 0.35589018, 4.086497]\n",
      "352: Train Loss: [2.2609663, 0.3246841, 4.1972485] | Test Loss: [2.2962003, 0.3459419, 4.2464585]\n",
      "353: Train Loss: [2.1236749, 0.30846044, 3.9388893] | Test Loss: [2.242808, 0.28160927, 4.204007]\n",
      "354: Train Loss: [2.124112, 0.33499375, 3.9132302] | Test Loss: [2.1859083, 0.24829867, 4.123518]\n",
      "355: Train Loss: [2.0753713, 0.28830966, 3.8624327] | Test Loss: [2.2395785, 0.35416615, 4.124991]\n",
      "356: Train Loss: [2.2036698, 0.37330246, 4.034037] | Test Loss: [2.2356408, 0.31461576, 4.156666]\n",
      "357: Train Loss: [2.2262118, 0.29947126, 4.152952] | Test Loss: [2.314271, 0.36589032, 4.2626514]\n",
      "358: Train Loss: [2.257893, 0.38484824, 4.130938] | Test Loss: [2.136751, 0.30043977, 3.973062]\n",
      "359: Train Loss: [2.2913973, 0.31608173, 4.266713] | Test Loss: [2.1666467, 0.3318838, 4.0014095]\n",
      "360: Train Loss: [2.07745, 0.36945835, 3.7854416] | Test Loss: [2.1392717, 0.3688891, 3.9096544]\n",
      "361: Train Loss: [2.24563, 0.34807384, 4.143186] | Test Loss: [2.37702, 0.34522843, 4.408811]\n",
      "362: Train Loss: [2.07901, 0.39404437, 3.7639759] | Test Loss: [2.2813964, 0.31216878, 4.250624]\n",
      "363: Train Loss: [2.1657975, 0.350318, 3.9812768] | Test Loss: [2.3133483, 0.34506315, 4.2816334]\n",
      "364: Train Loss: [2.156458, 0.37258822, 3.9403274] | Test Loss: [2.1865458, 0.3575907, 4.015501]\n",
      "365: Train Loss: [2.1243634, 0.29266664, 3.9560604] | Test Loss: [2.2228866, 0.32961273, 4.1161604]\n",
      "366: Train Loss: [2.1043167, 0.36889708, 3.8397365] | Test Loss: [2.3556054, 0.40667152, 4.304539]\n",
      "367: Train Loss: [2.1193337, 0.35847634, 3.8801913] | Test Loss: [2.1784916, 0.35007533, 4.006908]\n",
      "368: Train Loss: [2.1702247, 0.32037455, 4.020075] | Test Loss: [2.0290613, 0.3461754, 3.7119472]\n",
      "369: Train Loss: [2.1509068, 0.3032936, 3.9985201] | Test Loss: [2.1987822, 0.35264334, 4.044921]\n",
      "370: Train Loss: [2.0127687, 0.2910447, 3.7344928] | Test Loss: [2.220599, 0.34525552, 4.0959425]\n",
      "371: Train Loss: [2.2050784, 0.33181533, 4.0783415] | Test Loss: [2.0421882, 0.32966995, 3.7547064]\n",
      "372: Train Loss: [2.0416505, 0.36561713, 3.7176838] | Test Loss: [2.1531608, 0.36205798, 3.9442635]\n",
      "373: Train Loss: [2.2290225, 0.30777964, 4.150265] | Test Loss: [2.3551016, 0.3798986, 4.3303046]\n",
      "374: Train Loss: [2.1298838, 0.36730978, 3.892458] | Test Loss: [2.220161, 0.31099495, 4.129327]\n",
      "375: Train Loss: [2.3182685, 0.3275778, 4.3089595] | Test Loss: [2.2475643, 0.2508111, 4.2443175]\n",
      "376: Train Loss: [2.1626852, 0.32968047, 3.9956896] | Test Loss: [2.2987268, 0.41899747, 4.1784563]\n",
      "377: Train Loss: [2.0524328, 0.30513808, 3.7997274] | Test Loss: [2.3252926, 0.42530164, 4.2252836]\n",
      "378: Train Loss: [2.1470153, 0.41304246, 3.8809881] | Test Loss: [2.271491, 0.29672125, 4.2462606]\n",
      "379: Train Loss: [2.196145, 0.33085927, 4.061431] | Test Loss: [2.2449918, 0.3592763, 4.1307073]\n",
      "380: Train Loss: [2.1691272, 0.28303015, 4.0552244] | Test Loss: [2.1701453, 0.30754712, 4.0327435]\n",
      "381: Train Loss: [2.0561872, 0.32758737, 3.7847867] | Test Loss: [2.2098436, 0.37355945, 4.046128]\n",
      "382: Train Loss: [2.350298, 0.4561167, 4.244479] | Test Loss: [2.3011765, 0.30631733, 4.296036]\n",
      "383: Train Loss: [2.1948175, 0.3231698, 4.0664654] | Test Loss: [2.1411726, 0.33641097, 3.9459345]\n",
      "384: Train Loss: [2.2194936, 0.3679466, 4.0710406] | Test Loss: [2.2319274, 0.3526705, 4.111184]\n",
      "385: Train Loss: [2.0622175, 0.3831292, 3.7413058] | Test Loss: [2.2340698, 0.4295571, 4.0385823]\n",
      "386: Train Loss: [2.1150532, 0.30379176, 3.9263144] | Test Loss: [2.2076917, 0.29662973, 4.1187534]\n",
      "387: Train Loss: [2.2353745, 0.3365776, 4.1341715] | Test Loss: [2.1182013, 0.4115436, 3.824859]\n",
      "388: Train Loss: [2.2915163, 0.38180906, 4.2012234] | Test Loss: [2.2393858, 0.350944, 4.1278276]\n",
      "389: Train Loss: [1.9932986, 0.35959214, 3.627005] | Test Loss: [2.2038214, 0.31668648, 4.090956]\n",
      "390: Train Loss: [2.0479624, 0.29438588, 3.801539] | Test Loss: [2.1139388, 0.30638856, 3.921489]\n",
      "391: Train Loss: [2.2959065, 0.36652586, 4.2252874] | Test Loss: [2.18342, 0.30110535, 4.0657344]\n",
      "392: Train Loss: [2.0293362, 0.2739197, 3.7847526] | Test Loss: [2.1229792, 0.31890023, 3.927058]\n",
      "393: Train Loss: [2.0469275, 0.3129261, 3.7809289] | Test Loss: [2.0523806, 0.30391178, 3.8008494]\n",
      "394: Train Loss: [1.9957094, 0.30675742, 3.6846614] | Test Loss: [2.1958323, 0.28122234, 4.110442]\n",
      "395: Train Loss: [2.157916, 0.31714883, 3.9986832] | Test Loss: [2.3622165, 0.36301708, 4.361416]\n",
      "396: Train Loss: [2.1918464, 0.36009228, 4.0236006] | Test Loss: [2.2547843, 0.34501588, 4.1645527]\n",
      "397: Train Loss: [2.268794, 0.36896876, 4.168619] | Test Loss: [2.213452, 0.36952695, 4.0573773]\n",
      "398: Train Loss: [2.2031293, 0.3197262, 4.0865326] | Test Loss: [2.118041, 0.36057433, 3.8755078]\n",
      "399: Train Loss: [2.0727832, 0.26944125, 3.8761253] | Test Loss: [2.3214262, 0.380643, 4.2622094]\n",
      "400: Train Loss: [2.128756, 0.33745003, 3.9200618] | Test Loss: [2.3730567, 0.31326336, 4.43285]\n",
      "401: Train Loss: [2.146902, 0.32006505, 3.973739] | Test Loss: [2.3656511, 0.3331048, 4.3981977]\n",
      "402: Train Loss: [2.2571595, 0.34805357, 4.1662655] | Test Loss: [2.1394691, 0.36257976, 3.9163585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403: Train Loss: [2.1603587, 0.35181007, 3.9689074] | Test Loss: [2.3033252, 0.33421186, 4.2724385]\n",
      "404: Train Loss: [2.0441234, 0.37173793, 3.7165089] | Test Loss: [2.2144232, 0.32511196, 4.1037345]\n",
      "405: Train Loss: [2.0977266, 0.33651048, 3.8589427] | Test Loss: [2.403656, 0.28641126, 4.5209007]\n",
      "406: Train Loss: [2.047799, 0.32615438, 3.769444] | Test Loss: [2.3631053, 0.3081938, 4.418017]\n",
      "407: Train Loss: [2.280167, 0.35538006, 4.204954] | Test Loss: [2.2406507, 0.30920815, 4.172093]\n",
      "408: Train Loss: [2.2293563, 0.36487994, 4.0938325] | Test Loss: [2.2329335, 0.40391898, 4.061948]\n",
      "409: Train Loss: [2.222313, 0.34834027, 4.096286] | Test Loss: [2.1154695, 0.311778, 3.919161]\n",
      "410: Train Loss: [2.1447453, 0.28649533, 4.0029955] | Test Loss: [2.129542, 0.38808125, 3.871003]\n",
      "411: Train Loss: [2.0172887, 0.34496084, 3.6896164] | Test Loss: [2.1204624, 0.3115851, 3.92934]\n",
      "412: Train Loss: [2.1452022, 0.28986597, 4.0005383] | Test Loss: [2.2023373, 0.33610386, 4.0685706]\n",
      "413: Train Loss: [2.1465268, 0.34373078, 3.949323] | Test Loss: [2.2401464, 0.3082469, 4.1720457]\n",
      "414: Train Loss: [2.0014215, 0.29524168, 3.7076013] | Test Loss: [2.4585872, 0.40887433, 4.5083]\n",
      "415: Train Loss: [2.2693634, 0.3086535, 4.2300735] | Test Loss: [2.2493415, 0.3475331, 4.1511497]\n",
      "416: Train Loss: [2.1003585, 0.3562681, 3.8444488] | Test Loss: [2.2874093, 0.323272, 4.2515464]\n",
      "417: Train Loss: [2.1485305, 0.3347664, 3.9622948] | Test Loss: [2.1746798, 0.3118731, 4.0374866]\n",
      "418: Train Loss: [2.1784124, 0.37260973, 3.9842153] | Test Loss: [2.1368537, 0.34261537, 3.931092]\n",
      "419: Train Loss: [2.1721675, 0.31912038, 4.0252147] | Test Loss: [2.2905512, 0.3461049, 4.2349973]\n",
      "420: Train Loss: [2.1008966, 0.30532268, 3.8964705] | Test Loss: [2.2274604, 0.3890978, 4.065823]\n",
      "421: Train Loss: [2.120052, 0.3523422, 3.887762] | Test Loss: [2.36503, 0.3058316, 4.4242287]\n",
      "422: Train Loss: [2.0849235, 0.32298964, 3.8468575] | Test Loss: [2.3094072, 0.34305462, 4.2757597]\n",
      "423: Train Loss: [2.2115223, 0.27490088, 4.148144] | Test Loss: [2.114345, 0.36601242, 3.8626778]\n",
      "424: Train Loss: [2.1585252, 0.40121093, 3.9158394] | Test Loss: [2.0547533, 0.38808632, 3.7214205]\n",
      "425: Train Loss: [2.1347096, 0.29973772, 3.9696813] | Test Loss: [2.2502632, 0.32658166, 4.173945]\n",
      "426: Train Loss: [2.233084, 0.29002994, 4.176138] | Test Loss: [2.283109, 0.31534705, 4.2508707]\n",
      "427: Train Loss: [2.1808069, 0.28807732, 4.0735364] | Test Loss: [2.1286225, 0.35694683, 3.900298]\n",
      "428: Train Loss: [2.2547283, 0.32089812, 4.1885586] | Test Loss: [2.207088, 0.397102, 4.017074]\n",
      "429: Train Loss: [2.227375, 0.3302057, 4.124544] | Test Loss: [2.11068, 0.32583356, 3.895527]\n",
      "430: Train Loss: [2.1940506, 0.32966852, 4.0584326] | Test Loss: [2.223218, 0.31429723, 4.1321387]\n",
      "431: Train Loss: [2.2335443, 0.33790088, 4.1291876] | Test Loss: [2.3203173, 0.32975826, 4.3108764]\n",
      "432: Train Loss: [2.0823386, 0.30769587, 3.8569813] | Test Loss: [2.2892985, 0.36716327, 4.211434]\n",
      "433: Train Loss: [2.0884416, 0.28897196, 3.8879113] | Test Loss: [2.3827448, 0.38525406, 4.3802357]\n",
      "434: Train Loss: [2.094421, 0.31709647, 3.871745] | Test Loss: [2.2059207, 0.413445, 3.9983962]\n",
      "435: Train Loss: [2.2114315, 0.36603805, 4.056825] | Test Loss: [2.1338294, 0.27270797, 3.9949505]\n",
      "436: Train Loss: [2.125717, 0.31211108, 3.939323] | Test Loss: [2.4161816, 0.40495053, 4.4274125]\n",
      "437: Train Loss: [2.2542017, 0.32091808, 4.187485] | Test Loss: [2.208527, 0.34580427, 4.07125]\n",
      "438: Train Loss: [2.1040435, 0.3099508, 3.8981364] | Test Loss: [2.0180407, 0.36249065, 3.6735907]\n",
      "439: Train Loss: [2.200017, 0.36417377, 4.03586] | Test Loss: [2.3161385, 0.31194237, 4.3203344]\n",
      "440: Train Loss: [2.2648156, 0.3564839, 4.173147] | Test Loss: [2.2326908, 0.32968363, 4.135698]\n",
      "441: Train Loss: [2.1461413, 0.32350606, 3.9687767] | Test Loss: [2.2707677, 0.39646664, 4.1450686]\n",
      "442: Train Loss: [2.070175, 0.35253063, 3.7878194] | Test Loss: [2.2665215, 0.37991598, 4.1531267]\n",
      "443: Train Loss: [2.2647767, 0.37329057, 4.156263] | Test Loss: [2.2872975, 0.36318585, 4.211409]\n",
      "444: Train Loss: [1.926884, 0.3228734, 3.5308948] | Test Loss: [2.2066653, 0.37749043, 4.03584]\n",
      "445: Train Loss: [2.1080837, 0.35929096, 3.8568764] | Test Loss: [2.1846519, 0.36524233, 4.004061]\n",
      "446: Train Loss: [2.2049487, 0.35125196, 4.0586452] | Test Loss: [2.2949739, 0.30215526, 4.2877927]\n",
      "447: Train Loss: [2.0988998, 0.32592684, 3.871873] | Test Loss: [2.2592897, 0.29165483, 4.226925]\n",
      "448: Train Loss: [2.1488998, 0.31152686, 3.9862726] | Test Loss: [2.2177022, 0.33561084, 4.0997934]\n",
      "449: Train Loss: [2.2122085, 0.37965494, 4.044762] | Test Loss: [2.1641448, 0.35524553, 3.9730442]\n",
      "450: Train Loss: [2.1068165, 0.32680094, 3.886832] | Test Loss: [2.1890922, 0.35334164, 4.0248427]\n",
      "451: Train Loss: [2.0990534, 0.31367436, 3.8844323] | Test Loss: [2.09229, 0.40375692, 3.7808228]\n",
      "452: Train Loss: [2.2197726, 0.40163234, 4.037913] | Test Loss: [2.318698, 0.32706216, 4.3103337]\n",
      "453: Train Loss: [2.1971116, 0.36462542, 4.0295978] | Test Loss: [2.1042264, 0.40638408, 3.8020685]\n",
      "454: Train Loss: [2.0563114, 0.32969955, 3.782923] | Test Loss: [2.2561219, 0.3178954, 4.1943483]\n",
      "455: Train Loss: [2.2378793, 0.33266076, 4.143098] | Test Loss: [2.111984, 0.30770388, 3.916264]\n",
      "456: Train Loss: [2.085932, 0.321616, 3.850248] | Test Loss: [2.2543428, 0.28296578, 4.22572]\n",
      "457: Train Loss: [2.352247, 0.44357437, 4.2609196] | Test Loss: [2.2080882, 0.3546794, 4.0614967]\n",
      "458: Train Loss: [2.2107766, 0.344475, 4.0770783] | Test Loss: [2.1385846, 0.2819906, 3.9951785]\n",
      "459: Train Loss: [2.3419433, 0.38770202, 4.2961845] | Test Loss: [2.3148024, 0.3663754, 4.2632294]\n",
      "460: Train Loss: [2.27576, 0.36903083, 4.182489] | Test Loss: [2.3731935, 0.38576582, 4.360621]\n",
      "461: Train Loss: [2.2417967, 0.33718336, 4.14641] | Test Loss: [2.3437514, 0.3970041, 4.2904987]\n",
      "462: Train Loss: [2.2602303, 0.39908332, 4.1213775] | Test Loss: [2.1996539, 0.29643103, 4.1028767]\n",
      "463: Train Loss: [2.2141411, 0.40601927, 4.022263] | Test Loss: [2.312922, 0.37628716, 4.249557]\n",
      "464: Train Loss: [2.168447, 0.3545472, 3.982347] | Test Loss: [2.1339023, 0.33180696, 3.9359975]\n",
      "465: Train Loss: [2.280054, 0.37524062, 4.1848674] | Test Loss: [2.2317417, 0.3627562, 4.100727]\n",
      "466: Train Loss: [2.2535324, 0.28716654, 4.219898] | Test Loss: [2.3564038, 0.3679742, 4.3448334]\n",
      "467: Train Loss: [2.2450109, 0.3685007, 4.121521] | Test Loss: [2.1060805, 0.37193012, 3.840231]\n",
      "468: Train Loss: [2.0078602, 0.29608047, 3.71964] | Test Loss: [2.358766, 0.28287673, 4.4346557]\n",
      "469: Train Loss: [2.1377633, 0.34194458, 3.9335818] | Test Loss: [2.0053883, 0.3542901, 3.6564865]\n",
      "470: Train Loss: [2.0784016, 0.27949616, 3.8773072] | Test Loss: [2.3714728, 0.4297362, 4.3132095]\n",
      "471: Train Loss: [2.066544, 0.2526073, 3.8804808] | Test Loss: [2.2828104, 0.31731087, 4.24831]\n",
      "472: Train Loss: [2.2053633, 0.27543736, 4.135289] | Test Loss: [2.353826, 0.32120496, 4.386447]\n",
      "473: Train Loss: [2.1692564, 0.3335055, 4.0050073] | Test Loss: [2.2958264, 0.3582162, 4.2334366]\n",
      "474: Train Loss: [2.0302832, 0.28703463, 3.773532] | Test Loss: [2.2432947, 0.355952, 4.1306376]\n",
      "475: Train Loss: [2.1440802, 0.3390341, 3.949126] | Test Loss: [2.2505407, 0.3064383, 4.194643]\n",
      "476: Train Loss: [2.1654215, 0.30309284, 4.02775] | Test Loss: [2.106771, 0.2977294, 3.9158127]\n",
      "477: Train Loss: [2.1554122, 0.2964327, 4.014392] | Test Loss: [2.079279, 0.33683798, 3.82172]\n",
      "478: Train Loss: [2.221891, 0.40451023, 4.039272] | Test Loss: [2.318831, 0.3298118, 4.30785]\n",
      "479: Train Loss: [2.143453, 0.3270925, 3.959813] | Test Loss: [2.244786, 0.36135286, 4.128219]\n",
      "480: Train Loss: [2.1867647, 0.28231126, 4.091218] | Test Loss: [2.288951, 0.33907276, 4.238829]\n",
      "481: Train Loss: [2.13404, 0.31993333, 3.9481468] | Test Loss: [2.0692089, 0.30620965, 3.8322082]\n",
      "482: Train Loss: [2.2366228, 0.328645, 4.144601] | Test Loss: [2.1899629, 0.31784654, 4.0620794]\n",
      "483: Train Loss: [2.0857327, 0.3413729, 3.8300927] | Test Loss: [2.2783682, 0.31901124, 4.2377253]\n",
      "484: Train Loss: [1.9243115, 0.35216376, 3.4964592] | Test Loss: [2.3019052, 0.44906268, 4.1547475]\n",
      "485: Train Loss: [2.217311, 0.3566209, 4.078001] | Test Loss: [2.2032948, 0.31954837, 4.087041]\n",
      "486: Train Loss: [2.1883557, 0.3969061, 3.9798052] | Test Loss: [2.285809, 0.47821257, 4.0934057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487: Train Loss: [2.2450178, 0.36724913, 4.1227865] | Test Loss: [2.1614082, 0.3647803, 3.9580362]\n",
      "488: Train Loss: [2.1536975, 0.3207285, 3.9866664] | Test Loss: [2.1428912, 0.32063907, 3.9651432]\n",
      "489: Train Loss: [2.1926801, 0.37813932, 4.0072207] | Test Loss: [2.2900522, 0.35930258, 4.220802]\n",
      "490: Train Loss: [2.0871537, 0.35090148, 3.8234057] | Test Loss: [2.1906462, 0.38952637, 3.9917662]\n",
      "491: Train Loss: [2.2328012, 0.3557877, 4.1098146] | Test Loss: [2.0752068, 0.34977078, 3.800643]\n",
      "492: Train Loss: [2.186262, 0.41551903, 3.9570048] | Test Loss: [2.4193673, 0.34648603, 4.4922485]\n",
      "493: Train Loss: [2.172015, 0.33151156, 4.0125184] | Test Loss: [2.1441977, 0.32233176, 3.9660637]\n",
      "494: Train Loss: [2.1647851, 0.29991323, 4.029657] | Test Loss: [2.118313, 0.38751373, 3.8491125]\n",
      "495: Train Loss: [2.1114938, 0.27012125, 3.9528663] | Test Loss: [2.187499, 0.3753004, 3.9996977]\n",
      "496: Train Loss: [2.2552052, 0.26401144, 4.246399] | Test Loss: [2.272588, 0.3343964, 4.2107797]\n",
      "497: Train Loss: [2.2883344, 0.47231174, 4.1043572] | Test Loss: [2.2190282, 0.35988143, 4.078175]\n",
      "498: Train Loss: [2.0036137, 0.30076745, 3.70646] | Test Loss: [2.1238396, 0.30921623, 3.938463]\n",
      "499: Train Loss: [2.1791046, 0.4159084, 3.9423006] | Test Loss: [2.077941, 0.38162124, 3.7742608]\n",
      "500: Train Loss: [2.2002592, 0.323501, 4.0770173] | Test Loss: [2.1869078, 0.40828207, 3.9655335]\n",
      "501: Train Loss: [2.2628574, 0.41490293, 4.1108117] | Test Loss: [2.1247752, 0.3123787, 3.9371715]\n",
      "502: Train Loss: [2.2398353, 0.3671551, 4.1125154] | Test Loss: [2.233409, 0.3605291, 4.106289]\n",
      "503: Train Loss: [2.166049, 0.33145767, 4.0006404] | Test Loss: [2.1513097, 0.34598637, 3.956633]\n",
      "504: Train Loss: [2.2454836, 0.3410913, 4.149876] | Test Loss: [2.2907493, 0.34052274, 4.240976]\n",
      "505: Train Loss: [2.229611, 0.3762467, 4.082975] | Test Loss: [2.2790475, 0.36753115, 4.1905637]\n",
      "506: Train Loss: [2.3574936, 0.34671158, 4.3682756] | Test Loss: [2.2528703, 0.36187503, 4.1438656]\n",
      "507: Train Loss: [2.2095633, 0.3592612, 4.0598655] | Test Loss: [2.4185247, 0.37253693, 4.4645123]\n",
      "508: Train Loss: [2.163319, 0.38550162, 3.9411364] | Test Loss: [2.2520168, 0.348758, 4.1552753]\n",
      "509: Train Loss: [2.1137607, 0.34516248, 3.882359] | Test Loss: [2.3383331, 0.28144363, 4.3952227]\n",
      "510: Train Loss: [2.0477777, 0.3674084, 3.728147] | Test Loss: [2.1024613, 0.3226091, 3.8823135]\n",
      "511: Train Loss: [2.1242101, 0.3104561, 3.9379642] | Test Loss: [2.35357, 0.3073491, 4.399791]\n",
      "512: Train Loss: [2.0460196, 0.3341376, 3.7579014] | Test Loss: [2.3813167, 0.33007738, 4.432556]\n",
      "513: Train Loss: [2.1644702, 0.33347344, 3.995467] | Test Loss: [2.3278782, 0.3812335, 4.274523]\n",
      "514: Train Loss: [2.2999995, 0.31715864, 4.2828403] | Test Loss: [2.2581656, 0.3351715, 4.1811595]\n",
      "515: Train Loss: [2.1724615, 0.3888414, 3.9560819] | Test Loss: [2.1687725, 0.31057084, 4.026974]\n",
      "516: Train Loss: [2.195995, 0.33640176, 4.0555882] | Test Loss: [2.1664934, 0.34042525, 3.9925616]\n",
      "517: Train Loss: [2.1430643, 0.3318754, 3.954253] | Test Loss: [2.2023098, 0.38675794, 4.017862]\n",
      "518: Train Loss: [2.062486, 0.32615718, 3.7988145] | Test Loss: [2.303125, 0.3795554, 4.2266946]\n",
      "519: Train Loss: [2.2847507, 0.3047166, 4.264785] | Test Loss: [2.1880765, 0.43412253, 3.9420304]\n",
      "520: Train Loss: [2.132012, 0.29595983, 3.9680638] | Test Loss: [2.1099715, 0.28921083, 3.9307323]\n",
      "521: Train Loss: [2.1915872, 0.36045408, 4.0227203] | Test Loss: [2.4272478, 0.43367478, 4.4208207]\n",
      "522: Train Loss: [2.3278577, 0.33650783, 4.3192077] | Test Loss: [2.1577082, 0.31908855, 3.9963279]\n",
      "523: Train Loss: [2.2306256, 0.42997643, 4.031275] | Test Loss: [2.3672593, 0.42247495, 4.3120437]\n",
      "Epoch 3\n",
      "0: Train Loss: [2.0589032, 0.34233898, 3.7754676] | Test Loss: [2.1861556, 0.35360318, 4.0187078]\n",
      "1: Train Loss: [2.0198815, 0.39802885, 3.6417341] | Test Loss: [2.3980143, 0.32131833, 4.4747105]\n",
      "2: Train Loss: [2.1345458, 0.37874058, 3.890351] | Test Loss: [2.2497792, 0.3188811, 4.1806774]\n",
      "3: Train Loss: [2.1422763, 0.35345075, 3.9311018] | Test Loss: [2.1368012, 0.3475148, 3.9260879]\n",
      "4: Train Loss: [1.8695245, 0.3700825, 3.3689663] | Test Loss: [2.1501133, 0.31659278, 3.9836338]\n",
      "5: Train Loss: [1.9555845, 0.31448206, 3.596687] | Test Loss: [2.299852, 0.34378412, 4.2559195]\n",
      "6: Train Loss: [2.0569859, 0.3242182, 3.7897534] | Test Loss: [2.3406234, 0.3381382, 4.3431087]\n",
      "7: Train Loss: [1.9282378, 0.36812592, 3.4883497] | Test Loss: [2.278659, 0.38691777, 4.1704006]\n",
      "8: Train Loss: [2.156207, 0.40459058, 3.9078236] | Test Loss: [2.3769324, 0.30405554, 4.449809]\n",
      "9: Train Loss: [1.986335, 0.32070205, 3.651968] | Test Loss: [2.1366935, 0.38783532, 3.8855515]\n",
      "10: Train Loss: [2.1562123, 0.41940895, 3.8930156] | Test Loss: [2.2421057, 0.32546407, 4.158747]\n",
      "11: Train Loss: [2.0183837, 0.25249675, 3.7842705] | Test Loss: [2.2211115, 0.32822278, 4.1140003]\n",
      "12: Train Loss: [1.8617256, 0.29176325, 3.4316878] | Test Loss: [2.3169851, 0.35945028, 4.27452]\n",
      "13: Train Loss: [1.9192735, 0.33472627, 3.5038207] | Test Loss: [2.277769, 0.32679576, 4.2287426]\n",
      "14: Train Loss: [1.9733315, 0.30208814, 3.6445746] | Test Loss: [2.3762217, 0.39539832, 4.357045]\n",
      "15: Train Loss: [2.0684185, 0.3547423, 3.7820945] | Test Loss: [2.1848722, 0.371847, 3.9978974]\n",
      "16: Train Loss: [2.0013154, 0.38222295, 3.6204076] | Test Loss: [2.1050146, 0.36928686, 3.8407423]\n",
      "17: Train Loss: [2.1062846, 0.35461816, 3.8579512] | Test Loss: [2.2981846, 0.34992218, 4.246447]\n",
      "18: Train Loss: [1.9556578, 0.3433712, 3.5679445] | Test Loss: [2.2820425, 0.3224572, 4.2416277]\n",
      "19: Train Loss: [2.2136173, 0.32682997, 4.1004047] | Test Loss: [2.165651, 0.29643407, 4.0348682]\n",
      "20: Train Loss: [2.0742824, 0.346351, 3.8022137] | Test Loss: [2.1519427, 0.3629722, 3.9409134]\n",
      "21: Train Loss: [2.0413368, 0.34785306, 3.7348206] | Test Loss: [2.2916067, 0.36806753, 4.2151456]\n",
      "22: Train Loss: [1.9290987, 0.3460427, 3.5121548] | Test Loss: [2.2180278, 0.36453104, 4.0715246]\n",
      "23: Train Loss: [1.9493976, 0.3232929, 3.5755022] | Test Loss: [2.2055125, 0.37307242, 4.0379524]\n",
      "24: Train Loss: [2.19059, 0.3558297, 4.02535] | Test Loss: [2.2916133, 0.32211456, 4.261112]\n",
      "25: Train Loss: [2.0340738, 0.44207904, 3.6260686] | Test Loss: [2.0085936, 0.3450597, 3.6721272]\n",
      "26: Train Loss: [2.160135, 0.5132999, 3.8069701] | Test Loss: [2.2333019, 0.33663237, 4.1299715]\n",
      "27: Train Loss: [2.108543, 0.3175804, 3.8995056] | Test Loss: [2.3099515, 0.3770362, 4.242867]\n",
      "28: Train Loss: [2.0655951, 0.34751487, 3.7836754] | Test Loss: [2.4007673, 0.39417493, 4.4073596]\n",
      "29: Train Loss: [2.0539055, 0.3521032, 3.7557077] | Test Loss: [2.244279, 0.3667966, 4.1217613]\n",
      "30: Train Loss: [2.0336406, 0.3246491, 3.7426322] | Test Loss: [2.2659445, 0.32183012, 4.2100587]\n",
      "31: Train Loss: [2.0491111, 0.3841015, 3.7141206] | Test Loss: [2.643299, 0.34710437, 4.9394937]\n",
      "32: Train Loss: [2.1341429, 0.35301185, 3.915274] | Test Loss: [2.185973, 0.3703636, 4.001582]\n",
      "33: Train Loss: [2.045079, 0.32354, 3.766618] | Test Loss: [2.2678957, 0.3817826, 4.154009]\n",
      "34: Train Loss: [2.1466746, 0.2842051, 4.0091443] | Test Loss: [2.2122366, 0.38823017, 4.036243]\n",
      "35: Train Loss: [2.0068574, 0.33856302, 3.6751516] | Test Loss: [2.277531, 0.33709028, 4.2179713]\n",
      "36: Train Loss: [2.0891008, 0.37120458, 3.806997] | Test Loss: [2.2115624, 0.333107, 4.090018]\n",
      "37: Train Loss: [2.160904, 0.36281332, 3.9589944] | Test Loss: [2.2441227, 0.3637995, 4.124446]\n",
      "38: Train Loss: [2.0528805, 0.35784703, 3.747914] | Test Loss: [2.0728168, 0.38225064, 3.7633832]\n",
      "39: Train Loss: [1.997431, 0.3388737, 3.6559885] | Test Loss: [2.4095712, 0.3690744, 4.450068]\n",
      "40: Train Loss: [1.9998554, 0.35159013, 3.6481206] | Test Loss: [2.2300837, 0.3203984, 4.139769]\n",
      "41: Train Loss: [2.071366, 0.34861583, 3.7941165] | Test Loss: [2.0903938, 0.352332, 3.8284557]\n",
      "42: Train Loss: [1.8794802, 0.33790383, 3.4210567] | Test Loss: [2.4486518, 0.32602897, 4.5712748]\n",
      "43: Train Loss: [2.039088, 0.31734952, 3.7608263] | Test Loss: [2.1427157, 0.40196863, 3.8834627]\n",
      "44: Train Loss: [2.1173491, 0.3163432, 3.9183552] | Test Loss: [2.2606552, 0.37468395, 4.1466265]\n",
      "45: Train Loss: [2.1182244, 0.31209084, 3.9243581] | Test Loss: [2.28073, 0.47507238, 4.0863876]\n",
      "46: Train Loss: [1.9257938, 0.3227177, 3.5288699] | Test Loss: [2.4245195, 0.41529128, 4.433748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47: Train Loss: [2.06464, 0.30505258, 3.8242273] | Test Loss: [2.1625593, 0.31274697, 4.0123715]\n",
      "48: Train Loss: [2.0954647, 0.29545125, 3.8954782] | Test Loss: [2.284873, 0.27620223, 4.293544]\n",
      "49: Train Loss: [2.0242348, 0.4898557, 3.558614] | Test Loss: [2.2350705, 0.28371814, 4.186423]\n",
      "50: Train Loss: [2.0891185, 0.34248003, 3.8357568] | Test Loss: [2.3727372, 0.34844863, 4.3970256]\n",
      "51: Train Loss: [1.964387, 0.31399015, 3.614784] | Test Loss: [2.184859, 0.35422972, 4.015488]\n",
      "52: Train Loss: [1.9172821, 0.3099702, 3.524594] | Test Loss: [2.4103975, 0.42190745, 4.3988876]\n",
      "53: Train Loss: [2.0255678, 0.32416677, 3.7269688] | Test Loss: [2.2958887, 0.33396095, 4.2578163]\n",
      "54: Train Loss: [1.9543227, 0.34742856, 3.5612168] | Test Loss: [2.2812176, 0.3380665, 4.2243686]\n",
      "55: Train Loss: [2.0611637, 0.34613323, 3.776194] | Test Loss: [2.310112, 0.3201691, 4.300055]\n",
      "56: Train Loss: [2.10536, 0.41999054, 3.7907293] | Test Loss: [2.4896321, 0.37131143, 4.607953]\n",
      "57: Train Loss: [2.1949153, 0.3375807, 4.05225] | Test Loss: [2.225052, 0.31267452, 4.1374297]\n",
      "58: Train Loss: [2.171595, 0.39591798, 3.9472723] | Test Loss: [2.2203848, 0.32600003, 4.1147695]\n",
      "59: Train Loss: [2.0517771, 0.32969153, 3.7738628] | Test Loss: [2.2209651, 0.3179889, 4.1239414]\n",
      "60: Train Loss: [1.9157889, 0.32735297, 3.5042248] | Test Loss: [2.197257, 0.36593908, 4.028575]\n",
      "61: Train Loss: [2.1532779, 0.3257069, 3.9808488] | Test Loss: [2.230757, 0.33283204, 4.128682]\n",
      "62: Train Loss: [1.9885024, 0.3685244, 3.6084805] | Test Loss: [2.1574817, 0.30654895, 4.0084143]\n",
      "63: Train Loss: [1.9678087, 0.26495653, 3.670661] | Test Loss: [2.2782104, 0.43218774, 4.1242332]\n",
      "64: Train Loss: [1.963628, 0.35790336, 3.5693526] | Test Loss: [2.2746425, 0.3598749, 4.18941]\n",
      "65: Train Loss: [2.0346308, 0.44853318, 3.6207283] | Test Loss: [2.3157566, 0.3049239, 4.326589]\n",
      "66: Train Loss: [2.156312, 0.32495338, 3.9876707] | Test Loss: [2.4036098, 0.39114624, 4.4160733]\n",
      "67: Train Loss: [2.0436456, 0.31399852, 3.7732925] | Test Loss: [2.309049, 0.35312176, 4.264976]\n",
      "68: Train Loss: [1.9720798, 0.3193669, 3.6247926] | Test Loss: [2.27503, 0.37667227, 4.1733875]\n",
      "69: Train Loss: [2.1198707, 0.34213838, 3.8976028] | Test Loss: [2.3425353, 0.34319645, 4.341874]\n",
      "70: Train Loss: [2.0920386, 0.39398873, 3.7900887] | Test Loss: [2.114771, 0.38737497, 3.842167]\n",
      "71: Train Loss: [2.044799, 0.31698576, 3.7726126] | Test Loss: [2.2107828, 0.35405505, 4.0675106]\n",
      "72: Train Loss: [2.1671805, 0.40064812, 3.9337132] | Test Loss: [2.2210164, 0.3153501, 4.1266828]\n",
      "73: Train Loss: [2.0523863, 0.3378508, 3.7669218] | Test Loss: [2.322612, 0.3544843, 4.29074]\n",
      "74: Train Loss: [2.247557, 0.33800933, 4.1571045] | Test Loss: [2.2314475, 0.3171527, 4.1457424]\n",
      "75: Train Loss: [2.1236267, 0.32244667, 3.9248068] | Test Loss: [2.2117455, 0.32689804, 4.096593]\n",
      "76: Train Loss: [2.01808, 0.29785687, 3.738303] | Test Loss: [2.2776203, 0.37921974, 4.176021]\n",
      "77: Train Loss: [2.0395212, 0.35496002, 3.7240825] | Test Loss: [2.2910361, 0.3917575, 4.190315]\n",
      "78: Train Loss: [2.1184163, 0.34534654, 3.891486] | Test Loss: [2.2597733, 0.32143816, 4.198108]\n",
      "79: Train Loss: [2.1955156, 0.452109, 3.9389224] | Test Loss: [2.5048618, 0.3884166, 4.621307]\n",
      "80: Train Loss: [2.088422, 0.37907258, 3.7977717] | Test Loss: [2.5018806, 0.32543394, 4.6783276]\n",
      "81: Train Loss: [2.130098, 0.4077902, 3.8524058] | Test Loss: [2.2969882, 0.36667114, 4.2273054]\n",
      "82: Train Loss: [2.1086352, 0.34698993, 3.8702803] | Test Loss: [2.1920848, 0.37652612, 4.0076437]\n",
      "83: Train Loss: [2.0699062, 0.37023377, 3.7695785] | Test Loss: [2.2248673, 0.32452404, 4.125211]\n",
      "84: Train Loss: [2.189381, 0.36744845, 4.0113134] | Test Loss: [2.1840227, 0.35043705, 4.017608]\n",
      "85: Train Loss: [2.0351806, 0.3289492, 3.741412] | Test Loss: [2.261858, 0.36924666, 4.1544695]\n",
      "86: Train Loss: [2.1077178, 0.28435275, 3.931083] | Test Loss: [2.2515934, 0.44238958, 4.060797]\n",
      "87: Train Loss: [2.1361825, 0.3847529, 3.8876123] | Test Loss: [2.1870346, 0.33360296, 4.0404663]\n",
      "88: Train Loss: [1.9703177, 0.32431823, 3.6163173] | Test Loss: [2.3073413, 0.33312958, 4.2815533]\n",
      "89: Train Loss: [2.1969516, 0.36155456, 4.0323486] | Test Loss: [2.2768474, 0.36941162, 4.1842833]\n",
      "90: Train Loss: [2.1253889, 0.3721992, 3.8785784] | Test Loss: [2.3807373, 0.43371415, 4.32776]\n",
      "91: Train Loss: [2.114418, 0.38383964, 3.8449962] | Test Loss: [2.1830213, 0.34070966, 4.025333]\n",
      "92: Train Loss: [2.1189563, 0.35602918, 3.8818834] | Test Loss: [2.2707915, 0.346186, 4.195397]\n",
      "93: Train Loss: [2.1394727, 0.3301872, 3.9487584] | Test Loss: [2.312029, 0.3448587, 4.279199]\n",
      "94: Train Loss: [2.051621, 0.32031047, 3.7829313] | Test Loss: [2.4581835, 0.4967875, 4.4195795]\n",
      "95: Train Loss: [2.0070548, 0.3132285, 3.700881] | Test Loss: [2.321017, 0.4309339, 4.2111]\n",
      "96: Train Loss: [2.1241848, 0.4244874, 3.8238823] | Test Loss: [2.2232447, 0.29957542, 4.146914]\n",
      "97: Train Loss: [2.1432452, 0.39969796, 3.8867927] | Test Loss: [2.2014678, 0.3846926, 4.018243]\n",
      "98: Train Loss: [2.0847, 0.34422764, 3.8251724] | Test Loss: [2.3258862, 0.30898187, 4.3427906]\n",
      "99: Train Loss: [1.9872017, 0.28935394, 3.6850495] | Test Loss: [2.1771839, 0.40509793, 3.94927]\n",
      "100: Train Loss: [2.1081831, 0.3175291, 3.898837] | Test Loss: [2.3209352, 0.39231727, 4.249553]\n",
      "101: Train Loss: [2.0425997, 0.3404892, 3.7447102] | Test Loss: [2.4100826, 0.36062816, 4.459537]\n",
      "102: Train Loss: [2.0656455, 0.3601051, 3.7711859] | Test Loss: [2.1935935, 0.35439336, 4.0327935]\n",
      "103: Train Loss: [2.1623695, 0.3426868, 3.982052] | Test Loss: [2.3379047, 0.3834215, 4.292388]\n",
      "104: Train Loss: [2.1048603, 0.31271905, 3.8970015] | Test Loss: [2.3177252, 0.38602644, 4.249424]\n",
      "105: Train Loss: [2.1119328, 0.3404652, 3.8834002] | Test Loss: [2.29568, 0.26718506, 4.324175]\n",
      "106: Train Loss: [2.227228, 0.38629863, 4.068157] | Test Loss: [2.3002007, 0.39842358, 4.2019777]\n",
      "107: Train Loss: [2.1097376, 0.3135002, 3.905975] | Test Loss: [2.2017393, 0.37788814, 4.0255904]\n",
      "108: Train Loss: [2.0799265, 0.31616762, 3.8436852] | Test Loss: [2.285802, 0.3029116, 4.268692]\n",
      "109: Train Loss: [2.0439851, 0.3324017, 3.7555685] | Test Loss: [2.2198417, 0.30863035, 4.131053]\n",
      "110: Train Loss: [1.9809675, 0.3434192, 3.618516] | Test Loss: [2.1755404, 0.36224964, 3.9888313]\n",
      "111: Train Loss: [2.079511, 0.35375547, 3.8052664] | Test Loss: [2.2850976, 0.3368263, 4.233369]\n",
      "112: Train Loss: [2.0791616, 0.29935855, 3.8589647] | Test Loss: [2.2081738, 0.34930804, 4.0670395]\n",
      "113: Train Loss: [2.0794473, 0.37013185, 3.7887628] | Test Loss: [2.3559372, 0.32479158, 4.387083]\n",
      "114: Train Loss: [2.0904164, 0.3581311, 3.8227017] | Test Loss: [2.09893, 0.33185738, 3.8660026]\n",
      "115: Train Loss: [2.0199637, 0.3127463, 3.727181] | Test Loss: [2.328584, 0.3448779, 4.31229]\n",
      "116: Train Loss: [2.1988127, 0.35091352, 4.046712] | Test Loss: [2.3321986, 0.29282016, 4.3715773]\n",
      "117: Train Loss: [2.0597007, 0.37277433, 3.746627] | Test Loss: [2.3175812, 0.31162927, 4.323533]\n",
      "118: Train Loss: [2.0709891, 0.33991763, 3.8020606] | Test Loss: [2.2536657, 0.3636425, 4.1436887]\n",
      "119: Train Loss: [2.0303404, 0.31974468, 3.740936] | Test Loss: [2.1833572, 0.35187647, 4.014838]\n",
      "120: Train Loss: [2.1002731, 0.33443147, 3.8661146] | Test Loss: [2.318046, 0.40367198, 4.2324204]\n",
      "121: Train Loss: [2.203806, 0.31689504, 4.090717] | Test Loss: [2.3835995, 0.46550196, 4.3016973]\n",
      "122: Train Loss: [2.1301293, 0.34832898, 3.9119296] | Test Loss: [2.2961059, 0.32366824, 4.2685432]\n",
      "123: Train Loss: [2.1610644, 0.29918224, 4.0229464] | Test Loss: [2.3065937, 0.31220552, 4.300982]\n",
      "124: Train Loss: [2.1427264, 0.30831528, 3.9771373] | Test Loss: [2.1756113, 0.33493525, 4.0162873]\n",
      "125: Train Loss: [2.260248, 0.28562868, 4.234867] | Test Loss: [2.1048758, 0.31335965, 3.8963919]\n",
      "126: Train Loss: [1.9963421, 0.28290373, 3.7097805] | Test Loss: [2.3487027, 0.34279832, 4.354607]\n",
      "127: Train Loss: [2.0834775, 0.26358488, 3.9033701] | Test Loss: [2.4130876, 0.5030309, 4.3231444]\n",
      "128: Train Loss: [2.024859, 0.32396257, 3.7257552] | Test Loss: [2.1742132, 0.40232036, 3.946106]\n",
      "129: Train Loss: [2.2156098, 0.28953668, 4.141683] | Test Loss: [2.2297971, 0.28652877, 4.1730657]\n",
      "130: Train Loss: [2.1074336, 0.326652, 3.888215] | Test Loss: [2.2573447, 0.3791742, 4.135515]\n",
      "131: Train Loss: [1.9991664, 0.28468138, 3.7136514] | Test Loss: [2.2017581, 0.37683773, 4.0266786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132: Train Loss: [2.0453732, 0.36359572, 3.7271507] | Test Loss: [2.2942502, 0.33545402, 4.2530465]\n",
      "133: Train Loss: [2.086708, 0.3442908, 3.8291252] | Test Loss: [2.3014653, 0.35737017, 4.24556]\n",
      "134: Train Loss: [2.1840508, 0.37464765, 3.993454] | Test Loss: [2.3017433, 0.29849437, 4.304992]\n",
      "135: Train Loss: [1.9981915, 0.31790376, 3.6784792] | Test Loss: [2.1910698, 0.3417288, 4.040411]\n",
      "136: Train Loss: [2.0778635, 0.31006, 3.8456671] | Test Loss: [2.3550894, 0.29710367, 4.413075]\n",
      "137: Train Loss: [2.2957246, 0.44257724, 4.148872] | Test Loss: [2.3076303, 0.35195416, 4.2633066]\n",
      "138: Train Loss: [2.1483386, 0.32085606, 3.975821] | Test Loss: [2.2005863, 0.30769488, 4.0934777]\n",
      "139: Train Loss: [2.03009, 0.276151, 3.7840292] | Test Loss: [2.4479237, 0.3760351, 4.519812]\n",
      "140: Train Loss: [2.164731, 0.29483607, 4.034626] | Test Loss: [2.3116963, 0.4058935, 4.2174993]\n",
      "141: Train Loss: [2.1570063, 0.34876078, 3.9652517] | Test Loss: [2.3192706, 0.37541917, 4.263122]\n",
      "142: Train Loss: [2.1143746, 0.40024245, 3.828507] | Test Loss: [2.2300963, 0.31385186, 4.146341]\n",
      "143: Train Loss: [1.9788494, 0.2985299, 3.659169] | Test Loss: [2.3587573, 0.32944605, 4.3880687]\n",
      "144: Train Loss: [1.9462261, 0.35739934, 3.535053] | Test Loss: [2.2067845, 0.35924402, 4.054325]\n",
      "145: Train Loss: [2.0783145, 0.28397042, 3.8726585] | Test Loss: [2.1717558, 0.33366874, 4.009843]\n",
      "146: Train Loss: [2.1216087, 0.32891047, 3.914307] | Test Loss: [2.3713446, 0.33098856, 4.4117007]\n",
      "147: Train Loss: [2.0255518, 0.35245797, 3.6986458] | Test Loss: [2.1947882, 0.38708118, 4.0024953]\n",
      "148: Train Loss: [2.16544, 0.38455296, 3.9463272] | Test Loss: [2.2797284, 0.37082088, 4.188636]\n",
      "149: Train Loss: [2.3131518, 0.42175105, 4.2045527] | Test Loss: [2.3272004, 0.35549513, 4.298906]\n",
      "150: Train Loss: [2.1654146, 0.37317622, 3.957653] | Test Loss: [2.0477195, 0.39713946, 3.6982994]\n",
      "151: Train Loss: [2.185167, 0.36802194, 4.002312] | Test Loss: [2.3500173, 0.35479483, 4.3452396]\n",
      "152: Train Loss: [2.0943344, 0.32881337, 3.8598554] | Test Loss: [2.2410889, 0.3812564, 4.100921]\n",
      "153: Train Loss: [2.1326413, 0.37022376, 3.895059] | Test Loss: [2.2792728, 0.3554798, 4.203066]\n",
      "154: Train Loss: [1.9537439, 0.31078467, 3.5967033] | Test Loss: [2.4529948, 0.36341277, 4.542577]\n",
      "155: Train Loss: [1.9930751, 0.30487907, 3.681271] | Test Loss: [2.226599, 0.33925527, 4.1139426]\n",
      "156: Train Loss: [2.1442657, 0.37150985, 3.9170213] | Test Loss: [2.32792, 0.36824653, 4.2875934]\n",
      "157: Train Loss: [1.9773892, 0.29051405, 3.6642644] | Test Loss: [2.309618, 0.42805266, 4.1911836]\n",
      "158: Train Loss: [2.2052948, 0.28195515, 4.1286345] | Test Loss: [2.3573709, 0.28402895, 4.4307127]\n",
      "159: Train Loss: [2.041242, 0.31893286, 3.763551] | Test Loss: [2.2682793, 0.33084992, 4.2057085]\n",
      "160: Train Loss: [2.0163858, 0.37085405, 3.6619177] | Test Loss: [2.1651556, 0.33667645, 3.9936347]\n",
      "161: Train Loss: [2.1064467, 0.2894425, 3.923451] | Test Loss: [2.2949815, 0.3055865, 4.2843766]\n",
      "162: Train Loss: [2.0233686, 0.33850473, 3.7082324] | Test Loss: [2.1582386, 0.38886267, 3.9276147]\n",
      "163: Train Loss: [2.2679555, 0.4294298, 4.106481] | Test Loss: [2.2883568, 0.33468735, 4.2420263]\n",
      "164: Train Loss: [2.1296306, 0.30164057, 3.9576206] | Test Loss: [2.2692263, 0.3652076, 4.173245]\n",
      "165: Train Loss: [2.128903, 0.3777922, 3.8800137] | Test Loss: [2.3341303, 0.3297055, 4.338555]\n",
      "166: Train Loss: [2.07275, 0.2590773, 3.8864226] | Test Loss: [2.2188504, 0.38209912, 4.0556016]\n",
      "167: Train Loss: [2.1090355, 0.33499596, 3.8830752] | Test Loss: [2.067488, 0.30955878, 3.825417]\n",
      "168: Train Loss: [2.1404817, 0.2765971, 4.0043664] | Test Loss: [2.2398365, 0.34249762, 4.137175]\n",
      "169: Train Loss: [2.0897627, 0.39155486, 3.7879705] | Test Loss: [2.257283, 0.39014614, 4.1244197]\n",
      "170: Train Loss: [2.104606, 0.42131236, 3.7878995] | Test Loss: [2.1260421, 0.3569161, 3.895168]\n",
      "171: Train Loss: [2.1481962, 0.35733348, 3.939059] | Test Loss: [2.2623618, 0.3221781, 4.2025456]\n",
      "172: Train Loss: [2.1980133, 0.33394235, 4.062084] | Test Loss: [2.1144016, 0.40215778, 3.8266454]\n",
      "173: Train Loss: [2.0582733, 0.38176832, 3.7347782] | Test Loss: [2.185379, 0.3968519, 3.973906]\n",
      "174: Train Loss: [2.0383124, 0.3435631, 3.7330618] | Test Loss: [2.4848614, 0.4097107, 4.560012]\n",
      "175: Train Loss: [2.0194888, 0.3951723, 3.6438053] | Test Loss: [2.3226354, 0.3376486, 4.3076224]\n",
      "176: Train Loss: [2.0012383, 0.38753453, 3.6149423] | Test Loss: [2.300232, 0.30685577, 4.293608]\n",
      "177: Train Loss: [2.1072266, 0.34604484, 3.8684084] | Test Loss: [2.0926423, 0.32057762, 3.864707]\n",
      "178: Train Loss: [2.2055163, 0.3611721, 4.0498605] | Test Loss: [2.2074537, 0.30079535, 4.114112]\n",
      "179: Train Loss: [2.0956075, 0.31262973, 3.8785853] | Test Loss: [2.3055427, 0.35434783, 4.2567377]\n",
      "180: Train Loss: [2.1810937, 0.31294265, 4.049245] | Test Loss: [2.2838213, 0.34182712, 4.225816]\n",
      "181: Train Loss: [2.125048, 0.3255905, 3.9245055] | Test Loss: [2.271155, 0.36923832, 4.173072]\n",
      "182: Train Loss: [2.0644267, 0.3182551, 3.8105984] | Test Loss: [2.246476, 0.32090703, 4.1720448]\n",
      "183: Train Loss: [2.0600226, 0.3586511, 3.761394] | Test Loss: [2.243322, 0.34595641, 4.1406875]\n",
      "184: Train Loss: [2.1436265, 0.3843498, 3.9029033] | Test Loss: [2.3093276, 0.35323566, 4.2654195]\n",
      "185: Train Loss: [2.211063, 0.3410385, 4.081087] | Test Loss: [2.3168192, 0.28098324, 4.352655]\n",
      "186: Train Loss: [2.1256843, 0.29437888, 3.9569898] | Test Loss: [2.3048053, 0.3917568, 4.2178535]\n",
      "187: Train Loss: [2.0385058, 0.2829033, 3.7941084] | Test Loss: [2.1952326, 0.34482515, 4.04564]\n",
      "188: Train Loss: [2.0196273, 0.43076897, 3.6084857] | Test Loss: [2.265509, 0.30276653, 4.2282515]\n",
      "189: Train Loss: [2.074389, 0.28251255, 3.8662653] | Test Loss: [2.2552454, 0.35538235, 4.1551085]\n",
      "190: Train Loss: [2.0700421, 0.29498714, 3.8450973] | Test Loss: [2.3058653, 0.29546145, 4.316269]\n",
      "191: Train Loss: [1.9941818, 0.34868434, 3.6396792] | Test Loss: [2.2245986, 0.3865598, 4.0626373]\n",
      "192: Train Loss: [2.184466, 0.37116545, 3.9977663] | Test Loss: [2.3421698, 0.38257438, 4.301765]\n",
      "193: Train Loss: [2.130996, 0.35712445, 3.9048676] | Test Loss: [2.3253548, 0.28371236, 4.3669972]\n",
      "194: Train Loss: [2.1259303, 0.3056242, 3.9462364] | Test Loss: [2.1808007, 0.37931937, 3.982282]\n",
      "195: Train Loss: [2.2558942, 0.34642774, 4.1653605] | Test Loss: [2.4261882, 0.36077312, 4.4916034]\n",
      "196: Train Loss: [2.1104167, 0.31077865, 3.9100547] | Test Loss: [2.3123963, 0.3496377, 4.275155]\n",
      "197: Train Loss: [2.1787539, 0.2935611, 4.0639467] | Test Loss: [2.2748613, 0.35944015, 4.1902823]\n",
      "198: Train Loss: [2.261711, 0.43114057, 4.0922813] | Test Loss: [2.269068, 0.38311467, 4.155021]\n",
      "199: Train Loss: [2.1498957, 0.44724873, 3.8525424] | Test Loss: [2.2551475, 0.31439906, 4.1958957]\n",
      "200: Train Loss: [2.0664098, 0.31884184, 3.8139777] | Test Loss: [1.9880918, 0.3406563, 3.6355274]\n",
      "201: Train Loss: [2.0830023, 0.3367759, 3.8292289] | Test Loss: [2.1992037, 0.33036086, 4.0680466]\n",
      "202: Train Loss: [2.0215752, 0.39656347, 3.646587] | Test Loss: [2.2760007, 0.38386407, 4.1681376]\n",
      "203: Train Loss: [2.091113, 0.30100715, 3.8812191] | Test Loss: [2.42242, 0.33566007, 4.50918]\n",
      "204: Train Loss: [1.977917, 0.3406414, 3.6151924] | Test Loss: [2.34288, 0.32694212, 4.358818]\n",
      "205: Train Loss: [2.0051844, 0.34458408, 3.6657846] | Test Loss: [2.1799467, 0.43574837, 3.9241447]\n",
      "206: Train Loss: [2.0159028, 0.31643212, 3.7153735] | Test Loss: [2.2121477, 0.29757127, 4.1267242]\n",
      "207: Train Loss: [2.1272306, 0.2812942, 3.9731672] | Test Loss: [2.2254763, 0.39916357, 4.051789]\n",
      "208: Train Loss: [2.0992296, 0.33253488, 3.8659244] | Test Loss: [2.3743954, 0.37461594, 4.3741746]\n",
      "209: Train Loss: [1.9986527, 0.27131516, 3.7259903] | Test Loss: [2.313834, 0.36619407, 4.2614737]\n",
      "210: Train Loss: [2.1345592, 0.32898894, 3.9401293] | Test Loss: [2.1510093, 0.36322197, 3.9387965]\n",
      "211: Train Loss: [2.1452508, 0.373908, 3.9165938] | Test Loss: [2.138944, 0.36558363, 3.9123042]\n",
      "212: Train Loss: [2.2616751, 0.45933425, 4.064016] | Test Loss: [2.163262, 0.41450992, 3.9120138]\n",
      "213: Train Loss: [2.1091144, 0.40997612, 3.8082528] | Test Loss: [2.2057405, 0.35037124, 4.0611095]\n",
      "214: Train Loss: [2.0523407, 0.39769426, 3.7069874] | Test Loss: [2.0999265, 0.33857217, 3.8612807]\n",
      "215: Train Loss: [2.165813, 0.39667892, 3.934947] | Test Loss: [2.2715595, 0.39623454, 4.1468844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216: Train Loss: [2.1269596, 0.2882127, 3.9657066] | Test Loss: [2.3340082, 0.3039494, 4.364067]\n",
      "217: Train Loss: [2.11534, 0.3017159, 3.928964] | Test Loss: [2.3390684, 0.45095372, 4.2271833]\n",
      "218: Train Loss: [1.9047731, 0.30778828, 3.5017579] | Test Loss: [2.2898502, 0.31238645, 4.267314]\n",
      "219: Train Loss: [2.1362534, 0.3606533, 3.9118536] | Test Loss: [2.2405913, 0.29691994, 4.1842628]\n",
      "220: Train Loss: [2.1270883, 0.37879682, 3.8753798] | Test Loss: [2.2294304, 0.5306347, 3.9282262]\n",
      "221: Train Loss: [2.0695088, 0.38324803, 3.7557697] | Test Loss: [2.2528813, 0.32089382, 4.184869]\n",
      "222: Train Loss: [1.9478796, 0.3532039, 3.542555] | Test Loss: [2.2714105, 0.33710134, 4.2057195]\n",
      "223: Train Loss: [2.0375187, 0.33648404, 3.7385535] | Test Loss: [2.3771973, 0.37890023, 4.3754945]\n",
      "224: Train Loss: [2.1797073, 0.323389, 4.0360255] | Test Loss: [2.2281272, 0.352897, 4.1033573]\n",
      "225: Train Loss: [1.9625169, 0.32689923, 3.5981345] | Test Loss: [2.2185361, 0.3858981, 4.051174]\n",
      "226: Train Loss: [1.9849695, 0.34059456, 3.6293445] | Test Loss: [2.245814, 0.369335, 4.122293]\n",
      "227: Train Loss: [2.1195278, 0.33027524, 3.9087806] | Test Loss: [2.0868845, 0.30815518, 3.8656137]\n",
      "228: Train Loss: [2.1396666, 0.32203755, 3.9572957] | Test Loss: [2.3599586, 0.39566743, 4.3242497]\n",
      "229: Train Loss: [2.1308339, 0.32956368, 3.932104] | Test Loss: [2.2455058, 0.38110805, 4.109904]\n",
      "230: Train Loss: [2.157817, 0.38460734, 3.9310262] | Test Loss: [2.149263, 0.33285868, 3.965667]\n",
      "231: Train Loss: [2.0837624, 0.29895398, 3.868571] | Test Loss: [2.127297, 0.32534575, 3.9292479]\n",
      "232: Train Loss: [2.2285912, 0.3337464, 4.123436] | Test Loss: [2.212123, 0.36661267, 4.0576334]\n",
      "233: Train Loss: [2.1318731, 0.30283654, 3.9609096] | Test Loss: [2.3552165, 0.30930778, 4.4011254]\n",
      "234: Train Loss: [2.0745223, 0.30923015, 3.8398142] | Test Loss: [2.3945475, 0.36811557, 4.4209795]\n",
      "235: Train Loss: [2.278163, 0.35521007, 4.2011156] | Test Loss: [2.298009, 0.32825354, 4.267764]\n",
      "236: Train Loss: [2.104927, 0.35477266, 3.8550816] | Test Loss: [2.3858705, 0.3509872, 4.4207535]\n",
      "237: Train Loss: [1.8618168, 0.33599827, 3.3876352] | Test Loss: [2.080776, 0.337321, 3.8242311]\n",
      "238: Train Loss: [2.1691465, 0.37042418, 3.9678688] | Test Loss: [2.2144718, 0.34136873, 4.087575]\n",
      "239: Train Loss: [2.1482916, 0.35026598, 3.9463172] | Test Loss: [2.4814115, 0.3572925, 4.6055303]\n",
      "240: Train Loss: [2.1284783, 0.35203546, 3.9049213] | Test Loss: [2.394917, 0.41369325, 4.3761406]\n",
      "241: Train Loss: [2.110496, 0.32087868, 3.9001136] | Test Loss: [2.343648, 0.32416987, 4.363126]\n",
      "242: Train Loss: [2.0677373, 0.3265442, 3.8089304] | Test Loss: [2.1388152, 0.31393817, 3.9636922]\n",
      "243: Train Loss: [2.0625355, 0.31207103, 3.813] | Test Loss: [2.2214653, 0.2938289, 4.1491017]\n",
      "244: Train Loss: [2.2477832, 0.36376834, 4.131798] | Test Loss: [2.267837, 0.3519173, 4.183757]\n",
      "245: Train Loss: [1.9705154, 0.36677572, 3.574255] | Test Loss: [2.1567464, 0.4240858, 3.8894072]\n",
      "246: Train Loss: [2.145975, 0.37251678, 3.9194334] | Test Loss: [2.2897532, 0.30539832, 4.274108]\n",
      "247: Train Loss: [2.0481765, 0.3378766, 3.7584765] | Test Loss: [2.208576, 0.40066537, 4.0164866]\n",
      "248: Train Loss: [2.2986128, 0.32051045, 4.2767153] | Test Loss: [2.408863, 0.40265244, 4.415074]\n",
      "249: Train Loss: [2.152493, 0.3195319, 3.985454] | Test Loss: [2.3703346, 0.30685455, 4.4338145]\n",
      "250: Train Loss: [1.983242, 0.28007326, 3.686411] | Test Loss: [2.1133387, 0.31410655, 3.9125707]\n",
      "251: Train Loss: [2.1109247, 0.3570892, 3.8647604] | Test Loss: [2.255148, 0.3317551, 4.1785407]\n",
      "252: Train Loss: [2.0716207, 0.34158224, 3.801659] | Test Loss: [2.3213677, 0.33577847, 4.306957]\n",
      "253: Train Loss: [2.1908236, 0.30330178, 4.0783453] | Test Loss: [2.2543445, 0.35552865, 4.15316]\n",
      "254: Train Loss: [2.1130126, 0.30953997, 3.9164853] | Test Loss: [2.1904578, 0.3330541, 4.0478616]\n",
      "255: Train Loss: [2.1796346, 0.33632028, 4.0229487] | Test Loss: [2.219631, 0.3864952, 4.052767]\n",
      "256: Train Loss: [2.0947828, 0.31769502, 3.8718705] | Test Loss: [2.4132814, 0.4391931, 4.3873696]\n",
      "257: Train Loss: [2.1542726, 0.3420507, 3.9664946] | Test Loss: [2.1906393, 0.32953724, 4.051741]\n",
      "258: Train Loss: [2.1587968, 0.29984665, 4.017747] | Test Loss: [2.2083068, 0.30571273, 4.110901]\n",
      "259: Train Loss: [1.9724445, 0.26872286, 3.6761663] | Test Loss: [2.3660116, 0.32068738, 4.411336]\n",
      "260: Train Loss: [2.1377609, 0.3690159, 3.906506] | Test Loss: [2.247551, 0.31204963, 4.183052]\n",
      "261: Train Loss: [2.103398, 0.29965186, 3.9071443] | Test Loss: [2.2519786, 0.35610452, 4.147853]\n",
      "262: Train Loss: [2.17794, 0.3999736, 3.9559062] | Test Loss: [2.1888103, 0.2745962, 4.1030245]\n",
      "263: Train Loss: [2.0726433, 0.3837632, 3.7615232] | Test Loss: [2.1099496, 0.31229508, 3.9076042]\n",
      "264: Train Loss: [2.0182555, 0.28858078, 3.7479303] | Test Loss: [2.3439264, 0.4538377, 4.234015]\n",
      "265: Train Loss: [1.8902217, 0.27211604, 3.5083275] | Test Loss: [2.2444098, 0.3590643, 4.1297555]\n",
      "266: Train Loss: [2.1408474, 0.29437697, 3.987318] | Test Loss: [2.1735492, 0.3183325, 4.0287657]\n",
      "267: Train Loss: [2.195991, 0.31427947, 4.0777025] | Test Loss: [2.2883475, 0.35165536, 4.2250395]\n",
      "268: Train Loss: [2.0892215, 0.34613514, 3.8323076] | Test Loss: [2.2036705, 0.37576357, 4.0315776]\n",
      "269: Train Loss: [2.042652, 0.27408352, 3.8112202] | Test Loss: [2.236054, 0.46969724, 4.0024104]\n",
      "270: Train Loss: [2.0875394, 0.3107788, 3.8643003] | Test Loss: [2.2596745, 0.37787592, 4.1414733]\n",
      "271: Train Loss: [2.0085695, 0.3535919, 3.6635473] | Test Loss: [2.26756, 0.27197897, 4.263141]\n",
      "272: Train Loss: [2.0556948, 0.3527385, 3.7586513] | Test Loss: [2.180251, 0.35142645, 4.009075]\n",
      "273: Train Loss: [2.242508, 0.30202976, 4.1829863] | Test Loss: [2.228078, 0.36010984, 4.096046]\n",
      "274: Train Loss: [2.1353168, 0.28650117, 3.9841323] | Test Loss: [2.2751966, 0.298107, 4.252286]\n",
      "275: Train Loss: [2.089142, 0.3554295, 3.8228545] | Test Loss: [2.268841, 0.3979965, 4.1396856]\n",
      "276: Train Loss: [2.1110153, 0.32370815, 3.8983223] | Test Loss: [2.2432072, 0.36205864, 4.124356]\n",
      "277: Train Loss: [2.15341, 0.31188816, 3.9949317] | Test Loss: [2.2762353, 0.3013905, 4.25108]\n",
      "278: Train Loss: [2.2558048, 0.32224366, 4.189366] | Test Loss: [2.34099, 0.36734658, 4.3146334]\n",
      "279: Train Loss: [2.033321, 0.3631564, 3.7034853] | Test Loss: [2.2621114, 0.34859234, 4.1756306]\n",
      "280: Train Loss: [2.1329248, 0.37054077, 3.895309] | Test Loss: [2.175101, 0.30844784, 4.0417542]\n",
      "281: Train Loss: [2.0321841, 0.3811062, 3.6832619] | Test Loss: [2.268525, 0.36793947, 4.1691103]\n",
      "282: Train Loss: [2.191483, 0.3301418, 4.052824] | Test Loss: [2.2690945, 0.3200542, 4.218135]\n",
      "283: Train Loss: [2.119586, 0.36965844, 3.8695138] | Test Loss: [2.2604103, 0.36030555, 4.160515]\n",
      "284: Train Loss: [2.1044793, 0.35986018, 3.8490987] | Test Loss: [2.271194, 0.33714253, 4.2052455]\n",
      "285: Train Loss: [2.1143947, 0.35391048, 3.874879] | Test Loss: [2.0527167, 0.3355931, 3.7698402]\n",
      "286: Train Loss: [2.038161, 0.3511141, 3.7252078] | Test Loss: [2.2808068, 0.2851163, 4.2764974]\n",
      "287: Train Loss: [1.9073309, 0.35845008, 3.4562116] | Test Loss: [2.2844214, 0.35938805, 4.209455]\n",
      "288: Train Loss: [2.2321947, 0.32354203, 4.140847] | Test Loss: [2.1886096, 0.38445204, 3.9927673]\n",
      "289: Train Loss: [2.2714663, 0.40431952, 4.1386127] | Test Loss: [2.0636578, 0.3378179, 3.7894974]\n",
      "290: Train Loss: [2.002467, 0.35626552, 3.6486685] | Test Loss: [2.279569, 0.3857704, 4.1733675]\n",
      "291: Train Loss: [2.071149, 0.36822024, 3.7740781] | Test Loss: [2.220252, 0.36583105, 4.074673]\n",
      "292: Train Loss: [2.314442, 0.35677794, 4.2721057] | Test Loss: [2.4028687, 0.3177642, 4.487973]\n",
      "293: Train Loss: [2.121333, 0.33917084, 3.9034948] | Test Loss: [2.2941678, 0.43972075, 4.148615]\n",
      "294: Train Loss: [2.1023245, 0.32555053, 3.8790984] | Test Loss: [2.3079984, 0.30905968, 4.306937]\n",
      "295: Train Loss: [2.146516, 0.31320664, 3.9798255] | Test Loss: [2.3381944, 0.30619025, 4.3701987]\n",
      "296: Train Loss: [2.1865, 0.3493142, 4.023686] | Test Loss: [2.3230145, 0.35573584, 4.290293]\n",
      "297: Train Loss: [2.0292695, 0.3935131, 3.665026] | Test Loss: [2.4664478, 0.47215232, 4.4607434]\n",
      "298: Train Loss: [2.2275002, 0.3802218, 4.0747786] | Test Loss: [2.0615723, 0.40030608, 3.7228384]\n",
      "299: Train Loss: [2.1675909, 0.34726158, 3.9879203] | Test Loss: [2.294371, 0.36775586, 4.220986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300: Train Loss: [2.0255003, 0.33116272, 3.719838] | Test Loss: [2.1570263, 0.3373388, 3.976714]\n",
      "301: Train Loss: [2.2057252, 0.39298266, 4.018468] | Test Loss: [2.2046099, 0.34680355, 4.062416]\n",
      "302: Train Loss: [2.2340539, 0.33818865, 4.129919] | Test Loss: [2.1677465, 0.30097535, 4.034518]\n",
      "303: Train Loss: [2.0627038, 0.38602158, 3.739386] | Test Loss: [2.0298686, 0.37698576, 3.6827517]\n",
      "304: Train Loss: [2.0203893, 0.31196034, 3.7288182] | Test Loss: [2.3682485, 0.38584006, 4.350657]\n",
      "305: Train Loss: [2.2136247, 0.30423674, 4.1230125] | Test Loss: [2.2894258, 0.34034222, 4.2385097]\n",
      "306: Train Loss: [2.1460762, 0.3510776, 3.9410748] | Test Loss: [2.2373, 0.33522272, 4.139377]\n",
      "307: Train Loss: [2.0591497, 0.4281826, 3.6901166] | Test Loss: [2.3519034, 0.3304534, 4.3733535]\n",
      "308: Train Loss: [1.9770011, 0.2627925, 3.6912096] | Test Loss: [2.1835628, 0.36580968, 4.0013156]\n",
      "309: Train Loss: [2.083144, 0.32341653, 3.8428714] | Test Loss: [2.3272789, 0.36608285, 4.288475]\n",
      "310: Train Loss: [2.1384695, 0.4035185, 3.8734202] | Test Loss: [2.4438586, 0.35626483, 4.531452]\n",
      "311: Train Loss: [1.9791356, 0.36516127, 3.59311] | Test Loss: [2.2611718, 0.316266, 4.2060776]\n",
      "312: Train Loss: [2.2089655, 0.35352555, 4.0644054] | Test Loss: [2.174183, 0.43582797, 3.9125376]\n",
      "313: Train Loss: [2.1616275, 0.4037574, 3.9194975] | Test Loss: [2.2057657, 0.3918017, 4.0197296]\n",
      "314: Train Loss: [2.0528324, 0.3416582, 3.7640064] | Test Loss: [2.1215863, 0.38137466, 3.861798]\n",
      "315: Train Loss: [2.1060226, 0.3059928, 3.9060524] | Test Loss: [2.1824746, 0.4713786, 3.8935704]\n",
      "316: Train Loss: [2.1707237, 0.37180036, 3.969647] | Test Loss: [2.2437994, 0.335726, 4.151873]\n",
      "317: Train Loss: [2.2529879, 0.29612115, 4.2098546] | Test Loss: [2.2552242, 0.3299187, 4.1805296]\n",
      "318: Train Loss: [2.0482957, 0.34119022, 3.7554014] | Test Loss: [2.2496128, 0.34955394, 4.1496716]\n",
      "319: Train Loss: [2.1756566, 0.29376888, 4.057544] | Test Loss: [2.3017378, 0.30732283, 4.2961526]\n",
      "320: Train Loss: [2.136034, 0.4190846, 3.8529835] | Test Loss: [2.172337, 0.3125419, 4.032132]\n",
      "321: Train Loss: [1.917687, 0.31171814, 3.523656] | Test Loss: [2.3400695, 0.33779448, 4.3423448]\n",
      "322: Train Loss: [2.050866, 0.37986603, 3.7218657] | Test Loss: [2.228117, 0.36195713, 4.094277]\n",
      "323: Train Loss: [2.2549782, 0.35392198, 4.1560345] | Test Loss: [2.390432, 0.4003328, 4.380531]\n",
      "324: Train Loss: [1.9963107, 0.304921, 3.6877005] | Test Loss: [2.3200142, 0.3510288, 4.2889996]\n",
      "325: Train Loss: [2.1125672, 0.33826208, 3.8868723] | Test Loss: [2.2937014, 0.36050332, 4.2268996]\n",
      "326: Train Loss: [2.0879784, 0.45127463, 3.7246823] | Test Loss: [2.2538903, 0.37115175, 4.1366286]\n",
      "327: Train Loss: [2.0982807, 0.31563163, 3.8809297] | Test Loss: [2.0904675, 0.34494662, 3.835988]\n",
      "328: Train Loss: [2.2097862, 0.3905021, 4.0290704] | Test Loss: [2.1804504, 0.3154617, 4.0454392]\n",
      "329: Train Loss: [2.1190221, 0.32398275, 3.9140613] | Test Loss: [2.3188531, 0.3316066, 4.3061]\n",
      "330: Train Loss: [2.1265132, 0.30042848, 3.9525979] | Test Loss: [2.171428, 0.36793482, 3.974921]\n",
      "331: Train Loss: [2.1905713, 0.36747533, 4.013667] | Test Loss: [2.718113, 0.3059867, 5.130239]\n",
      "332: Train Loss: [2.1613016, 0.33083504, 3.9917684] | Test Loss: [2.3225098, 0.36601347, 4.279006]\n",
      "333: Train Loss: [2.0321558, 0.30089825, 3.7634132] | Test Loss: [2.2365406, 0.32692078, 4.14616]\n",
      "334: Train Loss: [2.0244718, 0.36685756, 3.6820858] | Test Loss: [2.135457, 0.34043044, 3.9304838]\n",
      "335: Train Loss: [2.225346, 0.5063273, 3.944365] | Test Loss: [2.1768577, 0.3452258, 4.0084896]\n",
      "336: Train Loss: [2.1366725, 0.3345191, 3.9388258] | Test Loss: [2.3849332, 0.3983572, 4.371509]\n",
      "337: Train Loss: [2.162815, 0.35186887, 3.9737616] | Test Loss: [2.180048, 0.38792196, 3.9721742]\n",
      "338: Train Loss: [2.1226935, 0.3451451, 3.9002419] | Test Loss: [2.1786504, 0.31871724, 4.0385838]\n",
      "339: Train Loss: [2.0247555, 0.3210311, 3.72848] | Test Loss: [2.2517612, 0.3804159, 4.1231065]\n",
      "340: Train Loss: [2.1771727, 0.3238061, 4.030539] | Test Loss: [2.206595, 0.28293765, 4.1302524]\n",
      "341: Train Loss: [2.0917253, 0.37625405, 3.8071966] | Test Loss: [2.2718024, 0.345154, 4.198451]\n",
      "342: Train Loss: [2.0846055, 0.3676182, 3.8015928] | Test Loss: [2.3017886, 0.41137198, 4.192205]\n",
      "343: Train Loss: [2.2465634, 0.32414123, 4.168986] | Test Loss: [2.1265414, 0.3217638, 3.9313188]\n",
      "344: Train Loss: [2.1123729, 0.30990422, 3.9148414] | Test Loss: [2.412424, 0.321712, 4.503136]\n",
      "345: Train Loss: [2.1071906, 0.31195703, 3.9024243] | Test Loss: [2.377147, 0.3354557, 4.418838]\n",
      "346: Train Loss: [1.9711859, 0.31518045, 3.6271913] | Test Loss: [2.37523, 0.4082639, 4.3421965]\n",
      "347: Train Loss: [2.1042898, 0.32795158, 3.8806279] | Test Loss: [2.2543778, 0.3307698, 4.1779857]\n",
      "348: Train Loss: [2.1357856, 0.329792, 3.9417791] | Test Loss: [2.221721, 0.3574589, 4.085983]\n",
      "349: Train Loss: [2.2290547, 0.34393445, 4.114175] | Test Loss: [2.3810058, 0.3521068, 4.409905]\n",
      "350: Train Loss: [2.0713818, 0.35777014, 3.7849934] | Test Loss: [2.2362113, 0.31926042, 4.153162]\n",
      "351: Train Loss: [2.153575, 0.36136192, 3.9457881] | Test Loss: [2.4549723, 0.3868416, 4.5231028]\n",
      "352: Train Loss: [2.0627403, 0.30338928, 3.8220913] | Test Loss: [2.2575297, 0.4040606, 4.110999]\n",
      "353: Train Loss: [2.084026, 0.31153774, 3.8565145] | Test Loss: [2.3427866, 0.3168682, 4.368705]\n",
      "354: Train Loss: [2.1878774, 0.30696848, 4.068786] | Test Loss: [2.2073615, 0.32946923, 4.0852537]\n",
      "355: Train Loss: [2.3221695, 0.467357, 4.176982] | Test Loss: [2.2217784, 0.3356362, 4.1079206]\n",
      "356: Train Loss: [2.0014162, 0.31117475, 3.6916575] | Test Loss: [2.3235054, 0.39513782, 4.251873]\n",
      "357: Train Loss: [2.0251465, 0.30605605, 3.744237] | Test Loss: [2.1115043, 0.39307678, 3.8299317]\n",
      "358: Train Loss: [1.9800022, 0.35296008, 3.6070442] | Test Loss: [2.2754512, 0.36379108, 4.1871114]\n",
      "359: Train Loss: [2.2918918, 0.4538729, 4.129911] | Test Loss: [2.1011045, 0.38732424, 3.814885]\n",
      "360: Train Loss: [2.2046127, 0.30398235, 4.105243] | Test Loss: [2.172632, 0.30715573, 4.0381083]\n",
      "361: Train Loss: [2.2153776, 0.38919237, 4.0415626] | Test Loss: [2.3174317, 0.33352527, 4.301338]\n",
      "362: Train Loss: [2.0258465, 0.3483776, 3.7033155] | Test Loss: [2.2190769, 0.35596514, 4.0821886]\n",
      "363: Train Loss: [2.2032516, 0.34118515, 4.065318] | Test Loss: [2.3882616, 0.36866888, 4.407854]\n",
      "364: Train Loss: [2.0670142, 0.33194485, 3.8020837] | Test Loss: [2.3192225, 0.34595087, 4.292494]\n",
      "365: Train Loss: [2.1891017, 0.31813973, 4.060064] | Test Loss: [2.1451674, 0.36455464, 3.9257803]\n",
      "366: Train Loss: [2.1621585, 0.38358167, 3.940735] | Test Loss: [2.3957238, 0.3779871, 4.4134607]\n",
      "367: Train Loss: [2.1566787, 0.2970849, 4.0162725] | Test Loss: [2.2289698, 0.36808836, 4.0898514]\n",
      "368: Train Loss: [2.0540218, 0.32717595, 3.7808678] | Test Loss: [2.3411825, 0.34234032, 4.3400245]\n",
      "369: Train Loss: [1.949664, 0.3478752, 3.5514529] | Test Loss: [2.2486458, 0.34482482, 4.152467]\n",
      "370: Train Loss: [2.093688, 0.29380798, 3.8935683] | Test Loss: [2.191046, 0.30945873, 4.0726333]\n",
      "371: Train Loss: [2.2078567, 0.35578388, 4.0599294] | Test Loss: [2.217559, 0.36024883, 4.074869]\n",
      "372: Train Loss: [1.957, 0.32118574, 3.5928142] | Test Loss: [2.3413062, 0.4168836, 4.265729]\n",
      "373: Train Loss: [2.2138371, 0.3472721, 4.0804024] | Test Loss: [2.1709828, 0.29361796, 4.0483475]\n",
      "374: Train Loss: [2.160656, 0.3474436, 3.9738681] | Test Loss: [2.3352482, 0.36600572, 4.3044906]\n",
      "375: Train Loss: [2.1969814, 0.32123518, 4.0727277] | Test Loss: [2.287116, 0.2994396, 4.2747927]\n",
      "376: Train Loss: [2.1883318, 0.34839553, 4.0282683] | Test Loss: [2.2095203, 0.28906158, 4.129979]\n",
      "377: Train Loss: [2.0393422, 0.33164984, 3.7470345] | Test Loss: [2.132896, 0.32572705, 3.9400647]\n",
      "378: Train Loss: [2.157799, 0.3432141, 3.972384] | Test Loss: [2.1359859, 0.35189256, 3.9200792]\n",
      "379: Train Loss: [2.1794477, 0.35206687, 4.0068283] | Test Loss: [2.4848862, 0.43746433, 4.532308]\n",
      "380: Train Loss: [2.262601, 0.367568, 4.157634] | Test Loss: [2.2858062, 0.33041382, 4.2411985]\n",
      "381: Train Loss: [2.1237721, 0.31553853, 3.9320056] | Test Loss: [2.1548414, 0.32492048, 3.9847624]\n",
      "382: Train Loss: [2.0985548, 0.35679573, 3.840314] | Test Loss: [2.2937517, 0.40035492, 4.1871486]\n",
      "383: Train Loss: [2.099259, 0.2835108, 3.9150069] | Test Loss: [2.3386595, 0.41294304, 4.264376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384: Train Loss: [2.0759122, 0.3713422, 3.780482] | Test Loss: [2.1490064, 0.3531757, 3.944837]\n",
      "385: Train Loss: [2.0902019, 0.27101904, 3.9093845] | Test Loss: [2.2916064, 0.31886888, 4.264344]\n",
      "386: Train Loss: [2.0015147, 0.2750947, 3.7279348] | Test Loss: [2.3615484, 0.41111732, 4.31198]\n",
      "387: Train Loss: [2.211555, 0.33854353, 4.0845666] | Test Loss: [2.2742698, 0.28325978, 4.26528]\n",
      "388: Train Loss: [2.1382, 0.30857313, 3.9678268] | Test Loss: [2.1227252, 0.2866237, 3.9588268]\n",
      "389: Train Loss: [2.2105684, 0.36479372, 4.056343] | Test Loss: [2.094373, 0.3379682, 3.8507776]\n",
      "390: Train Loss: [2.0734134, 0.32971776, 3.817109] | Test Loss: [2.3033156, 0.33232188, 4.274309]\n",
      "391: Train Loss: [2.2032852, 0.37527484, 4.031296] | Test Loss: [1.8797649, 0.5066201, 3.2529097]\n",
      "392: Train Loss: [2.0366979, 0.36734664, 3.7060492] | Test Loss: [2.2462857, 0.37399167, 4.11858]\n",
      "393: Train Loss: [2.042533, 0.3663702, 3.7186959] | Test Loss: [2.308288, 0.3754661, 4.24111]\n",
      "394: Train Loss: [2.1001637, 0.39447397, 3.8058534] | Test Loss: [2.3351889, 0.4235889, 4.246789]\n",
      "395: Train Loss: [2.2314024, 0.40793362, 4.054871] | Test Loss: [2.2568393, 0.30432373, 4.209355]\n",
      "396: Train Loss: [2.0539937, 0.3371081, 3.7708793] | Test Loss: [2.383656, 0.35001755, 4.4172945]\n",
      "397: Train Loss: [2.110864, 0.31388363, 3.9078443] | Test Loss: [2.2115808, 0.4169629, 4.0061984]\n",
      "398: Train Loss: [1.9327188, 0.27611548, 3.589322] | Test Loss: [2.1048462, 0.33562, 3.8740726]\n",
      "399: Train Loss: [2.1503086, 0.3867103, 3.913907] | Test Loss: [2.278864, 0.32311463, 4.234613]\n",
      "400: Train Loss: [2.1340017, 0.30478585, 3.9632177] | Test Loss: [2.340323, 0.420718, 4.2599277]\n",
      "401: Train Loss: [2.0405068, 0.33880734, 3.742206] | Test Loss: [2.1684647, 0.37502503, 3.9619043]\n",
      "402: Train Loss: [1.983029, 0.28429347, 3.6817646] | Test Loss: [2.2785082, 0.33446044, 4.222556]\n",
      "403: Train Loss: [2.0329745, 0.3145195, 3.7514296] | Test Loss: [2.0387669, 0.4022472, 3.6752868]\n",
      "404: Train Loss: [2.0182517, 0.32818478, 3.7083185] | Test Loss: [2.4492192, 0.4301109, 4.4683275]\n",
      "405: Train Loss: [2.0430079, 0.33252168, 3.753494] | Test Loss: [2.216857, 0.325907, 4.107807]\n",
      "406: Train Loss: [2.345239, 0.5094651, 4.1810126] | Test Loss: [2.3137128, 0.4235465, 4.2038794]\n",
      "407: Train Loss: [2.1425986, 0.31507248, 3.970125] | Test Loss: [2.2531526, 0.3233638, 4.1829414]\n",
      "408: Train Loss: [2.0629945, 0.3593086, 3.7666802] | Test Loss: [2.1977808, 0.30811888, 4.087443]\n",
      "409: Train Loss: [2.0746405, 0.32665667, 3.8226242] | Test Loss: [2.1806378, 0.29571092, 4.0655646]\n",
      "410: Train Loss: [2.0816178, 0.32547644, 3.837759] | Test Loss: [2.138158, 0.36000824, 3.916308]\n",
      "411: Train Loss: [1.9575514, 0.33492568, 3.580177] | Test Loss: [2.2494211, 0.35249987, 4.1463423]\n",
      "412: Train Loss: [2.202794, 0.41571867, 3.9898694] | Test Loss: [2.1575303, 0.37738308, 3.9376774]\n",
      "413: Train Loss: [2.0281544, 0.35940897, 3.6968997] | Test Loss: [2.2884064, 0.30112767, 4.2756853]\n",
      "414: Train Loss: [2.2977083, 0.38487965, 4.210537] | Test Loss: [2.1877508, 0.3635939, 4.0119076]\n",
      "415: Train Loss: [2.1680403, 0.37359208, 3.9624884] | Test Loss: [2.2068315, 0.37618327, 4.0374794]\n",
      "416: Train Loss: [1.988483, 0.3808229, 3.596143] | Test Loss: [2.274282, 0.36716333, 4.181401]\n",
      "417: Train Loss: [2.0841844, 0.3977795, 3.7705894] | Test Loss: [2.304493, 0.41102976, 4.197956]\n",
      "418: Train Loss: [2.138503, 0.35875255, 3.9182534] | Test Loss: [2.2302775, 0.37040162, 4.090153]\n",
      "419: Train Loss: [2.106531, 0.400402, 3.8126597] | Test Loss: [2.3490903, 0.34829104, 4.3498898]\n",
      "420: Train Loss: [2.1889222, 0.4678784, 3.909966] | Test Loss: [2.3294473, 0.3363945, 4.3225]\n",
      "421: Train Loss: [2.003313, 0.29517025, 3.7114558] | Test Loss: [2.3956249, 0.3214979, 4.469752]\n",
      "422: Train Loss: [2.1257486, 0.31609008, 3.9354074] | Test Loss: [2.2603383, 0.36943167, 4.151245]\n",
      "423: Train Loss: [2.141544, 0.30921963, 3.9738684] | Test Loss: [2.3363736, 0.33242556, 4.3403215]\n",
      "424: Train Loss: [2.1778827, 0.5288534, 3.8269117] | Test Loss: [2.3674603, 0.36734867, 4.367572]\n",
      "425: Train Loss: [2.1935856, 0.3156894, 4.0714817] | Test Loss: [2.2075577, 0.30312335, 4.111992]\n",
      "426: Train Loss: [2.2254164, 0.40462264, 4.0462103] | Test Loss: [2.2428997, 0.3461962, 4.139603]\n",
      "427: Train Loss: [2.0291274, 0.3641511, 3.6941037] | Test Loss: [2.2718408, 0.34154427, 4.2021375]\n",
      "428: Train Loss: [2.2176073, 0.32925808, 4.1059566] | Test Loss: [2.3291247, 0.4045131, 4.2537365]\n",
      "429: Train Loss: [2.060872, 0.32783294, 3.7939115] | Test Loss: [2.308814, 0.39916387, 4.2184644]\n",
      "430: Train Loss: [2.1194305, 0.27896336, 3.9598975] | Test Loss: [2.2838035, 0.3247817, 4.242825]\n",
      "431: Train Loss: [2.2033179, 0.32877094, 4.0778646] | Test Loss: [2.2282405, 0.34546894, 4.111012]\n",
      "432: Train Loss: [2.13641, 0.35063648, 3.9221835] | Test Loss: [2.2020364, 0.4889242, 3.9151487]\n",
      "433: Train Loss: [2.0436804, 0.30163464, 3.785726] | Test Loss: [2.1980262, 0.383575, 4.0124774]\n",
      "434: Train Loss: [2.0532055, 0.3418043, 3.7646067] | Test Loss: [2.205929, 0.40047604, 4.011382]\n",
      "435: Train Loss: [2.241539, 0.4051707, 4.077907] | Test Loss: [2.2660868, 0.28438452, 4.247789]\n",
      "436: Train Loss: [2.0332365, 0.31242034, 3.7540529] | Test Loss: [2.3132474, 0.34103578, 4.285459]\n",
      "437: Train Loss: [2.1760252, 0.31965527, 4.032395] | Test Loss: [2.3539789, 0.3598743, 4.3480835]\n",
      "438: Train Loss: [2.1021855, 0.38052174, 3.8238492] | Test Loss: [2.414568, 0.36248338, 4.4666524]\n",
      "439: Train Loss: [2.0991075, 0.33325723, 3.8649576] | Test Loss: [2.1043375, 0.370083, 3.838592]\n",
      "440: Train Loss: [1.9994261, 0.30784035, 3.691012] | Test Loss: [2.1136498, 0.348899, 3.8784008]\n",
      "441: Train Loss: [2.0312803, 0.29064524, 3.7719152] | Test Loss: [2.2080033, 0.321634, 4.0943727]\n",
      "442: Train Loss: [2.1178143, 0.34343514, 3.8921936] | Test Loss: [2.2250834, 0.28514758, 4.165019]\n",
      "443: Train Loss: [2.2084172, 0.31810448, 4.09873] | Test Loss: [2.3427165, 0.51505536, 4.1703777]\n",
      "444: Train Loss: [2.172167, 0.393106, 3.9512281] | Test Loss: [2.1323192, 0.39403266, 3.8706057]\n",
      "445: Train Loss: [2.055456, 0.33943865, 3.7714734] | Test Loss: [2.3899384, 0.40769878, 4.372178]\n",
      "446: Train Loss: [2.2464862, 0.35991699, 4.133055] | Test Loss: [2.2128477, 0.32291988, 4.1027756]\n",
      "447: Train Loss: [1.9586747, 0.27181515, 3.6455343] | Test Loss: [2.2009842, 0.33604243, 4.065926]\n",
      "448: Train Loss: [2.2327268, 0.3358625, 4.129591] | Test Loss: [2.1503253, 0.36123842, 3.9394124]\n",
      "449: Train Loss: [2.1139915, 0.32509425, 3.9028888] | Test Loss: [2.1689768, 0.33964407, 3.9983094]\n",
      "450: Train Loss: [2.0252306, 0.4498553, 3.6006062] | Test Loss: [2.3953931, 0.35560358, 4.4351826]\n",
      "451: Train Loss: [2.1573036, 0.40532097, 3.9092863] | Test Loss: [1.9314208, 0.27322543, 3.589616]\n",
      "452: Train Loss: [2.054697, 0.38637015, 3.7230241] | Test Loss: [2.2013898, 0.41025928, 3.9925203]\n",
      "453: Train Loss: [1.9530866, 0.36081135, 3.5453618] | Test Loss: [2.238024, 0.35734707, 4.118701]\n",
      "454: Train Loss: [2.0635965, 0.33192098, 3.7952719] | Test Loss: [2.2363107, 0.30629677, 4.1663246]\n",
      "455: Train Loss: [2.1193273, 0.33249202, 3.9061627] | Test Loss: [2.3568034, 0.37728217, 4.3363247]\n",
      "456: Train Loss: [2.1716497, 0.28431275, 4.0589867] | Test Loss: [2.4873993, 0.50566643, 4.4691324]\n",
      "457: Train Loss: [2.146435, 0.34424844, 3.9486217] | Test Loss: [2.3610535, 0.35365662, 4.36845]\n",
      "458: Train Loss: [2.0702379, 0.32881707, 3.8116589] | Test Loss: [2.3159494, 0.3519743, 4.2799244]\n",
      "459: Train Loss: [2.1444776, 0.29912442, 3.989831] | Test Loss: [2.360999, 0.32920432, 4.392794]\n",
      "460: Train Loss: [2.3673308, 0.37832016, 4.3563414] | Test Loss: [2.27233, 0.4576421, 4.087018]\n",
      "461: Train Loss: [2.1625135, 0.39862314, 3.926404] | Test Loss: [2.1778748, 0.40568292, 3.9500666]\n",
      "462: Train Loss: [2.0894034, 0.30203047, 3.8767762] | Test Loss: [2.2968726, 0.32633018, 4.267415]\n",
      "463: Train Loss: [2.1832812, 0.3088486, 4.057714] | Test Loss: [2.3435693, 0.35448512, 4.3326535]\n",
      "464: Train Loss: [2.1260724, 0.38225123, 3.8698938] | Test Loss: [2.2869322, 0.36607695, 4.2077875]\n",
      "465: Train Loss: [2.1518927, 0.39661276, 3.9071724] | Test Loss: [2.1859627, 0.36867207, 4.0032535]\n",
      "466: Train Loss: [2.1354144, 0.34286526, 3.9279635] | Test Loss: [2.4219513, 0.3200226, 4.52388]\n",
      "467: Train Loss: [2.1708143, 0.30559888, 4.03603] | Test Loss: [2.2506194, 0.31813908, 4.1830997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468: Train Loss: [2.1295915, 0.346852, 3.9123309] | Test Loss: [2.120478, 0.407384, 3.833572]\n",
      "469: Train Loss: [2.1752822, 0.3275796, 4.022985] | Test Loss: [2.263203, 0.29961076, 4.226795]\n",
      "470: Train Loss: [2.2532766, 0.3190281, 4.1875253] | Test Loss: [2.2114084, 0.45484623, 3.9679706]\n",
      "471: Train Loss: [2.2525678, 0.36168453, 4.143451] | Test Loss: [2.3054705, 0.32371497, 4.2872257]\n",
      "472: Train Loss: [2.068182, 0.27803382, 3.8583302] | Test Loss: [2.1215317, 0.30866, 3.9344037]\n",
      "473: Train Loss: [2.1663573, 0.38205445, 3.9506602] | Test Loss: [2.2888958, 0.34166792, 4.2361236]\n",
      "474: Train Loss: [2.24866, 0.29742402, 4.1998963] | Test Loss: [2.1459138, 0.32540378, 3.9664237]\n",
      "475: Train Loss: [2.1031144, 0.3136914, 3.8925374] | Test Loss: [2.1805034, 0.32021037, 4.0407963]\n",
      "476: Train Loss: [2.1048539, 0.3236994, 3.8860083] | Test Loss: [2.2174268, 0.34061256, 4.094241]\n",
      "477: Train Loss: [2.1548362, 0.45013285, 3.8595395] | Test Loss: [2.2137387, 0.37263858, 4.0548387]\n",
      "478: Train Loss: [2.1738503, 0.35090894, 3.9967918] | Test Loss: [2.275363, 0.40341967, 4.1473064]\n",
      "479: Train Loss: [2.073588, 0.3519322, 3.7952437] | Test Loss: [2.1626685, 0.30774945, 4.0175877]\n",
      "480: Train Loss: [2.0538492, 0.3317948, 3.7759035] | Test Loss: [2.2220156, 0.34780666, 4.096225]\n",
      "481: Train Loss: [2.119672, 0.27932593, 3.9600182] | Test Loss: [2.3630784, 0.5580346, 4.1681223]\n",
      "482: Train Loss: [2.0963085, 0.34918675, 3.8434303] | Test Loss: [2.168405, 0.38092154, 3.9558887]\n",
      "483: Train Loss: [2.2231774, 0.37882355, 4.067531] | Test Loss: [2.264005, 0.4067268, 4.121283]\n",
      "484: Train Loss: [2.1735075, 0.34872407, 3.998291] | Test Loss: [2.180341, 0.38654712, 3.974135]\n",
      "485: Train Loss: [2.1880376, 0.33148324, 4.044592] | Test Loss: [2.378118, 0.34700668, 4.4092293]\n",
      "486: Train Loss: [2.01511, 0.34722, 3.683] | Test Loss: [2.0869803, 0.3442024, 3.8297582]\n",
      "487: Train Loss: [2.2226048, 0.40822262, 4.036987] | Test Loss: [2.4073746, 0.32510787, 4.489641]\n",
      "488: Train Loss: [2.1557949, 0.33634964, 3.9752402] | Test Loss: [2.3300152, 0.38582125, 4.274209]\n",
      "489: Train Loss: [2.3923233, 0.36557385, 4.4190726] | Test Loss: [2.266726, 0.35033882, 4.183113]\n",
      "490: Train Loss: [2.1386824, 0.3157694, 3.9615953] | Test Loss: [2.196489, 0.35628122, 4.036697]\n",
      "491: Train Loss: [2.0823164, 0.36899474, 3.795638] | Test Loss: [2.356576, 0.38435704, 4.328795]\n",
      "492: Train Loss: [2.0909805, 0.3613193, 3.8206418] | Test Loss: [2.3018572, 0.35527268, 4.2484417]\n",
      "493: Train Loss: [1.9559238, 0.34329203, 3.5685556] | Test Loss: [2.3173904, 0.34579837, 4.2889824]\n",
      "494: Train Loss: [2.1160321, 0.29216728, 3.939897] | Test Loss: [2.2372973, 0.37077647, 4.103818]\n",
      "495: Train Loss: [2.0369494, 0.28294593, 3.790953] | Test Loss: [2.1726232, 0.36676764, 3.9784784]\n",
      "496: Train Loss: [2.1799235, 0.30058944, 4.0592575] | Test Loss: [2.2286706, 0.34845504, 4.1088862]\n",
      "497: Train Loss: [2.156732, 0.30209422, 4.01137] | Test Loss: [2.2656503, 0.33945766, 4.191843]\n",
      "498: Train Loss: [2.105487, 0.3259929, 3.8849814] | Test Loss: [2.4712205, 0.37539154, 4.5670495]\n",
      "499: Train Loss: [2.208245, 0.3478309, 4.0686593] | Test Loss: [2.238354, 0.36649755, 4.1102104]\n",
      "500: Train Loss: [1.966125, 0.29218328, 3.6400669] | Test Loss: [2.3173273, 0.38026005, 4.2543945]\n",
      "501: Train Loss: [2.2067428, 0.33168992, 4.0817957] | Test Loss: [2.2698805, 0.35011578, 4.1896453]\n",
      "502: Train Loss: [2.0889142, 0.35003817, 3.8277903] | Test Loss: [2.252266, 0.34943983, 4.1550922]\n",
      "503: Train Loss: [2.2304187, 0.33816823, 4.122669] | Test Loss: [2.2233214, 0.3715052, 4.0751376]\n",
      "504: Train Loss: [2.078618, 0.32461405, 3.832622] | Test Loss: [2.211543, 0.30269727, 4.120389]\n",
      "505: Train Loss: [2.0653827, 0.3302334, 3.8005319] | Test Loss: [2.091887, 0.31569374, 3.8680801]\n",
      "506: Train Loss: [2.1341748, 0.34208676, 3.9262629] | Test Loss: [2.1020083, 0.3336377, 3.8703787]\n",
      "507: Train Loss: [2.2575347, 0.44088456, 4.074185] | Test Loss: [2.4537375, 0.32862404, 4.5788507]\n",
      "508: Train Loss: [2.148883, 0.34505996, 3.9527063] | Test Loss: [2.102021, 0.40065402, 3.8033879]\n",
      "509: Train Loss: [2.1770127, 0.30973902, 4.0442863] | Test Loss: [2.2079315, 0.40173265, 4.0141306]\n",
      "510: Train Loss: [2.2143896, 0.32118386, 4.1075954] | Test Loss: [2.2132595, 0.32940146, 4.0971174]\n",
      "511: Train Loss: [2.1794333, 0.3571579, 4.001709] | Test Loss: [2.2509916, 0.2633564, 4.238627]\n",
      "512: Train Loss: [2.2108378, 0.36172727, 4.0599484] | Test Loss: [2.396751, 0.38031054, 4.4131913]\n",
      "513: Train Loss: [2.057532, 0.4050516, 3.7100127] | Test Loss: [2.2694027, 0.44607556, 4.09273]\n",
      "514: Train Loss: [2.0661886, 0.32609588, 3.8062813] | Test Loss: [2.1324804, 0.3914563, 3.8735044]\n",
      "515: Train Loss: [2.2083182, 0.38186637, 4.03477] | Test Loss: [2.168271, 0.29504693, 4.0414953]\n",
      "516: Train Loss: [2.2309873, 0.33177015, 4.1302047] | Test Loss: [2.1995804, 0.37131828, 4.0278425]\n",
      "517: Train Loss: [2.1248972, 0.32140678, 3.9283876] | Test Loss: [2.246977, 0.34047687, 4.153477]\n",
      "518: Train Loss: [2.028273, 0.33308885, 3.7234573] | Test Loss: [2.20424, 0.3132371, 4.095243]\n",
      "519: Train Loss: [2.3727686, 0.43372568, 4.3118114] | Test Loss: [2.1810632, 0.2887664, 4.07336]\n",
      "520: Train Loss: [2.0709033, 0.35711485, 3.7846918] | Test Loss: [2.326539, 0.3514037, 4.3016744]\n",
      "521: Train Loss: [2.142888, 0.39431736, 3.891459] | Test Loss: [2.3798206, 0.37862033, 4.381021]\n",
      "522: Train Loss: [1.9891682, 0.3648496, 3.6134868] | Test Loss: [2.1007133, 0.3220273, 3.8793993]\n",
      "523: Train Loss: [2.1016219, 0.31328878, 3.889955] | Test Loss: [2.2123742, 0.32421824, 4.10053]\n",
      "Epoch 4\n",
      "0: Train Loss: [1.9681513, 0.33956146, 3.5967412] | Test Loss: [2.239539, 0.32923666, 4.1498413]\n",
      "1: Train Loss: [1.935513, 0.3097785, 3.5612476] | Test Loss: [2.4502413, 0.38006842, 4.5204144]\n",
      "2: Train Loss: [1.8130738, 0.30941024, 3.3167372] | Test Loss: [2.2247937, 0.36696228, 4.082625]\n",
      "3: Train Loss: [1.9038564, 0.3291309, 3.478582] | Test Loss: [2.2137017, 0.37844262, 4.0489607]\n",
      "4: Train Loss: [1.8270154, 0.29324296, 3.3607879] | Test Loss: [2.3375838, 0.37878698, 4.2963805]\n",
      "5: Train Loss: [1.989176, 0.30199054, 3.6763616] | Test Loss: [2.194707, 0.30282113, 4.0865927]\n",
      "6: Train Loss: [2.060743, 0.3435608, 3.7779253] | Test Loss: [2.3440566, 0.3384875, 4.3496256]\n",
      "7: Train Loss: [2.0245688, 0.3308098, 3.7183278] | Test Loss: [2.1793816, 0.36628658, 3.9924765]\n",
      "8: Train Loss: [1.9827983, 0.31330657, 3.65229] | Test Loss: [2.2878845, 0.31644204, 4.259327]\n",
      "9: Train Loss: [2.0552506, 0.31303617, 3.797465] | Test Loss: [2.217495, 0.38577655, 4.0492134]\n",
      "10: Train Loss: [1.9067829, 0.3350435, 3.4785223] | Test Loss: [2.2039144, 0.39476272, 4.0130663]\n",
      "11: Train Loss: [1.9335771, 0.3785645, 3.4885895] | Test Loss: [2.1026485, 0.31417438, 3.8911226]\n",
      "12: Train Loss: [1.9767566, 0.42515957, 3.5283535] | Test Loss: [2.3467493, 0.34602416, 4.3474746]\n",
      "13: Train Loss: [1.9030224, 0.36951804, 3.4365268] | Test Loss: [2.2842298, 0.36544517, 4.2030144]\n",
      "14: Train Loss: [2.0047235, 0.4216768, 3.5877702] | Test Loss: [2.1844816, 0.38193095, 3.9870324]\n",
      "15: Train Loss: [1.8931618, 0.35664755, 3.429676] | Test Loss: [2.3220854, 0.34450224, 4.2996683]\n",
      "16: Train Loss: [1.878882, 0.3420369, 3.4157271] | Test Loss: [2.4877276, 0.3720182, 4.603437]\n",
      "17: Train Loss: [2.116674, 0.36410064, 3.8692472] | Test Loss: [2.245967, 0.40490708, 4.0870266]\n",
      "18: Train Loss: [1.8877987, 0.3293162, 3.4462812] | Test Loss: [2.210782, 0.3965514, 4.0250125]\n",
      "19: Train Loss: [2.0714867, 0.52289486, 3.6200786] | Test Loss: [2.2789683, 0.34163755, 4.216299]\n",
      "20: Train Loss: [1.9004226, 0.33090383, 3.4699414] | Test Loss: [2.223569, 0.35582688, 4.091311]\n",
      "21: Train Loss: [1.9597614, 0.31520176, 3.604321] | Test Loss: [2.1020534, 0.2882449, 3.9158618]\n",
      "22: Train Loss: [2.0428119, 0.37178585, 3.713838] | Test Loss: [2.2039335, 0.3907498, 4.017117]\n",
      "23: Train Loss: [1.9420623, 0.3769304, 3.507194] | Test Loss: [2.1164536, 0.3638469, 3.8690603]\n",
      "24: Train Loss: [1.987581, 0.36318743, 3.6119747] | Test Loss: [2.0091934, 0.34607944, 3.6723075]\n",
      "25: Train Loss: [1.9101028, 0.38621628, 3.4339895] | Test Loss: [2.1587903, 0.35641316, 3.9611673]\n",
      "26: Train Loss: [1.87642, 0.43396437, 3.3188758] | Test Loss: [2.0988746, 0.36531052, 3.8324387]\n",
      "27: Train Loss: [2.0606768, 0.3176765, 3.803677] | Test Loss: [2.2435231, 0.32584867, 4.1611977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28: Train Loss: [1.8615898, 0.32890898, 3.3942707] | Test Loss: [2.2939978, 0.38213623, 4.205859]\n",
      "29: Train Loss: [1.8091967, 0.3046199, 3.3137734] | Test Loss: [2.185682, 0.37371668, 3.9976473]\n",
      "30: Train Loss: [2.0621786, 0.24121313, 3.883144] | Test Loss: [2.4326048, 0.38812774, 4.477082]\n",
      "31: Train Loss: [2.0552688, 0.4709896, 3.6395478] | Test Loss: [2.2743514, 0.47783965, 4.0708632]\n",
      "32: Train Loss: [2.0843096, 0.39711517, 3.771504] | Test Loss: [2.319228, 0.3580828, 4.280373]\n",
      "33: Train Loss: [1.9617783, 0.32841465, 3.595142] | Test Loss: [2.1676843, 0.3302961, 4.0050726]\n",
      "34: Train Loss: [1.9677072, 0.28053403, 3.6548803] | Test Loss: [2.5681427, 0.38926822, 4.747017]\n",
      "35: Train Loss: [1.9458764, 0.35277143, 3.5389812] | Test Loss: [2.2832096, 0.31837377, 4.2480454]\n",
      "36: Train Loss: [1.8565389, 0.2981708, 3.414907] | Test Loss: [2.1669629, 0.36145052, 3.9724753]\n",
      "37: Train Loss: [1.9390666, 0.3276792, 3.5504541] | Test Loss: [2.3886518, 0.33884767, 4.438456]\n",
      "38: Train Loss: [1.9925413, 0.33653855, 3.648544] | Test Loss: [2.2203615, 0.34196883, 4.098754]\n",
      "39: Train Loss: [1.9427531, 0.30935216, 3.576154] | Test Loss: [2.371268, 0.3476519, 4.394884]\n",
      "40: Train Loss: [2.0434072, 0.31786904, 3.7689455] | Test Loss: [2.2560284, 0.31591597, 4.196141]\n",
      "41: Train Loss: [1.9775496, 0.43945783, 3.5156412] | Test Loss: [2.3039718, 0.33687073, 4.271073]\n",
      "42: Train Loss: [2.009494, 0.33686706, 3.6821213] | Test Loss: [2.3652139, 0.29832545, 4.432102]\n",
      "43: Train Loss: [1.9826359, 0.31657714, 3.6486945] | Test Loss: [2.2379065, 0.34978223, 4.1260304]\n",
      "44: Train Loss: [2.0598204, 0.3263818, 3.793259] | Test Loss: [2.3065693, 0.32899055, 4.284148]\n",
      "45: Train Loss: [1.9527327, 0.36730576, 3.5381596] | Test Loss: [2.3810804, 0.30930766, 4.452853]\n",
      "46: Train Loss: [1.9944383, 0.42684755, 3.5620291] | Test Loss: [2.3348413, 0.36738884, 4.302294]\n",
      "47: Train Loss: [1.997, 0.38644636, 3.6075537] | Test Loss: [2.4048417, 0.3288701, 4.480813]\n",
      "48: Train Loss: [2.0335658, 0.3344958, 3.7326355] | Test Loss: [2.2052228, 0.33011147, 4.080334]\n",
      "49: Train Loss: [1.9361584, 0.3516229, 3.520694] | Test Loss: [2.3622742, 0.34220105, 4.382347]\n",
      "50: Train Loss: [2.1184132, 0.3829457, 3.853881] | Test Loss: [2.293599, 0.3389966, 4.2482014]\n",
      "51: Train Loss: [2.105458, 0.40636376, 3.804552] | Test Loss: [2.1421034, 0.41267, 3.8715367]\n",
      "52: Train Loss: [1.9007745, 0.34029084, 3.4612582] | Test Loss: [2.221123, 0.34827483, 4.0939713]\n",
      "53: Train Loss: [2.119558, 0.28599313, 3.9531229] | Test Loss: [2.2474701, 0.39623925, 4.098701]\n",
      "54: Train Loss: [1.9984764, 0.34719616, 3.6497567] | Test Loss: [2.1485877, 0.334356, 3.9628193]\n",
      "55: Train Loss: [1.8869348, 0.3243927, 3.4494767] | Test Loss: [2.2230875, 0.45869482, 3.9874804]\n",
      "56: Train Loss: [2.0451343, 0.3344434, 3.755825] | Test Loss: [2.1625772, 0.33181858, 3.9933355]\n",
      "57: Train Loss: [1.9225031, 0.27429006, 3.5707161] | Test Loss: [2.5574794, 0.35530275, 4.759656]\n",
      "58: Train Loss: [1.8866026, 0.31552207, 3.457683] | Test Loss: [2.415084, 0.4118666, 4.418301]\n",
      "59: Train Loss: [1.8671014, 0.29437888, 3.439824] | Test Loss: [2.201144, 0.3251441, 4.0771437]\n",
      "60: Train Loss: [1.9791775, 0.319268, 3.639087] | Test Loss: [2.0170267, 0.32285827, 3.7111952]\n",
      "61: Train Loss: [2.0205564, 0.33903176, 3.702081] | Test Loss: [2.2638624, 0.37038228, 4.1573424]\n",
      "62: Train Loss: [1.8735986, 0.31224075, 3.4349563] | Test Loss: [2.3390226, 0.37601033, 4.302035]\n",
      "63: Train Loss: [2.001341, 0.38975814, 3.612924] | Test Loss: [2.3076913, 0.369007, 4.2463756]\n",
      "64: Train Loss: [2.0603647, 0.36928996, 3.7514393] | Test Loss: [2.2782042, 0.32570007, 4.230708]\n",
      "65: Train Loss: [2.0496094, 0.3237324, 3.7754862] | Test Loss: [2.1824305, 0.39662334, 3.9682379]\n",
      "66: Train Loss: [1.9727879, 0.41371483, 3.5318608] | Test Loss: [2.2124398, 0.37387344, 4.0510063]\n",
      "67: Train Loss: [2.0084605, 0.29341328, 3.723508] | Test Loss: [2.1480927, 0.34168792, 3.9544973]\n",
      "68: Train Loss: [2.0838327, 0.3372705, 3.8303952] | Test Loss: [2.1276963, 0.32124028, 3.9341524]\n",
      "69: Train Loss: [1.8170719, 0.34969148, 3.2844524] | Test Loss: [2.272261, 0.37141114, 4.1731105]\n",
      "70: Train Loss: [2.0076528, 0.38242617, 3.6328795] | Test Loss: [2.3717394, 0.40616024, 4.3373184]\n",
      "71: Train Loss: [2.0518317, 0.37342098, 3.7302425] | Test Loss: [2.352038, 0.32808343, 4.3759923]\n",
      "72: Train Loss: [1.7640297, 0.31722984, 3.2108297] | Test Loss: [2.2597392, 0.34563988, 4.1738386]\n",
      "73: Train Loss: [1.891119, 0.30681074, 3.4754274] | Test Loss: [2.4641597, 0.3208538, 4.6074657]\n",
      "74: Train Loss: [2.0868638, 0.31747687, 3.8562505] | Test Loss: [2.2160404, 0.28160715, 4.1504736]\n",
      "75: Train Loss: [1.9325153, 0.3448493, 3.5201812] | Test Loss: [2.2348485, 0.34583902, 4.123858]\n",
      "76: Train Loss: [1.9727385, 0.26936612, 3.676111] | Test Loss: [2.3323092, 0.4208911, 4.243727]\n",
      "77: Train Loss: [2.0425167, 0.3458514, 3.739182] | Test Loss: [2.2655842, 0.31865457, 4.212514]\n",
      "78: Train Loss: [2.085444, 0.38485807, 3.7860298] | Test Loss: [2.1996324, 0.32044572, 4.0788193]\n",
      "79: Train Loss: [2.126897, 0.38001648, 3.8737779] | Test Loss: [2.1815279, 0.3762778, 3.986778]\n",
      "80: Train Loss: [1.9827472, 0.33451635, 3.630978] | Test Loss: [2.4020472, 0.28992847, 4.514166]\n",
      "81: Train Loss: [2.0590136, 0.298707, 3.8193202] | Test Loss: [2.274484, 0.35457236, 4.1943955]\n",
      "82: Train Loss: [1.9576994, 0.472928, 3.4424708] | Test Loss: [2.1028583, 0.3455233, 3.8601933]\n",
      "83: Train Loss: [2.0681973, 0.3357137, 3.8006809] | Test Loss: [2.1334982, 0.37419567, 3.8928008]\n",
      "84: Train Loss: [2.0788305, 0.3368234, 3.8208375] | Test Loss: [2.3676138, 0.3618628, 4.373365]\n",
      "85: Train Loss: [1.9095657, 0.30690193, 3.5122294] | Test Loss: [2.354368, 0.3520644, 4.3566713]\n",
      "86: Train Loss: [1.9852912, 0.3258791, 3.6447034] | Test Loss: [2.3055394, 0.33508933, 4.2759895]\n",
      "87: Train Loss: [2.1338124, 0.35404298, 3.9135818] | Test Loss: [2.4006178, 0.43298596, 4.36825]\n",
      "88: Train Loss: [1.8457868, 0.30201137, 3.3895624] | Test Loss: [2.389997, 0.33550712, 4.444487]\n",
      "89: Train Loss: [1.850091, 0.28162035, 3.4185617] | Test Loss: [2.1936946, 0.32211566, 4.065274]\n",
      "90: Train Loss: [2.0027664, 0.45924196, 3.5462906] | Test Loss: [2.1378582, 0.2587632, 4.016953]\n",
      "91: Train Loss: [2.01304, 0.38988855, 3.6361914] | Test Loss: [2.236232, 0.40533218, 4.067132]\n",
      "92: Train Loss: [1.9438069, 0.3709656, 3.5166483] | Test Loss: [2.2351363, 0.500171, 3.9701014]\n",
      "93: Train Loss: [1.9454994, 0.3992035, 3.4917953] | Test Loss: [2.4709222, 0.3994938, 4.542351]\n",
      "94: Train Loss: [2.192266, 0.36854166, 4.0159903] | Test Loss: [2.3766594, 0.39159775, 4.361721]\n",
      "95: Train Loss: [1.8980362, 0.37239438, 3.4236782] | Test Loss: [2.2929335, 0.32968634, 4.256181]\n",
      "96: Train Loss: [1.9549185, 0.3536459, 3.5561912] | Test Loss: [2.0806415, 0.309235, 3.852048]\n",
      "97: Train Loss: [2.0518239, 0.31997722, 3.7836704] | Test Loss: [2.265953, 0.3369114, 4.194995]\n",
      "98: Train Loss: [2.0315876, 0.33421767, 3.7289574] | Test Loss: [2.2532418, 0.39625177, 4.110232]\n",
      "99: Train Loss: [2.0062327, 0.41492262, 3.5975428] | Test Loss: [2.2766833, 0.34139487, 4.2119718]\n",
      "100: Train Loss: [2.1016302, 0.34522468, 3.8580358] | Test Loss: [2.1617055, 0.3173047, 4.0061064]\n",
      "101: Train Loss: [2.2100077, 0.39512914, 4.024886] | Test Loss: [2.3869336, 0.37721622, 4.396651]\n",
      "102: Train Loss: [2.2737374, 0.351316, 4.196159] | Test Loss: [2.1632915, 0.40275505, 3.923828]\n",
      "103: Train Loss: [1.9952376, 0.3182002, 3.672275] | Test Loss: [2.4095554, 0.38936937, 4.4297414]\n",
      "104: Train Loss: [2.0682707, 0.4417853, 3.694756] | Test Loss: [2.193425, 0.35278246, 4.0340676]\n",
      "105: Train Loss: [2.0416508, 0.38668296, 3.6966186] | Test Loss: [2.1832697, 0.39654148, 3.9699981]\n",
      "106: Train Loss: [1.9734254, 0.37550363, 3.5713472] | Test Loss: [2.3705153, 0.36370245, 4.3773284]\n",
      "107: Train Loss: [1.9532845, 0.37672603, 3.5298429] | Test Loss: [2.2173936, 0.3418779, 4.0929093]\n",
      "108: Train Loss: [2.0820053, 0.33657917, 3.8274314] | Test Loss: [2.0832741, 0.33578008, 3.8307683]\n",
      "109: Train Loss: [2.1132934, 0.36542663, 3.86116] | Test Loss: [2.3226118, 0.398365, 4.2468586]\n",
      "110: Train Loss: [1.9088203, 0.36186498, 3.4557755] | Test Loss: [2.1268082, 0.44901285, 3.8046036]\n",
      "111: Train Loss: [2.0341828, 0.3828586, 3.685507] | Test Loss: [2.2981954, 0.37307, 4.223321]\n",
      "112: Train Loss: [2.0202894, 0.35336757, 3.6872115] | Test Loss: [2.184276, 0.46432924, 3.9042232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: Train Loss: [1.8933327, 0.30979645, 3.476869] | Test Loss: [2.423891, 0.32865834, 4.519124]\n",
      "114: Train Loss: [2.0823698, 0.3056693, 3.8590703] | Test Loss: [2.368707, 0.35913143, 4.3782825]\n",
      "115: Train Loss: [2.08697, 0.44922718, 3.7247128] | Test Loss: [2.3472567, 0.38214076, 4.3123727]\n",
      "116: Train Loss: [2.1651974, 0.35255414, 3.9778407] | Test Loss: [2.1614292, 0.3439654, 3.978893]\n",
      "117: Train Loss: [2.0622633, 0.3121029, 3.8124235] | Test Loss: [2.3238513, 0.33276895, 4.314934]\n",
      "118: Train Loss: [2.1313329, 0.3595401, 3.9031258] | Test Loss: [2.2699714, 0.33726907, 4.2026734]\n",
      "119: Train Loss: [1.9075763, 0.29727152, 3.5178812] | Test Loss: [2.162965, 0.31614175, 4.0097885]\n",
      "120: Train Loss: [2.04875, 0.49002782, 3.607472] | Test Loss: [2.2694206, 0.32728693, 4.2115545]\n",
      "121: Train Loss: [2.078896, 0.37397954, 3.7838125] | Test Loss: [2.5003324, 0.31726786, 4.683397]\n",
      "122: Train Loss: [2.077848, 0.31999496, 3.835701] | Test Loss: [2.0975473, 0.36242265, 3.832672]\n",
      "123: Train Loss: [2.0221007, 0.32948148, 3.71472] | Test Loss: [2.2815435, 0.33171776, 4.231369]\n",
      "124: Train Loss: [2.0620804, 0.31260186, 3.811559] | Test Loss: [2.3459942, 0.31462896, 4.3773594]\n",
      "125: Train Loss: [1.9829354, 0.34190446, 3.6239665] | Test Loss: [2.395698, 0.4746699, 4.316726]\n",
      "126: Train Loss: [1.9770547, 0.4136708, 3.5404387] | Test Loss: [2.206196, 0.32128307, 4.0911093]\n",
      "127: Train Loss: [2.0966537, 0.31005928, 3.8832483] | Test Loss: [2.2372448, 0.3731104, 4.1013794]\n",
      "128: Train Loss: [2.0562093, 0.37454432, 3.7378745] | Test Loss: [2.4206092, 0.33555186, 4.5056667]\n",
      "129: Train Loss: [2.0057957, 0.26862723, 3.742964] | Test Loss: [2.3932106, 0.38980505, 4.3966165]\n",
      "130: Train Loss: [1.9131229, 0.31374237, 3.5125034] | Test Loss: [2.2306113, 0.30702934, 4.1541934]\n",
      "131: Train Loss: [2.045365, 0.3328815, 3.7578487] | Test Loss: [2.0795455, 0.38256747, 3.7765234]\n",
      "132: Train Loss: [2.0005167, 0.34126797, 3.6597655] | Test Loss: [2.3156447, 0.3046531, 4.3266363]\n",
      "133: Train Loss: [1.880046, 0.33394584, 3.4261463] | Test Loss: [2.1827374, 0.37568474, 3.98979]\n",
      "134: Train Loss: [2.0361252, 0.36052054, 3.71173] | Test Loss: [2.1276588, 0.30115938, 3.9541583]\n",
      "135: Train Loss: [2.0298882, 0.3469197, 3.7128565] | Test Loss: [2.371825, 0.34121263, 4.402437]\n",
      "136: Train Loss: [2.1650376, 0.38613996, 3.9439352] | Test Loss: [2.1981072, 0.24252802, 4.1536865]\n",
      "137: Train Loss: [1.9948865, 0.31344268, 3.6763303] | Test Loss: [2.107774, 0.31117806, 3.90437]\n",
      "138: Train Loss: [2.020641, 0.3128709, 3.7284114] | Test Loss: [2.1498702, 0.28974506, 4.0099955]\n",
      "139: Train Loss: [2.0568135, 0.31368324, 3.7999437] | Test Loss: [2.2298627, 0.42364573, 4.0360794]\n",
      "140: Train Loss: [1.9110893, 0.30799338, 3.5141852] | Test Loss: [2.3057501, 0.38116074, 4.2303395]\n",
      "141: Train Loss: [2.037466, 0.4189322, 3.6560001] | Test Loss: [2.3870106, 0.39788198, 4.376139]\n",
      "142: Train Loss: [2.1057317, 0.38471496, 3.8267484] | Test Loss: [2.1601706, 0.32083714, 3.999504]\n",
      "143: Train Loss: [1.9164928, 0.28546792, 3.5475178] | Test Loss: [2.2935386, 0.34095547, 4.246122]\n",
      "144: Train Loss: [2.0072374, 0.35790446, 3.6565702] | Test Loss: [2.1984293, 0.33262822, 4.0642304]\n",
      "145: Train Loss: [2.134577, 0.31185722, 3.9572968] | Test Loss: [2.2795873, 0.44540924, 4.1137652]\n",
      "146: Train Loss: [1.9359236, 0.3540263, 3.5178208] | Test Loss: [2.288809, 0.40489176, 4.172726]\n",
      "147: Train Loss: [2.0527937, 0.5253102, 3.5802772] | Test Loss: [2.3849735, 0.3974787, 4.3724685]\n",
      "148: Train Loss: [1.9191248, 0.335642, 3.5026076] | Test Loss: [2.2638476, 0.4005831, 4.127112]\n",
      "149: Train Loss: [2.0376625, 0.35856414, 3.716761] | Test Loss: [2.34924, 0.3222381, 4.376242]\n",
      "150: Train Loss: [2.0518208, 0.33642873, 3.7672129] | Test Loss: [2.1829755, 0.31474814, 4.051203]\n",
      "151: Train Loss: [1.9981242, 0.37159345, 3.624655] | Test Loss: [2.2658122, 0.31301528, 4.218609]\n",
      "152: Train Loss: [2.2686646, 0.34380612, 4.193523] | Test Loss: [2.2789855, 0.34994084, 4.20803]\n",
      "153: Train Loss: [2.0679078, 0.30166692, 3.8341486] | Test Loss: [2.1410232, 0.30607867, 3.9759674]\n",
      "154: Train Loss: [1.977153, 0.30767202, 3.6466339] | Test Loss: [2.3091419, 0.4322762, 4.1860075]\n",
      "155: Train Loss: [2.1869087, 0.42701215, 3.9468052] | Test Loss: [2.241825, 0.4151373, 4.068513]\n",
      "156: Train Loss: [1.9700769, 0.25791866, 3.6822352] | Test Loss: [2.1924355, 0.34150413, 4.043367]\n",
      "157: Train Loss: [1.9509828, 0.2929097, 3.609056] | Test Loss: [2.2759519, 0.36427465, 4.187629]\n",
      "158: Train Loss: [2.0362692, 0.36748272, 3.7050555] | Test Loss: [2.1835017, 0.29835945, 4.068644]\n",
      "159: Train Loss: [2.1597512, 0.37971273, 3.9397895] | Test Loss: [2.3307874, 0.2885355, 4.3730392]\n",
      "160: Train Loss: [1.9655719, 0.34331667, 3.587827] | Test Loss: [2.2130997, 0.34479624, 4.0814033]\n",
      "161: Train Loss: [1.953941, 0.34454176, 3.5633402] | Test Loss: [2.1985428, 0.34986055, 4.047225]\n",
      "162: Train Loss: [1.9280838, 0.36044067, 3.4957268] | Test Loss: [2.5088778, 0.33029187, 4.6874638]\n",
      "163: Train Loss: [2.0351474, 0.34185588, 3.7284389] | Test Loss: [2.42313, 0.35857257, 4.4876876]\n",
      "164: Train Loss: [2.0061505, 0.32599625, 3.6863046] | Test Loss: [2.336578, 0.36235315, 4.3108025]\n",
      "165: Train Loss: [1.9505496, 0.38086164, 3.5202374] | Test Loss: [2.3674152, 0.42204925, 4.3127813]\n",
      "166: Train Loss: [1.9621525, 0.30285907, 3.621446] | Test Loss: [2.455298, 0.3022323, 4.6083636]\n",
      "167: Train Loss: [1.919343, 0.29686725, 3.5418186] | Test Loss: [2.3774898, 0.30195078, 4.4530287]\n",
      "168: Train Loss: [2.0789127, 0.28578663, 3.8720388] | Test Loss: [2.342627, 0.32311454, 4.3621397]\n",
      "169: Train Loss: [2.1309228, 0.4155643, 3.8462815] | Test Loss: [2.1717584, 0.3863942, 3.9571228]\n",
      "170: Train Loss: [2.0438526, 0.34818855, 3.7395167] | Test Loss: [2.1487923, 0.41441736, 3.883167]\n",
      "171: Train Loss: [2.0756187, 0.27689716, 3.8743405] | Test Loss: [2.2865741, 0.32724226, 4.245906]\n",
      "172: Train Loss: [1.9836041, 0.35285217, 3.614356] | Test Loss: [2.3847718, 0.4771106, 4.2924333]\n",
      "173: Train Loss: [1.9639419, 0.35718757, 3.5706964] | Test Loss: [2.336598, 0.37668765, 4.2965083]\n",
      "174: Train Loss: [1.9786544, 0.3235728, 3.633736] | Test Loss: [2.209465, 0.3443798, 4.07455]\n",
      "175: Train Loss: [2.0816011, 0.41789076, 3.7453115] | Test Loss: [2.345647, 0.32376188, 4.3675323]\n",
      "176: Train Loss: [2.0615065, 0.41621295, 3.7068] | Test Loss: [2.2451084, 0.33280176, 4.157415]\n",
      "177: Train Loss: [2.0247617, 0.32023242, 3.729291] | Test Loss: [2.3070862, 0.39872798, 4.2154446]\n",
      "178: Train Loss: [1.9353067, 0.33397913, 3.5366342] | Test Loss: [2.2336822, 0.38545775, 4.081907]\n",
      "179: Train Loss: [2.0575895, 0.29703188, 3.8181472] | Test Loss: [2.3687706, 0.33085662, 4.4066844]\n",
      "180: Train Loss: [1.9626081, 0.30798036, 3.617236] | Test Loss: [2.176256, 0.41576782, 3.936744]\n",
      "181: Train Loss: [2.1639726, 0.47095263, 3.8569925] | Test Loss: [2.2145636, 0.30582866, 4.1232986]\n",
      "182: Train Loss: [2.1908064, 0.38301086, 3.9986017] | Test Loss: [2.236992, 0.35560828, 4.1183753]\n",
      "183: Train Loss: [1.9604063, 0.35121143, 3.5696013] | Test Loss: [2.195611, 0.33667818, 4.054544]\n",
      "184: Train Loss: [2.1128876, 0.37618923, 3.849586] | Test Loss: [2.225389, 0.34769985, 4.1030784]\n",
      "185: Train Loss: [2.077219, 0.36905083, 3.785387] | Test Loss: [2.2645497, 0.38853693, 4.1405625]\n",
      "186: Train Loss: [2.0466998, 0.38298592, 3.7104135] | Test Loss: [2.4289207, 0.3418188, 4.5160227]\n",
      "187: Train Loss: [2.122397, 0.35906103, 3.885733] | Test Loss: [2.2881534, 0.36602467, 4.2102823]\n",
      "188: Train Loss: [2.013369, 0.37144822, 3.65529] | Test Loss: [2.3042972, 0.37021443, 4.23838]\n",
      "189: Train Loss: [2.082333, 0.30993095, 3.8547351] | Test Loss: [2.1604295, 0.33125055, 3.9896083]\n",
      "190: Train Loss: [1.9189882, 0.26830977, 3.5696666] | Test Loss: [2.2925823, 0.34749767, 4.237667]\n",
      "191: Train Loss: [2.0729284, 0.24518329, 3.9006734] | Test Loss: [2.0600438, 0.31035087, 3.809737]\n",
      "192: Train Loss: [2.0689156, 0.3627982, 3.775033] | Test Loss: [2.5148675, 0.36490643, 4.664829]\n",
      "193: Train Loss: [2.0467327, 0.41910067, 3.6743646] | Test Loss: [2.3558364, 0.43728453, 4.2743883]\n",
      "194: Train Loss: [2.0781815, 0.40972382, 3.7466393] | Test Loss: [2.3083394, 0.35593095, 4.260748]\n",
      "195: Train Loss: [2.067823, 0.29959768, 3.8360484] | Test Loss: [2.2648604, 0.3481472, 4.1815734]\n",
      "196: Train Loss: [2.157646, 0.32934836, 3.9859433] | Test Loss: [2.228245, 0.46923584, 3.9872544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197: Train Loss: [1.8371915, 0.35322124, 3.3211617] | Test Loss: [2.1588259, 0.3832175, 3.9344342]\n",
      "198: Train Loss: [2.0393777, 0.31319672, 3.7655587] | Test Loss: [2.0943694, 0.34220898, 3.84653]\n",
      "199: Train Loss: [2.135599, 0.34972125, 3.9214766] | Test Loss: [2.3662703, 0.32597858, 4.406562]\n",
      "200: Train Loss: [2.0867865, 0.35828888, 3.8152843] | Test Loss: [2.2453296, 0.3811348, 4.1095243]\n",
      "201: Train Loss: [2.130211, 0.30487636, 3.955546] | Test Loss: [2.4343147, 0.33994687, 4.5286827]\n",
      "202: Train Loss: [2.0636837, 0.39524022, 3.7321274] | Test Loss: [2.3454845, 0.41972977, 4.2712393]\n",
      "203: Train Loss: [2.041696, 0.4222914, 3.6611009] | Test Loss: [2.3324525, 0.35924897, 4.305656]\n",
      "204: Train Loss: [2.0866444, 0.35883805, 3.8144507] | Test Loss: [2.2818465, 0.3112452, 4.252448]\n",
      "205: Train Loss: [2.0200443, 0.36361954, 3.6764693] | Test Loss: [2.4282568, 0.33306545, 4.523448]\n",
      "206: Train Loss: [2.081183, 0.33179194, 3.830574] | Test Loss: [2.2771633, 0.35435054, 4.199976]\n",
      "207: Train Loss: [2.0761201, 0.38128048, 3.7709596] | Test Loss: [2.1663117, 0.36616206, 3.9664617]\n",
      "208: Train Loss: [2.1335442, 0.3094557, 3.9576325] | Test Loss: [2.3682504, 0.3955475, 4.3409534]\n",
      "209: Train Loss: [2.0916183, 0.42678943, 3.756447] | Test Loss: [2.1887755, 0.3049087, 4.0726423]\n",
      "210: Train Loss: [2.1196992, 0.36694443, 3.8724542] | Test Loss: [2.3588834, 0.3719479, 4.345819]\n",
      "211: Train Loss: [2.0831316, 0.34134948, 3.8249135] | Test Loss: [2.3219929, 0.36198622, 4.2819996]\n",
      "212: Train Loss: [2.002366, 0.37504098, 3.6296911] | Test Loss: [2.291483, 0.3411673, 4.2417984]\n",
      "213: Train Loss: [1.9409065, 0.33198857, 3.5498245] | Test Loss: [2.18893, 0.3491552, 4.028705]\n",
      "214: Train Loss: [1.9251777, 0.3409302, 3.5094252] | Test Loss: [2.2543578, 0.37159276, 4.137123]\n",
      "215: Train Loss: [1.9721419, 0.40856627, 3.5357175] | Test Loss: [2.2970376, 0.387701, 4.206374]\n",
      "216: Train Loss: [2.0889966, 0.41656134, 3.7614322] | Test Loss: [2.1379359, 0.38236636, 3.8935053]\n",
      "217: Train Loss: [2.119253, 0.26085642, 3.9776492] | Test Loss: [2.1392503, 0.3558302, 3.9226706]\n",
      "218: Train Loss: [2.0299056, 0.3180332, 3.741778] | Test Loss: [2.3380625, 0.32848927, 4.3476357]\n",
      "219: Train Loss: [2.0679646, 0.35430878, 3.7816203] | Test Loss: [2.2560227, 0.2662946, 4.245751]\n",
      "220: Train Loss: [1.9714279, 0.2960503, 3.6468055] | Test Loss: [2.3104851, 0.4175217, 4.203449]\n",
      "221: Train Loss: [2.106163, 0.27633774, 3.9359884] | Test Loss: [2.4300613, 0.37600037, 4.4841223]\n",
      "222: Train Loss: [2.1017501, 0.30884165, 3.8946588] | Test Loss: [2.2289722, 0.36511433, 4.09283]\n",
      "223: Train Loss: [1.9497111, 0.31146523, 3.587957] | Test Loss: [2.2706153, 0.34968677, 4.191544]\n",
      "224: Train Loss: [2.1284542, 0.39380664, 3.863102] | Test Loss: [2.2348635, 0.41631737, 4.0534096]\n",
      "225: Train Loss: [1.9833251, 0.2961831, 3.6704671] | Test Loss: [2.3026826, 0.29462045, 4.310745]\n",
      "226: Train Loss: [2.1289935, 0.33963576, 3.9183514] | Test Loss: [2.4221249, 0.42136306, 4.422887]\n",
      "227: Train Loss: [1.9636438, 0.3492308, 3.5780568] | Test Loss: [2.3105187, 0.3577249, 4.263313]\n",
      "228: Train Loss: [2.0396714, 0.32963428, 3.7497084] | Test Loss: [2.2220366, 0.36676747, 4.077306]\n",
      "229: Train Loss: [1.999636, 0.35479385, 3.6444783] | Test Loss: [2.2421365, 0.34083351, 4.1434393]\n",
      "230: Train Loss: [2.1208186, 0.4754943, 3.7661428] | Test Loss: [2.404471, 0.37649873, 4.432443]\n",
      "231: Train Loss: [2.1301928, 0.32928547, 3.9311001] | Test Loss: [2.2599719, 0.3726147, 4.147329]\n",
      "232: Train Loss: [2.0725474, 0.33374682, 3.8113482] | Test Loss: [2.2628653, 0.36330038, 4.1624303]\n",
      "233: Train Loss: [1.9802027, 0.29460436, 3.665801] | Test Loss: [2.2424252, 0.32364374, 4.1612067]\n",
      "234: Train Loss: [2.1555932, 0.40477288, 3.9064136] | Test Loss: [2.3151436, 0.38006812, 4.250219]\n",
      "235: Train Loss: [2.0828948, 0.31296122, 3.8528283] | Test Loss: [2.3842366, 0.33252063, 4.4359527]\n",
      "236: Train Loss: [2.1514, 0.316051, 3.9867492] | Test Loss: [2.3116276, 0.33984503, 4.28341]\n",
      "237: Train Loss: [2.062166, 0.30638468, 3.8179474] | Test Loss: [2.2281125, 0.26290292, 4.193322]\n",
      "238: Train Loss: [2.0217988, 0.3841566, 3.6594412] | Test Loss: [2.2335749, 0.3990481, 4.068102]\n",
      "239: Train Loss: [2.0113025, 0.31293806, 3.709667] | Test Loss: [2.2147062, 0.37747306, 4.0519395]\n",
      "240: Train Loss: [2.1629746, 0.35107544, 3.9748735] | Test Loss: [2.5454664, 0.4101935, 4.6807394]\n",
      "241: Train Loss: [2.203609, 0.3888695, 4.0183487] | Test Loss: [2.2519178, 0.36684543, 4.13699]\n",
      "242: Train Loss: [2.2508614, 0.3730673, 4.1286554] | Test Loss: [2.3620949, 0.37702173, 4.347168]\n",
      "243: Train Loss: [1.9804358, 0.33341265, 3.627459] | Test Loss: [2.3051128, 0.35155067, 4.258675]\n",
      "244: Train Loss: [1.9695787, 0.3739875, 3.56517] | Test Loss: [2.1550603, 0.39093143, 3.9191892]\n",
      "245: Train Loss: [1.9298406, 0.36954233, 3.4901388] | Test Loss: [2.3065548, 0.35162932, 4.2614803]\n",
      "246: Train Loss: [2.0769813, 0.29489076, 3.8590717] | Test Loss: [2.2370403, 0.31657633, 4.157504]\n",
      "247: Train Loss: [2.1319854, 0.306625, 3.957346] | Test Loss: [2.2345986, 0.41729465, 4.051903]\n",
      "248: Train Loss: [2.0377793, 0.35226, 3.7232985] | Test Loss: [2.1929753, 0.2856415, 4.100309]\n",
      "249: Train Loss: [2.0071642, 0.351862, 3.6624663] | Test Loss: [2.2659655, 0.38997564, 4.1419554]\n",
      "250: Train Loss: [1.9477023, 0.36275476, 3.5326498] | Test Loss: [2.4279773, 0.3357886, 4.520166]\n",
      "251: Train Loss: [2.1663063, 0.43575305, 3.8968594] | Test Loss: [2.241902, 0.41893116, 4.064873]\n",
      "252: Train Loss: [2.13864, 0.33146322, 3.9458168] | Test Loss: [2.320621, 0.3622167, 4.279025]\n",
      "253: Train Loss: [2.05681, 0.33509403, 3.7785258] | Test Loss: [2.27777, 0.38262475, 4.1729155]\n",
      "254: Train Loss: [2.0815094, 0.3611572, 3.8018615] | Test Loss: [2.3256533, 0.32746077, 4.323846]\n",
      "255: Train Loss: [2.1889567, 0.3135626, 4.064351] | Test Loss: [2.368382, 0.3323058, 4.404458]\n",
      "256: Train Loss: [2.13056, 0.32166255, 3.9394574] | Test Loss: [2.113691, 0.30279168, 3.9245906]\n",
      "257: Train Loss: [2.0061095, 0.37604028, 3.6361787] | Test Loss: [2.0929856, 0.37903425, 3.806937]\n",
      "258: Train Loss: [2.1485882, 0.39189002, 3.9052866] | Test Loss: [2.5151, 0.51250404, 4.517696]\n",
      "259: Train Loss: [2.1400075, 0.36552382, 3.9144912] | Test Loss: [2.2620473, 0.35791332, 4.166181]\n",
      "260: Train Loss: [2.1144671, 0.30380452, 3.9251297] | Test Loss: [2.3357291, 0.33300832, 4.33845]\n",
      "261: Train Loss: [2.100973, 0.30000323, 3.9019425] | Test Loss: [2.3003068, 0.3197096, 4.280904]\n",
      "262: Train Loss: [1.994038, 0.30065107, 3.687425] | Test Loss: [2.3273363, 0.3489387, 4.3057337]\n",
      "263: Train Loss: [2.08475, 0.30241635, 3.8670835] | Test Loss: [2.2132752, 0.3686881, 4.0578623]\n",
      "264: Train Loss: [2.027205, 0.3262987, 3.728111] | Test Loss: [2.2180228, 0.31442285, 4.121623]\n",
      "265: Train Loss: [2.14761, 0.33880532, 3.9564145] | Test Loss: [2.3345938, 0.32439452, 4.344793]\n",
      "266: Train Loss: [2.2079177, 0.39437303, 4.0214624] | Test Loss: [2.2058778, 0.30117288, 4.110583]\n",
      "267: Train Loss: [2.2416348, 0.41645634, 4.0668135] | Test Loss: [2.3004282, 0.3400338, 4.2608223]\n",
      "268: Train Loss: [2.1354902, 0.3326364, 3.938344] | Test Loss: [2.4777324, 0.37864402, 4.576821]\n",
      "269: Train Loss: [2.260547, 0.34657732, 4.1745167] | Test Loss: [2.3860292, 0.37132052, 4.400738]\n",
      "270: Train Loss: [2.242716, 0.42389494, 4.0615373] | Test Loss: [2.3360417, 0.3393251, 4.3327584]\n",
      "271: Train Loss: [2.1538951, 0.34933457, 3.9584558] | Test Loss: [2.3637223, 0.3813364, 4.3461084]\n",
      "272: Train Loss: [2.2858293, 0.31227523, 4.259383] | Test Loss: [2.4903316, 0.39170524, 4.5889583]\n",
      "273: Train Loss: [2.0131972, 0.33816618, 3.6882284] | Test Loss: [2.387475, 0.3901903, 4.38476]\n",
      "274: Train Loss: [2.2444139, 0.4477131, 4.041115] | Test Loss: [2.315368, 0.4938332, 4.136903]\n",
      "275: Train Loss: [2.1809688, 0.337487, 4.0244503] | Test Loss: [2.2221224, 0.33278936, 4.1114554]\n",
      "276: Train Loss: [2.0635939, 0.32420665, 3.802981] | Test Loss: [2.295316, 0.35247993, 4.238152]\n",
      "277: Train Loss: [2.0857935, 0.34320548, 3.8283813] | Test Loss: [2.5765138, 0.3352859, 4.8177414]\n",
      "278: Train Loss: [2.2159617, 0.29428232, 4.137641] | Test Loss: [2.3846147, 0.4712203, 4.298009]\n",
      "279: Train Loss: [2.1581843, 0.31182554, 4.004543] | Test Loss: [2.3444862, 0.41207936, 4.276893]\n",
      "280: Train Loss: [2.2188134, 0.31138128, 4.1262455] | Test Loss: [2.1871061, 0.30405188, 4.0701604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281: Train Loss: [2.260941, 0.3614435, 4.1604385] | Test Loss: [2.312114, 0.33658066, 4.2876472]\n",
      "282: Train Loss: [2.1712248, 0.3318267, 4.010623] | Test Loss: [2.1880941, 0.31327337, 4.062915]\n",
      "283: Train Loss: [2.1663268, 0.30640456, 4.026249] | Test Loss: [2.4593546, 0.30757716, 4.611132]\n",
      "284: Train Loss: [2.0590723, 0.26148474, 3.8566597] | Test Loss: [2.3736322, 0.3212441, 4.42602]\n",
      "285: Train Loss: [2.0489118, 0.31316283, 3.7846606] | Test Loss: [2.4460397, 0.3580982, 4.5339813]\n",
      "286: Train Loss: [2.074566, 0.3627528, 3.7863789] | Test Loss: [2.2992766, 0.42022127, 4.178332]\n",
      "287: Train Loss: [2.2312438, 0.3205087, 4.141979] | Test Loss: [2.3778305, 0.3995304, 4.3561306]\n",
      "288: Train Loss: [2.287036, 0.39610156, 4.1779704] | Test Loss: [2.3107688, 0.3721733, 4.2493644]\n",
      "289: Train Loss: [2.2167673, 0.3332193, 4.100315] | Test Loss: [2.2540123, 0.33646232, 4.171562]\n",
      "290: Train Loss: [2.0687027, 0.34285662, 3.794549] | Test Loss: [2.3451755, 0.3734894, 4.3168616]\n",
      "291: Train Loss: [2.134102, 0.3106241, 3.95758] | Test Loss: [2.4335918, 0.36866805, 4.4985156]\n",
      "292: Train Loss: [2.112034, 0.32871646, 3.895352] | Test Loss: [2.4677348, 0.3342474, 4.601222]\n",
      "293: Train Loss: [2.2122927, 0.36461952, 4.0599656] | Test Loss: [2.2620413, 0.33315986, 4.1909227]\n",
      "294: Train Loss: [2.2758305, 0.3376948, 4.2139664] | Test Loss: [2.2196271, 0.3585677, 4.0806866]\n",
      "295: Train Loss: [2.2669506, 0.41540027, 4.1185007] | Test Loss: [2.3121738, 0.36141902, 4.2629285]\n",
      "296: Train Loss: [2.1780992, 0.39068636, 3.9655118] | Test Loss: [2.1664321, 0.3788777, 3.9539864]\n",
      "297: Train Loss: [2.1823144, 0.330574, 4.0340548] | Test Loss: [2.271584, 0.37457642, 4.1685915]\n",
      "298: Train Loss: [2.1922011, 0.2977144, 4.086688] | Test Loss: [2.3126698, 0.3752118, 4.250128]\n",
      "299: Train Loss: [2.0974503, 0.33508438, 3.8598163] | Test Loss: [2.2783482, 0.3702505, 4.1864457]\n",
      "300: Train Loss: [2.1144314, 0.3146879, 3.9141748] | Test Loss: [2.310436, 0.40829122, 4.2125807]\n",
      "301: Train Loss: [2.135966, 0.37013867, 3.9017932] | Test Loss: [2.5976064, 0.36351067, 4.831702]\n",
      "302: Train Loss: [2.278394, 0.46659935, 4.0901885] | Test Loss: [2.5862088, 0.34091628, 4.8315015]\n",
      "303: Train Loss: [2.3317974, 0.35103512, 4.3125596] | Test Loss: [2.296051, 0.33936596, 4.252736]\n",
      "304: Train Loss: [2.18209, 0.34951103, 4.014669] | Test Loss: [2.416445, 0.33901078, 4.4938793]\n",
      "305: Train Loss: [2.0159624, 0.30498904, 3.7269356] | Test Loss: [2.224794, 0.2803016, 4.1692863]\n",
      "306: Train Loss: [2.0602329, 0.31179106, 3.8086748] | Test Loss: [2.2989995, 0.2960823, 4.3019166]\n",
      "307: Train Loss: [2.0409098, 0.3230871, 3.7587326] | Test Loss: [2.2195332, 0.42200345, 4.017063]\n",
      "308: Train Loss: [2.0501719, 0.34821194, 3.752132] | Test Loss: [2.361903, 0.3555071, 4.368299]\n",
      "309: Train Loss: [2.151353, 0.34436655, 3.9583395] | Test Loss: [2.5153067, 0.42966458, 4.600949]\n",
      "310: Train Loss: [2.4952917, 0.34587187, 4.6447115] | Test Loss: [2.5857303, 0.40814978, 4.763311]\n",
      "311: Train Loss: [2.1323488, 0.31798962, 3.9467077] | Test Loss: [2.311251, 0.3074804, 4.3150215]\n",
      "312: Train Loss: [2.38114, 0.48419657, 4.2780833] | Test Loss: [2.3519106, 0.36224777, 4.341573]\n",
      "313: Train Loss: [2.1707294, 0.2933161, 4.048143] | Test Loss: [2.4552794, 0.35381642, 4.556742]\n",
      "314: Train Loss: [2.0742612, 0.40219954, 3.7463229] | Test Loss: [2.445308, 0.44595706, 4.4446588]\n",
      "315: Train Loss: [2.4634428, 0.36192966, 4.5649557] | Test Loss: [2.213047, 0.32484573, 4.1012483]\n",
      "316: Train Loss: [2.2210886, 0.3153965, 4.126781] | Test Loss: [1.9784285, 0.5098389, 3.4470181]\n",
      "317: Train Loss: [2.216471, 0.37727436, 4.0556674] | Test Loss: [2.2166038, 0.3526086, 4.080599]\n",
      "318: Train Loss: [2.0791333, 0.344893, 3.8133736] | Test Loss: [2.5336359, 0.3534662, 4.7138057]\n",
      "319: Train Loss: [2.2152495, 0.31882897, 4.11167] | Test Loss: [2.3524034, 0.32309473, 4.381712]\n",
      "320: Train Loss: [2.1622264, 0.2868891, 4.037564] | Test Loss: [2.3288248, 0.36153165, 4.296118]\n",
      "321: Train Loss: [2.0613348, 0.29256788, 3.8301017] | Test Loss: [2.3035386, 0.43273205, 4.174345]\n",
      "322: Train Loss: [2.1695323, 0.39803767, 3.9410272] | Test Loss: [2.2834797, 0.33624986, 4.2307096]\n",
      "323: Train Loss: [2.3417492, 0.40240663, 4.2810917] | Test Loss: [2.205714, 0.33606642, 4.0753617]\n",
      "324: Train Loss: [2.164524, 0.38457587, 3.944472] | Test Loss: [2.3816342, 0.30510214, 4.458166]\n",
      "325: Train Loss: [2.2523699, 0.33690095, 4.1678386] | Test Loss: [2.3372934, 0.39842865, 4.2761583]\n",
      "326: Train Loss: [2.2008958, 0.37040424, 4.0313873] | Test Loss: [2.3853621, 0.3122007, 4.4585238]\n",
      "327: Train Loss: [2.2445457, 0.3817201, 4.1073713] | Test Loss: [2.253464, 0.32632294, 4.180605]\n",
      "328: Train Loss: [2.2472496, 0.36897793, 4.125521] | Test Loss: [2.3737257, 0.3504737, 4.3969774]\n",
      "329: Train Loss: [2.320385, 0.3476254, 4.2931447] | Test Loss: [2.2241836, 0.3162396, 4.132128]\n",
      "330: Train Loss: [2.2139807, 0.30868566, 4.1192756] | Test Loss: [2.4548352, 0.42882556, 4.480845]\n",
      "331: Train Loss: [2.261132, 0.39870793, 4.123556] | Test Loss: [2.5235167, 0.45921132, 4.587822]\n",
      "332: Train Loss: [2.1469538, 0.34012207, 3.9537857] | Test Loss: [2.2102015, 0.4162522, 4.004151]\n",
      "333: Train Loss: [2.0792844, 0.3520567, 3.806512] | Test Loss: [2.408066, 0.42321253, 4.3929195]\n",
      "334: Train Loss: [2.1243248, 0.3051839, 3.9434655] | Test Loss: [2.3155234, 0.2835892, 4.3474574]\n",
      "335: Train Loss: [2.3564556, 0.35941198, 4.353499] | Test Loss: [2.335654, 0.303457, 4.3678513]\n",
      "336: Train Loss: [2.2080593, 0.39565384, 4.020465] | Test Loss: [2.355653, 0.3137627, 4.3975434]\n",
      "337: Train Loss: [2.196253, 0.38258725, 4.0099187] | Test Loss: [2.4284573, 0.3757685, 4.481146]\n",
      "338: Train Loss: [2.265604, 0.41155905, 4.119649] | Test Loss: [2.48479, 0.44937506, 4.520205]\n",
      "339: Train Loss: [2.1967034, 0.38882938, 4.0045776] | Test Loss: [2.488701, 0.3680237, 4.6093783]\n",
      "340: Train Loss: [2.357025, 0.5064637, 4.2075863] | Test Loss: [2.4907072, 0.3752186, 4.606196]\n",
      "341: Train Loss: [2.286072, 0.29429913, 4.277845] | Test Loss: [2.5821204, 0.3902204, 4.7740207]\n",
      "342: Train Loss: [2.2440813, 0.35143763, 4.136725] | Test Loss: [2.5934043, 0.46668485, 4.720124]\n",
      "343: Train Loss: [2.3491209, 0.3508417, 4.3474] | Test Loss: [2.6244397, 0.35436878, 4.8945107]\n",
      "344: Train Loss: [2.25941, 0.3342014, 4.1846185] | Test Loss: [2.4615707, 0.36826026, 4.554881]\n",
      "345: Train Loss: [2.2411618, 0.2855661, 4.1967573] | Test Loss: [2.5442553, 0.39072672, 4.697784]\n",
      "346: Train Loss: [2.3484004, 0.33422783, 4.3625727] | Test Loss: [2.4298146, 0.30823463, 4.5513945]\n",
      "347: Train Loss: [2.491191, 0.34906992, 4.6333117] | Test Loss: [2.720526, 0.396197, 5.044855]\n",
      "348: Train Loss: [2.4491203, 0.3795362, 4.5187044] | Test Loss: [2.4160295, 0.36019748, 4.4718614]\n",
      "349: Train Loss: [2.4896097, 0.36603227, 4.6131873] | Test Loss: [2.5742142, 0.4119826, 4.736446]\n",
      "350: Train Loss: [2.3716433, 0.3103963, 4.4328904] | Test Loss: [2.5543773, 0.34717923, 4.761575]\n",
      "351: Train Loss: [2.3508651, 0.28984717, 4.411883] | Test Loss: [2.587306, 0.34594443, 4.8286676]\n",
      "352: Train Loss: [2.5005002, 0.41172892, 4.5892715] | Test Loss: [2.3649726, 0.3676573, 4.362288]\n",
      "353: Train Loss: [2.445644, 0.33162996, 4.559658] | Test Loss: [2.5268965, 0.33384517, 4.719948]\n",
      "354: Train Loss: [2.3091846, 0.4144888, 4.2038803] | Test Loss: [2.3707206, 0.3206167, 4.4208245]\n",
      "355: Train Loss: [2.36239, 0.4100049, 4.314775] | Test Loss: [2.4489982, 0.3077695, 4.590227]\n",
      "356: Train Loss: [2.2630007, 0.3515612, 4.1744404] | Test Loss: [2.6109025, 0.34442025, 4.8773847]\n",
      "357: Train Loss: [2.514394, 0.38812092, 4.640667] | Test Loss: [2.5018232, 0.3386546, 4.664992]\n",
      "358: Train Loss: [2.4881432, 0.31724733, 4.659039] | Test Loss: [2.6803486, 0.38073242, 4.9799647]\n",
      "359: Train Loss: [2.362971, 0.38856497, 4.337377] | Test Loss: [2.6051269, 0.46587947, 4.7443743]\n",
      "360: Train Loss: [2.2772615, 0.30876115, 4.245762] | Test Loss: [2.5980902, 0.36348844, 4.832692]\n",
      "361: Train Loss: [2.4169838, 0.3788174, 4.45515] | Test Loss: [2.412677, 0.4097985, 4.4155555]\n",
      "362: Train Loss: [2.325787, 0.29900065, 4.3525734] | Test Loss: [2.7960093, 0.34300706, 5.2490115]\n",
      "363: Train Loss: [2.244666, 0.33306888, 4.1562634] | Test Loss: [2.5665886, 0.33838502, 4.794792]\n",
      "364: Train Loss: [2.3710577, 0.35329464, 4.3888206] | Test Loss: [2.4924886, 0.36445776, 4.6205196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365: Train Loss: [2.384552, 0.31113353, 4.4579706] | Test Loss: [2.4689004, 0.36359233, 4.5742087]\n",
      "366: Train Loss: [2.2398121, 0.3317461, 4.147878] | Test Loss: [2.3609107, 0.33472192, 4.3870993]\n",
      "367: Train Loss: [2.2543592, 0.3397562, 4.1689625] | Test Loss: [2.6241255, 0.32203585, 4.926215]\n",
      "368: Train Loss: [2.5795603, 0.36794925, 4.791171] | Test Loss: [2.513283, 0.3615872, 4.664979]\n",
      "369: Train Loss: [2.394312, 0.31865183, 4.469972] | Test Loss: [2.5155845, 0.40521637, 4.6259527]\n",
      "370: Train Loss: [2.3815684, 0.37882543, 4.3843117] | Test Loss: [2.383789, 0.43235514, 4.335223]\n",
      "371: Train Loss: [2.4990497, 0.32857338, 4.669526] | Test Loss: [2.5006366, 0.3589809, 4.642292]\n",
      "372: Train Loss: [2.4763863, 0.3663469, 4.586426] | Test Loss: [2.3921525, 0.38852537, 4.3957796]\n",
      "373: Train Loss: [2.3648996, 0.2792858, 4.4505134] | Test Loss: [2.3252199, 0.35852426, 4.2919154]\n",
      "374: Train Loss: [2.3652453, 0.44927597, 4.2812147] | Test Loss: [2.5356135, 0.30404413, 4.767183]\n",
      "375: Train Loss: [2.4373963, 0.41846138, 4.4563313] | Test Loss: [2.5427275, 0.39634788, 4.689107]\n",
      "376: Train Loss: [2.4849782, 0.3661519, 4.6038046] | Test Loss: [2.503973, 0.39388663, 4.6140594]\n",
      "377: Train Loss: [2.4148474, 0.32794404, 4.5017505] | Test Loss: [2.5191815, 0.34940356, 4.6889596]\n",
      "378: Train Loss: [2.3540041, 0.31695762, 4.391051] | Test Loss: [2.5183234, 0.3711234, 4.6655235]\n",
      "379: Train Loss: [2.4744678, 0.35316604, 4.5957694] | Test Loss: [2.5658243, 0.43997243, 4.691676]\n",
      "380: Train Loss: [2.372229, 0.29517007, 4.449288] | Test Loss: [2.4146705, 0.31326714, 4.5160737]\n",
      "381: Train Loss: [2.3263013, 0.38705117, 4.2655516] | Test Loss: [2.4277303, 0.5136505, 4.34181]\n",
      "382: Train Loss: [2.4294538, 0.31186885, 4.547039] | Test Loss: [2.4228792, 0.37260017, 4.4731584]\n",
      "383: Train Loss: [2.3821898, 0.33479637, 4.429583] | Test Loss: [2.5037608, 0.33534992, 4.6721716]\n",
      "384: Train Loss: [2.1930108, 0.351063, 4.034959] | Test Loss: [2.4835346, 0.3146219, 4.652447]\n",
      "385: Train Loss: [2.490677, 0.3604006, 4.6209536] | Test Loss: [2.657751, 0.39590293, 4.919599]\n",
      "386: Train Loss: [2.3125386, 0.38651103, 4.2385664] | Test Loss: [2.555337, 0.36143917, 4.7492347]\n",
      "387: Train Loss: [2.4657714, 0.34545463, 4.586088] | Test Loss: [2.3428087, 0.34384173, 4.341776]\n",
      "388: Train Loss: [2.3901486, 0.3277824, 4.452515] | Test Loss: [2.3676436, 0.37792975, 4.3573575]\n",
      "389: Train Loss: [2.3817782, 0.3720859, 4.3914704] | Test Loss: [2.5179887, 0.3363454, 4.699632]\n",
      "390: Train Loss: [2.425808, 0.3741603, 4.4774556] | Test Loss: [2.605499, 0.3720466, 4.8389516]\n",
      "391: Train Loss: [2.6482804, 0.37078542, 4.9257755] | Test Loss: [2.7146375, 0.39474583, 5.034529]\n",
      "392: Train Loss: [2.4024475, 0.3281833, 4.4767118] | Test Loss: [2.5399718, 0.26069266, 4.819251]\n",
      "393: Train Loss: [2.514707, 0.35391992, 4.675494] | Test Loss: [2.4632483, 0.37304288, 4.5534534]\n",
      "394: Train Loss: [2.5151699, 0.29344442, 4.736895] | Test Loss: [2.4890525, 0.311351, 4.6667542]\n",
      "395: Train Loss: [2.3252695, 0.48712108, 4.163418] | Test Loss: [2.5187926, 0.3861925, 4.651393]\n",
      "396: Train Loss: [2.3650901, 0.31076583, 4.4194145] | Test Loss: [2.4193037, 0.34066156, 4.497946]\n",
      "397: Train Loss: [2.314611, 0.31307596, 4.316146] | Test Loss: [2.4038115, 0.3733358, 4.434287]\n",
      "398: Train Loss: [2.4611356, 0.36982048, 4.5524507] | Test Loss: [2.571516, 0.31153417, 4.8314977]\n",
      "399: Train Loss: [2.2872167, 0.3132457, 4.2611876] | Test Loss: [2.4653685, 0.35902458, 4.5717125]\n",
      "400: Train Loss: [2.317672, 0.28834167, 4.3470025] | Test Loss: [2.3592708, 0.44701913, 4.2715225]\n",
      "401: Train Loss: [2.5551925, 0.3536534, 4.7567315] | Test Loss: [2.3400717, 0.33938906, 4.3407545]\n",
      "402: Train Loss: [2.3354027, 0.35802016, 4.312785] | Test Loss: [2.610693, 0.46857005, 4.7528157]\n",
      "403: Train Loss: [2.3332427, 0.31582734, 4.350658] | Test Loss: [2.6202426, 0.43001056, 4.8104744]\n",
      "404: Train Loss: [2.3897874, 0.35007012, 4.429505] | Test Loss: [2.422632, 0.36706716, 4.4781966]\n",
      "405: Train Loss: [2.589248, 0.34923372, 4.8292623] | Test Loss: [2.5850375, 0.3054489, 4.864626]\n",
      "406: Train Loss: [2.5063157, 0.40917483, 4.6034565] | Test Loss: [2.2934062, 0.3595608, 4.2272515]\n",
      "407: Train Loss: [2.3162923, 0.38167968, 4.250905] | Test Loss: [2.4736142, 0.3493828, 4.5978456]\n",
      "408: Train Loss: [2.3330283, 0.32533276, 4.340724] | Test Loss: [2.4081116, 0.41337246, 4.4028506]\n",
      "409: Train Loss: [2.4459984, 0.3569192, 4.5350776] | Test Loss: [2.5392816, 0.3620831, 4.7164803]\n",
      "410: Train Loss: [2.3672152, 0.30368295, 4.4307475] | Test Loss: [2.5653865, 0.32299584, 4.8077774]\n",
      "411: Train Loss: [2.2602217, 0.38606304, 4.1343803] | Test Loss: [2.424725, 0.36039764, 4.4890523]\n",
      "412: Train Loss: [2.2199738, 0.34093896, 4.0990086] | Test Loss: [2.6451924, 0.40720645, 4.883178]\n",
      "413: Train Loss: [2.251519, 0.34181604, 4.161222] | Test Loss: [2.5357096, 0.38894793, 4.6824713]\n",
      "414: Train Loss: [2.381569, 0.3465785, 4.416559] | Test Loss: [2.5474324, 0.38758954, 4.7072754]\n",
      "415: Train Loss: [2.335236, 0.29315358, 4.3773184] | Test Loss: [2.4511313, 0.44085103, 4.4614115]\n",
      "416: Train Loss: [2.433152, 0.367176, 4.499128] | Test Loss: [2.4954793, 0.34271297, 4.648246]\n",
      "417: Train Loss: [2.3862185, 0.34628153, 4.4261556] | Test Loss: [2.4981906, 0.30143553, 4.694946]\n",
      "418: Train Loss: [2.290782, 0.3300989, 4.251465] | Test Loss: [2.5780482, 0.33941415, 4.8166823]\n",
      "419: Train Loss: [2.3795846, 0.32815173, 4.4310174] | Test Loss: [2.4040122, 0.33425286, 4.4737716]\n",
      "420: Train Loss: [2.242906, 0.28448305, 4.201329] | Test Loss: [2.4419425, 0.35574496, 4.52814]\n",
      "421: Train Loss: [2.4389048, 0.30667776, 4.5711317] | Test Loss: [2.4728239, 0.4783817, 4.467266]\n",
      "422: Train Loss: [2.3416138, 0.4181932, 4.265034] | Test Loss: [2.3968964, 0.5088308, 4.284962]\n",
      "423: Train Loss: [2.399189, 0.38654077, 4.411837] | Test Loss: [2.369895, 0.2974979, 4.442292]\n",
      "424: Train Loss: [2.3239226, 0.4096498, 4.2381954] | Test Loss: [2.29488, 0.35160702, 4.238153]\n",
      "425: Train Loss: [2.2179482, 0.29443434, 4.141462] | Test Loss: [2.4826431, 0.4029162, 4.5623703]\n",
      "426: Train Loss: [2.310784, 0.36519355, 4.256375] | Test Loss: [2.596733, 0.3502719, 4.8431945]\n",
      "427: Train Loss: [2.2460952, 0.32571974, 4.1664705] | Test Loss: [2.4589715, 0.33007503, 4.5878677]\n",
      "428: Train Loss: [2.3000937, 0.35943958, 4.240748] | Test Loss: [2.4792595, 0.36240572, 4.596113]\n",
      "429: Train Loss: [2.1670206, 0.34142166, 3.9926193] | Test Loss: [2.4239035, 0.34005815, 4.5077486]\n",
      "430: Train Loss: [2.3889565, 0.4153558, 4.3625574] | Test Loss: [2.5461173, 0.3818039, 4.7104306]\n",
      "431: Train Loss: [2.3325248, 0.34877327, 4.316276] | Test Loss: [2.3974216, 0.38476977, 4.4100733]\n",
      "432: Train Loss: [2.261836, 0.3665309, 4.157141] | Test Loss: [2.3994699, 0.44332263, 4.355617]\n",
      "433: Train Loss: [2.4739132, 0.40376374, 4.5440626] | Test Loss: [2.492146, 0.33911636, 4.6451755]\n",
      "434: Train Loss: [2.374887, 0.3403453, 4.4094286] | Test Loss: [2.3467922, 0.35048577, 4.3430986]\n",
      "435: Train Loss: [2.3177714, 0.3577835, 4.2777596] | Test Loss: [2.6345255, 0.34439296, 4.9246583]\n",
      "436: Train Loss: [2.4533982, 0.41311812, 4.493678] | Test Loss: [2.3019345, 0.361969, 4.2419]\n",
      "437: Train Loss: [2.526778, 0.32437745, 4.7291784] | Test Loss: [2.263211, 0.4612873, 4.0651345]\n",
      "438: Train Loss: [2.298292, 0.3678464, 4.2287374] | Test Loss: [2.4466102, 0.32018617, 4.5730343]\n",
      "439: Train Loss: [2.2738454, 0.32930604, 4.2183847] | Test Loss: [2.489003, 0.37261218, 4.605394]\n",
      "440: Train Loss: [2.2773147, 0.34286234, 4.211767] | Test Loss: [2.5564287, 0.3516609, 4.7611966]\n",
      "441: Train Loss: [2.3717241, 0.391659, 4.3517895] | Test Loss: [2.3240988, 0.42154598, 4.2266517]\n",
      "442: Train Loss: [2.3246875, 0.29612124, 4.353254] | Test Loss: [2.309831, 0.35276872, 4.266893]\n",
      "443: Train Loss: [2.3542104, 0.38009462, 4.328326] | Test Loss: [2.427779, 0.4029134, 4.4526443]\n",
      "444: Train Loss: [2.3507428, 0.3892416, 4.312244] | Test Loss: [2.5406387, 0.3577589, 4.7235184]\n",
      "445: Train Loss: [2.4286025, 0.3132525, 4.5439525] | Test Loss: [2.4928632, 0.3177982, 4.667928]\n",
      "446: Train Loss: [2.2996328, 0.33623564, 4.26303] | Test Loss: [2.3258955, 0.42745152, 4.2243395]\n",
      "447: Train Loss: [2.260973, 0.35211995, 4.169826] | Test Loss: [2.4200764, 0.33611864, 4.504034]\n",
      "448: Train Loss: [2.1143436, 0.321079, 3.9076085] | Test Loss: [2.3231893, 0.3102546, 4.336124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449: Train Loss: [2.4207957, 0.33166137, 4.50993] | Test Loss: [2.5610714, 0.37761316, 4.7445297]\n",
      "450: Train Loss: [2.4045575, 0.27957126, 4.529544] | Test Loss: [2.5485458, 0.4461968, 4.650895]\n",
      "451: Train Loss: [2.295563, 0.3405975, 4.2505283] | Test Loss: [2.62373, 0.37323153, 4.8742285]\n",
      "452: Train Loss: [2.4639359, 0.33555305, 4.5923185] | Test Loss: [2.3960238, 0.3642466, 4.427801]\n",
      "453: Train Loss: [2.28842, 0.3734626, 4.2033772] | Test Loss: [2.4835887, 0.3471535, 4.6200237]\n",
      "454: Train Loss: [2.1642027, 0.26816422, 4.060241] | Test Loss: [2.5938227, 0.3335957, 4.8540497]\n",
      "455: Train Loss: [2.444488, 0.3282858, 4.5606904] | Test Loss: [2.4030156, 0.34029275, 4.4657383]\n",
      "456: Train Loss: [2.488228, 0.38875997, 4.587696] | Test Loss: [2.5056133, 0.32223314, 4.6889935]\n",
      "457: Train Loss: [2.4388096, 0.33228683, 4.5453324] | Test Loss: [2.6276128, 0.41403353, 4.8411922]\n",
      "458: Train Loss: [2.324202, 0.33079726, 4.317607] | Test Loss: [2.3368907, 0.34869182, 4.3250895]\n",
      "459: Train Loss: [2.2284732, 0.31061912, 4.146327] | Test Loss: [2.3858583, 0.32755056, 4.444166]\n",
      "460: Train Loss: [2.3478386, 0.3303421, 4.365335] | Test Loss: [2.555712, 0.3394891, 4.771935]\n",
      "461: Train Loss: [2.313621, 0.309532, 4.31771] | Test Loss: [2.5828753, 0.5518648, 4.613886]\n",
      "462: Train Loss: [2.381427, 0.36710128, 4.395753] | Test Loss: [2.5181935, 0.40839455, 4.6279926]\n",
      "463: Train Loss: [2.4925113, 0.42868027, 4.556342] | Test Loss: [2.444531, 0.30411702, 4.5849447]\n",
      "464: Train Loss: [2.379508, 0.36042473, 4.3985915] | Test Loss: [2.4662642, 0.33466363, 4.5978646]\n",
      "465: Train Loss: [2.3837137, 0.31013155, 4.457296] | Test Loss: [2.5612774, 0.3597384, 4.7628164]\n",
      "466: Train Loss: [2.2866297, 0.3384886, 4.234771] | Test Loss: [2.2566264, 0.42951334, 4.0837393]\n",
      "467: Train Loss: [2.6172068, 0.3360585, 4.898355] | Test Loss: [2.4038215, 0.3639948, 4.4436483]\n",
      "468: Train Loss: [2.308872, 0.3740681, 4.2436757] | Test Loss: [2.4412515, 0.29789358, 4.5846095]\n",
      "469: Train Loss: [2.2880096, 0.43423384, 4.1417856] | Test Loss: [2.5022662, 0.35571042, 4.648822]\n",
      "470: Train Loss: [2.4518769, 0.4910926, 4.412661] | Test Loss: [2.4746163, 0.34002966, 4.609203]\n",
      "471: Train Loss: [2.2705781, 0.32930496, 4.211851] | Test Loss: [2.3233447, 0.38675225, 4.2599373]\n",
      "472: Train Loss: [2.4093933, 0.3366369, 4.4821496] | Test Loss: [2.537603, 0.41153756, 4.663668]\n",
      "473: Train Loss: [2.292599, 0.38104942, 4.2041483] | Test Loss: [2.3875325, 0.45852184, 4.316543]\n",
      "474: Train Loss: [2.273807, 0.31501964, 4.2325945] | Test Loss: [2.352982, 0.37930608, 4.326658]\n",
      "475: Train Loss: [2.564787, 0.47687122, 4.652703] | Test Loss: [2.3556952, 0.2807484, 4.430642]\n",
      "476: Train Loss: [2.2899058, 0.40081018, 4.1790013] | Test Loss: [2.4727876, 0.30926904, 4.6363063]\n",
      "477: Train Loss: [2.4538777, 0.3223664, 4.585389] | Test Loss: [2.469239, 0.32936198, 4.609116]\n",
      "478: Train Loss: [2.4156854, 0.3753578, 4.456013] | Test Loss: [2.5870094, 0.40552443, 4.7684946]\n",
      "479: Train Loss: [2.357869, 0.3397103, 4.3760276] | Test Loss: [2.42799, 0.39127693, 4.464703]\n",
      "480: Train Loss: [2.47642, 0.3221675, 4.6306725] | Test Loss: [2.6641629, 0.38186774, 4.946458]\n",
      "481: Train Loss: [2.2936344, 0.35277024, 4.2344985] | Test Loss: [2.5555882, 0.3477676, 4.763409]\n",
      "482: Train Loss: [2.451074, 0.39168215, 4.5104656] | Test Loss: [2.6111917, 0.4369333, 4.78545]\n",
      "483: Train Loss: [2.4793293, 0.36709437, 4.591564] | Test Loss: [2.4615374, 0.30866563, 4.614409]\n",
      "484: Train Loss: [2.6866307, 0.41883004, 4.9544315] | Test Loss: [2.5710106, 0.37637803, 4.765643]\n",
      "485: Train Loss: [2.3586004, 0.41101173, 4.306189] | Test Loss: [2.5072691, 0.41805732, 4.596481]\n",
      "486: Train Loss: [2.3001966, 0.30134377, 4.2990494] | Test Loss: [2.5757568, 0.36046365, 4.79105]\n",
      "487: Train Loss: [2.4106786, 0.31875482, 4.5026026] | Test Loss: [2.5337853, 0.37147528, 4.6960955]\n",
      "488: Train Loss: [2.4880004, 0.3235431, 4.6524577] | Test Loss: [2.53384, 0.39043686, 4.677243]\n",
      "489: Train Loss: [2.3216217, 0.35684416, 4.2863994] | Test Loss: [2.5013928, 0.39135298, 4.6114326]\n",
      "490: Train Loss: [2.3576736, 0.38631508, 4.3290324] | Test Loss: [2.446294, 0.39060414, 4.501984]\n",
      "491: Train Loss: [2.4176095, 0.38652974, 4.448689] | Test Loss: [2.596162, 0.37770078, 4.8146234]\n",
      "492: Train Loss: [2.3939376, 0.3678375, 4.4200377] | Test Loss: [2.530192, 0.31782773, 4.742556]\n",
      "493: Train Loss: [2.3721335, 0.36011463, 4.3841524] | Test Loss: [2.4282818, 0.33786923, 4.5186944]\n",
      "494: Train Loss: [2.21277, 0.30106416, 4.124476] | Test Loss: [2.6137526, 0.3837579, 4.843747]\n",
      "495: Train Loss: [2.5365412, 0.31469375, 4.7583885] | Test Loss: [2.5165632, 0.34152642, 4.6916]\n",
      "496: Train Loss: [2.3652298, 0.33971125, 4.3907485] | Test Loss: [2.4644628, 0.4105061, 4.5184193]\n",
      "497: Train Loss: [2.5013666, 0.34741122, 4.655322] | Test Loss: [2.6263216, 0.41091725, 4.841726]\n",
      "498: Train Loss: [2.2628736, 0.3891407, 4.1366067] | Test Loss: [2.433532, 0.31578308, 4.551281]\n",
      "499: Train Loss: [2.44354, 0.32480103, 4.562279] | Test Loss: [2.5806992, 0.34131274, 4.8200855]\n",
      "500: Train Loss: [2.3795235, 0.4123113, 4.346736] | Test Loss: [2.5374036, 0.34874442, 4.726063]\n",
      "501: Train Loss: [2.3488984, 0.2940998, 4.403697] | Test Loss: [2.5621903, 0.38694057, 4.73744]\n",
      "502: Train Loss: [2.5497398, 0.36825252, 4.731227] | Test Loss: [2.5973756, 0.39818397, 4.7965674]\n",
      "503: Train Loss: [2.4665487, 0.36990497, 4.5631924] | Test Loss: [2.5056248, 0.39249292, 4.618757]\n",
      "504: Train Loss: [2.4434896, 0.36527038, 4.521709] | Test Loss: [2.4950309, 0.3837471, 4.6063147]\n",
      "505: Train Loss: [2.4511015, 0.391521, 4.510682] | Test Loss: [2.5489101, 0.39030856, 4.707512]\n",
      "506: Train Loss: [2.3742456, 0.31671208, 4.4317794] | Test Loss: [2.4863293, 0.35701975, 4.6156387]\n",
      "507: Train Loss: [2.6737356, 0.38624713, 4.961224] | Test Loss: [2.438569, 0.34855908, 4.528579]\n",
      "508: Train Loss: [2.3105006, 0.356288, 4.2647133] | Test Loss: [2.4354074, 0.33175594, 4.5390587]\n",
      "509: Train Loss: [2.4543679, 0.3600522, 4.5486836] | Test Loss: [2.5411644, 0.31865612, 4.763673]\n",
      "510: Train Loss: [2.5067272, 0.36223865, 4.651216] | Test Loss: [2.4644806, 0.33816016, 4.5908012]\n",
      "511: Train Loss: [2.409104, 0.38978064, 4.4284277] | Test Loss: [2.6358356, 0.3633416, 4.9083295]\n",
      "512: Train Loss: [2.3613927, 0.32533115, 4.3974543] | Test Loss: [2.56321, 0.36225665, 4.7641635]\n",
      "513: Train Loss: [2.4934888, 0.3500226, 4.636955] | Test Loss: [2.4068918, 0.33756292, 4.4762206]\n",
      "514: Train Loss: [2.4257488, 0.32382292, 4.5276747] | Test Loss: [2.4721673, 0.3969029, 4.5474315]\n",
      "515: Train Loss: [2.2091308, 0.4105052, 4.007756] | Test Loss: [2.4034257, 0.34157988, 4.4652715]\n",
      "516: Train Loss: [2.352033, 0.37278768, 4.3312783] | Test Loss: [2.6226864, 0.36807555, 4.8772974]\n",
      "517: Train Loss: [2.2994313, 0.32655993, 4.2723026] | Test Loss: [2.5646312, 0.3297696, 4.799493]\n",
      "518: Train Loss: [2.3582773, 0.35519966, 4.361355] | Test Loss: [2.5563896, 0.40517825, 4.707601]\n",
      "519: Train Loss: [2.3941905, 0.41812378, 4.3702574] | Test Loss: [2.306568, 0.38478795, 4.228348]\n",
      "520: Train Loss: [2.6054308, 0.4309153, 4.7799463] | Test Loss: [2.5330155, 0.39507565, 4.670955]\n",
      "521: Train Loss: [2.4842498, 0.30178306, 4.6667166] | Test Loss: [2.7127655, 0.4014793, 5.0240517]\n",
      "522: Train Loss: [2.537447, 0.36983353, 4.7050605] | Test Loss: [2.500821, 0.36624828, 4.635394]\n",
      "523: Train Loss: [2.422586, 0.3810779, 4.464094] | Test Loss: [2.408528, 0.31159592, 4.5054603]\n",
      "Epoch 5\n",
      "0: Train Loss: [2.1520538, 0.33158135, 3.9725266] | Test Loss: [2.3255396, 0.40222266, 4.2488565]\n",
      "1: Train Loss: [2.1417105, 0.35042968, 3.9329915] | Test Loss: [2.3704278, 0.36288258, 4.377973]\n",
      "2: Train Loss: [2.4284992, 0.3892336, 4.467765] | Test Loss: [2.324886, 0.39475414, 4.255018]\n",
      "3: Train Loss: [2.3505087, 0.337538, 4.363479] | Test Loss: [2.5080163, 0.399075, 4.6169577]\n",
      "4: Train Loss: [2.2593546, 0.34428778, 4.1744213] | Test Loss: [2.6488483, 0.45425823, 4.843438]\n",
      "5: Train Loss: [2.284576, 0.36164925, 4.207503] | Test Loss: [2.4888318, 0.35719135, 4.620472]\n",
      "6: Train Loss: [2.3825955, 0.4569756, 4.3082156] | Test Loss: [2.4927773, 0.39168093, 4.593874]\n",
      "7: Train Loss: [2.1897836, 0.3420589, 4.0375085] | Test Loss: [2.409137, 0.3295506, 4.4887233]\n",
      "8: Train Loss: [2.421796, 0.3525144, 4.491078] | Test Loss: [2.5416925, 0.39955008, 4.683835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: Train Loss: [2.2456145, 0.40747833, 4.0837507] | Test Loss: [2.4239318, 0.36964926, 4.4782143]\n",
      "10: Train Loss: [2.4187114, 0.32584268, 4.51158] | Test Loss: [2.6164684, 0.37386966, 4.859067]\n",
      "11: Train Loss: [2.4479682, 0.3132854, 4.582651] | Test Loss: [2.5382724, 0.2987546, 4.77779]\n",
      "12: Train Loss: [2.2277195, 0.3687676, 4.0866714] | Test Loss: [2.5212305, 0.3583823, 4.6840787]\n",
      "13: Train Loss: [2.2514422, 0.3910956, 4.1117887] | Test Loss: [2.627104, 0.331924, 4.922284]\n",
      "14: Train Loss: [2.191596, 0.44641435, 3.9367776] | Test Loss: [2.4926567, 0.37313902, 4.6121745]\n",
      "15: Train Loss: [2.272401, 0.37297693, 4.1718254] | Test Loss: [2.6145408, 0.4320892, 4.7969923]\n",
      "16: Train Loss: [2.277025, 0.36677095, 4.187279] | Test Loss: [2.6120396, 0.357701, 4.8663783]\n",
      "17: Train Loss: [2.3640919, 0.37493667, 4.353247] | Test Loss: [2.3684332, 0.34847707, 4.3883896]\n",
      "18: Train Loss: [2.313344, 0.37168163, 4.2550063] | Test Loss: [2.5037584, 0.30507308, 4.7024436]\n",
      "19: Train Loss: [2.236616, 0.34846398, 4.124768] | Test Loss: [2.6070964, 0.3757916, 4.8384013]\n",
      "20: Train Loss: [2.3424673, 0.34124607, 4.3436885] | Test Loss: [2.309849, 0.3801321, 4.239566]\n",
      "21: Train Loss: [2.3081393, 0.38166368, 4.234615] | Test Loss: [2.463057, 0.28330246, 4.642812]\n",
      "22: Train Loss: [2.3575695, 0.4108035, 4.3043356] | Test Loss: [2.6292005, 0.45353913, 4.804862]\n",
      "23: Train Loss: [2.3340986, 0.3069276, 4.3612695] | Test Loss: [2.4352124, 0.3625269, 4.507898]\n",
      "24: Train Loss: [2.3308618, 0.3121577, 4.349566] | Test Loss: [2.6618803, 0.39155194, 4.9322085]\n",
      "25: Train Loss: [2.320944, 0.46079296, 4.181095] | Test Loss: [2.4718509, 0.35214326, 4.5915585]\n",
      "26: Train Loss: [2.2561367, 0.3523139, 4.1599593] | Test Loss: [2.4476733, 0.34419608, 4.5511503]\n",
      "27: Train Loss: [2.3777325, 0.36626115, 4.389204] | Test Loss: [2.3586016, 0.35393938, 4.3632636]\n",
      "28: Train Loss: [2.15098, 0.35792089, 3.944039] | Test Loss: [2.5372598, 0.3831748, 4.6913447]\n",
      "29: Train Loss: [2.2199657, 0.3418017, 4.0981297] | Test Loss: [2.583395, 0.4508721, 4.715918]\n",
      "30: Train Loss: [2.5188198, 0.40583727, 4.6318026] | Test Loss: [2.5673356, 0.36802506, 4.7666464]\n",
      "31: Train Loss: [2.3843627, 0.42048442, 4.348241] | Test Loss: [2.445459, 0.35673428, 4.5341835]\n",
      "32: Train Loss: [2.192645, 0.4201808, 3.965109] | Test Loss: [1.2885593, 0.20699562, 2.370123]\n",
      "33: Train Loss: [2.3021114, 0.31591314, 4.2883096] | Test Loss: [2.5702477, 0.38527328, 4.755222]\n",
      "34: Train Loss: [2.1773276, 0.3531304, 4.001525] | Test Loss: [2.3348603, 0.36222667, 4.307494]\n",
      "35: Train Loss: [2.1427221, 0.36430365, 3.9211407] | Test Loss: [2.563153, 0.33902228, 4.787284]\n",
      "36: Train Loss: [2.2350063, 0.32477188, 4.145241] | Test Loss: [2.5819542, 0.3933584, 4.7705503]\n",
      "37: Train Loss: [2.1718583, 0.32213682, 4.0215797] | Test Loss: [2.3418345, 0.31013247, 4.3735366]\n",
      "38: Train Loss: [2.1889913, 0.34174192, 4.0362406] | Test Loss: [2.4901936, 0.35274428, 4.627643]\n",
      "39: Train Loss: [2.2676773, 0.3690762, 4.1662784] | Test Loss: [2.6082296, 0.35985756, 4.8566017]\n",
      "40: Train Loss: [2.2672207, 0.3252884, 4.209153] | Test Loss: [2.4476244, 0.3827853, 4.5124636]\n",
      "41: Train Loss: [2.3006392, 0.31526566, 4.2860126] | Test Loss: [2.559185, 0.3497345, 4.7686357]\n",
      "42: Train Loss: [2.2589293, 0.33195588, 4.1859026] | Test Loss: [2.5068302, 0.38700587, 4.6266546]\n",
      "43: Train Loss: [2.4512165, 0.40279347, 4.4996395] | Test Loss: [2.4315903, 0.34539223, 4.5177884]\n",
      "44: Train Loss: [2.2657726, 0.31637228, 4.215173] | Test Loss: [2.5763655, 0.3706072, 4.7821236]\n",
      "45: Train Loss: [2.1376173, 0.37429363, 3.900941] | Test Loss: [2.585041, 0.39634997, 4.773732]\n",
      "46: Train Loss: [2.31192, 0.3526962, 4.2711434] | Test Loss: [2.5076172, 0.29532686, 4.7199078]\n",
      "47: Train Loss: [2.2556393, 0.48419997, 4.0270786] | Test Loss: [2.4048812, 0.41357577, 4.396187]\n",
      "48: Train Loss: [2.3411345, 0.34648073, 4.3357882] | Test Loss: [2.3714817, 0.2984129, 4.4445505]\n",
      "49: Train Loss: [2.3763351, 0.38112426, 4.371546] | Test Loss: [2.5724597, 0.3498149, 4.7951045]\n",
      "50: Train Loss: [2.2819893, 0.3552089, 4.20877] | Test Loss: [2.6132293, 0.33156857, 4.89489]\n",
      "51: Train Loss: [2.4311588, 0.37954485, 4.482773] | Test Loss: [2.4514358, 0.43890652, 4.463965]\n",
      "52: Train Loss: [2.3967855, 0.37767938, 4.4158916] | Test Loss: [2.6279619, 0.34571028, 4.9102135]\n",
      "53: Train Loss: [2.2678754, 0.28477627, 4.2509747] | Test Loss: [2.3634343, 0.45085275, 4.2760158]\n",
      "54: Train Loss: [2.4037747, 0.38334498, 4.4242043] | Test Loss: [2.3721113, 0.32765868, 4.416564]\n",
      "55: Train Loss: [2.2352862, 0.31792647, 4.152646] | Test Loss: [2.5762131, 0.37738115, 4.775045]\n",
      "56: Train Loss: [2.243194, 0.44586375, 4.0405245] | Test Loss: [2.5810208, 0.39966992, 4.7623715]\n",
      "57: Train Loss: [2.3619027, 0.4030723, 4.320733] | Test Loss: [2.3271995, 0.3824956, 4.2719035]\n",
      "58: Train Loss: [2.298724, 0.38354632, 4.2139015] | Test Loss: [2.5617526, 0.3627663, 4.760739]\n",
      "59: Train Loss: [2.4302518, 0.51587564, 4.344628] | Test Loss: [2.4616594, 0.36347857, 4.55984]\n",
      "60: Train Loss: [2.3097076, 0.34324798, 4.2761674] | Test Loss: [2.5329325, 0.40181577, 4.664049]\n",
      "61: Train Loss: [2.3817914, 0.38562837, 4.3779545] | Test Loss: [2.3510675, 0.37872928, 4.3234057]\n",
      "62: Train Loss: [2.215787, 0.31806767, 4.1135063] | Test Loss: [2.8513186, 0.34891218, 5.353725]\n",
      "63: Train Loss: [2.2704906, 0.4335614, 4.10742] | Test Loss: [2.672788, 0.3029458, 5.04263]\n",
      "64: Train Loss: [2.3588219, 0.37938207, 4.3382616] | Test Loss: [2.4080942, 0.28934819, 4.52684]\n",
      "65: Train Loss: [2.3317373, 0.4735896, 4.189885] | Test Loss: [2.6230488, 0.51088464, 4.735213]\n",
      "66: Train Loss: [2.4328468, 0.3675014, 4.4981923] | Test Loss: [2.4376028, 0.38324636, 4.491959]\n",
      "67: Train Loss: [2.1948552, 0.3426677, 4.047043] | Test Loss: [2.4625847, 0.36716992, 4.5579996]\n",
      "68: Train Loss: [2.1295545, 0.33123955, 3.9278696] | Test Loss: [2.5016348, 0.34807128, 4.6551986]\n",
      "69: Train Loss: [2.4054766, 0.36208218, 4.448871] | Test Loss: [2.4057329, 0.3590106, 4.452455]\n",
      "70: Train Loss: [2.204808, 0.354967, 4.054649] | Test Loss: [2.4243317, 0.3307574, 4.5179057]\n",
      "71: Train Loss: [2.1962874, 0.36705187, 4.0255227] | Test Loss: [2.5875013, 0.38764432, 4.7873583]\n",
      "72: Train Loss: [2.1811473, 0.34353092, 4.0187635] | Test Loss: [2.553682, 0.31143224, 4.795932]\n",
      "73: Train Loss: [2.3134465, 0.3550955, 4.2717977] | Test Loss: [2.5078943, 0.31312636, 4.702662]\n",
      "74: Train Loss: [2.3654294, 0.36370587, 4.367153] | Test Loss: [2.28842, 0.34559932, 4.2312407]\n",
      "75: Train Loss: [2.4393315, 0.43633264, 4.4423304] | Test Loss: [2.5589266, 0.32449958, 4.7933536]\n",
      "76: Train Loss: [2.2972798, 0.33612996, 4.2584295] | Test Loss: [2.3749657, 0.38352853, 4.3664026]\n",
      "77: Train Loss: [2.3628206, 0.3178226, 4.407819] | Test Loss: [2.666724, 0.34908953, 4.9843583]\n",
      "78: Train Loss: [2.2875237, 0.32612708, 4.2489204] | Test Loss: [2.3909976, 0.42971686, 4.352278]\n",
      "79: Train Loss: [2.135128, 0.36975783, 3.9004984] | Test Loss: [2.384272, 0.42847034, 4.340074]\n",
      "80: Train Loss: [2.3623323, 0.46037883, 4.264286] | Test Loss: [2.490122, 0.3500451, 4.630199]\n",
      "81: Train Loss: [2.2608063, 0.33577672, 4.185836] | Test Loss: [2.4981952, 0.33674264, 4.659648]\n",
      "82: Train Loss: [2.2130222, 0.3542892, 4.0717554] | Test Loss: [2.505486, 0.3768902, 4.634082]\n",
      "83: Train Loss: [2.2316358, 0.35931683, 4.103955] | Test Loss: [2.4785907, 0.36153024, 4.595651]\n",
      "84: Train Loss: [2.2187834, 0.34855723, 4.0890093] | Test Loss: [2.5031831, 0.34583417, 4.660532]\n",
      "85: Train Loss: [2.2954957, 0.39730513, 4.1936865] | Test Loss: [2.4285626, 0.34263694, 4.514488]\n",
      "86: Train Loss: [2.3269546, 0.34106296, 4.312846] | Test Loss: [2.5801022, 0.35229248, 4.807912]\n",
      "87: Train Loss: [2.4406872, 0.3824799, 4.4988947] | Test Loss: [2.621762, 0.4315491, 4.811975]\n",
      "88: Train Loss: [2.091783, 0.47391674, 3.7096496] | Test Loss: [2.526326, 0.39573404, 4.656918]\n",
      "89: Train Loss: [2.2567217, 0.33966395, 4.1737795] | Test Loss: [2.4682105, 0.39418694, 4.542234]\n",
      "90: Train Loss: [2.3913264, 0.3567695, 4.4258833] | Test Loss: [2.3553321, 0.34449452, 4.36617]\n",
      "91: Train Loss: [2.2380989, 0.3354634, 4.140734] | Test Loss: [2.4311244, 0.37555683, 4.486692]\n",
      "92: Train Loss: [2.411333, 0.35673928, 4.465927] | Test Loss: [2.6997724, 0.3314735, 5.0680714]\n",
      "93: Train Loss: [2.2659996, 0.33189598, 4.2001033] | Test Loss: [2.4918876, 0.37239197, 4.611383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94: Train Loss: [2.1754093, 0.33282682, 4.017992] | Test Loss: [2.5089686, 0.37884948, 4.6390877]\n",
      "95: Train Loss: [2.2614748, 0.411316, 4.111634] | Test Loss: [2.4725454, 0.3969861, 4.548105]\n",
      "96: Train Loss: [2.219052, 0.40582117, 4.032283] | Test Loss: [2.5553384, 0.3436577, 4.7670193]\n",
      "97: Train Loss: [2.3541782, 0.34048828, 4.367868] | Test Loss: [2.6199207, 0.3947419, 4.8450994]\n",
      "98: Train Loss: [2.2992961, 0.4589512, 4.1396413] | Test Loss: [2.5651853, 0.37422493, 4.7561455]\n",
      "99: Train Loss: [2.4094772, 0.37976888, 4.4391856] | Test Loss: [2.3570693, 0.3615312, 4.3526073]\n",
      "100: Train Loss: [2.3110895, 0.4230101, 4.1991687] | Test Loss: [2.5697188, 0.40820417, 4.7312336]\n",
      "101: Train Loss: [2.2425106, 0.37537873, 4.1096425] | Test Loss: [2.3849888, 0.36330047, 4.4066772]\n",
      "102: Train Loss: [2.1998987, 0.34272623, 4.057071] | Test Loss: [2.4967377, 0.46903366, 4.5244417]\n",
      "103: Train Loss: [2.4815357, 0.39907798, 4.5639935] | Test Loss: [2.5938933, 0.33507162, 4.852715]\n",
      "104: Train Loss: [2.2229962, 0.37851658, 4.067476] | Test Loss: [2.4820693, 0.42996266, 4.534176]\n",
      "105: Train Loss: [2.2988718, 0.39689773, 4.2008457] | Test Loss: [2.409472, 0.29771066, 4.5212336]\n",
      "106: Train Loss: [2.2002013, 0.31436172, 4.086041] | Test Loss: [2.593212, 0.43811405, 4.7483096]\n",
      "107: Train Loss: [2.1886804, 0.394322, 3.9830387] | Test Loss: [2.3855386, 0.37595233, 4.395125]\n",
      "108: Train Loss: [2.410604, 0.35636368, 4.464844] | Test Loss: [2.5489664, 0.37910992, 4.718823]\n",
      "109: Train Loss: [2.2338305, 0.3356165, 4.1320443] | Test Loss: [2.60542, 0.3264234, 4.8844166]\n",
      "110: Train Loss: [2.2338593, 0.38316947, 4.084549] | Test Loss: [2.342986, 0.31516695, 4.3708053]\n",
      "111: Train Loss: [2.1977615, 0.36043048, 4.0350924] | Test Loss: [2.2766767, 0.36604813, 4.187305]\n",
      "112: Train Loss: [2.3682752, 0.44179535, 4.294755] | Test Loss: [2.490645, 0.3393542, 4.641936]\n",
      "113: Train Loss: [2.3149111, 0.43689534, 4.192927] | Test Loss: [2.5444634, 0.39337298, 4.695554]\n",
      "114: Train Loss: [2.4176443, 0.37303755, 4.462251] | Test Loss: [2.5089166, 0.34068528, 4.677148]\n",
      "115: Train Loss: [2.3379514, 0.38241214, 4.293491] | Test Loss: [2.4865656, 0.39656034, 4.576571]\n",
      "116: Train Loss: [2.3006055, 0.35601512, 4.245196] | Test Loss: [2.4589086, 0.41215438, 4.505663]\n",
      "117: Train Loss: [2.2229662, 0.3908589, 4.0550737] | Test Loss: [2.3776898, 0.3256506, 4.429729]\n",
      "118: Train Loss: [2.322663, 0.3372825, 4.3080435] | Test Loss: [2.3755767, 0.38133016, 4.3698235]\n",
      "119: Train Loss: [2.2852879, 0.36549336, 4.2050824] | Test Loss: [2.6215222, 0.3737684, 4.869276]\n",
      "120: Train Loss: [2.2431183, 0.3281198, 4.158117] | Test Loss: [2.66086, 0.45804265, 4.8636775]\n",
      "121: Train Loss: [2.2114987, 0.35963464, 4.0633626] | Test Loss: [2.3745594, 0.35516253, 4.393956]\n",
      "122: Train Loss: [2.2844822, 0.33348432, 4.2354803] | Test Loss: [2.2998662, 0.34980252, 4.24993]\n",
      "123: Train Loss: [2.2784054, 0.30458575, 4.252225] | Test Loss: [2.4041662, 0.3196175, 4.488715]\n",
      "124: Train Loss: [2.4153626, 0.4103907, 4.4203343] | Test Loss: [2.4713898, 0.3505223, 4.592257]\n",
      "125: Train Loss: [2.2131054, 0.301701, 4.12451] | Test Loss: [2.3778403, 0.3669692, 4.3887115]\n",
      "126: Train Loss: [2.3673434, 0.31342807, 4.421259] | Test Loss: [2.587185, 0.37963933, 4.7947307]\n",
      "127: Train Loss: [2.3084853, 0.4171356, 4.199835] | Test Loss: [2.4905977, 0.3338342, 4.6473613]\n",
      "128: Train Loss: [2.1263232, 0.31239268, 3.9402537] | Test Loss: [2.3859727, 0.35067067, 4.4212747]\n",
      "129: Train Loss: [2.205831, 0.38982257, 4.0218396] | Test Loss: [2.390583, 0.41160953, 4.3695564]\n",
      "130: Train Loss: [2.2121181, 0.41491744, 4.009319] | Test Loss: [2.3934097, 0.32219878, 4.4646206]\n",
      "131: Train Loss: [2.393001, 0.34002912, 4.445973] | Test Loss: [2.5474946, 0.37298253, 4.722007]\n",
      "132: Train Loss: [2.451741, 0.3699323, 4.53355] | Test Loss: [2.4120588, 0.3180056, 4.506112]\n",
      "133: Train Loss: [2.3245556, 0.385849, 4.2632623] | Test Loss: [2.458728, 0.36400074, 4.5534554]\n",
      "134: Train Loss: [2.2015235, 0.32013255, 4.0829144] | Test Loss: [2.7093096, 0.43114668, 4.9874725]\n",
      "135: Train Loss: [2.3881896, 0.37261412, 4.403765] | Test Loss: [2.425705, 0.3574763, 4.4939337]\n",
      "136: Train Loss: [2.3513498, 0.33843577, 4.364264] | Test Loss: [2.4304762, 0.3466279, 4.5143247]\n",
      "137: Train Loss: [2.3952203, 0.3459777, 4.444463] | Test Loss: [2.628569, 0.33746555, 4.919672]\n",
      "138: Train Loss: [2.2788944, 0.30080128, 4.2569876] | Test Loss: [2.5756435, 0.3711992, 4.780088]\n",
      "139: Train Loss: [2.2142818, 0.31368944, 4.1148744] | Test Loss: [2.4814816, 0.4063394, 4.5566235]\n",
      "140: Train Loss: [2.2287107, 0.38229465, 4.0751266] | Test Loss: [2.5265572, 0.34507036, 4.708044]\n",
      "141: Train Loss: [2.2727106, 0.326806, 4.218615] | Test Loss: [2.5246162, 0.38238758, 4.666845]\n",
      "142: Train Loss: [2.1713636, 0.33265582, 4.0100713] | Test Loss: [2.6412761, 0.41319093, 4.8693614]\n",
      "143: Train Loss: [2.5075502, 0.4575835, 4.557517] | Test Loss: [2.613718, 0.37074792, 4.856688]\n",
      "144: Train Loss: [2.2709663, 0.36149004, 4.1804423] | Test Loss: [2.5757284, 0.38538533, 4.7660713]\n",
      "145: Train Loss: [2.3323593, 0.3870379, 4.277681] | Test Loss: [2.3980026, 0.4336646, 4.3623405]\n",
      "146: Train Loss: [2.332354, 0.31176677, 4.3529415] | Test Loss: [2.448068, 0.35937223, 4.5367637]\n",
      "147: Train Loss: [2.3091512, 0.338413, 4.279889] | Test Loss: [2.4577742, 0.36382133, 4.551727]\n",
      "148: Train Loss: [2.3555923, 0.3535626, 4.357622] | Test Loss: [2.538277, 0.46838382, 4.60817]\n",
      "149: Train Loss: [2.34059, 0.32405654, 4.3571234] | Test Loss: [2.4172728, 0.3996328, 4.4349127]\n",
      "150: Train Loss: [2.3733814, 0.4672372, 4.2795258] | Test Loss: [2.5356762, 0.46859246, 4.60276]\n",
      "151: Train Loss: [2.3540907, 0.3304154, 4.377766] | Test Loss: [2.4602993, 0.33282062, 4.587778]\n",
      "152: Train Loss: [2.2377412, 0.38524422, 4.090238] | Test Loss: [2.204229, 0.35652068, 4.0519376]\n",
      "153: Train Loss: [2.22454, 0.3542116, 4.094868] | Test Loss: [2.5076585, 0.41899762, 4.596319]\n",
      "154: Train Loss: [2.3182247, 0.32178777, 4.3146615] | Test Loss: [2.4239364, 0.36632416, 4.481549]\n",
      "155: Train Loss: [2.3073936, 0.33236715, 4.28242] | Test Loss: [2.6484447, 0.353705, 4.9431844]\n",
      "156: Train Loss: [2.360591, 0.36059928, 4.3605824] | Test Loss: [2.5752246, 0.34033507, 4.8101144]\n",
      "157: Train Loss: [2.3263526, 0.30604652, 4.3466587] | Test Loss: [2.618988, 0.39563486, 4.8423414]\n",
      "158: Train Loss: [2.1363933, 0.29585594, 3.9769309] | Test Loss: [2.6047497, 0.3511853, 4.858314]\n",
      "159: Train Loss: [2.2695513, 0.32779717, 4.2113056] | Test Loss: [2.4518616, 0.37384528, 4.529878]\n",
      "160: Train Loss: [2.3776448, 0.32786208, 4.4274273] | Test Loss: [2.6156764, 0.37091994, 4.8604326]\n",
      "161: Train Loss: [2.2083454, 0.3198501, 4.096841] | Test Loss: [2.3246753, 0.3984438, 4.250907]\n",
      "162: Train Loss: [2.271144, 0.3587884, 4.1834993] | Test Loss: [2.5192757, 0.360279, 4.6782722]\n",
      "163: Train Loss: [2.277474, 0.27636898, 4.2785788] | Test Loss: [2.5909758, 0.38749364, 4.794458]\n",
      "164: Train Loss: [2.443496, 0.33938032, 4.5476117] | Test Loss: [2.3972187, 0.32029808, 4.474139]\n",
      "165: Train Loss: [2.1625755, 0.34644184, 3.9787092] | Test Loss: [2.6020918, 0.3681393, 4.8360443]\n",
      "166: Train Loss: [2.3097355, 0.40124834, 4.2182226] | Test Loss: [2.4492722, 0.3954772, 4.503067]\n",
      "167: Train Loss: [2.4411283, 0.3559641, 4.5262923] | Test Loss: [2.4324594, 0.34182096, 4.523098]\n",
      "168: Train Loss: [2.4063773, 0.29923308, 4.5135217] | Test Loss: [2.502896, 0.35753122, 4.648261]\n",
      "169: Train Loss: [2.4143891, 0.3654192, 4.463359] | Test Loss: [2.454707, 0.3621858, 4.547228]\n",
      "170: Train Loss: [2.441776, 0.37711293, 4.506439] | Test Loss: [2.3237615, 0.3817075, 4.2658153]\n",
      "171: Train Loss: [2.4363263, 0.33364743, 4.5390053] | Test Loss: [2.4642658, 0.37705034, 4.5514812]\n",
      "172: Train Loss: [2.3153005, 0.32815737, 4.3024435] | Test Loss: [2.5835242, 0.39265713, 4.774391]\n",
      "173: Train Loss: [2.40796, 0.40505132, 4.4108686] | Test Loss: [2.6593645, 0.37063226, 4.9480968]\n",
      "174: Train Loss: [2.3149548, 0.29629418, 4.3336153] | Test Loss: [2.8058584, 0.43873847, 5.1729784]\n",
      "175: Train Loss: [2.3994522, 0.37229988, 4.4266047] | Test Loss: [2.4434886, 0.36298016, 4.523997]\n",
      "176: Train Loss: [2.372251, 0.3451252, 4.399377] | Test Loss: [2.402589, 0.35916832, 4.4460096]\n",
      "177: Train Loss: [2.35286, 0.36357808, 4.3421416] | Test Loss: [2.6573844, 0.39794084, 4.916828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178: Train Loss: [2.2782345, 0.32869792, 4.227771] | Test Loss: [2.5325224, 0.3337497, 4.731295]\n",
      "179: Train Loss: [2.2554727, 0.49322692, 4.0177183] | Test Loss: [2.4213827, 0.34458476, 4.4981804]\n",
      "180: Train Loss: [2.4732513, 0.32919574, 4.6173067] | Test Loss: [2.349568, 0.34852543, 4.3506103]\n",
      "181: Train Loss: [2.4013665, 0.36594287, 4.43679] | Test Loss: [2.3839364, 0.40173405, 4.366139]\n",
      "182: Train Loss: [2.278857, 0.37118685, 4.1865273] | Test Loss: [2.7008696, 0.27556685, 5.126172]\n",
      "183: Train Loss: [2.291031, 0.35387823, 4.2281837] | Test Loss: [2.412488, 0.42674974, 4.3982263]\n",
      "184: Train Loss: [2.2410233, 0.36717254, 4.114874] | Test Loss: [2.3662372, 0.3665586, 4.365916]\n",
      "185: Train Loss: [2.3237667, 0.29994032, 4.3475933] | Test Loss: [2.6535006, 0.4688016, 4.8381996]\n",
      "186: Train Loss: [2.4714181, 0.35107693, 4.591759] | Test Loss: [2.444685, 0.3947181, 4.494652]\n",
      "187: Train Loss: [2.290753, 0.3200135, 4.2614923] | Test Loss: [2.616876, 0.31736222, 4.9163895]\n",
      "188: Train Loss: [2.3926828, 0.3938793, 4.391486] | Test Loss: [2.4847398, 0.3263884, 4.643091]\n",
      "189: Train Loss: [2.3111439, 0.32394093, 4.298347] | Test Loss: [2.5399787, 0.34220245, 4.737755]\n",
      "190: Train Loss: [2.3365235, 0.3190701, 4.3539767] | Test Loss: [2.6499083, 0.32140964, 4.978407]\n",
      "191: Train Loss: [2.327144, 0.36647913, 4.287809] | Test Loss: [2.5330086, 0.32208526, 4.743932]\n",
      "192: Train Loss: [2.3724113, 0.35004494, 4.394778] | Test Loss: [2.503321, 0.33967793, 4.666964]\n",
      "193: Train Loss: [2.3549352, 0.3233503, 4.38652] | Test Loss: [2.5295863, 0.3246513, 4.7345214]\n",
      "194: Train Loss: [2.3746986, 0.3818538, 4.3675437] | Test Loss: [2.512887, 0.3804937, 4.6452804]\n",
      "195: Train Loss: [2.1602573, 0.36096236, 3.9595523] | Test Loss: [2.6724458, 0.40681896, 4.9380727]\n",
      "196: Train Loss: [2.4012723, 0.3774317, 4.4251127] | Test Loss: [2.5337253, 0.3296924, 4.737758]\n",
      "197: Train Loss: [2.370095, 0.3605546, 4.3796353] | Test Loss: [2.4390357, 0.36740875, 4.5106626]\n",
      "198: Train Loss: [2.3318841, 0.4739998, 4.1897683] | Test Loss: [2.480393, 0.36108297, 4.599703]\n",
      "199: Train Loss: [2.2893226, 0.35256732, 4.226078] | Test Loss: [2.5097387, 0.34584853, 4.673629]\n",
      "200: Train Loss: [2.277508, 0.36787182, 4.1871443] | Test Loss: [2.6723197, 0.39088932, 4.95375]\n",
      "201: Train Loss: [2.3895154, 0.2794708, 4.49956] | Test Loss: [2.575136, 0.3470351, 4.803237]\n",
      "202: Train Loss: [2.3829334, 0.30109325, 4.4647737] | Test Loss: [2.4987657, 0.33988807, 4.6576433]\n",
      "203: Train Loss: [2.4353096, 0.34264725, 4.527972] | Test Loss: [2.5155227, 0.4641872, 4.5668583]\n",
      "204: Train Loss: [2.3370795, 0.37557274, 4.2985864] | Test Loss: [2.3350558, 0.40003318, 4.2700787]\n",
      "205: Train Loss: [2.429542, 0.38634944, 4.472735] | Test Loss: [2.6427774, 0.45645377, 4.829101]\n",
      "206: Train Loss: [2.2918444, 0.41734737, 4.1663413] | Test Loss: [2.6715174, 0.37186918, 4.9711657]\n",
      "207: Train Loss: [2.4658265, 0.39573628, 4.535917] | Test Loss: [2.683781, 0.3381507, 5.0294113]\n",
      "208: Train Loss: [2.1742463, 0.37135354, 3.9771392] | Test Loss: [2.5254557, 0.3389313, 4.7119803]\n",
      "209: Train Loss: [2.3999002, 0.38796604, 4.4118342] | Test Loss: [2.4499304, 0.37569007, 4.524171]\n",
      "210: Train Loss: [2.4707098, 0.3484658, 4.5929537] | Test Loss: [2.3139095, 0.36281645, 4.2650027]\n",
      "211: Train Loss: [2.3703132, 0.37629756, 4.364329] | Test Loss: [2.5674698, 0.3978822, 4.7370577]\n",
      "212: Train Loss: [2.4700444, 0.33300596, 4.607083] | Test Loss: [2.7178462, 0.518957, 4.916735]\n",
      "213: Train Loss: [2.3177955, 0.3692618, 4.2663293] | Test Loss: [2.6533713, 0.341598, 4.9651446]\n",
      "214: Train Loss: [2.3649657, 0.41036874, 4.3195624] | Test Loss: [2.6903925, 0.4428587, 4.9379263]\n",
      "215: Train Loss: [2.270161, 0.3149689, 4.225353] | Test Loss: [2.5262935, 0.30109984, 4.7514873]\n",
      "216: Train Loss: [2.2448788, 0.42720369, 4.062554] | Test Loss: [2.3668394, 0.31101424, 4.4226646]\n",
      "217: Train Loss: [2.4397516, 0.35293806, 4.526565] | Test Loss: [2.643285, 0.37791187, 4.908658]\n",
      "218: Train Loss: [2.4850903, 0.4668474, 4.503333] | Test Loss: [2.5510502, 0.43670678, 4.6653934]\n",
      "219: Train Loss: [2.4346054, 0.408768, 4.4604425] | Test Loss: [2.4187376, 0.40518722, 4.432288]\n",
      "220: Train Loss: [2.477939, 0.42334402, 4.5325336] | Test Loss: [2.5337298, 0.38077533, 4.686684]\n",
      "221: Train Loss: [2.3654008, 0.39955807, 4.3312435] | Test Loss: [2.6396234, 0.3953658, 4.883881]\n",
      "222: Train Loss: [2.4429045, 0.35443014, 4.5313787] | Test Loss: [2.437708, 0.4637754, 4.4116406]\n",
      "223: Train Loss: [2.2823906, 0.31679708, 4.247984] | Test Loss: [2.58326, 0.40766189, 4.758858]\n",
      "224: Train Loss: [2.31287, 0.3567098, 4.26903] | Test Loss: [2.6803083, 0.47471967, 4.885897]\n",
      "225: Train Loss: [2.3117974, 0.39163172, 4.231963] | Test Loss: [2.443837, 0.35883412, 4.5288396]\n",
      "226: Train Loss: [2.5851955, 0.4250121, 4.745379] | Test Loss: [2.5359793, 0.3059133, 4.766045]\n",
      "227: Train Loss: [2.5129433, 0.41661566, 4.609271] | Test Loss: [2.6300948, 0.32891414, 4.9312754]\n",
      "228: Train Loss: [2.3795867, 0.32671857, 4.4324546] | Test Loss: [2.4283094, 0.3752953, 4.4813237]\n",
      "229: Train Loss: [2.278927, 0.36310443, 4.19475] | Test Loss: [2.3968592, 0.33720505, 4.4565134]\n",
      "230: Train Loss: [2.6269836, 0.4295642, 4.8244033] | Test Loss: [2.5642695, 0.3855493, 4.7429895]\n",
      "231: Train Loss: [2.4524488, 0.36195007, 4.542948] | Test Loss: [2.543025, 0.39083275, 4.695217]\n",
      "232: Train Loss: [2.4450452, 0.39383322, 4.4962573] | Test Loss: [2.394207, 0.3323732, 4.456041]\n",
      "233: Train Loss: [2.4246485, 0.4972058, 4.3520913] | Test Loss: [2.5544844, 0.3287792, 4.7801895]\n",
      "234: Train Loss: [2.4318948, 0.34081307, 4.5229764] | Test Loss: [2.6141183, 0.31589466, 4.912342]\n",
      "235: Train Loss: [2.3725364, 0.3630218, 4.382051] | Test Loss: [2.4545724, 0.35920218, 4.5499425]\n",
      "236: Train Loss: [2.3337686, 0.32109192, 4.346445] | Test Loss: [2.5850513, 0.3922886, 4.777814]\n",
      "237: Train Loss: [2.3485856, 0.32279623, 4.374375] | Test Loss: [2.6323762, 0.40440202, 4.8603506]\n",
      "238: Train Loss: [2.403804, 0.40464342, 4.4029646] | Test Loss: [2.4963613, 0.32727063, 4.665452]\n",
      "239: Train Loss: [2.4080014, 0.33846635, 4.4775367] | Test Loss: [2.5109832, 0.3702105, 4.651756]\n",
      "240: Train Loss: [2.4171565, 0.27713612, 4.5571766] | Test Loss: [2.5332503, 0.3969295, 4.6695714]\n",
      "241: Train Loss: [2.4434474, 0.32265222, 4.5642424] | Test Loss: [2.544423, 0.38995427, 4.698892]\n",
      "242: Train Loss: [2.246523, 0.34943834, 4.1436076] | Test Loss: [2.6974056, 0.36141285, 5.033398]\n",
      "243: Train Loss: [2.3878887, 0.3531898, 4.4225874] | Test Loss: [2.493716, 0.39308313, 4.594349]\n",
      "244: Train Loss: [2.31502, 0.31859466, 4.3114457] | Test Loss: [2.5196047, 0.33220974, 4.707]\n",
      "245: Train Loss: [2.2798176, 0.31502005, 4.244615] | Test Loss: [2.4514182, 0.36324266, 4.5395937]\n",
      "246: Train Loss: [2.416714, 0.40226123, 4.4311666] | Test Loss: [2.4759555, 0.31320468, 4.638706]\n",
      "247: Train Loss: [2.393917, 0.35153195, 4.436302] | Test Loss: [2.55733, 0.3362737, 4.778386]\n",
      "248: Train Loss: [2.3081229, 0.3561991, 4.2600465] | Test Loss: [2.4456456, 0.40781826, 4.483473]\n",
      "249: Train Loss: [2.5437784, 0.31457636, 4.7729807] | Test Loss: [2.497485, 0.35139418, 4.6435757]\n",
      "250: Train Loss: [2.3963966, 0.28964594, 4.503147] | Test Loss: [2.4990325, 0.35227537, 4.6457896]\n",
      "251: Train Loss: [2.3779993, 0.2919444, 4.464054] | Test Loss: [2.5017712, 0.3147443, 4.688798]\n",
      "252: Train Loss: [2.3617752, 0.38047484, 4.3430753] | Test Loss: [2.35797, 0.35063967, 4.3653]\n",
      "253: Train Loss: [2.3725855, 0.32262117, 4.4225497] | Test Loss: [2.569336, 0.49345797, 4.645214]\n",
      "254: Train Loss: [2.4694438, 0.31845307, 4.6204348] | Test Loss: [2.4541209, 0.36137757, 4.546864]\n",
      "255: Train Loss: [2.341095, 0.32773757, 4.3544526] | Test Loss: [2.5209265, 0.32935384, 4.712499]\n",
      "256: Train Loss: [2.3227894, 0.3518255, 4.293753] | Test Loss: [2.7140048, 0.5059193, 4.92209]\n",
      "257: Train Loss: [2.4841611, 0.30365363, 4.6646686] | Test Loss: [2.4715483, 0.38316184, 4.5599346]\n",
      "258: Train Loss: [2.2929766, 0.34434965, 4.2416034] | Test Loss: [2.6095035, 0.37093335, 4.8480735]\n",
      "259: Train Loss: [2.3149838, 0.38630655, 4.243661] | Test Loss: [2.4263215, 0.42427352, 4.4283695]\n",
      "260: Train Loss: [2.459968, 0.38077754, 4.539159] | Test Loss: [2.5922327, 0.35668024, 4.827785]\n",
      "261: Train Loss: [2.082929, 0.37506396, 3.7907937] | Test Loss: [2.4713986, 0.32192534, 4.620872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262: Train Loss: [2.388155, 0.37868968, 4.39762] | Test Loss: [2.6596591, 0.29786697, 5.0214515]\n",
      "263: Train Loss: [2.3117962, 0.331917, 4.2916756] | Test Loss: [2.6170297, 0.3414359, 4.8926234]\n",
      "264: Train Loss: [2.3706038, 0.35184842, 4.389359] | Test Loss: [2.579419, 0.39053032, 4.7683077]\n",
      "265: Train Loss: [2.2752783, 0.4102106, 4.140346] | Test Loss: [2.5318396, 0.38231236, 4.681367]\n",
      "266: Train Loss: [2.3386636, 0.31441796, 4.3629093] | Test Loss: [2.5863404, 0.3122544, 4.8604264]\n",
      "267: Train Loss: [2.4213295, 0.35109872, 4.4915605] | Test Loss: [2.5542016, 0.37140757, 4.7369957]\n",
      "268: Train Loss: [2.4275885, 0.34173465, 4.5134425] | Test Loss: [2.5549054, 0.35639212, 4.753419]\n",
      "269: Train Loss: [2.186093, 0.30589214, 4.066294] | Test Loss: [2.5208063, 0.35838053, 4.6832323]\n",
      "270: Train Loss: [2.368252, 0.30304462, 4.4334593] | Test Loss: [2.4805694, 0.36456615, 4.5965724]\n",
      "271: Train Loss: [2.4696639, 0.38945526, 4.5498724] | Test Loss: [2.6589448, 0.3505686, 4.967321]\n",
      "272: Train Loss: [2.4690943, 0.38330713, 4.5548816] | Test Loss: [2.4365122, 0.39394644, 4.479078]\n",
      "273: Train Loss: [2.382511, 0.3831949, 4.381827] | Test Loss: [2.5377877, 0.3852801, 4.690295]\n",
      "274: Train Loss: [2.422341, 0.40408942, 4.440593] | Test Loss: [2.3410347, 0.35392243, 4.328147]\n",
      "275: Train Loss: [2.4121737, 0.36105847, 4.463289] | Test Loss: [2.5187252, 0.36932698, 4.6681232]\n",
      "276: Train Loss: [2.2223785, 0.3517423, 4.0930147] | Test Loss: [2.4709132, 0.35750115, 4.5843253]\n",
      "277: Train Loss: [2.4802542, 0.35223985, 4.6082683] | Test Loss: [2.3883066, 0.35906062, 4.4175525]\n",
      "278: Train Loss: [2.2941537, 0.34557056, 4.242737] | Test Loss: [2.3762205, 0.35438108, 4.39806]\n",
      "279: Train Loss: [2.4536452, 0.36788666, 4.539404] | Test Loss: [2.5763998, 0.41866025, 4.7341394]\n",
      "280: Train Loss: [2.422946, 0.3301214, 4.5157704] | Test Loss: [2.5840015, 0.35607982, 4.811923]\n",
      "281: Train Loss: [2.2577007, 0.31400102, 4.2014003] | Test Loss: [2.471648, 0.36442915, 4.578867]\n",
      "282: Train Loss: [2.4899879, 0.35930935, 4.6206665] | Test Loss: [2.7473028, 0.34511033, 5.149495]\n",
      "283: Train Loss: [2.4011903, 0.33286893, 4.4695115] | Test Loss: [2.3792984, 0.32513183, 4.433465]\n",
      "284: Train Loss: [2.449982, 0.30260623, 4.5973577] | Test Loss: [2.790182, 0.44402775, 5.1363363]\n",
      "285: Train Loss: [2.4966695, 0.35509744, 4.638242] | Test Loss: [2.5201037, 0.33108985, 4.7091174]\n",
      "286: Train Loss: [2.436837, 0.38757792, 4.486096] | Test Loss: [2.620924, 0.3071695, 4.9346786]\n",
      "287: Train Loss: [2.6017118, 0.39109498, 4.8123283] | Test Loss: [2.5155396, 0.3888353, 4.642244]\n",
      "288: Train Loss: [2.355816, 0.3871634, 4.3244686] | Test Loss: [2.592978, 0.3570938, 4.828862]\n",
      "289: Train Loss: [2.568975, 0.3229969, 4.814953] | Test Loss: [2.5076325, 0.33153406, 4.683731]\n",
      "290: Train Loss: [2.3372083, 0.27884933, 4.3955674] | Test Loss: [2.417498, 0.3684692, 4.466527]\n",
      "291: Train Loss: [2.3984454, 0.431265, 4.365626] | Test Loss: [2.482434, 0.41296494, 4.5519032]\n",
      "292: Train Loss: [2.3375633, 0.32744053, 4.347686] | Test Loss: [2.5675497, 0.33285907, 4.8022404]\n",
      "293: Train Loss: [2.3825142, 0.28585848, 4.47917] | Test Loss: [2.5858772, 0.42405352, 4.7477007]\n",
      "294: Train Loss: [2.3720062, 0.315481, 4.428531] | Test Loss: [2.4486673, 0.457306, 4.4400287]\n",
      "295: Train Loss: [2.3973064, 0.3375578, 4.457055] | Test Loss: [2.5863788, 0.34161037, 4.831147]\n",
      "296: Train Loss: [2.4807262, 0.5120518, 4.4494004] | Test Loss: [2.5197947, 0.31297606, 4.7266135]\n",
      "297: Train Loss: [2.4635434, 0.2747813, 4.6523056] | Test Loss: [2.4968734, 0.31738323, 4.6763635]\n",
      "298: Train Loss: [2.4040751, 0.37756804, 4.430582] | Test Loss: [2.3898478, 0.39362454, 4.386071]\n",
      "299: Train Loss: [2.4163022, 0.3348255, 4.497779] | Test Loss: [2.6448624, 0.43674487, 4.85298]\n",
      "300: Train Loss: [2.4430153, 0.39122003, 4.4948106] | Test Loss: [2.5831137, 0.37647146, 4.789756]\n",
      "301: Train Loss: [2.4940324, 0.38719133, 4.6008735] | Test Loss: [2.6014402, 0.43198162, 4.770899]\n",
      "302: Train Loss: [2.5153542, 0.45464146, 4.576067] | Test Loss: [2.4192824, 0.27647266, 4.5620923]\n",
      "303: Train Loss: [2.535941, 0.4226567, 4.649225] | Test Loss: [2.7672927, 0.4223039, 5.112282]\n",
      "304: Train Loss: [2.3376215, 0.4654312, 4.2098117] | Test Loss: [2.4002678, 0.37292904, 4.4276066]\n",
      "305: Train Loss: [2.5381484, 0.34468216, 4.7316146] | Test Loss: [2.4738142, 0.3491538, 4.5984745]\n",
      "306: Train Loss: [2.4060109, 0.35046026, 4.4615617] | Test Loss: [2.5065975, 0.34097287, 4.672222]\n",
      "307: Train Loss: [2.498122, 0.4036627, 4.5925813] | Test Loss: [2.454124, 0.41397306, 4.494275]\n",
      "308: Train Loss: [2.4306667, 0.3512804, 4.510053] | Test Loss: [2.4469995, 0.30504945, 4.5889497]\n",
      "309: Train Loss: [2.461878, 0.32232207, 4.601434] | Test Loss: [2.4938107, 0.3946573, 4.592964]\n",
      "310: Train Loss: [2.486457, 0.3522304, 4.6206837] | Test Loss: [2.550051, 0.39653614, 4.7035656]\n",
      "311: Train Loss: [2.4068112, 0.39408073, 4.419542] | Test Loss: [2.5736775, 0.30498627, 4.8423686]\n",
      "312: Train Loss: [2.2161984, 0.2996684, 4.1327286] | Test Loss: [2.489839, 0.28578714, 4.693891]\n",
      "313: Train Loss: [2.4276762, 0.4033014, 4.452051] | Test Loss: [2.6363313, 0.35414708, 4.9185157]\n",
      "314: Train Loss: [2.548905, 0.32036972, 4.77744] | Test Loss: [2.4676268, 0.36517742, 4.570076]\n",
      "315: Train Loss: [2.3638597, 0.2996531, 4.4280663] | Test Loss: [2.6006217, 0.34910414, 4.8521395]\n",
      "316: Train Loss: [2.3428738, 0.29224092, 4.3935065] | Test Loss: [2.4448714, 0.3684548, 4.521288]\n",
      "317: Train Loss: [2.396618, 0.4076171, 4.3856187] | Test Loss: [2.5545805, 0.3844493, 4.7247114]\n",
      "318: Train Loss: [2.5004911, 0.40559953, 4.5953827] | Test Loss: [2.520434, 0.37871224, 4.6621556]\n",
      "319: Train Loss: [2.5144703, 0.36791822, 4.6610227] | Test Loss: [2.5121462, 0.36658278, 4.6577096]\n",
      "320: Train Loss: [2.523919, 0.417927, 4.6299114] | Test Loss: [2.5840967, 0.34944326, 4.81875]\n",
      "321: Train Loss: [2.3723485, 0.37702265, 4.3676744] | Test Loss: [2.603414, 0.40565357, 4.8011746]\n",
      "322: Train Loss: [2.3910978, 0.31682885, 4.465367] | Test Loss: [2.6039476, 0.35826588, 4.8496294]\n",
      "323: Train Loss: [2.4048746, 0.3832889, 4.4264603] | Test Loss: [2.6185806, 0.36172894, 4.875432]\n",
      "324: Train Loss: [2.3786576, 0.3486273, 4.408688] | Test Loss: [2.5734222, 0.35818818, 4.788656]\n",
      "325: Train Loss: [2.5811665, 0.4471115, 4.7152214] | Test Loss: [2.6350622, 0.3907845, 4.8793397]\n",
      "326: Train Loss: [2.3233638, 0.32475707, 4.3219705] | Test Loss: [2.735918, 0.4351195, 5.0367165]\n",
      "327: Train Loss: [2.429645, 0.4291286, 4.4301615] | Test Loss: [2.4895456, 0.28506786, 4.694023]\n",
      "328: Train Loss: [2.6371777, 0.33002663, 4.944329] | Test Loss: [2.519879, 0.42133257, 4.618426]\n",
      "329: Train Loss: [2.4665291, 0.33496958, 4.5980887] | Test Loss: [2.7719676, 0.40476882, 5.1391664]\n",
      "330: Train Loss: [2.4056375, 0.34977868, 4.4614964] | Test Loss: [2.3577387, 0.3056222, 4.4098554]\n",
      "331: Train Loss: [2.373088, 0.36762607, 4.3785496] | Test Loss: [2.441894, 0.37864158, 4.5051465]\n",
      "332: Train Loss: [2.50766, 0.3535091, 4.661811] | Test Loss: [2.2951255, 0.27286002, 4.317391]\n",
      "333: Train Loss: [2.5670903, 0.33907717, 4.7951035] | Test Loss: [2.4334037, 0.32476142, 4.542046]\n",
      "334: Train Loss: [2.5852442, 0.3509325, 4.8195558] | Test Loss: [2.5008392, 0.32841006, 4.6732683]\n",
      "335: Train Loss: [2.2738652, 0.3627982, 4.184932] | Test Loss: [2.3762243, 0.32930133, 4.423147]\n",
      "336: Train Loss: [2.3198345, 0.35621107, 4.2834578] | Test Loss: [2.4876351, 0.33625758, 4.639013]\n",
      "337: Train Loss: [2.6473002, 0.3221044, 4.972496] | Test Loss: [2.4869986, 0.33994374, 4.634053]\n",
      "338: Train Loss: [2.391977, 0.36800334, 4.415951] | Test Loss: [2.7275078, 0.32565102, 5.1293645]\n",
      "339: Train Loss: [2.5555193, 0.3435604, 4.7674785] | Test Loss: [2.5322087, 0.45720023, 4.6072173]\n",
      "340: Train Loss: [2.3726676, 0.3454387, 4.3998966] | Test Loss: [2.5537982, 0.36090678, 4.74669]\n",
      "341: Train Loss: [2.4680748, 0.29642287, 4.6397266] | Test Loss: [2.4635592, 0.39933914, 4.527779]\n",
      "342: Train Loss: [2.3796, 0.41107857, 4.3481216] | Test Loss: [2.5549176, 0.32378677, 4.7860484]\n",
      "343: Train Loss: [2.2276473, 0.3198374, 4.135457] | Test Loss: [2.4789696, 0.34716436, 4.610775]\n",
      "344: Train Loss: [2.3137832, 0.35571152, 4.271855] | Test Loss: [2.3615294, 0.34580782, 4.3772507]\n",
      "345: Train Loss: [2.3768384, 0.35622388, 4.397453] | Test Loss: [2.619637, 0.32419232, 4.9150815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346: Train Loss: [2.44823, 0.37049708, 4.525963] | Test Loss: [2.5037467, 0.39012575, 4.6173677]\n",
      "347: Train Loss: [2.3528166, 0.32578087, 4.3798523] | Test Loss: [2.6431026, 0.36364865, 4.922557]\n",
      "348: Train Loss: [2.420485, 0.34167498, 4.499295] | Test Loss: [2.465336, 0.38083842, 4.549834]\n",
      "349: Train Loss: [2.4460454, 0.31478706, 4.577304] | Test Loss: [2.6313913, 0.4100481, 4.8527346]\n",
      "350: Train Loss: [2.410397, 0.31215113, 4.508643] | Test Loss: [2.549883, 0.37912667, 4.720639]\n",
      "351: Train Loss: [2.406366, 0.3688871, 4.4438453] | Test Loss: [2.6479163, 0.43485266, 4.86098]\n",
      "352: Train Loss: [2.4579263, 0.3296954, 4.5861573] | Test Loss: [2.5610368, 0.34759623, 4.7744775]\n",
      "353: Train Loss: [2.345418, 0.31558535, 4.375251] | Test Loss: [2.4049478, 0.327823, 4.4820724]\n",
      "354: Train Loss: [2.505207, 0.3407387, 4.6696754] | Test Loss: [2.4278214, 0.36588752, 4.489755]\n",
      "355: Train Loss: [2.4130652, 0.3910492, 4.435081] | Test Loss: [2.5428674, 0.36759385, 4.718141]\n",
      "356: Train Loss: [2.4547448, 0.3753002, 4.534189] | Test Loss: [2.6289942, 0.38415882, 4.87383]\n",
      "357: Train Loss: [2.4160693, 0.40401408, 4.4281244] | Test Loss: [2.5780635, 0.33189145, 4.8242354]\n",
      "358: Train Loss: [2.3864915, 0.3508175, 4.4221654] | Test Loss: [2.3760293, 0.3943301, 4.3577285]\n",
      "359: Train Loss: [2.3184385, 0.34261906, 4.294258] | Test Loss: [2.6376495, 0.4538545, 4.8214445]\n",
      "360: Train Loss: [2.3037777, 0.3442149, 4.2633405] | Test Loss: [2.5366185, 0.34992903, 4.723308]\n",
      "361: Train Loss: [2.3340101, 0.27224264, 4.3957777] | Test Loss: [2.5051055, 0.3532323, 4.6569786]\n",
      "362: Train Loss: [2.4887767, 0.3292803, 4.648273] | Test Loss: [2.7662563, 0.37991458, 5.152598]\n",
      "363: Train Loss: [2.4448152, 0.33553118, 4.554099] | Test Loss: [2.4968054, 0.3864531, 4.6071577]\n",
      "364: Train Loss: [2.4411566, 0.30205786, 4.5802555] | Test Loss: [2.3679917, 0.40594432, 4.330039]\n",
      "365: Train Loss: [2.5341856, 0.35391614, 4.714455] | Test Loss: [2.510351, 0.4190006, 4.6017013]\n",
      "366: Train Loss: [2.3925033, 0.33394885, 4.451058] | Test Loss: [2.553933, 0.32380942, 4.784056]\n",
      "367: Train Loss: [2.4416108, 0.28916845, 4.5940533] | Test Loss: [2.6742845, 0.39133933, 4.9572296]\n",
      "368: Train Loss: [2.3986313, 0.33981073, 4.457452] | Test Loss: [2.5175595, 0.36403796, 4.671081]\n",
      "369: Train Loss: [2.5168664, 0.39278853, 4.6409445] | Test Loss: [2.5460136, 0.32131806, 4.770709]\n",
      "370: Train Loss: [2.3146935, 0.34020105, 4.289186] | Test Loss: [2.5782225, 0.34516543, 4.81128]\n",
      "371: Train Loss: [2.3443308, 0.3533352, 4.335326] | Test Loss: [2.5063734, 0.2979707, 4.714776]\n",
      "372: Train Loss: [2.4772243, 0.36796343, 4.5864854] | Test Loss: [2.4466088, 0.36285594, 4.5303617]\n",
      "373: Train Loss: [2.3474607, 0.3518675, 4.343054] | Test Loss: [2.5414183, 0.336034, 4.746803]\n",
      "374: Train Loss: [2.2030897, 0.34159482, 4.0645847] | Test Loss: [2.5400836, 0.35697016, 4.723197]\n",
      "375: Train Loss: [2.4007096, 0.42435664, 4.377063] | Test Loss: [2.6287806, 0.349331, 4.9082303]\n",
      "376: Train Loss: [2.3608634, 0.33284557, 4.388881] | Test Loss: [2.5685623, 0.317417, 4.8197074]\n",
      "377: Train Loss: [2.4267802, 0.36724207, 4.4863186] | Test Loss: [2.2104404, 0.41729534, 4.0035853]\n",
      "378: Train Loss: [2.4108305, 0.376713, 4.444948] | Test Loss: [2.5837243, 0.3347681, 4.83268]\n",
      "379: Train Loss: [2.278924, 0.3191096, 4.2387385] | Test Loss: [2.5776656, 0.40402222, 4.751309]\n",
      "380: Train Loss: [2.4603114, 0.2873687, 4.633254] | Test Loss: [2.4361444, 0.33591512, 4.5363736]\n",
      "381: Train Loss: [2.3118641, 0.3515745, 4.272154] | Test Loss: [2.58823, 0.3367701, 4.8396897]\n",
      "382: Train Loss: [2.323168, 0.31608385, 4.330252] | Test Loss: [2.5686297, 0.3286539, 4.8086057]\n",
      "383: Train Loss: [2.3955312, 0.35679895, 4.434263] | Test Loss: [2.6892736, 0.44743308, 4.931114]\n",
      "384: Train Loss: [2.3694954, 0.35746914, 4.3815217] | Test Loss: [2.7250383, 0.5378763, 4.9122005]\n",
      "385: Train Loss: [2.358911, 0.30043432, 4.417388] | Test Loss: [2.5646677, 0.40077627, 4.728559]\n",
      "386: Train Loss: [2.3490415, 0.33676213, 4.361321] | Test Loss: [2.4266112, 0.3346032, 4.518619]\n",
      "387: Train Loss: [2.3828144, 0.40043002, 4.3651986] | Test Loss: [2.5524898, 0.33928227, 4.7656975]\n",
      "388: Train Loss: [2.4677937, 0.37303987, 4.5625477] | Test Loss: [2.333407, 0.3803791, 4.2864347]\n",
      "389: Train Loss: [2.4451792, 0.34492075, 4.545438] | Test Loss: [2.5742533, 0.38301438, 4.7654924]\n",
      "390: Train Loss: [2.4116766, 0.31742716, 4.505926] | Test Loss: [2.506884, 0.3702833, 4.643485]\n",
      "391: Train Loss: [2.4239898, 0.37114304, 4.4768367] | Test Loss: [2.4892836, 0.32339978, 4.6551676]\n",
      "392: Train Loss: [2.328222, 0.30135652, 4.3550878] | Test Loss: [2.4935446, 0.46384358, 4.5232453]\n",
      "393: Train Loss: [2.4257956, 0.33549768, 4.5160933] | Test Loss: [2.5681775, 0.30508485, 4.83127]\n",
      "394: Train Loss: [2.3681672, 0.38824263, 4.3480916] | Test Loss: [2.7671537, 0.51567566, 5.018632]\n",
      "395: Train Loss: [2.3077357, 0.33775437, 4.277717] | Test Loss: [2.3767679, 0.32889837, 4.4246373]\n",
      "396: Train Loss: [2.2913694, 0.3543746, 4.2283645] | Test Loss: [2.4995787, 0.3958858, 4.6032715]\n",
      "397: Train Loss: [2.1860392, 0.29679346, 4.075285] | Test Loss: [2.5420718, 0.4210906, 4.663053]\n",
      "398: Train Loss: [2.3490016, 0.43155953, 4.2664437] | Test Loss: [2.449694, 0.35388726, 4.5455008]\n",
      "399: Train Loss: [2.4774237, 0.33794692, 4.6169004] | Test Loss: [2.658886, 0.39109656, 4.9266753]\n",
      "400: Train Loss: [2.450847, 0.41803762, 4.4836564] | Test Loss: [2.6046076, 0.35874426, 4.850471]\n",
      "401: Train Loss: [2.4999142, 0.37497932, 4.624849] | Test Loss: [2.788901, 0.37757045, 5.2002316]\n",
      "402: Train Loss: [2.3494263, 0.40146708, 4.297385] | Test Loss: [2.4950926, 0.35333058, 4.6368546]\n",
      "403: Train Loss: [2.5112388, 0.4115791, 4.6108985] | Test Loss: [2.3880339, 0.3080352, 4.4680324]\n",
      "404: Train Loss: [2.600345, 0.34962958, 4.8510604] | Test Loss: [2.4659262, 0.31985483, 4.6119976]\n",
      "405: Train Loss: [2.3160396, 0.2915008, 4.3405786] | Test Loss: [2.4562724, 0.31349197, 4.599053]\n",
      "406: Train Loss: [2.6341732, 0.3929802, 4.875366] | Test Loss: [2.5588298, 0.3499218, 4.767738]\n",
      "407: Train Loss: [2.3392746, 0.30814406, 4.370405] | Test Loss: [2.4674907, 0.3805843, 4.554397]\n",
      "408: Train Loss: [2.5237088, 0.3290495, 4.718368] | Test Loss: [2.64915, 0.35052997, 4.9477696]\n",
      "409: Train Loss: [2.4465618, 0.38349277, 4.5096307] | Test Loss: [2.4526734, 0.35562792, 4.549719]\n",
      "410: Train Loss: [2.471132, 0.3410607, 4.6012034] | Test Loss: [2.5430844, 0.3327933, 4.7533755]\n",
      "411: Train Loss: [2.5466108, 0.3337859, 4.7594357] | Test Loss: [2.6499946, 0.3133511, 4.986638]\n",
      "412: Train Loss: [2.4166546, 0.31561473, 4.5176945] | Test Loss: [2.5395057, 0.37565115, 4.70336]\n",
      "413: Train Loss: [2.535907, 0.37987545, 4.6919384] | Test Loss: [2.447681, 0.350943, 4.544419]\n",
      "414: Train Loss: [2.3966157, 0.39263645, 4.400595] | Test Loss: [2.3439395, 0.35392553, 4.3339534]\n",
      "415: Train Loss: [2.393134, 0.4044277, 4.3818407] | Test Loss: [2.509571, 0.34287128, 4.676271]\n",
      "416: Train Loss: [2.4548666, 0.36276102, 4.5469723] | Test Loss: [2.5290558, 0.36393788, 4.694174]\n",
      "417: Train Loss: [2.2939403, 0.3061218, 4.281759] | Test Loss: [2.4873734, 0.38547197, 4.589275]\n",
      "418: Train Loss: [2.4003499, 0.3299224, 4.4707775] | Test Loss: [2.4411755, 0.3650256, 4.5173254]\n",
      "419: Train Loss: [2.346917, 0.28679973, 4.407034] | Test Loss: [2.6992016, 0.5367271, 4.861676]\n",
      "420: Train Loss: [2.388537, 0.38683647, 4.3902373] | Test Loss: [2.5834525, 0.33468932, 4.832216]\n",
      "421: Train Loss: [2.2835872, 0.39562094, 4.1715536] | Test Loss: [2.5414472, 0.49446627, 4.588428]\n",
      "422: Train Loss: [2.4130163, 0.30932638, 4.5167065] | Test Loss: [2.3497124, 0.33871713, 4.3607078]\n",
      "423: Train Loss: [2.3677607, 0.35745493, 4.3780665] | Test Loss: [2.5879161, 0.46788704, 4.7079453]\n",
      "424: Train Loss: [2.3986485, 0.35155934, 4.445738] | Test Loss: [2.6016653, 0.38158828, 4.821742]\n",
      "425: Train Loss: [2.4874828, 0.34059927, 4.6343665] | Test Loss: [2.531009, 0.31080136, 4.7512164]\n",
      "426: Train Loss: [2.4070225, 0.326221, 4.487824] | Test Loss: [2.529324, 0.34640458, 4.7122436]\n",
      "427: Train Loss: [2.290078, 0.35055456, 4.2296014] | Test Loss: [2.673513, 0.40272295, 4.944303]\n",
      "428: Train Loss: [2.4476438, 0.34892154, 4.546366] | Test Loss: [2.4569035, 0.35821205, 4.555595]\n",
      "429: Train Loss: [2.3061123, 0.29630747, 4.315917] | Test Loss: [2.688134, 0.33946073, 5.036807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430: Train Loss: [2.3577836, 0.32510012, 4.390467] | Test Loss: [2.5791461, 0.3985483, 4.759744]\n",
      "431: Train Loss: [2.3470006, 0.33167845, 4.362323] | Test Loss: [2.5853548, 0.37716666, 4.793543]\n",
      "432: Train Loss: [2.4268663, 0.3626211, 4.4911113] | Test Loss: [2.648469, 0.3894564, 4.9074817]\n",
      "433: Train Loss: [2.2849405, 0.32053807, 4.249343] | Test Loss: [2.5154772, 0.4145954, 4.6163588]\n",
      "434: Train Loss: [2.3450625, 0.3503777, 4.3397474] | Test Loss: [2.1867914, 0.3634475, 4.010135]\n",
      "435: Train Loss: [2.4856462, 0.33742628, 4.6338663] | Test Loss: [2.4714286, 0.34311834, 4.599739]\n",
      "436: Train Loss: [2.3187413, 0.3265511, 4.3109317] | Test Loss: [2.4129305, 0.34723812, 4.478623]\n",
      "437: Train Loss: [2.4017282, 0.35760218, 4.445854] | Test Loss: [2.5777442, 0.3764491, 4.7790394]\n",
      "438: Train Loss: [2.3895335, 0.34459677, 4.43447] | Test Loss: [2.4190621, 0.31740057, 4.520724]\n",
      "439: Train Loss: [2.3760216, 0.37470648, 4.377337] | Test Loss: [2.395934, 0.4002095, 4.391659]\n",
      "440: Train Loss: [2.3205345, 0.3210577, 4.320011] | Test Loss: [2.5553675, 0.33735707, 4.773378]\n",
      "441: Train Loss: [2.3012502, 0.3734531, 4.2290473] | Test Loss: [2.60899, 0.46718082, 4.750799]\n",
      "442: Train Loss: [2.3425982, 0.31562802, 4.3695683] | Test Loss: [2.519718, 0.2898003, 4.7496357]\n",
      "443: Train Loss: [2.4634116, 0.393422, 4.533401] | Test Loss: [2.3799617, 0.3806577, 4.379266]\n",
      "444: Train Loss: [2.5093596, 0.45526674, 4.5634522] | Test Loss: [2.6689143, 0.32649633, 5.0113325]\n",
      "445: Train Loss: [2.5472257, 0.39798337, 4.696468] | Test Loss: [2.5731182, 0.37822282, 4.7680135]\n",
      "446: Train Loss: [2.3671067, 0.3697157, 4.3644977] | Test Loss: [2.5506525, 0.4383601, 4.662945]\n",
      "447: Train Loss: [2.5460503, 0.37272674, 4.7193737] | Test Loss: [2.6283855, 0.3424841, 4.914287]\n",
      "448: Train Loss: [2.4544322, 0.36414188, 4.5447226] | Test Loss: [2.5579596, 0.419909, 4.69601]\n",
      "449: Train Loss: [2.4089649, 0.382566, 4.435364] | Test Loss: [2.5411727, 0.32819855, 4.754147]\n",
      "450: Train Loss: [2.393941, 0.4568888, 4.330993] | Test Loss: [2.4551568, 0.37027577, 4.5400376]\n",
      "451: Train Loss: [2.4622812, 0.37232032, 4.5522423] | Test Loss: [2.5269606, 0.3605194, 4.693402]\n",
      "452: Train Loss: [2.3718412, 0.3035384, 4.440144] | Test Loss: [2.7738013, 0.27184567, 5.275757]\n",
      "453: Train Loss: [2.2583997, 0.3858327, 4.1309667] | Test Loss: [2.7042184, 0.36463982, 5.043797]\n",
      "454: Train Loss: [2.3547442, 0.34023178, 4.3692565] | Test Loss: [2.5147998, 0.43791106, 4.5916886]\n",
      "455: Train Loss: [2.4626088, 0.4061795, 4.519038] | Test Loss: [2.3456244, 0.3331642, 4.3580847]\n",
      "456: Train Loss: [2.3926675, 0.36797953, 4.4173555] | Test Loss: [2.5919664, 0.36293998, 4.820993]\n",
      "457: Train Loss: [2.3541527, 0.35239765, 4.355908] | Test Loss: [2.6415582, 0.34324607, 4.9398704]\n",
      "458: Train Loss: [2.330212, 0.3507711, 4.3096533] | Test Loss: [2.3504362, 0.3840868, 4.316786]\n",
      "459: Train Loss: [2.351537, 0.40181774, 4.301256] | Test Loss: [2.4690142, 0.36444378, 4.5735846]\n",
      "460: Train Loss: [2.4696317, 0.35033113, 4.588932] | Test Loss: [2.3652053, 0.34404698, 4.3863635]\n",
      "461: Train Loss: [2.335149, 0.37608182, 4.294216] | Test Loss: [2.6695244, 0.46505642, 4.8739924]\n",
      "462: Train Loss: [2.4063213, 0.32809827, 4.4845443] | Test Loss: [2.541603, 0.34338307, 4.739823]\n",
      "463: Train Loss: [2.5876143, 0.38630086, 4.7889276] | Test Loss: [2.527714, 0.39946988, 4.655958]\n",
      "464: Train Loss: [2.3918388, 0.40058288, 4.383095] | Test Loss: [2.491429, 0.34524187, 4.637616]\n",
      "465: Train Loss: [2.5983853, 0.37256646, 4.824204] | Test Loss: [2.4176836, 0.34724993, 4.488117]\n",
      "466: Train Loss: [2.3392515, 0.40499026, 4.273513] | Test Loss: [2.5193436, 0.38997772, 4.6487093]\n",
      "467: Train Loss: [2.4410882, 0.3649426, 4.517234] | Test Loss: [2.6214552, 0.35805634, 4.884854]\n",
      "468: Train Loss: [2.5675774, 0.34080338, 4.7943516] | Test Loss: [2.6064277, 0.37785098, 4.8350043]\n",
      "469: Train Loss: [2.4278874, 0.33697808, 4.518797] | Test Loss: [2.576169, 0.39200544, 4.7603326]\n",
      "470: Train Loss: [2.452519, 0.3348761, 4.570162] | Test Loss: [2.5126152, 0.38657063, 4.63866]\n",
      "471: Train Loss: [2.3194408, 0.34655526, 4.2923265] | Test Loss: [2.5027583, 0.39035648, 4.61516]\n",
      "472: Train Loss: [2.4233298, 0.3079895, 4.53867] | Test Loss: [2.4612072, 0.3602758, 4.5621386]\n",
      "473: Train Loss: [2.272863, 0.28356394, 4.2621617] | Test Loss: [2.2854385, 0.2750711, 4.295806]\n",
      "474: Train Loss: [2.4498544, 0.30376622, 4.5959425] | Test Loss: [2.6334174, 0.3665729, 4.900262]\n",
      "475: Train Loss: [2.3613, 0.39717546, 4.3254247] | Test Loss: [2.3958786, 0.29132852, 4.5004287]\n",
      "476: Train Loss: [2.3216598, 0.3641154, 4.2792044] | Test Loss: [2.5927732, 0.41257498, 4.7729716]\n",
      "477: Train Loss: [2.4388807, 0.37126037, 4.506501] | Test Loss: [2.6166143, 0.35447246, 4.878756]\n",
      "478: Train Loss: [2.3458338, 0.3320995, 4.359568] | Test Loss: [2.3764136, 0.34974882, 4.4030786]\n",
      "479: Train Loss: [2.2326615, 0.345949, 4.119374] | Test Loss: [2.5957327, 0.37675548, 4.81471]\n",
      "480: Train Loss: [2.455797, 0.29351228, 4.6180816] | Test Loss: [2.6696727, 0.4232838, 4.916062]\n",
      "481: Train Loss: [2.432734, 0.31371915, 4.5517488] | Test Loss: [2.5068235, 0.31536487, 4.6982822]\n",
      "482: Train Loss: [2.5113683, 0.32665753, 4.6960793] | Test Loss: [2.6054096, 0.38737062, 4.8234487]\n",
      "483: Train Loss: [2.541056, 0.38333145, 4.6987805] | Test Loss: [2.7193625, 0.52762604, 4.911099]\n",
      "484: Train Loss: [2.4809241, 0.31107476, 4.6507735] | Test Loss: [2.4128003, 0.39052787, 4.435073]\n",
      "485: Train Loss: [2.369983, 0.32708752, 4.4128785] | Test Loss: [2.6474319, 0.38187566, 4.912988]\n",
      "486: Train Loss: [2.422537, 0.3548383, 4.490236] | Test Loss: [2.670895, 0.35433435, 4.987456]\n",
      "487: Train Loss: [2.3714738, 0.31491396, 4.428034] | Test Loss: [2.7029507, 0.39609703, 5.0098042]\n",
      "488: Train Loss: [2.3950303, 0.33472326, 4.455337] | Test Loss: [2.3636105, 0.33013466, 4.397086]\n",
      "489: Train Loss: [2.4508514, 0.323099, 4.5786037] | Test Loss: [2.3841033, 0.3432009, 4.425006]\n",
      "490: Train Loss: [2.3756146, 0.41661334, 4.334616] | Test Loss: [2.5935342, 0.35675895, 4.8303094]\n",
      "491: Train Loss: [2.4724402, 0.3466204, 4.59826] | Test Loss: [2.5589502, 0.3624465, 4.755454]\n",
      "492: Train Loss: [2.2513077, 0.36344764, 4.139168] | Test Loss: [2.6253061, 0.33103597, 4.919576]\n",
      "493: Train Loss: [2.555203, 0.3498331, 4.760573] | Test Loss: [2.270339, 0.33615413, 4.204524]\n",
      "494: Train Loss: [2.5979543, 0.3325573, 4.8633513] | Test Loss: [2.5856524, 0.40575632, 4.765548]\n",
      "495: Train Loss: [2.3591294, 0.29666787, 4.421591] | Test Loss: [2.60045, 0.3727725, 4.8281274]\n",
      "496: Train Loss: [2.3902268, 0.40924662, 4.371207] | Test Loss: [2.7194302, 0.30478713, 5.1340733]\n",
      "497: Train Loss: [2.4974966, 0.36108673, 4.6339064] | Test Loss: [2.5651815, 0.34109458, 4.7892685]\n",
      "498: Train Loss: [2.3977814, 0.2994153, 4.4961476] | Test Loss: [2.3446875, 0.3129289, 4.3764462]\n",
      "499: Train Loss: [2.3798304, 0.33323354, 4.4264274] | Test Loss: [2.516943, 0.37725955, 4.656626]\n",
      "500: Train Loss: [2.312904, 0.3254508, 4.300357] | Test Loss: [2.412182, 0.4648381, 4.359526]\n",
      "501: Train Loss: [2.3080642, 0.3883461, 4.2277822] | Test Loss: [2.323812, 0.37799314, 4.269631]\n",
      "502: Train Loss: [2.2948627, 0.31602153, 4.273704] | Test Loss: [2.506394, 0.32901713, 4.6837707]\n",
      "503: Train Loss: [2.4017303, 0.33607087, 4.4673896] | Test Loss: [2.3948307, 0.36333176, 4.4263296]\n",
      "504: Train Loss: [2.3594964, 0.33398718, 4.3850055] | Test Loss: [2.574581, 0.3319713, 4.8171906]\n",
      "505: Train Loss: [2.4348564, 0.29852086, 4.571192] | Test Loss: [2.4543633, 0.4056417, 4.503085]\n",
      "506: Train Loss: [2.487231, 0.3236472, 4.650815] | Test Loss: [2.3588262, 0.32198745, 4.3956647]\n",
      "507: Train Loss: [2.3002136, 0.31107092, 4.289356] | Test Loss: [2.6154058, 0.3321691, 4.8986425]\n",
      "508: Train Loss: [2.399637, 0.32142895, 4.477845] | Test Loss: [2.4447465, 0.38294822, 4.5065446]\n",
      "509: Train Loss: [2.4068365, 0.35810927, 4.4555635] | Test Loss: [2.5009923, 0.36810187, 4.6338825]\n",
      "510: Train Loss: [2.4701037, 0.3223269, 4.6178803] | Test Loss: [2.6081307, 0.36628854, 4.8499727]\n",
      "511: Train Loss: [2.4608397, 0.37995377, 4.5417256] | Test Loss: [2.5939658, 0.41788423, 4.770047]\n",
      "512: Train Loss: [2.3317451, 0.30783448, 4.3556557] | Test Loss: [2.293974, 0.22134288, 4.366605]\n",
      "513: Train Loss: [2.3294442, 0.28571218, 4.373176] | Test Loss: [2.483753, 0.329743, 4.637763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514: Train Loss: [2.3212671, 0.31562933, 4.326905] | Test Loss: [2.4166167, 0.3975965, 4.435637]\n",
      "515: Train Loss: [2.4024615, 0.31245574, 4.4924674] | Test Loss: [2.3223398, 0.32169753, 4.322982]\n",
      "516: Train Loss: [2.5777657, 0.4175746, 4.737957] | Test Loss: [2.5336735, 0.3311607, 4.7361865]\n",
      "517: Train Loss: [2.609678, 0.41687754, 4.8024783] | Test Loss: [2.550904, 0.38413587, 4.7176723]\n",
      "518: Train Loss: [2.3282447, 0.3227697, 4.3337197] | Test Loss: [2.585569, 0.39161897, 4.779519]\n",
      "519: Train Loss: [2.4425652, 0.32224593, 4.5628843] | Test Loss: [2.5262785, 0.36727345, 4.6852837]\n",
      "520: Train Loss: [2.4471314, 0.3164094, 4.577853] | Test Loss: [2.5315745, 0.38318077, 4.6799684]\n",
      "521: Train Loss: [2.4770262, 0.36559278, 4.5884595] | Test Loss: [2.6041746, 0.33791187, 4.870437]\n",
      "522: Train Loss: [2.4936311, 0.45722702, 4.530035] | Test Loss: [2.6609857, 0.41925013, 4.9027214]\n",
      "523: Train Loss: [2.8022819, 0.34861353, 5.25595] | Test Loss: [2.6999671, 0.46637604, 4.9335585]\n",
      "Epoch 6\n",
      "0: Train Loss: [2.4608526, 0.37447748, 4.547228] | Test Loss: [2.5551014, 0.3766689, 4.733534]\n",
      "1: Train Loss: [2.3916464, 0.43042675, 4.352866] | Test Loss: [2.682663, 0.4238076, 4.9415183]\n",
      "2: Train Loss: [2.2868543, 0.371835, 4.2018733] | Test Loss: [2.6338458, 0.37603578, 4.891656]\n",
      "3: Train Loss: [2.36705, 0.36280906, 4.3712907] | Test Loss: [2.6822147, 0.37445664, 4.989973]\n",
      "4: Train Loss: [2.399479, 0.37635887, 4.422599] | Test Loss: [2.686294, 0.34443626, 5.028152]\n",
      "5: Train Loss: [2.4926836, 0.36173567, 4.6236315] | Test Loss: [2.6305656, 0.34747043, 4.913661]\n",
      "6: Train Loss: [2.3369422, 0.4464426, 4.227442] | Test Loss: [2.4771724, 0.34403786, 4.6103067]\n",
      "7: Train Loss: [2.2644048, 0.32853532, 4.2002745] | Test Loss: [2.6040623, 0.30898318, 4.8991413]\n",
      "8: Train Loss: [2.4298742, 0.3386638, 4.521085] | Test Loss: [2.4870334, 0.391928, 4.5821385]\n",
      "9: Train Loss: [2.3839142, 0.3684589, 4.3993697] | Test Loss: [2.5336, 0.31050614, 4.756694]\n",
      "10: Train Loss: [2.284204, 0.31837836, 4.2500296] | Test Loss: [2.5078359, 0.31383023, 4.7018414]\n",
      "11: Train Loss: [2.417838, 0.47351336, 4.3621626] | Test Loss: [2.5484426, 0.36983532, 4.72705]\n",
      "12: Train Loss: [2.4362133, 0.3435028, 4.5289235] | Test Loss: [2.5235384, 0.36056602, 4.6865106]\n",
      "13: Train Loss: [2.493626, 0.39118484, 4.5960674] | Test Loss: [2.6167998, 0.34698975, 4.88661]\n",
      "14: Train Loss: [2.3570023, 0.36834624, 4.3456583] | Test Loss: [2.6267045, 0.38961232, 4.8637967]\n",
      "15: Train Loss: [2.3386924, 0.35710102, 4.320284] | Test Loss: [2.5771184, 0.36689115, 4.7873454]\n",
      "16: Train Loss: [2.3164265, 0.35802954, 4.2748237] | Test Loss: [2.5915034, 0.3434588, 4.839548]\n",
      "17: Train Loss: [2.291566, 0.305881, 4.277251] | Test Loss: [2.425738, 0.37786806, 4.473608]\n",
      "18: Train Loss: [2.3458924, 0.32882154, 4.362963] | Test Loss: [2.6212232, 0.38406894, 4.8583775]\n",
      "19: Train Loss: [2.23466, 0.3299455, 4.1393743] | Test Loss: [2.5092554, 0.34144878, 4.677062]\n",
      "20: Train Loss: [2.5627713, 0.50246567, 4.623077] | Test Loss: [2.440379, 0.32400176, 4.556756]\n",
      "21: Train Loss: [2.332117, 0.3059726, 4.3582616] | Test Loss: [2.573093, 0.43671507, 4.7094707]\n",
      "22: Train Loss: [2.3108091, 0.3862916, 4.235327] | Test Loss: [2.6071012, 0.46228617, 4.7519164]\n",
      "23: Train Loss: [2.3818195, 0.3997188, 4.36392] | Test Loss: [2.7012916, 0.36038342, 5.0421996]\n",
      "24: Train Loss: [2.3753786, 0.30340797, 4.447349] | Test Loss: [2.5543602, 0.32838666, 4.7803335]\n",
      "25: Train Loss: [2.301326, 0.3639343, 4.2387176] | Test Loss: [2.6537948, 0.35002145, 4.957568]\n",
      "26: Train Loss: [2.354583, 0.41582146, 4.2933445] | Test Loss: [2.478193, 0.32420427, 4.6321816]\n",
      "27: Train Loss: [2.3901541, 0.4018546, 4.3784537] | Test Loss: [2.7014182, 0.38736925, 5.015467]\n",
      "28: Train Loss: [2.369685, 0.2925771, 4.4467926] | Test Loss: [2.5655904, 0.3391769, 4.7920036]\n",
      "29: Train Loss: [2.1985168, 0.33466232, 4.0623713] | Test Loss: [2.5126166, 0.35886317, 4.66637]\n",
      "30: Train Loss: [2.3108811, 0.30637047, 4.315392] | Test Loss: [2.6905189, 0.3949992, 4.9860387]\n",
      "31: Train Loss: [2.415678, 0.41835204, 4.413004] | Test Loss: [2.46511, 0.37049663, 4.5597234]\n",
      "32: Train Loss: [2.4267225, 0.3588855, 4.49456] | Test Loss: [2.6990833, 0.49615493, 4.902012]\n",
      "33: Train Loss: [2.2629547, 0.30858514, 4.2173243] | Test Loss: [2.5071626, 0.3669427, 4.6473823]\n",
      "34: Train Loss: [2.2105408, 0.4104108, 4.0106707] | Test Loss: [2.4327033, 0.44519904, 4.4202075]\n",
      "35: Train Loss: [2.1918082, 0.28849894, 4.0951176] | Test Loss: [2.5672, 0.3379107, 4.7964892]\n",
      "36: Train Loss: [2.3787613, 0.4479732, 4.3095493] | Test Loss: [2.7609255, 0.39969626, 5.1221547]\n",
      "37: Train Loss: [2.417718, 0.3874148, 4.448021] | Test Loss: [2.592278, 0.33028162, 4.8542743]\n",
      "38: Train Loss: [2.380842, 0.33651164, 4.4251723] | Test Loss: [2.4914668, 0.3502226, 4.632711]\n",
      "39: Train Loss: [2.5607152, 0.4520475, 4.669383] | Test Loss: [2.5669572, 0.3620549, 4.7718596]\n",
      "40: Train Loss: [2.2714972, 0.37643537, 4.166559] | Test Loss: [2.4999409, 0.4019314, 4.5979505]\n",
      "41: Train Loss: [2.4352968, 0.35450318, 4.5160904] | Test Loss: [2.6161826, 0.38377967, 4.8485856]\n",
      "42: Train Loss: [2.2655628, 0.36826494, 4.1628604] | Test Loss: [2.4502852, 0.31223497, 4.5883355]\n",
      "43: Train Loss: [2.2428606, 0.33804452, 4.1476765] | Test Loss: [2.4827437, 0.33339533, 4.632092]\n",
      "44: Train Loss: [2.311315, 0.36956084, 4.2530694] | Test Loss: [2.5920124, 0.3413781, 4.8426466]\n",
      "45: Train Loss: [2.2835312, 0.32604778, 4.2410145] | Test Loss: [2.6224189, 0.38361165, 4.861226]\n",
      "46: Train Loss: [2.5374048, 0.29971683, 4.7750926] | Test Loss: [2.6321704, 0.41098815, 4.8533525]\n",
      "47: Train Loss: [2.5033123, 0.3402992, 4.6663256] | Test Loss: [2.3788126, 0.33364785, 4.4239774]\n",
      "48: Train Loss: [2.1791472, 0.32705837, 4.031236] | Test Loss: [2.1843607, 0.3463895, 4.022332]\n",
      "49: Train Loss: [2.3719895, 0.32550576, 4.4184732] | Test Loss: [2.6000896, 0.33542415, 4.864755]\n",
      "50: Train Loss: [2.4483645, 0.37949258, 4.517236] | Test Loss: [2.5136242, 0.37594292, 4.6513057]\n",
      "51: Train Loss: [2.2881427, 0.31906164, 4.2572236] | Test Loss: [2.579477, 0.38649458, 4.7724595]\n",
      "52: Train Loss: [2.3232338, 0.39800328, 4.2484646] | Test Loss: [2.6034284, 0.3575003, 4.8493567]\n",
      "53: Train Loss: [2.3947396, 0.37411773, 4.4153614] | Test Loss: [2.6046412, 0.34947622, 4.859806]\n",
      "54: Train Loss: [2.312359, 0.36806208, 4.256656] | Test Loss: [2.4720185, 0.3646475, 4.5793896]\n",
      "55: Train Loss: [2.3624766, 0.4155807, 4.3093724] | Test Loss: [2.729569, 0.34363458, 5.1155033]\n",
      "56: Train Loss: [2.2391982, 0.33190456, 4.146492] | Test Loss: [2.4420252, 0.3304478, 4.5536027]\n",
      "57: Train Loss: [2.3898783, 0.36472398, 4.4150324] | Test Loss: [2.4600585, 0.32986698, 4.59025]\n",
      "58: Train Loss: [2.287809, 0.35781786, 4.2178] | Test Loss: [2.596941, 0.34591082, 4.847971]\n",
      "59: Train Loss: [2.4289827, 0.38923204, 4.4687333] | Test Loss: [2.457644, 0.38039425, 4.5348935]\n",
      "60: Train Loss: [2.432092, 0.31866595, 4.545518] | Test Loss: [2.6087646, 0.35534787, 4.8621817]\n",
      "61: Train Loss: [2.4360802, 0.31508452, 4.557076] | Test Loss: [2.6630602, 0.4068101, 4.91931]\n",
      "62: Train Loss: [2.3599315, 0.35972205, 4.360141] | Test Loss: [2.4331288, 0.41260573, 4.453652]\n",
      "63: Train Loss: [2.3273802, 0.37078118, 4.283979] | Test Loss: [2.630429, 0.4108861, 4.849972]\n",
      "64: Train Loss: [2.3806958, 0.32046586, 4.4409256] | Test Loss: [2.5412643, 0.3366551, 4.7458735]\n",
      "65: Train Loss: [2.442467, 0.3843359, 4.500598] | Test Loss: [2.7893636, 0.34691626, 5.231811]\n",
      "66: Train Loss: [2.2413063, 0.3388059, 4.1438065] | Test Loss: [2.6778157, 0.36297783, 4.9926534]\n",
      "67: Train Loss: [2.2323148, 0.3259808, 4.138649] | Test Loss: [2.5227544, 0.4299689, 4.61554]\n",
      "68: Train Loss: [2.2970343, 0.38516963, 4.208899] | Test Loss: [2.4791636, 0.3463889, 4.6119385]\n",
      "69: Train Loss: [2.4420524, 0.34729418, 4.5368104] | Test Loss: [2.4539988, 0.3109013, 4.5970964]\n",
      "70: Train Loss: [2.3393126, 0.3807506, 4.2978745] | Test Loss: [2.3383396, 0.3131911, 4.363488]\n",
      "71: Train Loss: [2.4197724, 0.32715568, 4.512389] | Test Loss: [2.7302577, 0.34051892, 5.1199965]\n",
      "72: Train Loss: [2.3900523, 0.35969013, 4.4204144] | Test Loss: [2.4666986, 0.43439752, 4.4989996]\n",
      "73: Train Loss: [2.342436, 0.3638705, 4.3210015] | Test Loss: [2.7044463, 0.4529083, 4.955984]\n",
      "74: Train Loss: [2.358826, 0.32993832, 4.3877134] | Test Loss: [2.696531, 0.428688, 4.964374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75: Train Loss: [2.3956244, 0.34930247, 4.4419465] | Test Loss: [2.3914466, 0.33558404, 4.447309]\n",
      "76: Train Loss: [2.352096, 0.3602499, 4.343942] | Test Loss: [2.4963565, 0.35662457, 4.6360884]\n",
      "77: Train Loss: [2.442671, 0.5142288, 4.3711133] | Test Loss: [2.57942, 0.37566552, 4.7831745]\n",
      "78: Train Loss: [2.4533086, 0.3463145, 4.5603027] | Test Loss: [2.4676065, 0.3311145, 4.604099]\n",
      "79: Train Loss: [2.43908, 0.3732825, 4.5048776] | Test Loss: [2.702053, 0.37677485, 5.0273314]\n",
      "80: Train Loss: [2.5110636, 0.320451, 4.7016764] | Test Loss: [2.50696, 0.32134563, 4.692574]\n",
      "81: Train Loss: [2.4633965, 0.3535673, 4.573226] | Test Loss: [2.3783817, 0.3913513, 4.365412]\n",
      "82: Train Loss: [2.3257644, 0.37462652, 4.276902] | Test Loss: [2.5347822, 0.30586305, 4.7637014]\n",
      "83: Train Loss: [2.4323628, 0.361757, 4.502969] | Test Loss: [2.6302867, 0.38433513, 4.8762383]\n",
      "84: Train Loss: [2.3224735, 0.34864056, 4.2963066] | Test Loss: [2.3591652, 0.35522383, 4.3631067]\n",
      "85: Train Loss: [2.3616664, 0.43209326, 4.2912397] | Test Loss: [2.677299, 0.4122597, 4.9423385]\n",
      "86: Train Loss: [2.2524104, 0.3093922, 4.195429] | Test Loss: [2.5849838, 0.4187817, 4.751186]\n",
      "87: Train Loss: [2.293367, 0.3593096, 4.227424] | Test Loss: [2.7110205, 0.32662508, 5.095416]\n",
      "88: Train Loss: [2.4214787, 0.41282386, 4.430134] | Test Loss: [2.6638412, 0.39852408, 4.929158]\n",
      "89: Train Loss: [2.3308344, 0.3933918, 4.268277] | Test Loss: [2.486463, 0.40118754, 4.5717387]\n",
      "90: Train Loss: [2.3276498, 0.32448664, 4.330813] | Test Loss: [2.4920306, 0.39499083, 4.5890703]\n",
      "91: Train Loss: [2.3669837, 0.37970102, 4.354266] | Test Loss: [2.3979287, 0.35868925, 4.437168]\n",
      "92: Train Loss: [2.2861278, 0.37427467, 4.197981] | Test Loss: [2.6052248, 0.36186072, 4.848589]\n",
      "93: Train Loss: [2.3657522, 0.30307722, 4.428427] | Test Loss: [2.6171112, 0.35288057, 4.881342]\n",
      "94: Train Loss: [2.3229978, 0.34574938, 4.3002462] | Test Loss: [2.657583, 0.37813833, 4.9370275]\n",
      "95: Train Loss: [2.4537728, 0.403812, 4.5037336] | Test Loss: [2.4493616, 0.44847977, 4.4502435]\n",
      "96: Train Loss: [2.3111534, 0.41602308, 4.2062836] | Test Loss: [2.7444708, 0.38968155, 5.0992603]\n",
      "97: Train Loss: [2.4282877, 0.36978194, 4.4867935] | Test Loss: [2.555437, 0.3742455, 4.7366285]\n",
      "98: Train Loss: [2.4222724, 0.31504822, 4.5294967] | Test Loss: [2.4819431, 0.3639473, 4.599939]\n",
      "99: Train Loss: [2.446872, 0.34965107, 4.544093] | Test Loss: [2.7095113, 0.3587099, 5.0603127]\n",
      "100: Train Loss: [2.4780254, 0.4205803, 4.5354705] | Test Loss: [2.4828389, 0.37044626, 4.5952315]\n",
      "101: Train Loss: [2.1971374, 0.34640658, 4.0478683] | Test Loss: [2.4691687, 0.34180093, 4.596536]\n",
      "102: Train Loss: [2.3926625, 0.36110458, 4.4242206] | Test Loss: [2.638102, 0.31240082, 4.9638033]\n",
      "103: Train Loss: [2.3338933, 0.43800476, 4.2297816] | Test Loss: [2.424722, 0.3681148, 4.481329]\n",
      "104: Train Loss: [2.3958004, 0.3961877, 4.395413] | Test Loss: [2.5524068, 0.3054231, 4.7993903]\n",
      "105: Train Loss: [2.219709, 0.2928586, 4.1465592] | Test Loss: [2.7884932, 0.4027717, 5.1742144]\n",
      "106: Train Loss: [2.4129653, 0.309294, 4.516637] | Test Loss: [2.3542683, 0.41587222, 4.2926645]\n",
      "107: Train Loss: [2.416013, 0.36617652, 4.4658494] | Test Loss: [2.487739, 0.32423636, 4.651242]\n",
      "108: Train Loss: [2.3543146, 0.33833992, 4.3702893] | Test Loss: [2.6697776, 0.37585288, 4.963702]\n",
      "109: Train Loss: [2.5642998, 0.3740594, 4.7545404] | Test Loss: [2.4548333, 0.39154667, 4.51812]\n",
      "110: Train Loss: [2.449936, 0.37999958, 4.519872] | Test Loss: [2.5141537, 0.33532593, 4.6929817]\n",
      "111: Train Loss: [2.3277407, 0.307861, 4.3476205] | Test Loss: [2.669423, 0.3541258, 4.98472]\n",
      "112: Train Loss: [2.4160156, 0.36976534, 4.462266] | Test Loss: [2.569603, 0.33357063, 4.8056355]\n",
      "113: Train Loss: [2.3873863, 0.33873802, 4.4360347] | Test Loss: [2.6319056, 0.3473755, 4.9164357]\n",
      "114: Train Loss: [2.4778912, 0.3460167, 4.6097655] | Test Loss: [2.4128294, 0.4315626, 4.3940964]\n",
      "115: Train Loss: [2.4724298, 0.3263263, 4.618533] | Test Loss: [2.6968656, 0.35832518, 5.035406]\n",
      "116: Train Loss: [2.5416813, 0.33381277, 4.74955] | Test Loss: [2.537466, 0.33869013, 4.736242]\n",
      "117: Train Loss: [2.4434357, 0.4008323, 4.486039] | Test Loss: [2.633742, 0.41462553, 4.8528585]\n",
      "118: Train Loss: [2.2703438, 0.3266259, 4.2140617] | Test Loss: [2.5844388, 0.35770774, 4.81117]\n",
      "119: Train Loss: [2.4369745, 0.33793592, 4.536013] | Test Loss: [2.7402685, 0.4275282, 5.0530086]\n",
      "120: Train Loss: [2.137008, 0.33605933, 3.9379566] | Test Loss: [2.5345585, 0.30746692, 4.76165]\n",
      "121: Train Loss: [2.5601273, 0.30852053, 4.811734] | Test Loss: [2.5621316, 0.39346966, 4.7307935]\n",
      "122: Train Loss: [2.451629, 0.36156893, 4.541689] | Test Loss: [2.613714, 0.33203152, 4.895396]\n",
      "123: Train Loss: [2.4103553, 0.34682798, 4.4738827] | Test Loss: [2.7761595, 0.36678165, 5.1855373]\n",
      "124: Train Loss: [2.4509563, 0.35927472, 4.542638] | Test Loss: [2.582675, 0.3474154, 4.8179345]\n",
      "125: Train Loss: [2.323605, 0.37145853, 4.2757516] | Test Loss: [2.716266, 0.41032097, 5.022211]\n",
      "126: Train Loss: [2.370261, 0.50271976, 4.237802] | Test Loss: [2.761825, 0.35921136, 5.1644387]\n",
      "127: Train Loss: [2.5247538, 0.377637, 4.6718707] | Test Loss: [2.5474565, 0.36067933, 4.734234]\n",
      "128: Train Loss: [2.623333, 0.4022291, 4.8444366] | Test Loss: [2.5289447, 0.37935653, 4.678533]\n",
      "129: Train Loss: [2.5717025, 0.35778794, 4.785617] | Test Loss: [2.6220503, 0.4150327, 4.8290677]\n",
      "130: Train Loss: [2.4565158, 0.32116023, 4.5918713] | Test Loss: [2.6008494, 0.34876212, 4.8529367]\n",
      "131: Train Loss: [2.4878693, 0.3502522, 4.6254864] | Test Loss: [2.6814797, 0.4348071, 4.928152]\n",
      "132: Train Loss: [2.459178, 0.34205568, 4.5763] | Test Loss: [2.6392052, 0.32052898, 4.9578815]\n",
      "133: Train Loss: [2.6394224, 0.35614958, 4.922695] | Test Loss: [2.6419313, 0.39682865, 4.887034]\n",
      "134: Train Loss: [2.397864, 0.34935987, 4.446368] | Test Loss: [2.6249466, 0.34857303, 4.90132]\n",
      "135: Train Loss: [2.4863403, 0.29622033, 4.6764603] | Test Loss: [2.646343, 0.43896675, 4.853719]\n",
      "136: Train Loss: [2.5489361, 0.35162646, 4.746246] | Test Loss: [2.442838, 0.40459338, 4.4810824]\n",
      "137: Train Loss: [2.3812022, 0.27799287, 4.4844117] | Test Loss: [2.4140592, 0.37391075, 4.4542074]\n",
      "138: Train Loss: [2.454983, 0.43307826, 4.4768877] | Test Loss: [2.653695, 0.24717832, 5.060212]\n",
      "139: Train Loss: [2.4583929, 0.3525086, 4.564277] | Test Loss: [2.828324, 0.3399317, 5.3167167]\n",
      "140: Train Loss: [2.4700742, 0.39557007, 4.544578] | Test Loss: [2.6806765, 0.35532892, 5.006024]\n",
      "141: Train Loss: [2.516241, 0.36385274, 4.668629] | Test Loss: [2.6420815, 0.3481017, 4.9360614]\n",
      "142: Train Loss: [2.4239535, 0.3159805, 4.5319266] | Test Loss: [2.5971324, 0.41354144, 4.7807236]\n",
      "143: Train Loss: [2.5131876, 0.3844744, 4.641901] | Test Loss: [2.520998, 0.3857495, 4.6562467]\n",
      "144: Train Loss: [2.3756912, 0.40033492, 4.3510475] | Test Loss: [2.6905217, 0.35802794, 5.0230155]\n",
      "145: Train Loss: [2.5785894, 0.3734713, 4.7837076] | Test Loss: [2.635533, 0.3703483, 4.9007177]\n",
      "146: Train Loss: [2.587474, 0.34638947, 4.828559] | Test Loss: [2.7414606, 0.35200778, 5.1309133]\n",
      "147: Train Loss: [2.3474996, 0.33200452, 4.3629947] | Test Loss: [2.5071323, 0.32446036, 4.689804]\n",
      "148: Train Loss: [2.4992406, 0.32519332, 4.673288] | Test Loss: [2.688323, 0.3845862, 4.9920597]\n",
      "149: Train Loss: [2.4254596, 0.31617358, 4.5347457] | Test Loss: [2.6789823, 0.35756436, 5.0004]\n",
      "150: Train Loss: [2.5247889, 0.32575488, 4.7238226] | Test Loss: [2.6880004, 0.45227048, 4.9237304]\n",
      "151: Train Loss: [2.4613624, 0.35587415, 4.5668507] | Test Loss: [2.6767497, 0.43522593, 4.9182734]\n",
      "152: Train Loss: [2.4502435, 0.34918642, 4.5513005] | Test Loss: [2.523896, 0.387379, 4.660413]\n",
      "153: Train Loss: [2.3054, 0.3763912, 4.2344084] | Test Loss: [2.7270174, 0.3725499, 5.081485]\n",
      "154: Train Loss: [2.2889168, 0.36545843, 4.212375] | Test Loss: [2.5779073, 0.29998598, 4.855829]\n",
      "155: Train Loss: [2.4232435, 0.35083187, 4.495655] | Test Loss: [2.6874926, 0.33105427, 5.043931]\n",
      "156: Train Loss: [2.4744556, 0.36280292, 4.586108] | Test Loss: [2.5912755, 0.34978992, 4.832761]\n",
      "157: Train Loss: [2.3284817, 0.3436293, 4.313334] | Test Loss: [2.4688606, 0.34157327, 4.596148]\n",
      "158: Train Loss: [2.2765565, 0.295216, 4.257897] | Test Loss: [2.7460241, 0.4909041, 5.001144]\n",
      "159: Train Loss: [2.4171731, 0.3360072, 4.498339] | Test Loss: [2.7054293, 0.39784527, 5.0130134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160: Train Loss: [2.5611815, 0.36277, 4.759593] | Test Loss: [2.6166165, 0.36999992, 4.863233]\n",
      "161: Train Loss: [2.4591303, 0.32319963, 4.595061] | Test Loss: [2.5856993, 0.40576, 4.765639]\n",
      "162: Train Loss: [2.4246438, 0.29052308, 4.5587645] | Test Loss: [2.68474, 0.37336785, 4.9961123]\n",
      "163: Train Loss: [2.4585018, 0.30947846, 4.6075253] | Test Loss: [2.5331028, 0.39705405, 4.6691513]\n",
      "164: Train Loss: [2.5972211, 0.39886445, 4.795578] | Test Loss: [2.6303954, 0.31634864, 4.9444423]\n",
      "165: Train Loss: [2.4962811, 0.32657617, 4.665986] | Test Loss: [2.527894, 0.32607698, 4.729711]\n",
      "166: Train Loss: [2.5021148, 0.33687168, 4.667358] | Test Loss: [2.445506, 0.3118375, 4.5791745]\n",
      "167: Train Loss: [2.6308448, 0.4194655, 4.842224] | Test Loss: [2.5948899, 0.3853583, 4.8044214]\n",
      "168: Train Loss: [2.503433, 0.34899107, 4.657875] | Test Loss: [2.6483736, 0.3345387, 4.9622087]\n",
      "169: Train Loss: [2.4448473, 0.38283488, 4.50686] | Test Loss: [2.5672956, 0.31044927, 4.824142]\n",
      "170: Train Loss: [2.5597465, 0.38424328, 4.7352495] | Test Loss: [2.6323671, 0.40550926, 4.859225]\n",
      "171: Train Loss: [2.4420862, 0.34262332, 4.541549] | Test Loss: [2.6624966, 0.41370866, 4.9112844]\n",
      "172: Train Loss: [2.5486355, 0.2916999, 4.805571] | Test Loss: [2.5630364, 0.37616754, 4.7499056]\n",
      "173: Train Loss: [2.4843569, 0.34042376, 4.62829] | Test Loss: [2.6944642, 0.4039377, 4.9849906]\n",
      "174: Train Loss: [2.3668694, 0.28126556, 4.452473] | Test Loss: [2.5159428, 0.3840305, 4.6478553]\n",
      "175: Train Loss: [2.550665, 0.38015136, 4.7211785] | Test Loss: [2.5773642, 0.40155697, 4.7531714]\n",
      "176: Train Loss: [2.3427794, 0.4841208, 4.201438] | Test Loss: [2.551889, 0.36923847, 4.7345395]\n",
      "177: Train Loss: [2.4854593, 0.29484004, 4.676079] | Test Loss: [2.5320647, 0.33715445, 4.726975]\n",
      "178: Train Loss: [2.4462547, 0.32730946, 4.5652] | Test Loss: [2.7162018, 0.3334956, 5.098908]\n",
      "179: Train Loss: [2.5764852, 0.3805182, 4.7724524] | Test Loss: [2.474384, 0.3625947, 4.5861735]\n",
      "180: Train Loss: [2.4390776, 0.34577236, 4.532383] | Test Loss: [2.5668178, 0.35850328, 4.775132]\n",
      "181: Train Loss: [2.397402, 0.32131907, 4.473485] | Test Loss: [2.580991, 0.38131478, 4.7806673]\n",
      "182: Train Loss: [2.336011, 0.3816106, 4.2904115] | Test Loss: [2.5929275, 0.35084122, 4.835014]\n",
      "183: Train Loss: [2.4031508, 0.32101202, 4.4852896] | Test Loss: [2.705173, 0.3506599, 5.059686]\n",
      "184: Train Loss: [2.440544, 0.3730437, 4.5080442] | Test Loss: [2.7502751, 0.32035968, 5.1801906]\n",
      "185: Train Loss: [2.436352, 0.36612892, 4.506575] | Test Loss: [2.5299544, 0.3763611, 4.683548]\n",
      "186: Train Loss: [2.5048554, 0.36370155, 4.6460094] | Test Loss: [2.659676, 0.34096068, 4.9783916]\n",
      "187: Train Loss: [2.3022957, 0.39725816, 4.207333] | Test Loss: [2.6027536, 0.3622595, 4.843248]\n",
      "188: Train Loss: [2.5322058, 0.33303884, 4.731373] | Test Loss: [2.6052392, 0.3999272, 4.810551]\n",
      "189: Train Loss: [2.4702094, 0.35013485, 4.590284] | Test Loss: [2.7097123, 0.35586405, 5.0635605]\n",
      "190: Train Loss: [2.496299, 0.329274, 4.663324] | Test Loss: [2.6844132, 0.4178551, 4.950971]\n",
      "191: Train Loss: [2.5181375, 0.38719192, 4.649083] | Test Loss: [2.5495331, 0.38938624, 4.70968]\n",
      "192: Train Loss: [2.3595157, 0.2930566, 4.425975] | Test Loss: [2.6682596, 0.48512715, 4.8513923]\n",
      "193: Train Loss: [2.3166828, 0.3291569, 4.3042088] | Test Loss: [2.4654727, 0.31420252, 4.616743]\n",
      "194: Train Loss: [2.5351436, 0.3675882, 4.702699] | Test Loss: [2.7694733, 0.37206343, 5.166883]\n",
      "195: Train Loss: [2.5183582, 0.40623316, 4.630483] | Test Loss: [2.5660937, 0.37433782, 4.7578497]\n",
      "196: Train Loss: [2.509164, 0.41621292, 4.602115] | Test Loss: [2.5438545, 0.32101366, 4.7666955]\n",
      "197: Train Loss: [2.596739, 0.35710967, 4.8363686] | Test Loss: [2.5113354, 0.3336845, 4.6889863]\n",
      "198: Train Loss: [2.4143906, 0.30208704, 4.5266943] | Test Loss: [2.4868464, 0.3642967, 4.609396]\n",
      "199: Train Loss: [2.4320858, 0.3321531, 4.532018] | Test Loss: [2.6945238, 0.37338617, 5.0156612]\n",
      "200: Train Loss: [2.7657864, 0.40205783, 5.129515] | Test Loss: [2.5522313, 0.4027838, 4.7016788]\n",
      "201: Train Loss: [2.4325447, 0.34440964, 4.52068] | Test Loss: [2.695167, 0.44914034, 4.9411936]\n",
      "202: Train Loss: [2.578438, 0.33368105, 4.823195] | Test Loss: [2.5529969, 0.30942175, 4.796572]\n",
      "203: Train Loss: [2.3611345, 0.40598047, 4.3162885] | Test Loss: [2.6458757, 0.32222942, 4.969522]\n",
      "204: Train Loss: [2.443174, 0.3294822, 4.5568657] | Test Loss: [2.6356626, 0.36855137, 4.902774]\n",
      "205: Train Loss: [2.494629, 0.34581637, 4.643441] | Test Loss: [2.5742881, 0.34958628, 4.79899]\n",
      "206: Train Loss: [2.6370177, 0.42693365, 4.8471017] | Test Loss: [2.491079, 0.3448084, 4.6373496]\n",
      "207: Train Loss: [2.4529364, 0.3478394, 4.5580335] | Test Loss: [2.676506, 0.39048895, 4.962523]\n",
      "208: Train Loss: [2.4432476, 0.39912695, 4.487368] | Test Loss: [2.4149406, 0.36733896, 4.462542]\n",
      "209: Train Loss: [2.4718082, 0.40495375, 4.5386624] | Test Loss: [2.67714, 0.41935486, 4.934925]\n",
      "210: Train Loss: [2.4467306, 0.3505147, 4.5429463] | Test Loss: [2.6273174, 0.345963, 4.908672]\n",
      "211: Train Loss: [2.3906548, 0.38790378, 4.393406] | Test Loss: [2.5920632, 0.3842025, 4.799924]\n",
      "212: Train Loss: [2.1967826, 0.3046696, 4.088896] | Test Loss: [2.64474, 0.3579676, 4.931513]\n",
      "213: Train Loss: [2.3999987, 0.33670783, 4.4632897] | Test Loss: [2.5383215, 0.37183577, 4.7048073]\n",
      "214: Train Loss: [2.56752, 0.42304757, 4.7119923] | Test Loss: [2.595282, 0.36235407, 4.82821]\n",
      "215: Train Loss: [2.585787, 0.46870634, 4.702868] | Test Loss: [2.5344605, 0.3827781, 4.686143]\n",
      "216: Train Loss: [2.4368174, 0.40380308, 4.469832] | Test Loss: [2.6382415, 0.3899247, 4.8865585]\n",
      "217: Train Loss: [2.4054608, 0.36310014, 4.4478216] | Test Loss: [2.3729062, 0.2994413, 4.446371]\n",
      "218: Train Loss: [2.4286153, 0.39508283, 4.4621477] | Test Loss: [2.6871262, 0.3944374, 4.979815]\n",
      "219: Train Loss: [2.399234, 0.34536898, 4.4530993] | Test Loss: [2.5689702, 0.3726529, 4.7652874]\n",
      "220: Train Loss: [2.439179, 0.35551262, 4.5228453] | Test Loss: [2.6792533, 0.37269923, 4.9858074]\n",
      "221: Train Loss: [2.5570688, 0.37253064, 4.741607] | Test Loss: [2.51488, 0.33195373, 4.6978064]\n",
      "222: Train Loss: [2.4380674, 0.333493, 4.5426416] | Test Loss: [2.6365147, 0.40046778, 4.8725615]\n",
      "223: Train Loss: [2.3601282, 0.38923484, 4.3310213] | Test Loss: [2.654926, 0.4585766, 4.8512754]\n",
      "224: Train Loss: [2.3658404, 0.40456942, 4.3271112] | Test Loss: [2.33055, 0.3254278, 4.335672]\n",
      "225: Train Loss: [2.3301277, 0.35689485, 4.3033605] | Test Loss: [2.7298062, 0.35228997, 5.107322]\n",
      "226: Train Loss: [2.3509307, 0.352451, 4.3494105] | Test Loss: [2.6727023, 0.33721662, 5.008188]\n",
      "227: Train Loss: [2.365389, 0.4056695, 4.3251085] | Test Loss: [2.4346004, 0.40408, 4.465121]\n",
      "228: Train Loss: [2.4410198, 0.38692588, 4.495114] | Test Loss: [2.294282, 0.571701, 4.016863]\n",
      "229: Train Loss: [2.437028, 0.38244992, 4.4916058] | Test Loss: [2.2512615, 0.33282357, 4.169699]\n",
      "230: Train Loss: [2.3095944, 0.39759687, 4.221592] | Test Loss: [2.7337837, 0.42848128, 5.0390863]\n",
      "231: Train Loss: [2.3682265, 0.3852888, 4.3511643] | Test Loss: [2.6595588, 0.39873415, 4.9203835]\n",
      "232: Train Loss: [2.4445546, 0.35244972, 4.5366592] | Test Loss: [2.6074944, 0.43730202, 4.7776866]\n",
      "233: Train Loss: [2.459548, 0.3443312, 4.5747647] | Test Loss: [2.6103816, 0.33449152, 4.8862715]\n",
      "234: Train Loss: [2.4277062, 0.31264058, 4.542772] | Test Loss: [2.6422522, 0.35929477, 4.9252095]\n",
      "235: Train Loss: [2.4209132, 0.41091722, 4.430909] | Test Loss: [2.7170923, 0.5916281, 4.8425565]\n",
      "236: Train Loss: [2.3377986, 0.29727188, 4.3783255] | Test Loss: [2.5282986, 0.39528343, 4.661314]\n",
      "237: Train Loss: [2.5125375, 0.48387265, 4.541202] | Test Loss: [2.6334763, 0.36839223, 4.89856]\n",
      "238: Train Loss: [2.6136594, 0.43318096, 4.794138] | Test Loss: [2.5753307, 0.4072767, 4.743385]\n",
      "239: Train Loss: [2.4204285, 0.3509823, 4.489875] | Test Loss: [2.5555508, 0.3704932, 4.740608]\n",
      "240: Train Loss: [2.4613216, 0.32449913, 4.598144] | Test Loss: [2.6179192, 0.34608895, 4.8897495]\n",
      "241: Train Loss: [2.506391, 0.33684987, 4.6759324] | Test Loss: [2.5217814, 0.32536992, 4.718193]\n",
      "242: Train Loss: [2.5182476, 0.4091943, 4.6273007] | Test Loss: [2.6415915, 0.3464903, 4.9366927]\n",
      "243: Train Loss: [2.5191731, 0.3605716, 4.677775] | Test Loss: [2.6534817, 0.335128, 4.9718356]\n",
      "244: Train Loss: [2.4428117, 0.3240743, 4.561549] | Test Loss: [2.6595018, 0.29519498, 5.0238085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245: Train Loss: [2.3137124, 0.38645592, 4.2409687] | Test Loss: [2.5667138, 0.43204957, 4.701378]\n",
      "246: Train Loss: [2.5194304, 0.35861844, 4.6802425] | Test Loss: [2.6405377, 0.3339093, 4.947166]\n",
      "247: Train Loss: [2.5178516, 0.3719918, 4.6637115] | Test Loss: [2.5538208, 0.33564144, 4.7720003]\n",
      "248: Train Loss: [2.431903, 0.4011803, 4.4626255] | Test Loss: [2.702287, 0.3679411, 5.036633]\n",
      "249: Train Loss: [2.3640807, 0.3828882, 4.345273] | Test Loss: [2.5523787, 0.37861508, 4.7261424]\n",
      "250: Train Loss: [2.5352478, 0.34958518, 4.7209105] | Test Loss: [2.4607124, 0.37058228, 4.550843]\n",
      "251: Train Loss: [2.4770129, 0.40965465, 4.544371] | Test Loss: [2.441712, 0.35494706, 4.5284767]\n",
      "252: Train Loss: [2.285561, 0.31462038, 4.2565017] | Test Loss: [2.4379377, 0.29198483, 4.5838904]\n",
      "253: Train Loss: [2.4504871, 0.3270805, 4.5738935] | Test Loss: [2.6510549, 0.34458598, 4.957524]\n",
      "254: Train Loss: [2.438582, 0.38815168, 4.4890122] | Test Loss: [2.6151195, 0.39264002, 4.837599]\n",
      "255: Train Loss: [2.3767428, 0.28903338, 4.4644523] | Test Loss: [2.7454925, 0.3435283, 5.1474566]\n",
      "256: Train Loss: [2.5194106, 0.31223968, 4.7265816] | Test Loss: [2.6765885, 0.42928115, 4.923896]\n",
      "257: Train Loss: [2.6835425, 0.3324931, 5.0345917] | Test Loss: [2.7389166, 0.4466533, 5.03118]\n",
      "258: Train Loss: [2.4886308, 0.38160887, 4.5956526] | Test Loss: [2.6586938, 0.32844, 4.9889474]\n",
      "259: Train Loss: [2.6033828, 0.35326415, 4.8535013] | Test Loss: [2.6034122, 0.3240949, 4.8827295]\n",
      "260: Train Loss: [2.5015342, 0.31386286, 4.6892056] | Test Loss: [2.4979517, 0.31065476, 4.685249]\n",
      "261: Train Loss: [2.465162, 0.38083717, 4.549487] | Test Loss: [2.7064734, 0.3819031, 5.0310435]\n",
      "262: Train Loss: [2.5035174, 0.34150866, 4.665526] | Test Loss: [2.5068903, 0.3936884, 4.6200924]\n",
      "263: Train Loss: [2.5511901, 0.35228541, 4.750095] | Test Loss: [2.56285, 0.40595204, 4.719748]\n",
      "264: Train Loss: [2.5732517, 0.41573793, 4.7307653] | Test Loss: [2.7083154, 0.38486162, 5.0317693]\n",
      "265: Train Loss: [2.6072776, 0.36486515, 4.84969] | Test Loss: [2.5243108, 0.32200113, 4.7266207]\n",
      "266: Train Loss: [2.5076392, 0.36402538, 4.6512527] | Test Loss: [2.67027, 0.41496322, 4.9255767]\n",
      "267: Train Loss: [2.524398, 0.34834728, 4.700449] | Test Loss: [2.6499236, 0.32869875, 4.9711485]\n",
      "268: Train Loss: [2.5569332, 0.29627085, 4.8175955] | Test Loss: [2.683369, 0.3653762, 5.001362]\n",
      "269: Train Loss: [2.5724006, 0.39451528, 4.7502856] | Test Loss: [2.5536575, 0.35440704, 4.752908]\n",
      "270: Train Loss: [2.7054856, 0.30584255, 5.105129] | Test Loss: [2.6073987, 0.34084076, 4.8739567]\n",
      "271: Train Loss: [2.4408183, 0.3977655, 4.483871] | Test Loss: [2.5660255, 0.32995135, 4.8020997]\n",
      "272: Train Loss: [2.5809345, 0.37782848, 4.7840405] | Test Loss: [2.5923607, 0.3311498, 4.853572]\n",
      "273: Train Loss: [2.5183213, 0.38699332, 4.649649] | Test Loss: [2.6353571, 0.37012804, 4.900586]\n",
      "274: Train Loss: [2.4564338, 0.40586573, 4.507002] | Test Loss: [2.5269654, 0.36782837, 4.6861024]\n",
      "275: Train Loss: [2.3898547, 0.40664294, 4.3730664] | Test Loss: [2.7090538, 0.35126644, 5.066841]\n",
      "276: Train Loss: [2.3661299, 0.33554173, 4.396718] | Test Loss: [2.6831481, 0.40155986, 4.9647365]\n",
      "277: Train Loss: [2.3897865, 0.3592648, 4.420308] | Test Loss: [2.5614822, 0.36638314, 4.7565813]\n",
      "278: Train Loss: [2.4802203, 0.34668005, 4.6137605] | Test Loss: [2.603581, 0.42722806, 4.779934]\n",
      "279: Train Loss: [2.5135767, 0.30736896, 4.7197847] | Test Loss: [2.6250424, 0.4061055, 4.8439794]\n",
      "280: Train Loss: [2.693358, 0.38734525, 4.9993706] | Test Loss: [2.646115, 0.40528315, 4.886947]\n",
      "281: Train Loss: [2.5286098, 0.334299, 4.7229204] | Test Loss: [2.7009962, 0.46378562, 4.9382067]\n",
      "282: Train Loss: [2.5406742, 0.40647194, 4.6748767] | Test Loss: [2.5860658, 0.29961705, 4.8725142]\n",
      "283: Train Loss: [2.6372585, 0.461137, 4.8133802] | Test Loss: [2.61617, 0.32805896, 4.904281]\n",
      "284: Train Loss: [2.4014366, 0.29850227, 4.5043707] | Test Loss: [2.42422, 0.42947713, 4.418963]\n",
      "285: Train Loss: [2.3634632, 0.34390968, 4.3830166] | Test Loss: [2.566488, 0.43446025, 4.698516]\n",
      "286: Train Loss: [2.5743415, 0.33426836, 4.8144145] | Test Loss: [2.4932935, 0.3815919, 4.6049953]\n",
      "287: Train Loss: [2.3905478, 0.3449349, 4.4361606] | Test Loss: [2.6719222, 0.3637215, 4.980123]\n",
      "288: Train Loss: [2.488264, 0.33712053, 4.6394076] | Test Loss: [2.6657925, 0.32178837, 5.0097966]\n",
      "289: Train Loss: [2.504419, 0.29227167, 4.7165666] | Test Loss: [2.547151, 0.34496984, 4.7493324]\n",
      "290: Train Loss: [2.593731, 0.37931848, 4.8081436] | Test Loss: [2.6823575, 0.43400237, 4.9307127]\n",
      "291: Train Loss: [2.441307, 0.58161473, 4.300999] | Test Loss: [2.4781067, 0.4060987, 4.5501146]\n",
      "292: Train Loss: [2.4805553, 0.29655778, 4.6645527] | Test Loss: [2.6008427, 0.34980872, 4.8518767]\n",
      "293: Train Loss: [2.3438973, 0.31765607, 4.3701386] | Test Loss: [2.4319644, 0.35669497, 4.5072336]\n",
      "294: Train Loss: [2.503496, 0.29687983, 4.710112] | Test Loss: [2.7008061, 0.34767056, 5.0539417]\n",
      "295: Train Loss: [2.536356, 0.34960654, 4.7231054] | Test Loss: [2.6405988, 0.41630718, 4.8648906]\n",
      "296: Train Loss: [2.5401607, 0.42472824, 4.655593] | Test Loss: [2.6651456, 0.32060072, 5.0096908]\n",
      "297: Train Loss: [2.4368644, 0.36514047, 4.5085883] | Test Loss: [2.5441854, 0.38035804, 4.7080126]\n",
      "298: Train Loss: [2.3740957, 0.36146724, 4.386724] | Test Loss: [2.6654334, 0.36966822, 4.961199]\n",
      "299: Train Loss: [2.3590434, 0.35534388, 4.362743] | Test Loss: [2.5583174, 0.31408387, 4.802551]\n",
      "300: Train Loss: [2.5536692, 0.3154973, 4.791841] | Test Loss: [2.5751944, 0.40751895, 4.74287]\n",
      "301: Train Loss: [2.5798428, 0.3493826, 4.810303] | Test Loss: [2.5809188, 0.379549, 4.7822886]\n",
      "302: Train Loss: [2.4054108, 0.37801266, 4.432809] | Test Loss: [2.6727967, 0.36572477, 4.979869]\n",
      "303: Train Loss: [2.2553341, 0.3329508, 4.1777177] | Test Loss: [2.4424045, 0.36731774, 4.5174913]\n",
      "304: Train Loss: [2.4729285, 0.45403966, 4.4918175] | Test Loss: [2.520338, 0.33441404, 4.706262]\n",
      "305: Train Loss: [2.5792434, 0.41650495, 4.741982] | Test Loss: [2.6398146, 0.3940957, 4.8855333]\n",
      "306: Train Loss: [2.5960014, 0.35078272, 4.84122] | Test Loss: [2.6067433, 0.3570469, 4.8564396]\n",
      "307: Train Loss: [2.3992815, 0.34459656, 4.4539666] | Test Loss: [2.6192293, 0.42106414, 4.8173947]\n",
      "308: Train Loss: [2.6015506, 0.41666225, 4.786439] | Test Loss: [2.5901003, 0.36324474, 4.816956]\n",
      "309: Train Loss: [2.3592017, 0.41585538, 4.302548] | Test Loss: [2.7030394, 0.415964, 4.9901147]\n",
      "310: Train Loss: [2.5321414, 0.3475363, 4.716747] | Test Loss: [2.5880234, 0.34137857, 4.834668]\n",
      "311: Train Loss: [2.4269574, 0.3610443, 4.4928703] | Test Loss: [2.50926, 0.37671673, 4.6418033]\n",
      "312: Train Loss: [2.429878, 0.3770675, 4.4826884] | Test Loss: [2.4331367, 0.35099775, 4.5152755]\n",
      "313: Train Loss: [2.4553525, 0.3576588, 4.553046] | Test Loss: [2.568487, 0.31429794, 4.8226757]\n",
      "314: Train Loss: [2.4820201, 0.46532267, 4.498718] | Test Loss: [2.617751, 0.45043722, 4.7850647]\n",
      "315: Train Loss: [2.3706481, 0.328899, 4.4123974] | Test Loss: [2.6284592, 0.3448259, 4.9120927]\n",
      "316: Train Loss: [2.4935467, 0.35654908, 4.630544] | Test Loss: [2.5863416, 0.34132355, 4.83136]\n",
      "317: Train Loss: [2.6448572, 0.3485802, 4.941134] | Test Loss: [2.558939, 0.3916739, 4.726204]\n",
      "318: Train Loss: [2.386426, 0.36440867, 4.4084435] | Test Loss: [2.5951614, 0.30932903, 4.880994]\n",
      "319: Train Loss: [2.5077255, 0.36049378, 4.6549573] | Test Loss: [2.631384, 0.3317365, 4.931031]\n",
      "320: Train Loss: [2.4151428, 0.34068653, 4.489599] | Test Loss: [2.6155772, 0.35130605, 4.8798485]\n",
      "321: Train Loss: [2.541195, 0.335459, 4.7469306] | Test Loss: [2.5208023, 0.32949764, 4.7121067]\n",
      "322: Train Loss: [2.5253258, 0.3449625, 4.705689] | Test Loss: [2.5617878, 0.28769952, 4.835876]\n",
      "323: Train Loss: [2.3505929, 0.41168636, 4.2894993] | Test Loss: [2.6691837, 0.2964853, 5.041882]\n",
      "324: Train Loss: [2.4522464, 0.4173172, 4.4871755] | Test Loss: [2.7331223, 0.39970502, 5.06654]\n",
      "325: Train Loss: [2.5548816, 0.40984353, 4.6999197] | Test Loss: [2.6320128, 0.3893886, 4.874637]\n",
      "326: Train Loss: [2.6660585, 0.33599, 4.996127] | Test Loss: [2.663692, 0.39291656, 4.9344673]\n",
      "327: Train Loss: [2.4477735, 0.352485, 4.5430617] | Test Loss: [2.4268656, 0.35248274, 4.5012484]\n",
      "328: Train Loss: [2.4282854, 0.34456757, 4.512003] | Test Loss: [2.5862813, 0.33692452, 4.835638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329: Train Loss: [2.4666843, 0.32903522, 4.6043334] | Test Loss: [2.541014, 0.38811558, 4.6939125]\n",
      "330: Train Loss: [2.4751856, 0.2738021, 4.676569] | Test Loss: [2.5574994, 0.39496678, 4.720032]\n",
      "331: Train Loss: [2.6080706, 0.43282002, 4.7833214] | Test Loss: [2.5022287, 0.3711273, 4.6333303]\n",
      "332: Train Loss: [2.2716212, 0.31586194, 4.2273808] | Test Loss: [2.6272206, 0.4654162, 4.789025]\n",
      "333: Train Loss: [2.5508604, 0.38397554, 4.7177453] | Test Loss: [2.501202, 0.32747957, 4.674925]\n",
      "334: Train Loss: [2.462029, 0.34073126, 4.583327] | Test Loss: [2.5960119, 0.3287641, 4.86326]\n",
      "335: Train Loss: [2.4820528, 0.36166453, 4.602441] | Test Loss: [2.5699158, 0.45063016, 4.6892014]\n",
      "336: Train Loss: [2.5942266, 0.3127007, 4.8757524] | Test Loss: [2.7193456, 0.34036013, 5.098331]\n",
      "337: Train Loss: [2.429975, 0.3727408, 4.4872093] | Test Loss: [2.5917401, 0.40851462, 4.774966]\n",
      "338: Train Loss: [2.5340738, 0.4146852, 4.6534624] | Test Loss: [2.5546787, 0.40750682, 4.7018504]\n",
      "339: Train Loss: [2.4871173, 0.3764839, 4.5977507] | Test Loss: [2.6967502, 0.3782967, 5.0152035]\n",
      "340: Train Loss: [2.2735708, 0.31881824, 4.2283235] | Test Loss: [2.5541017, 0.41232583, 4.6958776]\n",
      "341: Train Loss: [2.46102, 0.34951735, 4.5725226] | Test Loss: [2.6155467, 0.32817551, 4.902918]\n",
      "342: Train Loss: [2.4374213, 0.35505295, 4.5197897] | Test Loss: [2.618265, 0.40947506, 4.827055]\n",
      "343: Train Loss: [2.373561, 0.377124, 4.369998] | Test Loss: [2.3593717, 0.4912913, 4.227452]\n",
      "344: Train Loss: [2.597524, 0.4271179, 4.76793] | Test Loss: [2.578889, 0.3333359, 4.824442]\n",
      "345: Train Loss: [2.4976203, 0.37423608, 4.6210046] | Test Loss: [2.6174986, 0.35522312, 4.879774]\n",
      "346: Train Loss: [2.423831, 0.33124152, 4.5164204] | Test Loss: [2.680669, 0.40701714, 4.954321]\n",
      "347: Train Loss: [2.546855, 0.3469063, 4.7468038] | Test Loss: [2.6337454, 0.36577457, 4.901716]\n",
      "348: Train Loss: [2.5276828, 0.3483653, 4.7070003] | Test Loss: [2.516375, 0.51190543, 4.5208445]\n",
      "349: Train Loss: [2.4130669, 0.3713107, 4.454823] | Test Loss: [2.6243024, 0.32993597, 4.9186687]\n",
      "350: Train Loss: [2.430076, 0.35997775, 4.500174] | Test Loss: [2.5295887, 0.34422794, 4.7149496]\n",
      "351: Train Loss: [2.4992013, 0.3128592, 4.6855435] | Test Loss: [2.6003783, 0.33666727, 4.8640895]\n",
      "352: Train Loss: [2.465558, 0.33768994, 4.593426] | Test Loss: [2.439582, 0.3048822, 4.574282]\n",
      "353: Train Loss: [2.3702683, 0.5065795, 4.2339573] | Test Loss: [2.5624506, 0.53168553, 4.593216]\n",
      "354: Train Loss: [2.4222968, 0.3021118, 4.542482] | Test Loss: [2.5267513, 0.35523102, 4.6982718]\n",
      "355: Train Loss: [2.6856017, 0.37234345, 4.99886] | Test Loss: [2.5636292, 0.33223614, 4.795022]\n",
      "356: Train Loss: [2.557963, 0.2879646, 4.827961] | Test Loss: [2.5117679, 0.34678972, 4.676746]\n",
      "357: Train Loss: [2.477225, 0.31714147, 4.6373086] | Test Loss: [2.5685158, 0.34056592, 4.796466]\n",
      "358: Train Loss: [2.4472432, 0.33530992, 4.5591764] | Test Loss: [2.566878, 0.3673635, 4.7663927]\n",
      "359: Train Loss: [2.305093, 0.3152576, 4.2949286] | Test Loss: [2.7320652, 0.31105438, 5.153076]\n",
      "360: Train Loss: [2.4045281, 0.37087145, 4.4381847] | Test Loss: [2.838135, 0.3348022, 5.341468]\n",
      "361: Train Loss: [2.4273863, 0.2721222, 4.58265] | Test Loss: [2.5741708, 0.3424359, 4.805906]\n",
      "362: Train Loss: [2.5079854, 0.37940696, 4.636564] | Test Loss: [2.627803, 0.3870016, 4.8686047]\n",
      "363: Train Loss: [2.529088, 0.41951114, 4.6386647] | Test Loss: [2.646326, 0.28782448, 5.0048275]\n",
      "364: Train Loss: [2.4416761, 0.3241147, 4.5592375] | Test Loss: [2.4817817, 0.4459799, 4.5175834]\n",
      "365: Train Loss: [2.504166, 0.39744222, 4.6108894] | Test Loss: [2.6428995, 0.40695292, 4.878846]\n",
      "366: Train Loss: [2.4993625, 0.33201352, 4.6667113] | Test Loss: [2.6524508, 0.42130604, 4.8835955]\n",
      "367: Train Loss: [2.4598432, 0.31471452, 4.604972] | Test Loss: [2.5420015, 0.37358937, 4.7104135]\n",
      "368: Train Loss: [2.504738, 0.36205167, 4.6474247] | Test Loss: [2.5780876, 0.38039252, 4.7757826]\n",
      "369: Train Loss: [2.5737598, 0.363255, 4.7842646] | Test Loss: [2.7529314, 0.32411557, 5.181747]\n",
      "370: Train Loss: [2.4091437, 0.3241481, 4.494139] | Test Loss: [2.546733, 0.39896473, 4.694501]\n",
      "371: Train Loss: [2.442495, 0.41160163, 4.4733887] | Test Loss: [2.580998, 0.48058352, 4.681412]\n",
      "372: Train Loss: [2.4928796, 0.34525993, 4.640499] | Test Loss: [2.3445337, 0.3574665, 4.3316007]\n",
      "373: Train Loss: [2.4752424, 0.34928644, 4.601198] | Test Loss: [2.3756638, 0.3359039, 4.4154234]\n",
      "374: Train Loss: [2.322803, 0.34290338, 4.302703] | Test Loss: [2.4320147, 0.35894912, 4.50508]\n",
      "375: Train Loss: [2.618761, 0.3623103, 4.8752117] | Test Loss: [2.5389516, 0.3862883, 4.691615]\n",
      "376: Train Loss: [2.4990184, 0.31299913, 4.6850376] | Test Loss: [2.7254438, 0.35925317, 5.0916348]\n",
      "377: Train Loss: [2.3638518, 0.31366462, 4.414039] | Test Loss: [2.7638516, 0.47583777, 5.0518656]\n",
      "378: Train Loss: [2.4009526, 0.30639547, 4.4955096] | Test Loss: [2.9123547, 0.2263202, 5.598389]\n",
      "379: Train Loss: [2.4577005, 0.41037476, 4.5050263] | Test Loss: [2.511013, 0.3092025, 4.7128234]\n",
      "380: Train Loss: [2.4964676, 0.31439313, 4.678542] | Test Loss: [2.738842, 0.40740862, 5.0702753]\n",
      "381: Train Loss: [2.5712588, 0.386807, 4.7557106] | Test Loss: [2.5993617, 0.29243577, 4.9062877]\n",
      "382: Train Loss: [2.2973883, 0.39206913, 4.2027073] | Test Loss: [2.645684, 0.36792028, 4.9234476]\n",
      "383: Train Loss: [2.2861629, 0.31897008, 4.2533555] | Test Loss: [2.4662812, 0.30457604, 4.6279864]\n",
      "384: Train Loss: [2.2833712, 0.34129763, 4.225445] | Test Loss: [2.6711702, 0.41171002, 4.93063]\n",
      "385: Train Loss: [2.4127176, 0.3381932, 4.4872417] | Test Loss: [2.553568, 0.3852844, 4.7218513]\n",
      "386: Train Loss: [2.5039175, 0.34407073, 4.663764] | Test Loss: [2.46847, 0.4247015, 4.5122385]\n",
      "387: Train Loss: [2.3992426, 0.2954294, 4.503056] | Test Loss: [2.610562, 0.3940552, 4.827069]\n",
      "388: Train Loss: [2.5311964, 0.30281645, 4.7595763] | Test Loss: [2.6169088, 0.53265715, 4.7011604]\n",
      "389: Train Loss: [2.519671, 0.33788842, 4.7014537] | Test Loss: [2.5282538, 0.40180072, 4.654707]\n",
      "390: Train Loss: [2.6004078, 0.36771145, 4.833104] | Test Loss: [2.4821486, 0.32758006, 4.6367173]\n",
      "391: Train Loss: [2.3983028, 0.3364204, 4.460185] | Test Loss: [2.5962327, 0.359999, 4.832466]\n",
      "392: Train Loss: [2.4003587, 0.33243296, 4.4682846] | Test Loss: [2.655088, 0.35580412, 4.954372]\n",
      "393: Train Loss: [2.5007992, 0.34723824, 4.6543603] | Test Loss: [2.638994, 0.29087448, 4.9871135]\n",
      "394: Train Loss: [2.3079476, 0.34968764, 4.2662077] | Test Loss: [2.6077464, 0.39821517, 4.8172774]\n",
      "395: Train Loss: [2.4913244, 0.36737353, 4.6152754] | Test Loss: [2.6407483, 0.38495472, 4.8965416]\n",
      "396: Train Loss: [2.4509487, 0.35705242, 4.544845] | Test Loss: [2.4975297, 0.36443347, 4.630626]\n",
      "397: Train Loss: [2.2184772, 0.3319017, 4.105053] | Test Loss: [2.5905597, 0.338532, 4.8425875]\n",
      "398: Train Loss: [2.4406877, 0.3150317, 4.566344] | Test Loss: [2.529357, 0.35355854, 4.7051554]\n",
      "399: Train Loss: [2.4164925, 0.33003396, 4.502951] | Test Loss: [2.5605114, 0.41344845, 4.7075744]\n",
      "400: Train Loss: [2.6998427, 0.35133502, 5.0483503] | Test Loss: [2.4894514, 0.36852878, 4.610374]\n",
      "401: Train Loss: [2.3669345, 0.46812803, 4.265741] | Test Loss: [2.528168, 0.42814615, 4.6281896]\n",
      "402: Train Loss: [2.4981666, 0.32479513, 4.671538] | Test Loss: [2.5222447, 0.42013338, 4.624356]\n",
      "403: Train Loss: [2.594952, 0.30284736, 4.887057] | Test Loss: [2.4827344, 0.38552842, 4.5799403]\n",
      "404: Train Loss: [2.3901815, 0.3434125, 4.4369507] | Test Loss: [2.558725, 0.3210428, 4.796407]\n",
      "405: Train Loss: [2.2753415, 0.34314042, 4.2075424] | Test Loss: [2.6628876, 0.3681041, 4.957671]\n",
      "406: Train Loss: [2.4521782, 0.3255601, 4.5787964] | Test Loss: [2.8244355, 0.37689775, 5.271973]\n",
      "407: Train Loss: [2.3386688, 0.33613342, 4.341204] | Test Loss: [2.6470475, 0.32364094, 4.970454]\n",
      "408: Train Loss: [2.4840531, 0.32561228, 4.642494] | Test Loss: [2.7882864, 0.2609611, 5.315612]\n",
      "409: Train Loss: [2.4414802, 0.35568643, 4.5272737] | Test Loss: [2.3708591, 0.35101596, 4.3907022]\n",
      "410: Train Loss: [2.5847042, 0.4368486, 4.7325597] | Test Loss: [2.5918665, 0.31760758, 4.8661256]\n",
      "411: Train Loss: [2.4931135, 0.37195763, 4.6142693] | Test Loss: [2.6621618, 0.31199616, 5.0123277]\n",
      "412: Train Loss: [2.3605888, 0.3595452, 4.3616323] | Test Loss: [2.467801, 0.40449423, 4.531108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413: Train Loss: [2.602202, 0.3613202, 4.843084] | Test Loss: [2.8132057, 0.42546842, 5.200943]\n",
      "414: Train Loss: [2.355295, 0.34790248, 4.3626876] | Test Loss: [2.4886034, 0.37599072, 4.601216]\n",
      "415: Train Loss: [2.473719, 0.3446866, 4.6027513] | Test Loss: [2.5031238, 0.37204847, 4.634199]\n",
      "416: Train Loss: [2.4972997, 0.36246654, 4.632133] | Test Loss: [2.4804358, 0.31293973, 4.647932]\n",
      "417: Train Loss: [2.4591444, 0.34080857, 4.5774803] | Test Loss: [2.551691, 0.35250527, 4.750877]\n",
      "418: Train Loss: [2.6116672, 0.38772583, 4.8356085] | Test Loss: [2.3306978, 0.5712585, 4.090137]\n",
      "419: Train Loss: [2.3380294, 0.34598565, 4.3300734] | Test Loss: [2.5895, 0.34676653, 4.8322334]\n",
      "420: Train Loss: [2.3569298, 0.41938007, 4.2944794] | Test Loss: [2.6233175, 0.3360097, 4.9106255]\n",
      "421: Train Loss: [2.4160216, 0.3375141, 4.4945292] | Test Loss: [2.4782465, 0.43920362, 4.517289]\n",
      "422: Train Loss: [2.546668, 0.32987177, 4.7634645] | Test Loss: [2.8237145, 0.37700573, 5.2704234]\n",
      "423: Train Loss: [2.639563, 0.40934587, 4.8697805] | Test Loss: [2.642224, 0.34396327, 4.940485]\n",
      "424: Train Loss: [2.5580137, 0.34840664, 4.7676206] | Test Loss: [2.5728714, 0.34260356, 4.803139]\n",
      "425: Train Loss: [2.5281382, 0.3968724, 4.659404] | Test Loss: [2.5567565, 0.3202284, 4.7932844]\n",
      "426: Train Loss: [2.424923, 0.3257451, 4.524101] | Test Loss: [2.6753259, 0.35472655, 4.995925]\n",
      "427: Train Loss: [2.4186783, 0.31586242, 4.521494] | Test Loss: [2.5799122, 0.36601806, 4.7938066]\n",
      "428: Train Loss: [2.6817255, 0.326304, 5.037147] | Test Loss: [2.7002964, 0.3578588, 5.042734]\n",
      "429: Train Loss: [2.5654528, 0.40529972, 4.725606] | Test Loss: [2.3649082, 0.37881938, 4.350997]\n",
      "430: Train Loss: [2.4552915, 0.3209029, 4.58968] | Test Loss: [2.6430335, 0.43195054, 4.8541164]\n",
      "431: Train Loss: [2.3000298, 0.349544, 4.2505155] | Test Loss: [2.7661464, 0.42304212, 5.1092505]\n",
      "432: Train Loss: [2.5167108, 0.31410858, 4.719313] | Test Loss: [2.5906847, 0.39287665, 4.7884927]\n",
      "433: Train Loss: [2.6058803, 0.3403248, 4.8714356] | Test Loss: [2.7038255, 0.33306512, 5.074586]\n",
      "434: Train Loss: [2.4651086, 0.35811728, 4.5721] | Test Loss: [2.576945, 0.343982, 4.809908]\n",
      "435: Train Loss: [2.4574687, 0.38125405, 4.5336833] | Test Loss: [2.6343064, 0.33427277, 4.93434]\n",
      "436: Train Loss: [2.4111788, 0.2841929, 4.5381646] | Test Loss: [2.6382773, 0.3967227, 4.879832]\n",
      "437: Train Loss: [2.5418699, 0.4065483, 4.6771913] | Test Loss: [2.5787761, 0.34328625, 4.814266]\n",
      "438: Train Loss: [2.4152396, 0.34957644, 4.4809027] | Test Loss: [2.5858645, 0.42548418, 4.746245]\n",
      "439: Train Loss: [2.4860036, 0.33519685, 4.6368103] | Test Loss: [2.629536, 0.38987243, 4.8691993]\n",
      "440: Train Loss: [2.4377906, 0.3369576, 4.538624] | Test Loss: [2.6677496, 0.33730057, 4.9981985]\n",
      "441: Train Loss: [2.5541413, 0.47000206, 4.6382804] | Test Loss: [2.4762151, 0.33895653, 4.613474]\n",
      "442: Train Loss: [2.3783228, 0.32889473, 4.427751] | Test Loss: [2.5755892, 0.46774718, 4.683431]\n",
      "443: Train Loss: [2.4182835, 0.31001842, 4.5265484] | Test Loss: [2.7677274, 0.32519716, 5.2102575]\n",
      "444: Train Loss: [2.6176996, 0.34575665, 4.8896427] | Test Loss: [2.4330027, 0.37077677, 4.495229]\n",
      "445: Train Loss: [2.5692859, 0.31514144, 4.82343] | Test Loss: [2.4566946, 0.4211655, 4.4922237]\n",
      "446: Train Loss: [2.4537952, 0.36437944, 4.543211] | Test Loss: [2.6300955, 0.3287522, 4.931439]\n",
      "447: Train Loss: [2.521106, 0.3958078, 4.6464043] | Test Loss: [2.459517, 0.4016805, 4.5173535]\n",
      "448: Train Loss: [2.3924766, 0.4055264, 4.3794265] | Test Loss: [2.4928737, 0.3512388, 4.6345086]\n",
      "449: Train Loss: [2.5578322, 0.44642994, 4.6692348] | Test Loss: [2.61685, 0.41871995, 4.81498]\n",
      "450: Train Loss: [2.4593937, 0.32495418, 4.5938334] | Test Loss: [2.5765042, 0.3375237, 4.815485]\n",
      "451: Train Loss: [2.6072187, 0.35314825, 4.861289] | Test Loss: [2.7919073, 0.35846075, 5.2253537]\n",
      "452: Train Loss: [2.4972844, 0.38408652, 4.610482] | Test Loss: [2.6008465, 0.44678116, 4.754912]\n",
      "453: Train Loss: [2.5963514, 0.48449796, 4.7082047] | Test Loss: [2.688537, 0.4705434, 4.9065304]\n",
      "454: Train Loss: [2.5061688, 0.32645392, 4.685884] | Test Loss: [2.7883418, 0.37887225, 5.197811]\n",
      "455: Train Loss: [2.4469047, 0.39889058, 4.494919] | Test Loss: [2.5800831, 0.31246042, 4.847706]\n",
      "456: Train Loss: [2.5112534, 0.38162404, 4.6408825] | Test Loss: [2.6541145, 0.35901096, 4.949218]\n",
      "457: Train Loss: [2.5307124, 0.36232522, 4.6990995] | Test Loss: [2.3453026, 0.32572964, 4.3648753]\n",
      "458: Train Loss: [2.600515, 0.345964, 4.855066] | Test Loss: [2.633834, 0.39882323, 4.8688445]\n",
      "459: Train Loss: [2.6045837, 0.3922546, 4.816913] | Test Loss: [2.554404, 0.3230991, 4.785709]\n",
      "460: Train Loss: [2.5920262, 0.27758276, 4.90647] | Test Loss: [2.6408198, 0.3121274, 4.969512]\n",
      "461: Train Loss: [2.549683, 0.34074947, 4.758617] | Test Loss: [2.6562088, 0.37731785, 4.9350996]\n",
      "462: Train Loss: [2.556639, 0.41714323, 4.6961346] | Test Loss: [2.5267618, 0.31167433, 4.7418494]\n",
      "463: Train Loss: [2.3202782, 0.3567711, 4.2837853] | Test Loss: [2.5730314, 0.309869, 4.836194]\n",
      "464: Train Loss: [2.4718342, 0.3320479, 4.6116204] | Test Loss: [2.6083393, 0.34433943, 4.8723392]\n",
      "465: Train Loss: [2.4720237, 0.40314797, 4.5408993] | Test Loss: [2.7779808, 0.32613644, 5.229825]\n",
      "466: Train Loss: [2.4861763, 0.2983844, 4.6739683] | Test Loss: [2.5764868, 0.3981705, 4.754803]\n",
      "467: Train Loss: [2.389722, 0.3016405, 4.4778037] | Test Loss: [2.6742656, 0.41417748, 4.934354]\n",
      "468: Train Loss: [2.371999, 0.28114855, 4.4628496] | Test Loss: [2.9183786, 0.7465191, 5.090238]\n",
      "469: Train Loss: [2.5386887, 0.37045464, 4.7069225] | Test Loss: [2.7482636, 0.34401494, 5.152512]\n",
      "470: Train Loss: [2.481233, 0.38064983, 4.5818157] | Test Loss: [2.743153, 0.34168047, 5.1446257]\n",
      "471: Train Loss: [2.370589, 0.31007984, 4.431098] | Test Loss: [2.9265652, 0.51333433, 5.339796]\n",
      "472: Train Loss: [2.6300464, 0.35256383, 4.907529] | Test Loss: [2.5306468, 0.33501065, 4.726283]\n",
      "473: Train Loss: [2.531404, 0.37158185, 4.691226] | Test Loss: [2.5804265, 0.34279957, 4.8180532]\n",
      "474: Train Loss: [2.439928, 0.36492705, 4.5149293] | Test Loss: [2.406922, 0.41209486, 4.401749]\n",
      "475: Train Loss: [2.5256958, 0.32760176, 4.7237897] | Test Loss: [2.538162, 0.36177486, 4.714549]\n",
      "476: Train Loss: [2.432787, 0.35600796, 4.509566] | Test Loss: [2.4302552, 0.3278946, 4.5326157]\n",
      "477: Train Loss: [2.5961847, 0.31986725, 4.8725023] | Test Loss: [2.6601517, 0.38702053, 4.933283]\n",
      "478: Train Loss: [2.6554544, 0.3548852, 4.9560237] | Test Loss: [2.4958086, 0.3672506, 4.6243668]\n",
      "479: Train Loss: [2.467886, 0.34231004, 4.593462] | Test Loss: [2.6515849, 0.42247277, 4.880697]\n",
      "480: Train Loss: [2.3938878, 0.2924118, 4.4953637] | Test Loss: [2.7732105, 0.29604277, 5.250378]\n",
      "481: Train Loss: [2.7350557, 0.37136137, 5.09875] | Test Loss: [2.7362297, 0.34316096, 5.129298]\n",
      "482: Train Loss: [2.5925312, 0.4486389, 4.7364235] | Test Loss: [2.571154, 0.43391177, 4.7083964]\n",
      "483: Train Loss: [2.597819, 0.4098378, 4.7858005] | Test Loss: [2.482793, 0.41490752, 4.5506787]\n",
      "484: Train Loss: [2.4407063, 0.34115127, 4.5402613] | Test Loss: [2.661864, 0.38182992, 4.9418983]\n",
      "485: Train Loss: [2.4030707, 0.3165256, 4.489616] | Test Loss: [2.5109413, 0.30351037, 4.7183723]\n",
      "486: Train Loss: [2.4913464, 0.31100687, 4.6716857] | Test Loss: [2.5430515, 0.39473423, 4.6913686]\n",
      "487: Train Loss: [2.6614459, 0.33801502, 4.9848766] | Test Loss: [2.5050588, 0.37167013, 4.6384473]\n",
      "488: Train Loss: [2.4615324, 0.37302226, 4.5500426] | Test Loss: [2.4626915, 0.29942194, 4.6259613]\n",
      "489: Train Loss: [2.4652362, 0.365378, 4.5650945] | Test Loss: [2.7148554, 0.41844, 5.011271]\n",
      "490: Train Loss: [2.7268164, 0.31890357, 5.1347294] | Test Loss: [2.4802184, 0.3189383, 4.6414986]\n",
      "491: Train Loss: [2.4781084, 0.36027393, 4.595943] | Test Loss: [2.6241152, 0.33442417, 4.9138064]\n",
      "492: Train Loss: [2.5075254, 0.32961926, 4.6854315] | Test Loss: [2.528543, 0.34712875, 4.709957]\n",
      "493: Train Loss: [2.4929154, 0.33034426, 4.6554866] | Test Loss: [2.610394, 0.34745765, 4.87333]\n",
      "494: Train Loss: [2.4669418, 0.364513, 4.5693707] | Test Loss: [2.6023467, 0.3761731, 4.8285203]\n",
      "495: Train Loss: [2.5256476, 0.32168156, 4.729614] | Test Loss: [2.613401, 0.4438846, 4.782917]\n",
      "496: Train Loss: [2.5382495, 0.30040845, 4.7760906] | Test Loss: [2.5587833, 0.37337068, 4.744196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497: Train Loss: [2.5254877, 0.29324114, 4.7577343] | Test Loss: [2.5716295, 0.4355104, 4.707749]\n",
      "498: Train Loss: [2.426699, 0.37722924, 4.4761686] | Test Loss: [2.674202, 0.38521874, 4.9631853]\n",
      "499: Train Loss: [2.5165262, 0.34385577, 4.6891966] | Test Loss: [2.4798906, 0.36654022, 4.5932407]\n",
      "500: Train Loss: [2.3919556, 0.31524602, 4.468665] | Test Loss: [2.615177, 0.36075705, 4.869597]\n",
      "501: Train Loss: [2.4793882, 0.3385141, 4.620262] | Test Loss: [2.7033415, 0.35935324, 5.04733]\n",
      "502: Train Loss: [2.5172324, 0.40313542, 4.6313295] | Test Loss: [2.5182521, 0.32721, 4.7092943]\n",
      "503: Train Loss: [2.4611068, 0.3345914, 4.587622] | Test Loss: [2.5884523, 0.3182777, 4.858627]\n",
      "504: Train Loss: [2.438994, 0.40140826, 4.4765797] | Test Loss: [2.4701853, 0.36381903, 4.5765514]\n",
      "505: Train Loss: [2.430398, 0.33651957, 4.5242763] | Test Loss: [2.7152984, 0.4065463, 5.0240507]\n",
      "506: Train Loss: [2.5045106, 0.35576534, 4.653256] | Test Loss: [2.4739525, 0.37559086, 4.5723143]\n",
      "507: Train Loss: [2.426676, 0.35383245, 4.49952] | Test Loss: [2.5156522, 0.3687394, 4.6625648]\n",
      "508: Train Loss: [2.7062821, 0.2970185, 5.1155457] | Test Loss: [2.4370449, 0.32977372, 4.544316]\n",
      "509: Train Loss: [2.5488522, 0.31395352, 4.783751] | Test Loss: [2.7199674, 0.45113876, 4.9887958]\n",
      "510: Train Loss: [2.6081595, 0.38688105, 4.829438] | Test Loss: [2.7316728, 0.38508677, 5.078259]\n",
      "511: Train Loss: [2.3303902, 0.3433606, 4.31742] | Test Loss: [2.540458, 0.38794774, 4.6929684]\n",
      "512: Train Loss: [2.5645125, 0.3399442, 4.7890806] | Test Loss: [2.5384467, 0.30167532, 4.775218]\n",
      "513: Train Loss: [2.4274511, 0.33259478, 4.5223074] | Test Loss: [2.7452915, 0.37100524, 5.119578]\n",
      "514: Train Loss: [2.3195822, 0.31780368, 4.3213606] | Test Loss: [2.3612661, 0.36000916, 4.362523]\n",
      "515: Train Loss: [2.4186842, 0.32855198, 4.5088167] | Test Loss: [2.6428478, 0.30348274, 4.982213]\n",
      "516: Train Loss: [2.4921293, 0.434642, 4.549617] | Test Loss: [2.4995096, 0.33865798, 4.6603613]\n",
      "517: Train Loss: [2.3787458, 0.34761772, 4.409874] | Test Loss: [2.6844878, 0.44615966, 4.922816]\n",
      "518: Train Loss: [2.5160778, 0.35808814, 4.6740675] | Test Loss: [2.5814288, 0.3406082, 4.8222494]\n",
      "519: Train Loss: [2.6316137, 0.3429747, 4.920253] | Test Loss: [2.554226, 0.4105512, 4.697901]\n",
      "520: Train Loss: [2.5339165, 0.4833253, 4.5845075] | Test Loss: [2.608293, 0.36194488, 4.8546414]\n",
      "521: Train Loss: [2.5928354, 0.340179, 4.845492] | Test Loss: [2.5210972, 0.34668753, 4.695507]\n",
      "522: Train Loss: [2.697068, 0.4324955, 4.9616404] | Test Loss: [2.7388997, 0.34960935, 5.12819]\n",
      "523: Train Loss: [2.646944, 0.40698215, 4.886906] | Test Loss: [2.5155797, 0.40078738, 4.630372]\n",
      "524: Train Loss: [2.4683714, 0.44445318, 4.4922895] | Test Loss: [2.4768696, 0.3952121, 4.558527]\n",
      "Epoch 7\n",
      "0: Train Loss: [2.3667538, 0.37148035, 4.362027] | Test Loss: [2.731028, 0.40223294, 5.059823]\n",
      "1: Train Loss: [2.326097, 0.37838835, 4.2738056] | Test Loss: [2.4490936, 0.43402818, 4.464159]\n",
      "2: Train Loss: [2.3074255, 0.3747078, 4.2401433] | Test Loss: [2.7692366, 0.4078796, 5.130594]\n",
      "3: Train Loss: [2.2593145, 0.31907606, 4.199553] | Test Loss: [2.8554564, 0.29382837, 5.417084]\n",
      "4: Train Loss: [2.5066874, 0.352537, 4.6608377] | Test Loss: [2.6411393, 0.3633818, 4.9188967]\n",
      "5: Train Loss: [2.323795, 0.29581752, 4.351773] | Test Loss: [2.7146363, 0.34779012, 5.0814824]\n",
      "6: Train Loss: [2.5118482, 0.44974375, 4.5739527] | Test Loss: [2.6339126, 0.37898695, 4.8888383]\n",
      "7: Train Loss: [2.4924102, 0.4350616, 4.549759] | Test Loss: [2.5536215, 0.41296446, 4.6942787]\n",
      "8: Train Loss: [2.4143763, 0.38483074, 4.4439216] | Test Loss: [2.608621, 0.3749171, 4.8423247]\n",
      "9: Train Loss: [2.2858357, 0.37003314, 4.201638] | Test Loss: [2.4709601, 0.35351688, 4.588403]\n",
      "10: Train Loss: [2.258855, 0.3253351, 4.192375] | Test Loss: [2.594479, 0.33735648, 4.8516016]\n",
      "11: Train Loss: [2.6829212, 0.34312543, 5.022717] | Test Loss: [2.6249037, 0.42178896, 4.828018]\n",
      "12: Train Loss: [2.4030519, 0.34393683, 4.462167] | Test Loss: [2.4009619, 0.3269414, 4.4749823]\n",
      "13: Train Loss: [2.5256367, 0.37617698, 4.6750965] | Test Loss: [2.504109, 0.3103578, 4.69786]\n",
      "14: Train Loss: [2.3434262, 0.3411448, 4.345708] | Test Loss: [2.6998787, 0.32239416, 5.077363]\n",
      "15: Train Loss: [2.3184488, 0.33892462, 4.297973] | Test Loss: [2.664067, 0.29746714, 5.030667]\n",
      "16: Train Loss: [2.2709188, 0.34606233, 4.1957755] | Test Loss: [2.648579, 0.3770675, 4.92009]\n",
      "17: Train Loss: [2.2892494, 0.34010035, 4.2383986] | Test Loss: [2.7458339, 0.4070484, 5.0846195]\n",
      "18: Train Loss: [2.5045214, 0.31871325, 4.6903296] | Test Loss: [2.7080042, 0.32474777, 5.091261]\n",
      "19: Train Loss: [2.3685331, 0.31273088, 4.4243355] | Test Loss: [2.5890033, 0.39245334, 4.7855535]\n",
      "20: Train Loss: [2.4344184, 0.32782024, 4.5410166] | Test Loss: [2.52508, 0.3320063, 4.7181535]\n",
      "21: Train Loss: [2.4051483, 0.33753183, 4.4727645] | Test Loss: [2.696892, 0.39838538, 4.9953985]\n",
      "22: Train Loss: [2.3935378, 0.33430544, 4.45277] | Test Loss: [2.6582022, 0.42890096, 4.887503]\n",
      "23: Train Loss: [2.4038248, 0.35647705, 4.4511724] | Test Loss: [2.574298, 0.3601838, 4.788412]\n",
      "24: Train Loss: [2.4429908, 0.33803397, 4.5479474] | Test Loss: [2.5883615, 0.3397413, 4.836982]\n",
      "25: Train Loss: [2.4214044, 0.35559002, 4.487219] | Test Loss: [2.5400748, 0.41172934, 4.6684203]\n",
      "26: Train Loss: [2.3545742, 0.34139556, 4.367753] | Test Loss: [2.6344936, 0.37189245, 4.8970947]\n",
      "27: Train Loss: [2.3910422, 0.3339738, 4.4481106] | Test Loss: [2.6297336, 0.39793706, 4.8615303]\n",
      "28: Train Loss: [2.5170429, 0.3560455, 4.67804] | Test Loss: [2.513508, 0.39949137, 4.627525]\n",
      "29: Train Loss: [2.1852958, 0.35188872, 4.018703] | Test Loss: [2.7060878, 0.3733125, 5.038863]\n",
      "30: Train Loss: [2.513308, 0.34484243, 4.6817737] | Test Loss: [2.5068598, 0.43341246, 4.580307]\n",
      "31: Train Loss: [2.4283857, 0.30979058, 4.546981] | Test Loss: [2.6338124, 0.4177767, 4.8498483]\n",
      "32: Train Loss: [2.6407049, 0.3278276, 4.9535823] | Test Loss: [2.5998168, 0.4243036, 4.77533]\n",
      "33: Train Loss: [2.39856, 0.4080217, 4.389098] | Test Loss: [2.623902, 0.26718295, 4.9806213]\n",
      "34: Train Loss: [2.445591, 0.30166695, 4.589515] | Test Loss: [2.7284648, 0.35584688, 5.101083]\n",
      "35: Train Loss: [2.3950014, 0.36072516, 4.429278] | Test Loss: [2.493325, 0.3517893, 4.6348605]\n",
      "36: Train Loss: [2.2216787, 0.32656977, 4.116788] | Test Loss: [2.6806426, 0.46348968, 4.8977957]\n",
      "37: Train Loss: [2.326387, 0.4122123, 4.2405615] | Test Loss: [2.6466582, 0.38463154, 4.9086847]\n",
      "38: Train Loss: [2.4743814, 0.3503725, 4.5983906] | Test Loss: [2.612349, 0.47944093, 4.745257]\n",
      "39: Train Loss: [2.4490821, 0.34145764, 4.5567064] | Test Loss: [2.6661797, 0.39716056, 4.935199]\n",
      "40: Train Loss: [2.521348, 0.37424323, 4.6684527] | Test Loss: [2.795265, 0.39203605, 5.198494]\n",
      "41: Train Loss: [2.4006622, 0.38306254, 4.418262] | Test Loss: [2.7551756, 0.3753208, 5.1350303]\n",
      "42: Train Loss: [2.3742144, 0.35356134, 4.3948674] | Test Loss: [2.4884992, 0.3338144, 4.6431837]\n",
      "43: Train Loss: [2.3955994, 0.28330955, 4.5078893] | Test Loss: [2.6430817, 0.3586503, 4.927513]\n",
      "44: Train Loss: [2.4409158, 0.3324929, 4.549339] | Test Loss: [2.5243387, 0.41847634, 4.630201]\n",
      "45: Train Loss: [2.5667875, 0.3339113, 4.7996635] | Test Loss: [2.635587, 0.35231742, 4.9188566]\n",
      "46: Train Loss: [2.3610983, 0.3237999, 4.3983965] | Test Loss: [2.6171, 0.3307884, 4.9034114]\n",
      "47: Train Loss: [2.4312944, 0.35668552, 4.5059032] | Test Loss: [2.5626469, 0.34906346, 4.7762303]\n",
      "48: Train Loss: [2.5432599, 0.42966554, 4.656854] | Test Loss: [2.6079252, 0.36889204, 4.846958]\n",
      "49: Train Loss: [2.4850667, 0.30454862, 4.6655846] | Test Loss: [2.7687838, 0.3387818, 5.198786]\n",
      "50: Train Loss: [2.5721636, 0.54582137, 4.598506] | Test Loss: [2.584228, 0.41268906, 4.755767]\n",
      "51: Train Loss: [2.3361285, 0.3309229, 4.341334] | Test Loss: [2.4201405, 0.34342736, 4.496854]\n",
      "52: Train Loss: [2.4314842, 0.34152594, 4.5214424] | Test Loss: [2.6557255, 0.40655017, 4.9049006]\n",
      "53: Train Loss: [2.479413, 0.34375566, 4.6150703] | Test Loss: [2.4766028, 0.34162605, 4.6115794]\n",
      "54: Train Loss: [2.5425978, 0.35489154, 4.7303042] | Test Loss: [2.5367796, 0.35695282, 4.7166066]\n",
      "55: Train Loss: [2.445797, 0.37195083, 4.5196433] | Test Loss: [2.694749, 0.35896203, 5.030536]\n",
      "56: Train Loss: [2.5236745, 0.33571017, 4.711639] | Test Loss: [2.5392208, 0.3192902, 4.7591515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: Train Loss: [2.57298, 0.31526902, 4.830691] | Test Loss: [2.6585724, 0.45692495, 4.86022]\n",
      "58: Train Loss: [2.3690732, 0.36493343, 4.373213] | Test Loss: [2.5617163, 0.35580733, 4.7676253]\n",
      "59: Train Loss: [2.4339771, 0.299433, 4.568521] | Test Loss: [2.5539994, 0.32347003, 4.7845287]\n",
      "60: Train Loss: [2.468871, 0.4329272, 4.504815] | Test Loss: [2.6083295, 0.31530416, 4.901355]\n",
      "61: Train Loss: [2.388451, 0.35349384, 4.4234085] | Test Loss: [2.599133, 0.39516827, 4.8030977]\n",
      "62: Train Loss: [2.514578, 0.36171964, 4.6674366] | Test Loss: [2.5986254, 0.37008628, 4.8271646]\n",
      "63: Train Loss: [2.3902023, 0.36828834, 4.412116] | Test Loss: [2.7034807, 0.37367055, 5.033291]\n",
      "64: Train Loss: [2.4699883, 0.38941777, 4.550559] | Test Loss: [2.6301417, 0.36273792, 4.8975453]\n",
      "65: Train Loss: [2.3448734, 0.34189337, 4.3478537] | Test Loss: [2.6315813, 0.38460082, 4.878562]\n",
      "66: Train Loss: [2.3420856, 0.2927459, 4.391425] | Test Loss: [2.6954832, 0.37616718, 5.014799]\n",
      "67: Train Loss: [2.5549784, 0.3182629, 4.7916937] | Test Loss: [2.6901765, 0.36769918, 5.012654]\n",
      "68: Train Loss: [2.4552357, 0.32307374, 4.5873976] | Test Loss: [2.6800578, 0.42033568, 4.9397798]\n",
      "69: Train Loss: [2.4346685, 0.3173235, 4.5520134] | Test Loss: [2.7267406, 0.4645321, 4.9889493]\n",
      "70: Train Loss: [2.5052507, 0.45442817, 4.556073] | Test Loss: [2.5866702, 0.33977988, 4.8335605]\n",
      "71: Train Loss: [2.4809375, 0.432147, 4.529728] | Test Loss: [2.771519, 0.39484146, 5.148196]\n",
      "72: Train Loss: [2.4487684, 0.38376456, 4.513772] | Test Loss: [2.5847335, 0.4195397, 4.7499275]\n",
      "73: Train Loss: [2.495155, 0.34420118, 4.646109] | Test Loss: [2.7932925, 0.37586287, 5.210722]\n",
      "74: Train Loss: [2.2835355, 0.34775802, 4.219313] | Test Loss: [2.4838462, 0.35500997, 4.6126823]\n",
      "75: Train Loss: [2.4882123, 0.32510623, 4.6513186] | Test Loss: [2.631167, 0.4192797, 4.8430543]\n",
      "76: Train Loss: [2.3599236, 0.34821007, 4.3716373] | Test Loss: [2.4572802, 0.3150745, 4.599486]\n",
      "77: Train Loss: [2.5298538, 0.30477452, 4.754933] | Test Loss: [2.5868073, 0.36951518, 4.8040996]\n",
      "78: Train Loss: [2.2731428, 0.3850496, 4.161236] | Test Loss: [2.6131134, 0.36207646, 4.8641505]\n",
      "79: Train Loss: [2.3995962, 0.38745043, 4.411742] | Test Loss: [2.6166148, 0.3791133, 4.8541164]\n",
      "80: Train Loss: [2.4358952, 0.31516397, 4.5566263] | Test Loss: [2.609635, 0.3410628, 4.878207]\n",
      "81: Train Loss: [2.2907379, 0.33566582, 4.24581] | Test Loss: [2.6986473, 0.3129632, 5.0843315]\n",
      "82: Train Loss: [2.4943967, 0.45111772, 4.537676] | Test Loss: [2.4605315, 0.3730552, 4.548008]\n",
      "83: Train Loss: [2.4468184, 0.35317126, 4.5404654] | Test Loss: [2.6496398, 0.3750211, 4.9242587]\n",
      "84: Train Loss: [2.3241928, 0.33826312, 4.3101225] | Test Loss: [2.736096, 0.44149062, 5.030701]\n",
      "85: Train Loss: [2.3392992, 0.36050728, 4.318091] | Test Loss: [2.7511768, 0.3963319, 5.106022]\n",
      "86: Train Loss: [2.3478181, 0.32698128, 4.368655] | Test Loss: [2.5848908, 0.3548555, 4.814926]\n",
      "87: Train Loss: [2.4250724, 0.34188727, 4.5082574] | Test Loss: [2.640121, 0.3302618, 4.9499803]\n",
      "88: Train Loss: [2.4118826, 0.3702844, 4.4534807] | Test Loss: [2.6518054, 0.45236304, 4.851248]\n",
      "89: Train Loss: [2.2980728, 0.38662454, 4.2095213] | Test Loss: [2.344675, 0.3633182, 4.3260317]\n",
      "90: Train Loss: [2.5279984, 0.39905846, 4.6569386] | Test Loss: [2.4711194, 0.35683462, 4.5854044]\n",
      "91: Train Loss: [2.4477446, 0.32728446, 4.568205] | Test Loss: [2.7240398, 0.3841649, 5.063915]\n",
      "92: Train Loss: [2.412292, 0.35448825, 4.4700956] | Test Loss: [2.6724536, 0.3345037, 5.0104036]\n",
      "93: Train Loss: [2.4889355, 0.36658505, 4.6112857] | Test Loss: [1.4315351, 0.2063569, 2.6567132]\n",
      "94: Train Loss: [2.3566358, 0.3565517, 4.35672] | Test Loss: [2.529594, 0.31172162, 4.747466]\n",
      "95: Train Loss: [2.4721935, 0.44999382, 4.4943933] | Test Loss: [2.490589, 0.36592877, 4.615249]\n",
      "96: Train Loss: [2.5602293, 0.37851498, 4.741944] | Test Loss: [2.5773153, 0.3487764, 4.8058543]\n",
      "97: Train Loss: [2.2700028, 0.35284832, 4.187157] | Test Loss: [2.3470504, 0.37595168, 4.318149]\n",
      "98: Train Loss: [2.4629662, 0.31701908, 4.6089134] | Test Loss: [2.6368506, 0.3146874, 4.959014]\n",
      "99: Train Loss: [2.3081226, 0.26029515, 4.3559504] | Test Loss: [2.5819464, 0.41190782, 4.751985]\n",
      "100: Train Loss: [2.2914817, 0.29254243, 4.290421] | Test Loss: [2.6999004, 0.4384504, 4.9613504]\n",
      "101: Train Loss: [2.389002, 0.32733783, 4.4506664] | Test Loss: [2.6530604, 0.49112865, 4.8149924]\n",
      "102: Train Loss: [2.391776, 0.30179042, 4.481762] | Test Loss: [2.7604501, 0.38949046, 5.1314096]\n",
      "103: Train Loss: [2.4181368, 0.41646764, 4.419806] | Test Loss: [2.663613, 0.42990726, 4.897319]\n",
      "104: Train Loss: [2.5088015, 0.4019836, 4.615619] | Test Loss: [2.5368013, 0.37859777, 4.695005]\n",
      "105: Train Loss: [2.4286015, 0.44301188, 4.4141912] | Test Loss: [2.505085, 0.4127021, 4.597468]\n",
      "106: Train Loss: [2.5919683, 0.42629108, 4.7576456] | Test Loss: [2.7653604, 0.38821495, 5.1425056]\n",
      "107: Train Loss: [2.519696, 0.36051908, 4.678873] | Test Loss: [2.460824, 0.44092402, 4.480724]\n",
      "108: Train Loss: [2.530019, 0.37609866, 4.6839395] | Test Loss: [2.6006927, 0.41742167, 4.7839637]\n",
      "109: Train Loss: [2.6062257, 0.385585, 4.8268666] | Test Loss: [2.7523377, 0.34680834, 5.157867]\n",
      "110: Train Loss: [2.4332898, 0.3491525, 4.517427] | Test Loss: [2.593142, 0.3599151, 4.826369]\n",
      "111: Train Loss: [2.4427454, 0.30869186, 4.576799] | Test Loss: [2.7363894, 0.38899523, 5.0837836]\n",
      "112: Train Loss: [2.5699348, 0.31534457, 4.824525] | Test Loss: [2.644221, 0.3226445, 4.9657974]\n",
      "113: Train Loss: [2.4740553, 0.4675858, 4.4805245] | Test Loss: [2.7500815, 0.42909026, 5.0710726]\n",
      "114: Train Loss: [2.454242, 0.38745928, 4.5210247] | Test Loss: [2.6077416, 0.48088822, 4.734595]\n",
      "115: Train Loss: [2.3402243, 0.3995324, 4.280916] | Test Loss: [2.6503015, 0.35479647, 4.9458065]\n",
      "116: Train Loss: [2.4672332, 0.4400878, 4.4943786] | Test Loss: [2.5886662, 0.33062458, 4.846708]\n",
      "117: Train Loss: [2.396071, 0.29562035, 4.4965215] | Test Loss: [2.6198118, 0.36140057, 4.878223]\n",
      "118: Train Loss: [2.4036267, 0.34722325, 4.46003] | Test Loss: [2.521539, 0.3895151, 4.653563]\n",
      "119: Train Loss: [2.519004, 0.38378987, 4.654218] | Test Loss: [2.6669297, 0.41773, 4.9161296]\n",
      "120: Train Loss: [2.4052815, 0.35297644, 4.457587] | Test Loss: [2.6883981, 0.46907628, 4.90772]\n",
      "121: Train Loss: [2.542865, 0.38681614, 4.698914] | Test Loss: [2.6056957, 0.34055266, 4.8708386]\n",
      "122: Train Loss: [2.5692472, 0.36990035, 4.7685943] | Test Loss: [2.5524373, 0.3283797, 4.776495]\n",
      "123: Train Loss: [2.5189052, 0.45192307, 4.5858874] | Test Loss: [2.6172585, 0.30781722, 4.9266996]\n",
      "124: Train Loss: [2.396836, 0.41602746, 4.3776445] | Test Loss: [2.7012289, 0.36109388, 5.0413637]\n",
      "125: Train Loss: [2.2447078, 0.30234963, 4.187066] | Test Loss: [2.7587674, 0.32823402, 5.1893005]\n",
      "126: Train Loss: [2.3963296, 0.32182664, 4.470833] | Test Loss: [2.658183, 0.33031607, 4.98605]\n",
      "127: Train Loss: [2.4333537, 0.33557186, 4.5311356] | Test Loss: [2.6693604, 0.3726236, 4.9660974]\n",
      "128: Train Loss: [2.309856, 0.2903196, 4.3293924] | Test Loss: [2.709685, 0.39213753, 5.0272326]\n",
      "129: Train Loss: [2.2901134, 0.32730186, 4.252925] | Test Loss: [2.5299444, 0.34500152, 4.714887]\n",
      "130: Train Loss: [2.5811317, 0.36093727, 4.8013263] | Test Loss: [2.3963578, 0.41441113, 4.3783045]\n",
      "131: Train Loss: [2.6110964, 0.3940711, 4.8281217] | Test Loss: [2.7106078, 0.37616625, 5.045049]\n",
      "132: Train Loss: [2.3793209, 0.38543475, 4.373207] | Test Loss: [2.4392023, 0.36896366, 4.509441]\n",
      "133: Train Loss: [2.328651, 0.3255322, 4.3317695] | Test Loss: [2.5153573, 0.38415843, 4.646556]\n",
      "134: Train Loss: [2.3561788, 0.4327703, 4.2795873] | Test Loss: [2.5004158, 0.3595582, 4.6412735]\n",
      "135: Train Loss: [2.5077064, 0.41469672, 4.600716] | Test Loss: [2.4440427, 0.41072032, 4.477365]\n",
      "136: Train Loss: [2.355563, 0.38890693, 4.322219] | Test Loss: [2.514595, 0.36147952, 4.667711]\n",
      "137: Train Loss: [2.7087681, 0.3708141, 5.046722] | Test Loss: [2.59072, 0.36032242, 4.8211174]\n",
      "138: Train Loss: [2.505653, 0.35949966, 4.6518064] | Test Loss: [2.723546, 0.34987742, 5.0972147]\n",
      "139: Train Loss: [2.5919416, 0.46025595, 4.723627] | Test Loss: [2.5960846, 0.34935576, 4.8428135]\n",
      "140: Train Loss: [2.37223, 0.37554565, 4.3689146] | Test Loss: [2.5857024, 0.34410992, 4.827295]\n",
      "141: Train Loss: [2.4993958, 0.34849107, 4.6503005] | Test Loss: [2.7102907, 0.4580971, 4.9624844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142: Train Loss: [2.4337418, 0.30553678, 4.561947] | Test Loss: [2.6516325, 0.34827155, 4.9549937]\n",
      "143: Train Loss: [2.5110896, 0.39127463, 4.6309047] | Test Loss: [2.5671828, 0.3604604, 4.7739053]\n",
      "144: Train Loss: [2.3583283, 0.29861745, 4.4180393] | Test Loss: [2.6278117, 0.30883875, 4.9467845]\n",
      "145: Train Loss: [2.4251125, 0.30619243, 4.5440326] | Test Loss: [2.6653068, 0.34304693, 4.9875665]\n",
      "146: Train Loss: [2.434481, 0.378133, 4.490829] | Test Loss: [2.5871809, 0.39140585, 4.7829556]\n",
      "147: Train Loss: [2.30375, 0.31299397, 4.294506] | Test Loss: [2.7694411, 0.36785242, 5.17103]\n",
      "148: Train Loss: [2.4286988, 0.3619095, 4.495488] | Test Loss: [2.604876, 0.33130646, 4.8784456]\n",
      "149: Train Loss: [2.628231, 0.3287978, 4.9276643] | Test Loss: [2.511148, 0.3678029, 4.654493]\n",
      "150: Train Loss: [2.4303837, 0.41226557, 4.4485016] | Test Loss: [2.8064065, 0.46384913, 5.148964]\n",
      "151: Train Loss: [2.5454757, 0.45557895, 4.6353726] | Test Loss: [2.5580428, 0.37216365, 4.7439218]\n",
      "152: Train Loss: [2.4875658, 0.39832932, 4.5768023] | Test Loss: [2.4251485, 0.4549468, 4.39535]\n",
      "153: Train Loss: [2.4313831, 0.33721414, 4.5255523] | Test Loss: [2.6083465, 0.37968993, 4.837003]\n",
      "154: Train Loss: [2.431054, 0.3371588, 4.5249496] | Test Loss: [2.473102, 0.4264655, 4.5197387]\n",
      "155: Train Loss: [2.6256127, 0.41014618, 4.841079] | Test Loss: [2.7677085, 0.45782515, 5.077592]\n",
      "156: Train Loss: [2.370391, 0.32379577, 4.416986] | Test Loss: [2.5205812, 0.3628589, 4.6783037]\n",
      "157: Train Loss: [2.3454225, 0.34291467, 4.3479304] | Test Loss: [2.5407753, 0.3648726, 4.716678]\n",
      "158: Train Loss: [2.3617563, 0.30807725, 4.4154353] | Test Loss: [2.7929094, 0.4818954, 5.1039233]\n",
      "159: Train Loss: [2.470668, 0.3103696, 4.6309667] | Test Loss: [2.6327097, 0.42391175, 4.841508]\n",
      "160: Train Loss: [2.515418, 0.42504945, 4.605787] | Test Loss: [2.5111866, 0.33182582, 4.6905475]\n",
      "161: Train Loss: [2.3913026, 0.42859676, 4.354008] | Test Loss: [2.67001, 0.3075618, 5.0324583]\n",
      "162: Train Loss: [2.3431249, 0.2969973, 4.3892527] | Test Loss: [2.537682, 0.33584166, 4.7395225]\n",
      "163: Train Loss: [2.4214828, 0.34383482, 4.4991307] | Test Loss: [2.6705022, 0.33146626, 5.009538]\n",
      "164: Train Loss: [2.3852482, 0.3144006, 4.4560957] | Test Loss: [2.5507917, 0.38658756, 4.714996]\n",
      "165: Train Loss: [2.4610157, 0.3250189, 4.5970125] | Test Loss: [2.373679, 0.3570522, 4.3903055]\n",
      "166: Train Loss: [2.629056, 0.34467825, 4.9134336] | Test Loss: [2.4253647, 0.35276973, 4.4979596]\n",
      "167: Train Loss: [2.3593678, 0.34237114, 4.3763647] | Test Loss: [2.594636, 0.40946978, 4.7798023]\n",
      "168: Train Loss: [2.3961272, 0.41644597, 4.3758082] | Test Loss: [2.5298977, 0.33902842, 4.720767]\n",
      "169: Train Loss: [2.4381425, 0.33280733, 4.5434775] | Test Loss: [2.7182503, 0.41371286, 5.0227876]\n",
      "170: Train Loss: [2.3491364, 0.3284849, 4.3697877] | Test Loss: [2.5027957, 0.30797645, 4.697615]\n",
      "171: Train Loss: [2.3610415, 0.38742062, 4.3346624] | Test Loss: [2.5839262, 0.30760577, 4.8602467]\n",
      "172: Train Loss: [2.4407892, 0.33830443, 4.543274] | Test Loss: [2.6803713, 0.40459678, 4.956146]\n",
      "173: Train Loss: [2.503554, 0.3685726, 4.6385355] | Test Loss: [2.6112528, 0.44449967, 4.778006]\n",
      "174: Train Loss: [2.3665564, 0.30028024, 4.4328327] | Test Loss: [2.5560665, 0.36152297, 4.75061]\n",
      "175: Train Loss: [2.5591457, 0.34422952, 4.7740617] | Test Loss: [2.6662016, 0.38331515, 4.949088]\n",
      "176: Train Loss: [2.2139587, 0.31047773, 4.1174397] | Test Loss: [2.613146, 0.35552195, 4.87077]\n",
      "177: Train Loss: [2.449556, 0.40547755, 4.4936347] | Test Loss: [2.5269256, 0.36364472, 4.6902065]\n",
      "178: Train Loss: [2.3065672, 0.375418, 4.237716] | Test Loss: [2.693667, 0.35967618, 5.0276575]\n",
      "179: Train Loss: [2.4460464, 0.39407963, 4.498013] | Test Loss: [2.6141613, 0.27587762, 4.952445]\n",
      "180: Train Loss: [2.5214384, 0.2744383, 4.7684383] | Test Loss: [2.6877642, 0.4183758, 4.9571524]\n",
      "181: Train Loss: [2.5535274, 0.37098107, 4.7360735] | Test Loss: [2.568078, 0.34012812, 4.796028]\n",
      "182: Train Loss: [2.5596738, 0.39246753, 4.72688] | Test Loss: [2.5723164, 0.43249178, 4.712141]\n",
      "183: Train Loss: [2.3306115, 0.34452495, 4.316698] | Test Loss: [2.604718, 0.43919522, 4.770241]\n",
      "184: Train Loss: [2.5305908, 0.38358468, 4.677597] | Test Loss: [2.4191685, 0.3114727, 4.526864]\n",
      "185: Train Loss: [2.474148, 0.4201246, 4.5281715] | Test Loss: [2.617598, 0.5101335, 4.725063]\n",
      "186: Train Loss: [2.350207, 0.38437843, 4.3160357] | Test Loss: [2.4075913, 0.34869668, 4.466486]\n",
      "187: Train Loss: [2.3876069, 0.32110184, 4.454112] | Test Loss: [2.5888014, 0.36334628, 4.8142567]\n",
      "188: Train Loss: [2.4615202, 0.40803334, 4.515007] | Test Loss: [2.7252667, 0.33929065, 5.111243]\n",
      "189: Train Loss: [2.3900645, 0.32536313, 4.454766] | Test Loss: [2.5782604, 0.3813879, 4.775133]\n",
      "190: Train Loss: [2.530115, 0.34905577, 4.711174] | Test Loss: [2.490398, 0.36287197, 4.6179237]\n",
      "191: Train Loss: [2.2443776, 0.38194206, 4.106813] | Test Loss: [2.582215, 0.36147556, 4.8029547]\n",
      "192: Train Loss: [2.4885626, 0.36005282, 4.6170726] | Test Loss: [2.5593088, 0.403537, 4.7150807]\n",
      "193: Train Loss: [2.5291286, 0.3826678, 4.675589] | Test Loss: [2.5153472, 0.35882694, 4.6718674]\n",
      "194: Train Loss: [2.4485548, 0.3056593, 4.59145] | Test Loss: [2.5317051, 0.37866312, 4.684747]\n",
      "195: Train Loss: [2.3000865, 0.35133967, 4.248833] | Test Loss: [2.5342224, 0.46196562, 4.606479]\n",
      "196: Train Loss: [2.5136023, 0.35842097, 4.6687837] | Test Loss: [2.4714215, 0.36352673, 4.579316]\n",
      "197: Train Loss: [2.3837383, 0.3197208, 4.447756] | Test Loss: [2.7965214, 0.346743, 5.2462997]\n",
      "198: Train Loss: [2.3493707, 0.38205832, 4.3166833] | Test Loss: [2.509152, 0.41586852, 4.6024356]\n",
      "199: Train Loss: [2.403559, 0.38750854, 4.4196095] | Test Loss: [2.8103466, 0.37787026, 5.242823]\n",
      "200: Train Loss: [2.248534, 0.31384534, 4.183223] | Test Loss: [2.6478138, 0.36941352, 4.926214]\n",
      "201: Train Loss: [2.4882407, 0.3521321, 4.624349] | Test Loss: [2.6248467, 0.3284572, 4.921236]\n",
      "202: Train Loss: [2.3710003, 0.29210934, 4.449891] | Test Loss: [2.6948211, 0.44085768, 4.9487844]\n",
      "203: Train Loss: [2.314954, 0.34083903, 4.289069] | Test Loss: [2.6063907, 0.42370167, 4.7890797]\n",
      "204: Train Loss: [2.409564, 0.31115475, 4.507973] | Test Loss: [2.680903, 0.39313444, 4.9686713]\n",
      "205: Train Loss: [2.5300205, 0.3128876, 4.7471533] | Test Loss: [2.5230033, 0.4103355, 4.635671]\n",
      "206: Train Loss: [2.3059022, 0.3716358, 4.2401686] | Test Loss: [2.6580815, 0.36684546, 4.9493175]\n",
      "207: Train Loss: [2.5846276, 0.35967362, 4.8095818] | Test Loss: [2.559974, 0.3173737, 4.802574]\n",
      "208: Train Loss: [2.33709, 0.39233774, 4.281842] | Test Loss: [2.6550536, 0.2824486, 5.0276585]\n",
      "209: Train Loss: [2.4501922, 0.3397015, 4.560683] | Test Loss: [2.504482, 0.41532546, 4.5936384]\n",
      "210: Train Loss: [2.4274032, 0.3638678, 4.4909387] | Test Loss: [2.4287562, 0.32614356, 4.5313687]\n",
      "211: Train Loss: [2.4000614, 0.3580077, 4.442115] | Test Loss: [2.7048087, 0.337034, 5.072583]\n",
      "212: Train Loss: [2.5470865, 0.37315086, 4.721022] | Test Loss: [2.4518838, 0.32528022, 4.5784874]\n",
      "213: Train Loss: [2.6679106, 0.4029779, 4.932843] | Test Loss: [2.4867735, 0.32360357, 4.6499434]\n",
      "214: Train Loss: [2.4812195, 0.42141095, 4.541028] | Test Loss: [2.5601432, 0.35160017, 4.7686863]\n",
      "215: Train Loss: [2.472052, 0.33792138, 4.6061826] | Test Loss: [2.729035, 0.4135003, 5.0445695]\n",
      "216: Train Loss: [2.4305115, 0.33999646, 4.5210266] | Test Loss: [2.7198231, 0.37136012, 5.068286]\n",
      "217: Train Loss: [2.3426352, 0.3468645, 4.3384056] | Test Loss: [2.5852869, 0.2979666, 4.872607]\n",
      "218: Train Loss: [2.5048664, 0.37540454, 4.6343284] | Test Loss: [2.4545238, 0.42704242, 4.482005]\n",
      "219: Train Loss: [2.3120682, 0.39240697, 4.2317295] | Test Loss: [2.484426, 0.39856437, 4.5702877]\n",
      "220: Train Loss: [2.3489187, 0.30648553, 4.3913517] | Test Loss: [2.4821951, 0.36825433, 4.596136]\n",
      "221: Train Loss: [2.4649737, 0.36424652, 4.565701] | Test Loss: [2.7465177, 0.31154567, 5.1814895]\n",
      "222: Train Loss: [2.5010602, 0.36465502, 4.6374655] | Test Loss: [2.4849033, 0.38050044, 4.5893064]\n",
      "223: Train Loss: [2.482331, 0.47066173, 4.4940004] | Test Loss: [2.4999886, 0.35507125, 4.644906]\n",
      "224: Train Loss: [2.540125, 0.3792192, 4.7010307] | Test Loss: [2.4337912, 0.36148, 4.506102]\n",
      "225: Train Loss: [2.2049637, 0.36095616, 4.048971] | Test Loss: [2.3499975, 0.4694355, 4.2305593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226: Train Loss: [2.6750498, 0.43770406, 4.9123955] | Test Loss: [2.5010674, 0.3247782, 4.6773567]\n",
      "227: Train Loss: [2.4854147, 0.32655594, 4.6442738] | Test Loss: [2.6589856, 0.3947927, 4.9231787]\n",
      "228: Train Loss: [2.4536104, 0.3713041, 4.535917] | Test Loss: [2.6548338, 0.40332872, 4.9063387]\n",
      "229: Train Loss: [2.2985625, 0.37589225, 4.221233] | Test Loss: [2.4578395, 0.38671336, 4.5289655]\n",
      "230: Train Loss: [2.5479457, 0.40763572, 4.688256] | Test Loss: [2.7518964, 0.37101713, 5.132776]\n",
      "231: Train Loss: [2.2876375, 0.38833067, 4.1869445] | Test Loss: [2.7975452, 0.44991505, 5.1451755]\n",
      "232: Train Loss: [2.5008645, 0.3232536, 4.6784754] | Test Loss: [2.6881351, 0.34696704, 5.029303]\n",
      "233: Train Loss: [2.2730446, 0.31222475, 4.2338643] | Test Loss: [2.6570814, 0.37215936, 4.9420033]\n",
      "234: Train Loss: [2.3018389, 0.3554628, 4.2482147] | Test Loss: [2.6095574, 0.35920477, 4.85991]\n",
      "235: Train Loss: [2.229516, 0.30093944, 4.1580925] | Test Loss: [2.5218632, 0.35845459, 4.6852717]\n",
      "236: Train Loss: [2.4635632, 0.3500756, 4.5770507] | Test Loss: [2.5380588, 0.36078104, 4.7153363]\n",
      "237: Train Loss: [2.4079077, 0.3387347, 4.477081] | Test Loss: [2.4123492, 0.37329793, 4.4514003]\n",
      "238: Train Loss: [2.4306319, 0.36374733, 4.4975166] | Test Loss: [2.4175775, 0.3271333, 4.508022]\n",
      "239: Train Loss: [2.5731428, 0.43347216, 4.7128134] | Test Loss: [2.4664984, 0.35653424, 4.5764627]\n",
      "240: Train Loss: [2.4842443, 0.35736546, 4.611123] | Test Loss: [2.6855967, 0.34169397, 5.0294995]\n",
      "241: Train Loss: [2.320243, 0.35313034, 4.2873554] | Test Loss: [2.767619, 0.42641696, 5.108821]\n",
      "242: Train Loss: [2.363857, 0.34909475, 4.378619] | Test Loss: [2.6212022, 0.3643939, 4.8780107]\n",
      "243: Train Loss: [2.5292656, 0.35262352, 4.705908] | Test Loss: [2.8293724, 0.30247372, 5.3562713]\n",
      "244: Train Loss: [2.383007, 0.37163758, 4.3943763] | Test Loss: [2.5172777, 0.43621746, 4.598338]\n",
      "245: Train Loss: [2.3403523, 0.3479056, 4.332799] | Test Loss: [2.452279, 0.34864172, 4.5559163]\n",
      "246: Train Loss: [2.6082337, 0.3211269, 4.8953404] | Test Loss: [2.4236088, 0.3614669, 4.4857507]\n",
      "247: Train Loss: [2.5492985, 0.37339947, 4.725198] | Test Loss: [2.7860975, 0.45679322, 5.1154017]\n",
      "248: Train Loss: [2.474444, 0.3111299, 4.637758] | Test Loss: [2.760801, 0.36691603, 5.154686]\n",
      "249: Train Loss: [2.457931, 0.35745355, 4.5584087] | Test Loss: [2.4826396, 0.30096132, 4.6643176]\n",
      "250: Train Loss: [2.3736916, 0.30909652, 4.438287] | Test Loss: [2.6696637, 0.3384868, 5.0008407]\n",
      "251: Train Loss: [2.3073552, 0.36533648, 4.249374] | Test Loss: [2.7663867, 0.37255415, 5.160219]\n",
      "252: Train Loss: [2.4547057, 0.35091472, 4.558497] | Test Loss: [2.668959, 0.37622163, 4.961696]\n",
      "253: Train Loss: [2.2875497, 0.40493065, 4.170169] | Test Loss: [2.5185437, 0.37720287, 4.6598845]\n",
      "254: Train Loss: [2.2970915, 0.36735362, 4.2268295] | Test Loss: [2.5020347, 0.36503857, 4.639031]\n",
      "255: Train Loss: [2.592921, 0.35963508, 4.826207] | Test Loss: [2.6075153, 0.31059718, 4.9044333]\n",
      "256: Train Loss: [2.4098332, 0.39562404, 4.424042] | Test Loss: [2.4660761, 0.3443316, 4.5878205]\n",
      "257: Train Loss: [2.4635723, 0.40826395, 4.5188804] | Test Loss: [2.641849, 0.41383308, 4.869865]\n",
      "258: Train Loss: [2.4119678, 0.4305587, 4.393377] | Test Loss: [2.7392204, 0.39889878, 5.079542]\n",
      "259: Train Loss: [2.460884, 0.33996832, 4.5818] | Test Loss: [2.6445265, 0.38433123, 4.9047217]\n",
      "260: Train Loss: [2.421599, 0.33051795, 4.51268] | Test Loss: [2.5847764, 0.38832816, 4.7812247]\n",
      "261: Train Loss: [2.5943453, 0.33446756, 4.8542233] | Test Loss: [2.627977, 0.37712497, 4.878829]\n",
      "262: Train Loss: [2.5504513, 0.365863, 4.7350397] | Test Loss: [2.5757506, 0.41376826, 4.737733]\n",
      "263: Train Loss: [2.5969007, 0.28924546, 4.904556] | Test Loss: [2.482753, 0.3505872, 4.6149187]\n",
      "264: Train Loss: [2.321261, 0.40937844, 4.2331433] | Test Loss: [2.6987336, 0.48283586, 4.9146314]\n",
      "265: Train Loss: [2.5277162, 0.34111413, 4.7143183] | Test Loss: [2.5805225, 0.30761462, 4.8534303]\n",
      "266: Train Loss: [2.4543824, 0.36752614, 4.541239] | Test Loss: [2.6723595, 0.3461759, 4.998543]\n",
      "267: Train Loss: [2.4156606, 0.33685505, 4.4944663] | Test Loss: [2.53303, 0.35002187, 4.716038]\n",
      "268: Train Loss: [2.4952593, 0.31893754, 4.671581] | Test Loss: [2.4731278, 0.36638525, 4.57987]\n",
      "269: Train Loss: [2.4352887, 0.4079482, 4.4626293] | Test Loss: [2.4546459, 0.38601738, 4.5232744]\n",
      "270: Train Loss: [2.2754407, 0.3921578, 4.158724] | Test Loss: [2.4159658, 0.3406721, 4.4912596]\n",
      "271: Train Loss: [2.537502, 0.3399409, 4.735063] | Test Loss: [2.6116595, 0.38557407, 4.837745]\n",
      "272: Train Loss: [2.4148552, 0.32834134, 4.501369] | Test Loss: [2.4909697, 0.43717524, 4.544764]\n",
      "273: Train Loss: [2.4204435, 0.4174172, 4.42347] | Test Loss: [2.7629075, 0.28454667, 5.241268]\n",
      "274: Train Loss: [2.3679566, 0.3625755, 4.3733377] | Test Loss: [2.531813, 0.3711907, 4.6924353]\n",
      "275: Train Loss: [2.4887238, 0.35174307, 4.6257043] | Test Loss: [2.5597231, 0.45881066, 4.6606355]\n",
      "276: Train Loss: [2.4007576, 0.3596428, 4.441872] | Test Loss: [2.435621, 0.3754679, 4.4957743]\n",
      "277: Train Loss: [2.576143, 0.38636377, 4.765922] | Test Loss: [2.5993, 0.37457806, 4.824022]\n",
      "278: Train Loss: [2.3864067, 0.38033712, 4.392476] | Test Loss: [2.6538491, 0.3316438, 4.9760547]\n",
      "279: Train Loss: [2.460545, 0.36302385, 4.5580664] | Test Loss: [2.7028866, 0.37499636, 5.030777]\n",
      "280: Train Loss: [2.3607175, 0.2839705, 4.4374647] | Test Loss: [2.648374, 0.43091023, 4.865838]\n",
      "281: Train Loss: [2.4797542, 0.37524584, 4.5842624] | Test Loss: [2.6000922, 0.3001183, 4.900066]\n",
      "282: Train Loss: [2.2512805, 0.35647523, 4.1460857] | Test Loss: [2.6843367, 0.34393123, 5.024742]\n",
      "283: Train Loss: [2.5192628, 0.35590643, 4.682619] | Test Loss: [2.6327066, 0.37395468, 4.8914585]\n",
      "284: Train Loss: [2.4089189, 0.37296677, 4.444871] | Test Loss: [2.376175, 0.40517887, 4.347171]\n",
      "285: Train Loss: [2.5532682, 0.32815802, 4.7783785] | Test Loss: [2.390132, 0.36080357, 4.4194603]\n",
      "286: Train Loss: [2.6406975, 0.38089776, 4.9004974] | Test Loss: [2.6956193, 0.39466295, 4.996576]\n",
      "287: Train Loss: [2.4956279, 0.3303948, 4.660861] | Test Loss: [2.4760234, 0.39757693, 4.55447]\n",
      "288: Train Loss: [2.502114, 0.36029327, 4.6439347] | Test Loss: [2.6154094, 0.33049315, 4.900326]\n",
      "289: Train Loss: [2.5409014, 0.32457358, 4.7572293] | Test Loss: [2.553424, 0.39390287, 4.712945]\n",
      "290: Train Loss: [2.5561454, 0.3596254, 4.7526655] | Test Loss: [2.6199615, 0.3100298, 4.929893]\n",
      "291: Train Loss: [2.644743, 0.4168461, 4.8726397] | Test Loss: [2.533453, 0.39462066, 4.672285]\n",
      "292: Train Loss: [2.3813992, 0.35733536, 4.4054627] | Test Loss: [2.5310626, 0.33384347, 4.728282]\n",
      "293: Train Loss: [2.4267278, 0.32320765, 4.5302477] | Test Loss: [2.596805, 0.3782249, 4.8153853]\n",
      "294: Train Loss: [2.4421175, 0.3792287, 4.5050063] | Test Loss: [2.6504042, 0.40513247, 4.895676]\n",
      "295: Train Loss: [2.363495, 0.4125105, 4.31448] | Test Loss: [2.395509, 0.3540732, 4.436945]\n",
      "296: Train Loss: [2.5626197, 0.40554976, 4.7196894] | Test Loss: [2.6658413, 0.38240245, 4.9492803]\n",
      "297: Train Loss: [2.5039833, 0.3567124, 4.651254] | Test Loss: [2.5194187, 0.4528005, 4.5860367]\n",
      "298: Train Loss: [2.2324805, 0.3538862, 4.111075] | Test Loss: [2.6661205, 0.3918777, 4.9403634]\n",
      "299: Train Loss: [2.3134303, 0.39884463, 4.228016] | Test Loss: [2.6816628, 0.34344786, 5.019878]\n",
      "300: Train Loss: [2.3214824, 0.37542012, 4.2675447] | Test Loss: [2.799494, 0.3646548, 5.234333]\n",
      "301: Train Loss: [2.4484754, 0.32793468, 4.569016] | Test Loss: [2.5481343, 0.37334368, 4.722925]\n",
      "302: Train Loss: [2.2371, 0.3801367, 4.0940633] | Test Loss: [2.559105, 0.31505278, 4.803157]\n",
      "303: Train Loss: [2.484128, 0.332095, 4.636161] | Test Loss: [2.2001894, 0.3492196, 4.051159]\n",
      "304: Train Loss: [2.3315418, 0.33755857, 4.325525] | Test Loss: [2.6437416, 0.41467685, 4.8728065]\n",
      "305: Train Loss: [2.4647176, 0.45651346, 4.472922] | Test Loss: [2.4856555, 0.32980374, 4.641507]\n",
      "306: Train Loss: [2.4418976, 0.3912124, 4.492583] | Test Loss: [2.71571, 0.37264907, 5.0587707]\n",
      "307: Train Loss: [2.3985379, 0.3997383, 4.3973374] | Test Loss: [2.5683813, 0.39389828, 4.742864]\n",
      "308: Train Loss: [2.4776664, 0.4124937, 4.542839] | Test Loss: [2.4794583, 0.3646446, 4.594272]\n",
      "309: Train Loss: [2.4396198, 0.4027275, 4.476512] | Test Loss: [2.5480547, 0.3235756, 4.772534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310: Train Loss: [2.5506265, 0.38304242, 4.7182107] | Test Loss: [2.6639285, 0.38928586, 4.938571]\n",
      "311: Train Loss: [2.5158975, 0.32060924, 4.711186] | Test Loss: [2.6827717, 0.4802999, 4.8852434]\n",
      "312: Train Loss: [2.30451, 0.37878492, 4.230235] | Test Loss: [2.4764628, 0.46518117, 4.4877443]\n",
      "313: Train Loss: [2.4360237, 0.33761027, 4.534437] | Test Loss: [2.4517033, 0.29999703, 4.60341]\n",
      "314: Train Loss: [2.515944, 0.4170324, 4.614856] | Test Loss: [2.612389, 0.34367967, 4.8810987]\n",
      "315: Train Loss: [2.451345, 0.41254407, 4.4901457] | Test Loss: [2.4610825, 0.35982218, 4.5623426]\n",
      "316: Train Loss: [2.4206543, 0.34785607, 4.4934525] | Test Loss: [2.5994434, 0.32050297, 4.878384]\n",
      "317: Train Loss: [2.4020193, 0.34061483, 4.4634237] | Test Loss: [2.4531996, 0.40538752, 4.501012]\n",
      "318: Train Loss: [2.489748, 0.41454658, 4.5649495] | Test Loss: [2.5312364, 0.4400222, 4.622451]\n",
      "319: Train Loss: [2.3028028, 0.31323385, 4.2923717] | Test Loss: [2.61947, 0.36981803, 4.8691216]\n",
      "320: Train Loss: [2.6024468, 0.45636633, 4.748527] | Test Loss: [2.6463819, 0.40379393, 4.88897]\n",
      "321: Train Loss: [2.2628098, 0.37129003, 4.1543293] | Test Loss: [2.6413848, 0.34196502, 4.9408045]\n",
      "322: Train Loss: [2.3575253, 0.40128845, 4.313762] | Test Loss: [2.6284027, 0.37767658, 4.879129]\n",
      "323: Train Loss: [2.6958425, 0.3628172, 5.0288677] | Test Loss: [2.6311703, 0.3864254, 4.875915]\n",
      "324: Train Loss: [2.427427, 0.34264353, 4.5122104] | Test Loss: [2.6700957, 0.3460175, 4.994174]\n",
      "325: Train Loss: [2.3472474, 0.3148225, 4.379672] | Test Loss: [2.5698256, 0.42968348, 4.7099676]\n",
      "326: Train Loss: [2.6675959, 0.36769354, 4.9674983] | Test Loss: [2.5301306, 0.4014932, 4.658768]\n",
      "327: Train Loss: [2.4428682, 0.32903206, 4.5567045] | Test Loss: [2.5642936, 0.31429583, 4.8142915]\n",
      "328: Train Loss: [2.366952, 0.33857784, 4.395326] | Test Loss: [2.572085, 0.38481268, 4.759357]\n",
      "329: Train Loss: [2.602569, 0.40344873, 4.8016896] | Test Loss: [2.718653, 0.36713868, 5.070167]\n",
      "330: Train Loss: [2.4830234, 0.346851, 4.619196] | Test Loss: [2.5757298, 0.43719232, 4.7142673]\n",
      "331: Train Loss: [2.5966878, 0.3562277, 4.8371477] | Test Loss: [2.6831896, 0.4130889, 4.9532905]\n",
      "332: Train Loss: [2.5148005, 0.35226688, 4.6773343] | Test Loss: [2.5220947, 0.38051438, 4.663675]\n",
      "333: Train Loss: [2.541235, 0.44980523, 4.6326647] | Test Loss: [2.3251076, 0.3732806, 4.2769346]\n",
      "334: Train Loss: [2.4922018, 0.3586693, 4.6257343] | Test Loss: [2.4897385, 0.32398695, 4.65549]\n",
      "335: Train Loss: [2.5147789, 0.3806747, 4.648883] | Test Loss: [2.555958, 0.36861864, 4.7432976]\n",
      "336: Train Loss: [2.5719013, 0.40685254, 4.73695] | Test Loss: [2.63127, 0.4396373, 4.8229027]\n",
      "337: Train Loss: [2.3920202, 0.3581826, 4.425858] | Test Loss: [2.470149, 0.3491224, 4.5911756]\n",
      "338: Train Loss: [2.4601405, 0.3341079, 4.586173] | Test Loss: [2.6614661, 0.3672534, 4.955679]\n",
      "339: Train Loss: [2.6280916, 0.40241984, 4.853763] | Test Loss: [2.681315, 0.36932045, 4.9933095]\n",
      "340: Train Loss: [2.4000864, 0.36250088, 4.437672] | Test Loss: [2.6234136, 0.3652948, 4.881532]\n",
      "341: Train Loss: [2.4904997, 0.45051914, 4.5304804] | Test Loss: [2.5440493, 0.36245403, 4.7256446]\n",
      "342: Train Loss: [2.3374717, 0.30894536, 4.3659983] | Test Loss: [2.5664394, 0.37267902, 4.7601995]\n",
      "343: Train Loss: [2.3062267, 0.3472478, 4.265206] | Test Loss: [2.580186, 0.34380123, 4.8165708]\n",
      "344: Train Loss: [2.3535867, 0.35089317, 4.3562803] | Test Loss: [2.5847044, 0.3288098, 4.840599]\n",
      "345: Train Loss: [2.328773, 0.28422502, 4.373321] | Test Loss: [2.3231237, 0.41261598, 4.2336316]\n",
      "346: Train Loss: [2.471328, 0.3656054, 4.5770507] | Test Loss: [2.7304556, 0.30221805, 5.1586933]\n",
      "347: Train Loss: [2.4307775, 0.39270532, 4.4688497] | Test Loss: [2.4882102, 0.4090002, 4.56742]\n",
      "348: Train Loss: [2.4507244, 0.3421431, 4.5593057] | Test Loss: [2.5148873, 0.37111256, 4.6586623]\n",
      "349: Train Loss: [2.5388138, 0.5248204, 4.5528073] | Test Loss: [2.4742608, 0.38744542, 4.561076]\n",
      "350: Train Loss: [2.5119956, 0.3827917, 4.6411996] | Test Loss: [2.522607, 0.39851043, 4.6467037]\n",
      "351: Train Loss: [2.5100255, 0.40875062, 4.6113005] | Test Loss: [2.6643813, 0.37605298, 4.9527097]\n",
      "352: Train Loss: [2.5575485, 0.39093053, 4.7241664] | Test Loss: [2.5690494, 0.3800495, 4.758049]\n",
      "353: Train Loss: [2.4914339, 0.36497697, 4.617891] | Test Loss: [2.6558712, 0.40054524, 4.911197]\n",
      "354: Train Loss: [2.5082064, 0.33127502, 4.6851377] | Test Loss: [2.7008846, 0.36055118, 5.041218]\n",
      "355: Train Loss: [2.4238243, 0.3327318, 4.514917] | Test Loss: [2.5533466, 0.3609055, 4.7457876]\n",
      "356: Train Loss: [2.2807922, 0.34125495, 4.2203293] | Test Loss: [2.8821418, 0.6117321, 5.1525517]\n",
      "357: Train Loss: [2.5605738, 0.3807122, 4.7404356] | Test Loss: [2.5136664, 0.40032867, 4.627004]\n",
      "358: Train Loss: [2.452423, 0.34762272, 4.5572233] | Test Loss: [2.8291004, 0.4529534, 5.2052474]\n",
      "359: Train Loss: [2.3776035, 0.32092306, 4.434284] | Test Loss: [2.402724, 0.356438, 4.44901]\n",
      "360: Train Loss: [2.2906287, 0.32789716, 4.2533603] | Test Loss: [2.7316558, 0.39804733, 5.065264]\n",
      "361: Train Loss: [2.39213, 0.40182242, 4.382437] | Test Loss: [2.7372034, 0.35252553, 5.121881]\n",
      "362: Train Loss: [2.6687734, 0.33910832, 4.9984384] | Test Loss: [2.4673254, 0.34532544, 4.5893254]\n",
      "363: Train Loss: [2.5158455, 0.3632817, 4.6684093] | Test Loss: [2.75518, 0.35856932, 5.1517906]\n",
      "364: Train Loss: [2.6181784, 0.3727854, 4.863571] | Test Loss: [2.6202497, 0.36146998, 4.8790298]\n",
      "365: Train Loss: [2.5287247, 0.3734467, 4.684003] | Test Loss: [2.5756316, 0.37915462, 4.7721086]\n",
      "366: Train Loss: [2.5020666, 0.31650424, 4.687629] | Test Loss: [2.559726, 0.35917845, 4.7602735]\n",
      "367: Train Loss: [2.418282, 0.36900714, 4.467557] | Test Loss: [2.5503962, 0.3229545, 4.7778378]\n",
      "368: Train Loss: [2.5057442, 0.32656577, 4.6849227] | Test Loss: [2.5652664, 0.4275178, 4.703015]\n",
      "369: Train Loss: [2.4108925, 0.35451522, 4.46727] | Test Loss: [2.577197, 0.372302, 4.782092]\n",
      "370: Train Loss: [2.5114791, 0.34479725, 4.678161] | Test Loss: [2.5556195, 0.34651768, 4.7647214]\n",
      "371: Train Loss: [2.573089, 0.4273096, 4.7188683] | Test Loss: [2.4885552, 0.38022357, 4.5968866]\n",
      "372: Train Loss: [2.469451, 0.36985543, 4.5690465] | Test Loss: [2.6345518, 0.41764528, 4.851458]\n",
      "373: Train Loss: [2.5632157, 0.2956233, 4.830808] | Test Loss: [2.5487738, 0.4096559, 4.6878915]\n",
      "374: Train Loss: [2.3129454, 0.40090284, 4.224988] | Test Loss: [2.306519, 0.32527724, 4.2877607]\n",
      "375: Train Loss: [2.4032245, 0.33946595, 4.466983] | Test Loss: [2.5832, 0.34104717, 4.8253527]\n",
      "376: Train Loss: [2.4813824, 0.40748098, 4.5552835] | Test Loss: [2.4417171, 0.36398834, 4.519446]\n",
      "377: Train Loss: [2.449864, 0.35085586, 4.548872] | Test Loss: [2.612884, 0.3612699, 4.864498]\n",
      "378: Train Loss: [2.4858003, 0.37115398, 4.6004467] | Test Loss: [2.7977896, 0.36074856, 5.2348304]\n",
      "379: Train Loss: [2.584696, 0.4044103, 4.7649817] | Test Loss: [2.620864, 0.37447947, 4.8672485]\n",
      "380: Train Loss: [2.4938228, 0.29485893, 4.6927867] | Test Loss: [2.712611, 0.34932077, 5.075901]\n",
      "381: Train Loss: [2.4865563, 0.3381383, 4.6349745] | Test Loss: [2.6003063, 0.3354187, 4.865194]\n",
      "382: Train Loss: [2.413545, 0.34773225, 4.4793577] | Test Loss: [2.7468433, 0.43611187, 5.0575747]\n",
      "383: Train Loss: [2.322763, 0.34629303, 4.299233] | Test Loss: [2.5376937, 0.37179196, 4.7035956]\n",
      "384: Train Loss: [2.380711, 0.3772441, 4.384178] | Test Loss: [2.6390386, 0.49880826, 4.7792687]\n",
      "385: Train Loss: [2.4736693, 0.449028, 4.4983106] | Test Loss: [2.60398, 0.3900597, 4.8179007]\n",
      "386: Train Loss: [2.613739, 0.34201586, 4.8854623] | Test Loss: [2.3375554, 0.3671243, 4.3079867]\n",
      "387: Train Loss: [2.3624928, 0.39285278, 4.332133] | Test Loss: [2.5802927, 0.3614058, 4.7991796]\n",
      "388: Train Loss: [2.4776044, 0.3385357, 4.616673] | Test Loss: [2.6222064, 0.34489238, 4.8995204]\n",
      "389: Train Loss: [2.504559, 0.39282867, 4.6162896] | Test Loss: [2.6843603, 0.40955403, 4.9591665]\n",
      "390: Train Loss: [2.4473236, 0.3654138, 4.5292335] | Test Loss: [2.6538877, 0.4028986, 4.9048767]\n",
      "391: Train Loss: [2.508675, 0.39696965, 4.6203804] | Test Loss: [2.5431013, 0.43901515, 4.647187]\n",
      "392: Train Loss: [2.4845376, 0.39163128, 4.577444] | Test Loss: [2.7080994, 0.3716134, 5.044585]\n",
      "393: Train Loss: [2.511871, 0.32246092, 4.701281] | Test Loss: [2.726485, 0.24145526, 5.211515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394: Train Loss: [2.4837213, 0.42298636, 4.544456] | Test Loss: [2.5734575, 0.35828665, 4.788628]\n",
      "395: Train Loss: [2.626664, 0.4267657, 4.826562] | Test Loss: [2.6130981, 0.3398458, 4.8863506]\n",
      "396: Train Loss: [2.5165412, 0.37265116, 4.6604314] | Test Loss: [2.7005851, 0.34988138, 5.051289]\n",
      "397: Train Loss: [2.5518558, 0.3494509, 4.7542605] | Test Loss: [2.674738, 0.4192801, 4.930196]\n",
      "398: Train Loss: [2.654012, 0.39019343, 4.9178305] | Test Loss: [2.7833421, 0.3874215, 5.1792626]\n",
      "399: Train Loss: [2.5074995, 0.32273388, 4.692265] | Test Loss: [2.5473757, 0.35314518, 4.741606]\n",
      "400: Train Loss: [2.3758004, 0.37670273, 4.374898] | Test Loss: [2.611492, 0.34342435, 4.8795595]\n",
      "401: Train Loss: [2.3991027, 0.3767584, 4.421447] | Test Loss: [2.5704076, 0.35475567, 4.7860594]\n",
      "402: Train Loss: [2.5534935, 0.3342666, 4.7727203] | Test Loss: [2.4344952, 0.37132195, 4.4976683]\n",
      "403: Train Loss: [2.4459229, 0.36742318, 4.5244226] | Test Loss: [2.7684822, 0.34606698, 5.1908975]\n",
      "404: Train Loss: [2.395524, 0.40559986, 4.385448] | Test Loss: [2.625641, 0.43302113, 4.818261]\n",
      "405: Train Loss: [2.740191, 0.37424466, 5.1061373] | Test Loss: [2.5607595, 0.31682658, 4.8046927]\n",
      "406: Train Loss: [2.3311045, 0.36164442, 4.300565] | Test Loss: [2.4905167, 0.38953653, 4.591497]\n",
      "407: Train Loss: [2.525344, 0.3789454, 4.6717424] | Test Loss: [2.7044885, 0.40083987, 5.008137]\n",
      "408: Train Loss: [2.4934425, 0.39035568, 4.5965295] | Test Loss: [2.6825206, 0.36546597, 4.999575]\n",
      "409: Train Loss: [2.3334985, 0.2990911, 4.3679056] | Test Loss: [2.5736434, 0.34901172, 4.798275]\n",
      "410: Train Loss: [2.489783, 0.40236548, 4.5772004] | Test Loss: [2.505483, 0.397383, 4.6135826]\n",
      "411: Train Loss: [2.4711192, 0.35239464, 4.5898438] | Test Loss: [2.3870492, 0.33729953, 4.436799]\n",
      "412: Train Loss: [2.4887395, 0.3406247, 4.636854] | Test Loss: [2.642634, 0.40003031, 4.8852377]\n",
      "413: Train Loss: [2.4374857, 0.35589996, 4.5190716] | Test Loss: [2.4218652, 0.42583206, 4.417898]\n",
      "414: Train Loss: [2.456443, 0.37141085, 4.5414753] | Test Loss: [2.584078, 0.34476426, 4.823392]\n",
      "415: Train Loss: [2.4633024, 0.37285492, 4.55375] | Test Loss: [2.47162, 0.3624041, 4.5808363]\n",
      "416: Train Loss: [2.5292656, 0.29583967, 4.7626915] | Test Loss: [2.6785226, 0.4171545, 4.939891]\n",
      "417: Train Loss: [2.4618735, 0.40597728, 4.51777] | Test Loss: [2.4488027, 0.3501846, 4.547421]\n",
      "418: Train Loss: [2.4527254, 0.29928398, 4.606167] | Test Loss: [2.6187842, 0.37986502, 4.857703]\n",
      "419: Train Loss: [2.500342, 0.359478, 4.641206] | Test Loss: [2.5214214, 0.36212173, 4.6807213]\n",
      "420: Train Loss: [2.5514557, 0.37369516, 4.729216] | Test Loss: [2.6525257, 0.46226016, 4.842791]\n",
      "421: Train Loss: [2.5938337, 0.3695471, 4.8181205] | Test Loss: [2.7132785, 0.33238965, 5.094167]\n",
      "422: Train Loss: [2.5365887, 0.3867026, 4.686475] | Test Loss: [2.6086183, 0.3406388, 4.876598]\n",
      "423: Train Loss: [2.4444578, 0.40238512, 4.4865303] | Test Loss: [2.7152514, 0.32643783, 5.104065]\n",
      "424: Train Loss: [2.3115606, 0.31341428, 4.309707] | Test Loss: [2.715009, 0.41536462, 5.014653]\n",
      "425: Train Loss: [2.4255865, 0.34136996, 4.509803] | Test Loss: [2.3990421, 0.33975297, 4.458331]\n",
      "426: Train Loss: [2.5344942, 0.33097637, 4.738012] | Test Loss: [2.6321828, 0.33366933, 4.9306965]\n",
      "427: Train Loss: [2.5705276, 0.33930463, 4.8017507] | Test Loss: [2.575135, 0.37368643, 4.7765837]\n",
      "428: Train Loss: [2.5565867, 0.27745107, 4.8357224] | Test Loss: [2.4656467, 0.29870525, 4.6325884]\n",
      "429: Train Loss: [2.5395947, 0.31124836, 4.767941] | Test Loss: [2.8111084, 0.39388722, 5.2283297]\n",
      "430: Train Loss: [2.3430233, 0.33559862, 4.350448] | Test Loss: [2.6378376, 0.3628813, 4.912794]\n",
      "431: Train Loss: [2.6624928, 0.4558024, 4.869183] | Test Loss: [2.7915444, 0.41138268, 5.171706]\n",
      "432: Train Loss: [2.4458137, 0.31554216, 4.576085] | Test Loss: [2.5500002, 0.44261196, 4.657388]\n",
      "433: Train Loss: [2.5403962, 0.39417297, 4.6866193] | Test Loss: [2.6073124, 0.34516013, 4.869465]\n",
      "434: Train Loss: [2.3850615, 0.32109764, 4.449025] | Test Loss: [2.5320413, 0.35156238, 4.71252]\n",
      "435: Train Loss: [2.3793018, 0.3339067, 4.424697] | Test Loss: [2.5300715, 0.38642356, 4.6737194]\n",
      "436: Train Loss: [2.4762757, 0.46595752, 4.4865937] | Test Loss: [2.6449351, 0.3616696, 4.9282007]\n",
      "437: Train Loss: [2.5965445, 0.38977548, 4.8033137] | Test Loss: [2.4682248, 0.35073504, 4.5857143]\n",
      "438: Train Loss: [2.3775902, 0.32391065, 4.4312696] | Test Loss: [2.4803946, 0.40341437, 4.557375]\n",
      "439: Train Loss: [2.4595172, 0.31784388, 4.6011906] | Test Loss: [2.6172066, 0.40139282, 4.83302]\n",
      "440: Train Loss: [2.5114868, 0.39405355, 4.62892] | Test Loss: [2.5758367, 0.37846586, 4.7732077]\n",
      "441: Train Loss: [2.729356, 0.41453865, 5.0441732] | Test Loss: [2.5535994, 0.3094847, 4.797714]\n",
      "442: Train Loss: [2.6079636, 0.41892332, 4.7970037] | Test Loss: [2.544881, 0.37826666, 4.7114954]\n",
      "443: Train Loss: [2.4681919, 0.37697804, 4.559406] | Test Loss: [2.6947181, 0.4374993, 4.9519367]\n",
      "444: Train Loss: [2.5488443, 0.36641324, 4.7312756] | Test Loss: [2.3443274, 0.3618799, 4.326775]\n",
      "445: Train Loss: [2.4433286, 0.3705019, 4.5161552] | Test Loss: [2.6044307, 0.36104628, 4.847815]\n",
      "446: Train Loss: [2.423497, 0.3315863, 4.5154076] | Test Loss: [2.8352134, 0.4685573, 5.2018695]\n",
      "447: Train Loss: [2.321004, 0.3628795, 4.279128] | Test Loss: [2.6046665, 0.38775417, 4.821579]\n",
      "448: Train Loss: [2.5741289, 0.37023798, 4.77802] | Test Loss: [2.635347, 0.37674892, 4.8939447]\n",
      "449: Train Loss: [2.4121318, 0.3893647, 4.434899] | Test Loss: [2.46286, 0.3240188, 4.6017013]\n",
      "450: Train Loss: [2.3861873, 0.36718896, 4.4051857] | Test Loss: [2.538071, 0.38172406, 4.694418]\n",
      "451: Train Loss: [2.4869702, 0.3755268, 4.5984135] | Test Loss: [2.599791, 0.35493827, 4.844644]\n",
      "452: Train Loss: [2.4465628, 0.35763472, 4.535491] | Test Loss: [2.6899688, 0.3394388, 5.0404987]\n",
      "453: Train Loss: [2.3164225, 0.32403892, 4.308806] | Test Loss: [2.3682032, 0.32423404, 4.4121723]\n",
      "454: Train Loss: [2.4611318, 0.46322048, 4.459043] | Test Loss: [2.694995, 0.36063737, 5.0293527]\n",
      "455: Train Loss: [2.6175072, 0.36391962, 4.8710947] | Test Loss: [2.4941866, 0.3491911, 4.639182]\n",
      "456: Train Loss: [2.5196083, 0.33950216, 4.699714] | Test Loss: [2.6511817, 0.36030677, 4.9420567]\n",
      "457: Train Loss: [2.4246037, 0.3091758, 4.5400314] | Test Loss: [2.649331, 0.34982255, 4.9488397]\n",
      "458: Train Loss: [2.533708, 0.40396142, 4.663455] | Test Loss: [2.4135196, 0.401402, 4.4256372]\n",
      "459: Train Loss: [2.4851415, 0.38499507, 4.585288] | Test Loss: [2.619307, 0.4015292, 4.837085]\n",
      "460: Train Loss: [2.3584678, 0.37865213, 4.3382835] | Test Loss: [2.743648, 0.3920487, 5.0952473]\n",
      "461: Train Loss: [2.5476704, 0.41862193, 4.6767187] | Test Loss: [2.5629187, 0.32984516, 4.7959924]\n",
      "462: Train Loss: [2.4546812, 0.32629728, 4.583065] | Test Loss: [2.6062279, 0.31073833, 4.9017177]\n",
      "463: Train Loss: [2.5266082, 0.36526024, 4.6879563] | Test Loss: [2.3395882, 0.32570168, 4.3534746]\n",
      "464: Train Loss: [2.6634018, 0.34506616, 4.9817376] | Test Loss: [2.614918, 0.35661635, 4.8732195]\n",
      "465: Train Loss: [2.6937187, 0.4007369, 4.9867005] | Test Loss: [2.5883796, 0.3747377, 4.8020215]\n",
      "466: Train Loss: [2.412, 0.3790101, 4.4449897] | Test Loss: [2.59055, 0.41476873, 4.766331]\n",
      "467: Train Loss: [2.56922, 0.31750748, 4.820933] | Test Loss: [2.4024062, 0.39941022, 4.405402]\n",
      "468: Train Loss: [2.6370857, 0.3062566, 4.9679146] | Test Loss: [2.627407, 0.34898025, 4.9058337]\n",
      "469: Train Loss: [2.47915, 0.33599156, 4.6223087] | Test Loss: [2.417934, 0.3115661, 4.524302]\n",
      "470: Train Loss: [2.4906013, 0.29710034, 4.684102] | Test Loss: [2.6244433, 0.44299632, 4.80589]\n",
      "471: Train Loss: [2.5325105, 0.33731005, 4.727711] | Test Loss: [2.698906, 0.37950912, 5.018303]\n",
      "472: Train Loss: [2.4307177, 0.32998937, 4.531446] | Test Loss: [2.5209768, 0.34570223, 4.6962514]\n",
      "473: Train Loss: [2.5166337, 0.45700783, 4.5762596] | Test Loss: [2.5964234, 0.3592282, 4.8336186]\n",
      "474: Train Loss: [2.6460907, 0.32522598, 4.9669557] | Test Loss: [2.597477, 0.37509084, 4.8198633]\n",
      "475: Train Loss: [2.4517205, 0.32747298, 4.575968] | Test Loss: [2.7239234, 0.40182063, 5.046026]\n",
      "476: Train Loss: [2.4202714, 0.319913, 4.52063] | Test Loss: [2.613148, 0.37976024, 4.8465357]\n",
      "477: Train Loss: [2.4817147, 0.3590367, 4.6043925] | Test Loss: [2.6722498, 0.40157458, 4.942925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478: Train Loss: [2.4180894, 0.32896984, 4.507209] | Test Loss: [2.494319, 0.33883372, 4.649804]\n",
      "479: Train Loss: [2.5852835, 0.369834, 4.800733] | Test Loss: [2.6883674, 0.40287745, 4.9738574]\n",
      "480: Train Loss: [2.475206, 0.38386276, 4.566549] | Test Loss: [2.594866, 0.4061183, 4.7836137]\n",
      "481: Train Loss: [2.4184585, 0.3430034, 4.4939137] | Test Loss: [2.87441, 0.4458204, 5.3029995]\n",
      "482: Train Loss: [2.4052718, 0.3173579, 4.4931855] | Test Loss: [2.565034, 0.3979602, 4.7321076]\n",
      "483: Train Loss: [2.5682533, 0.3836134, 4.752893] | Test Loss: [2.6632783, 0.25259167, 5.073965]\n",
      "484: Train Loss: [2.5083249, 0.393469, 4.623181] | Test Loss: [2.53788, 0.31795472, 4.7578053]\n",
      "485: Train Loss: [2.547614, 0.38470578, 4.7105227] | Test Loss: [2.636951, 0.49443173, 4.77947]\n",
      "486: Train Loss: [2.4036741, 0.35715613, 4.450192] | Test Loss: [2.4835074, 0.3120568, 4.654958]\n",
      "487: Train Loss: [2.575366, 0.4959531, 4.654779] | Test Loss: [2.842345, 0.4326564, 5.2520337]\n",
      "488: Train Loss: [2.5187757, 0.37188876, 4.665663] | Test Loss: [2.549414, 0.35883296, 4.739995]\n",
      "489: Train Loss: [2.552031, 0.37788457, 4.7261777] | Test Loss: [2.5238502, 0.4040975, 4.643603]\n",
      "490: Train Loss: [2.572136, 0.384395, 4.7598767] | Test Loss: [2.6321507, 0.3447674, 4.9195337]\n",
      "491: Train Loss: [2.4702427, 0.36715963, 4.5733256] | Test Loss: [2.4399629, 0.3168171, 4.5631084]\n",
      "492: Train Loss: [2.4455981, 0.31114623, 4.58005] | Test Loss: [2.7082024, 0.41356277, 5.002842]\n",
      "493: Train Loss: [2.452174, 0.31747526, 4.5868726] | Test Loss: [2.6023767, 0.33776507, 4.866988]\n",
      "494: Train Loss: [2.3787382, 0.3469558, 4.4105206] | Test Loss: [2.5293202, 0.37594473, 4.682696]\n",
      "495: Train Loss: [2.5223167, 0.50081176, 4.543822] | Test Loss: [2.6453166, 0.38931566, 4.9013176]\n",
      "496: Train Loss: [2.4690783, 0.28501368, 4.653143] | Test Loss: [2.4587367, 0.40412745, 4.5133457]\n",
      "497: Train Loss: [2.4120753, 0.36082432, 4.4633265] | Test Loss: [2.482483, 0.3460133, 4.6189528]\n",
      "498: Train Loss: [2.4264205, 0.3065796, 4.5462613] | Test Loss: [2.7515726, 0.38544208, 5.117703]\n",
      "499: Train Loss: [2.4707398, 0.36729985, 4.5741796] | Test Loss: [2.6027815, 0.36802864, 4.8375344]\n",
      "500: Train Loss: [2.4875479, 0.322507, 4.652589] | Test Loss: [2.7843904, 0.39906517, 5.169716]\n",
      "501: Train Loss: [2.4467049, 0.3104489, 4.5829606] | Test Loss: [2.5963614, 0.39252266, 4.8002]\n",
      "502: Train Loss: [2.5127308, 0.37698096, 4.648481] | Test Loss: [2.4778552, 0.3562448, 4.599466]\n",
      "503: Train Loss: [2.4050725, 0.35622245, 4.4539223] | Test Loss: [2.6432195, 0.3395455, 4.9468937]\n",
      "504: Train Loss: [2.3529558, 0.31840232, 4.3875093] | Test Loss: [2.5791461, 0.3490471, 4.809245]\n",
      "505: Train Loss: [2.6286037, 0.45614502, 4.8010626] | Test Loss: [2.543659, 0.36001393, 4.727304]\n",
      "506: Train Loss: [2.3414788, 0.34082732, 4.34213] | Test Loss: [2.6087627, 0.31163716, 4.9058886]\n",
      "507: Train Loss: [2.6255336, 0.33360204, 4.917465] | Test Loss: [2.674867, 0.36246392, 4.98727]\n",
      "508: Train Loss: [2.501295, 0.29547808, 4.7071123] | Test Loss: [2.6778655, 0.42352092, 4.93221]\n",
      "509: Train Loss: [2.6454568, 0.32320842, 4.9677052] | Test Loss: [2.5160313, 0.37793976, 4.654123]\n",
      "510: Train Loss: [2.4278846, 0.33230883, 4.5234604] | Test Loss: [2.4866989, 0.39161265, 4.581785]\n",
      "511: Train Loss: [2.5907729, 0.35201627, 4.8295293] | Test Loss: [2.5478437, 0.40585187, 4.6898355]\n",
      "512: Train Loss: [2.5060406, 0.3503346, 4.6617465] | Test Loss: [2.5179176, 0.44244105, 4.5933943]\n",
      "513: Train Loss: [2.4403975, 0.38360974, 4.497185] | Test Loss: [2.439786, 0.26596808, 4.6136036]\n",
      "514: Train Loss: [2.5461347, 0.39660957, 4.6956596] | Test Loss: [2.6073337, 0.37207198, 4.842595]\n",
      "515: Train Loss: [2.6746266, 0.39599323, 4.95326] | Test Loss: [2.6907058, 0.3596302, 5.0217814]\n",
      "516: Train Loss: [2.693241, 0.3306402, 5.0558414] | Test Loss: [2.3266737, 0.35165915, 4.301688]\n",
      "517: Train Loss: [2.5825484, 0.33820626, 4.8268905] | Test Loss: [2.4875276, 0.441752, 4.5333033]\n",
      "518: Train Loss: [2.3294559, 0.31273305, 4.3461785] | Test Loss: [2.498088, 0.32971537, 4.6664605]\n",
      "519: Train Loss: [2.519702, 0.35963914, 4.6797647] | Test Loss: [2.7306566, 0.3577969, 5.103516]\n",
      "520: Train Loss: [2.5310225, 0.3981138, 4.6639314] | Test Loss: [2.8140166, 0.38051233, 5.247521]\n",
      "521: Train Loss: [2.400457, 0.33974987, 4.461164] | Test Loss: [2.6130543, 0.31269184, 4.913417]\n",
      "522: Train Loss: [2.506762, 0.33747712, 4.676047] | Test Loss: [2.579582, 0.41620934, 4.7429547]\n",
      "523: Train Loss: [2.337894, 0.3744747, 4.3013134] | Test Loss: [2.7519958, 0.3932609, 5.1107306]\n",
      "524: Train Loss: [2.153379, 0.28395465, 4.0228033] | Test Loss: [2.575849, 0.38283408, 4.768864]\n",
      "Epoch 8\n",
      "0: Train Loss: [2.380935, 0.37009507, 4.3917747] | Test Loss: [2.6007066, 0.4267785, 4.774635]\n",
      "1: Train Loss: [2.3162565, 0.36312887, 4.2693844] | Test Loss: [2.7127447, 0.3447715, 5.080718]\n",
      "2: Train Loss: [2.46664, 0.35551745, 4.5777626] | Test Loss: [2.4893093, 0.4043402, 4.5742784]\n",
      "3: Train Loss: [2.357008, 0.35443464, 4.3595815] | Test Loss: [2.5920968, 0.29552013, 4.8886733]\n",
      "4: Train Loss: [2.247891, 0.36262974, 4.133152] | Test Loss: [2.600211, 0.3379111, 4.8625107]\n",
      "5: Train Loss: [2.4888313, 0.38095003, 4.5967126] | Test Loss: [2.6283424, 0.31694412, 4.9397407]\n",
      "6: Train Loss: [2.3711102, 0.43496206, 4.307258] | Test Loss: [2.4256601, 0.32522553, 4.526095]\n",
      "7: Train Loss: [2.3315194, 0.31455877, 4.3484797] | Test Loss: [2.5679383, 0.3821392, 4.7537374]\n",
      "8: Train Loss: [2.3841937, 0.49192145, 4.276466] | Test Loss: [2.7956336, 0.4012662, 5.190001]\n",
      "9: Train Loss: [2.2361834, 0.34412038, 4.1282463] | Test Loss: [2.635636, 0.3774126, 4.8938594]\n",
      "10: Train Loss: [2.4717371, 0.36255303, 4.580921] | Test Loss: [2.5248036, 0.3579972, 4.69161]\n",
      "11: Train Loss: [2.4513183, 0.34556583, 4.5570707] | Test Loss: [2.6661718, 0.38352975, 4.948814]\n",
      "12: Train Loss: [2.183479, 0.33440053, 4.0325575] | Test Loss: [2.62811, 0.3412074, 4.9150124]\n",
      "13: Train Loss: [2.2232926, 0.37282622, 4.073759] | Test Loss: [2.661229, 0.4089974, 4.9134603]\n",
      "14: Train Loss: [2.2931328, 0.3405424, 4.2457232] | Test Loss: [2.702567, 0.4264367, 4.9786973]\n",
      "15: Train Loss: [2.3584807, 0.37779725, 4.3391643] | Test Loss: [2.5866773, 0.35743052, 4.815924]\n",
      "16: Train Loss: [2.2917657, 0.2927429, 4.2907887] | Test Loss: [2.539039, 0.3850581, 4.69302]\n",
      "17: Train Loss: [2.2578382, 0.36367878, 4.1519976] | Test Loss: [2.4663866, 0.4176402, 4.515133]\n",
      "18: Train Loss: [2.348822, 0.3506932, 4.346951] | Test Loss: [2.5746517, 0.28179637, 4.867507]\n",
      "19: Train Loss: [2.3635058, 0.37578052, 4.351231] | Test Loss: [2.467273, 0.34482357, 4.5897226]\n",
      "20: Train Loss: [2.3658006, 0.3405277, 4.3910737] | Test Loss: [2.718461, 0.31990626, 5.117016]\n",
      "21: Train Loss: [2.3335423, 0.36758006, 4.2995048] | Test Loss: [2.7230082, 0.37629813, 5.0697184]\n",
      "22: Train Loss: [2.293994, 0.365953, 4.222035] | Test Loss: [2.6250124, 0.40777904, 4.8422456]\n",
      "23: Train Loss: [2.4195185, 0.38798755, 4.4510493] | Test Loss: [2.501211, 0.39124283, 4.611179]\n",
      "24: Train Loss: [2.4471848, 0.32343233, 4.570937] | Test Loss: [2.6187186, 0.39331657, 4.8441205]\n",
      "25: Train Loss: [2.2646177, 0.34371454, 4.1855206] | Test Loss: [2.5619311, 0.38026324, 4.743599]\n",
      "26: Train Loss: [2.3182242, 0.37903517, 4.2574134] | Test Loss: [2.3544793, 0.38108665, 4.327872]\n",
      "27: Train Loss: [2.4417436, 0.43816847, 4.4453187] | Test Loss: [2.694015, 0.32554117, 5.062489]\n",
      "28: Train Loss: [2.2837083, 0.30815277, 4.259264] | Test Loss: [2.6283727, 0.39632678, 4.860419]\n",
      "29: Train Loss: [2.345622, 0.36598954, 4.3252544] | Test Loss: [2.600243, 0.29403877, 4.9064474]\n",
      "30: Train Loss: [2.3978627, 0.37536198, 4.4203634] | Test Loss: [2.7222264, 0.42245144, 5.0220013]\n",
      "31: Train Loss: [2.3582563, 0.3781579, 4.3383546] | Test Loss: [2.5900078, 0.3878954, 4.79212]\n",
      "32: Train Loss: [2.288288, 0.36547637, 4.2111] | Test Loss: [2.6392512, 0.39110148, 4.887401]\n",
      "33: Train Loss: [2.3233366, 0.43665832, 4.210015] | Test Loss: [2.532574, 0.3350394, 4.7301083]\n",
      "34: Train Loss: [2.341154, 0.35184094, 4.330467] | Test Loss: [2.6737328, 0.34146324, 5.0060024]\n",
      "35: Train Loss: [2.2647479, 0.3477985, 4.1816974] | Test Loss: [2.7419605, 0.3389704, 5.144951]\n",
      "36: Train Loss: [2.2128792, 0.35910308, 4.066655] | Test Loss: [2.4015698, 0.31623834, 4.4869013]\n",
      "37: Train Loss: [2.4686472, 0.35329834, 4.5839963] | Test Loss: [2.5400515, 0.38726804, 4.692835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38: Train Loss: [2.4411907, 0.3733579, 4.5090237] | Test Loss: [2.7014365, 0.35036728, 5.052506]\n",
      "39: Train Loss: [2.5297227, 0.37766322, 4.6817822] | Test Loss: [2.4463432, 0.48494804, 4.407738]\n",
      "40: Train Loss: [2.4243948, 0.30434752, 4.544442] | Test Loss: [2.505452, 0.4047426, 4.606161]\n",
      "41: Train Loss: [2.3347774, 0.3520164, 4.3175383] | Test Loss: [2.5726619, 0.427234, 4.7180896]\n",
      "42: Train Loss: [2.2168887, 0.33238202, 4.101395] | Test Loss: [2.510406, 0.36576772, 4.655044]\n",
      "43: Train Loss: [2.340939, 0.36937574, 4.3125024] | Test Loss: [2.4261494, 0.35367894, 4.4986196]\n",
      "44: Train Loss: [2.3868628, 0.3119245, 4.461801] | Test Loss: [2.6191535, 0.38844368, 4.8498635]\n",
      "45: Train Loss: [2.264946, 0.33903542, 4.1908565] | Test Loss: [2.5637121, 0.34533387, 4.78209]\n",
      "46: Train Loss: [2.3900027, 0.29890165, 4.481104] | Test Loss: [2.840756, 0.3876057, 5.293906]\n",
      "47: Train Loss: [2.3239658, 0.34427068, 4.303661] | Test Loss: [2.550284, 0.3434046, 4.757163]\n",
      "48: Train Loss: [2.3990824, 0.36589822, 4.4322667] | Test Loss: [2.675457, 0.3382754, 5.0126386]\n",
      "49: Train Loss: [2.3149052, 0.31665492, 4.3131557] | Test Loss: [2.7309608, 0.39336443, 5.0685573]\n",
      "50: Train Loss: [2.2529912, 0.29772967, 4.208253] | Test Loss: [2.5455656, 0.35695195, 4.7341795]\n",
      "51: Train Loss: [2.4314992, 0.37252635, 4.4904723] | Test Loss: [2.5928726, 0.3628957, 4.8228498]\n",
      "52: Train Loss: [2.2559905, 0.4185888, 4.0933924] | Test Loss: [2.5714872, 0.38314798, 4.759826]\n",
      "53: Train Loss: [2.3933027, 0.43635243, 4.350253] | Test Loss: [2.617383, 0.35508803, 4.879678]\n",
      "54: Train Loss: [2.4947858, 0.3467039, 4.6428676] | Test Loss: [2.7991886, 0.35267043, 5.2457066]\n",
      "55: Train Loss: [2.225033, 0.4371365, 4.0129294] | Test Loss: [2.6388621, 0.37840676, 4.8993177]\n",
      "56: Train Loss: [2.4805784, 0.37822488, 4.582932] | Test Loss: [2.6378248, 0.37422737, 4.901422]\n",
      "57: Train Loss: [2.2572763, 0.34511223, 4.1694403] | Test Loss: [2.6192095, 0.3885804, 4.8498387]\n",
      "58: Train Loss: [2.3453927, 0.37222826, 4.3185573] | Test Loss: [2.542488, 0.36686334, 4.718113]\n",
      "59: Train Loss: [2.5379465, 0.37699345, 4.6988993] | Test Loss: [2.521889, 0.36994684, 4.673831]\n",
      "60: Train Loss: [2.3336363, 0.28978533, 4.377487] | Test Loss: [2.633769, 0.34862134, 4.9189167]\n",
      "61: Train Loss: [2.339169, 0.32483706, 4.353501] | Test Loss: [2.8170493, 0.47421068, 5.159888]\n",
      "62: Train Loss: [2.247111, 0.35165507, 4.142567] | Test Loss: [2.481007, 0.36524904, 4.596765]\n",
      "63: Train Loss: [2.2747843, 0.3381389, 4.2114296] | Test Loss: [2.4023173, 0.35661468, 4.44802]\n",
      "64: Train Loss: [2.2917235, 0.35987082, 4.223576] | Test Loss: [2.350864, 0.40198305, 4.2997446]\n",
      "65: Train Loss: [2.1402147, 0.31335697, 3.9670725] | Test Loss: [2.4013593, 0.3649372, 4.4377813]\n",
      "66: Train Loss: [2.1343303, 0.3333285, 3.935332] | Test Loss: [2.5945027, 0.33046353, 4.858542]\n",
      "67: Train Loss: [2.3173215, 0.35548592, 4.279157] | Test Loss: [2.5578241, 0.33250812, 4.78314]\n",
      "68: Train Loss: [2.314286, 0.33278984, 4.295782] | Test Loss: [2.543629, 0.3277059, 4.759552]\n",
      "69: Train Loss: [2.205245, 0.33966804, 4.0708222] | Test Loss: [2.494112, 0.41258124, 4.5756426]\n",
      "70: Train Loss: [2.4911935, 0.37508497, 4.607302] | Test Loss: [2.7471395, 0.43836296, 5.055916]\n",
      "71: Train Loss: [2.3216763, 0.3956823, 4.24767] | Test Loss: [2.5024836, 0.3633722, 4.641595]\n",
      "72: Train Loss: [2.282575, 0.29720038, 4.2679496] | Test Loss: [2.528796, 0.37274158, 4.68485]\n",
      "73: Train Loss: [2.4094775, 0.31068736, 4.5082674] | Test Loss: [2.5463862, 0.32174665, 4.7710257]\n",
      "74: Train Loss: [2.25415, 0.3260041, 4.182296] | Test Loss: [2.7249255, 0.3590971, 5.090754]\n",
      "75: Train Loss: [2.3486757, 0.46370018, 4.233651] | Test Loss: [2.5376842, 0.42965508, 4.6457133]\n",
      "76: Train Loss: [2.3096771, 0.31415027, 4.305204] | Test Loss: [2.5563676, 0.41046622, 4.702269]\n",
      "77: Train Loss: [2.2604384, 0.38816512, 4.132712] | Test Loss: [2.0039039, 0.2436855, 3.764122]\n",
      "78: Train Loss: [2.3823948, 0.3043142, 4.4604754] | Test Loss: [2.4160225, 0.45463035, 4.3774147]\n",
      "79: Train Loss: [2.3657498, 0.39036238, 4.3411374] | Test Loss: [2.65635, 0.3615238, 4.951176]\n",
      "80: Train Loss: [2.3216734, 0.3211138, 4.322233] | Test Loss: [2.4782932, 0.3298383, 4.626748]\n",
      "81: Train Loss: [2.360345, 0.3605796, 4.3601103] | Test Loss: [2.54214, 0.3256054, 4.7586746]\n",
      "82: Train Loss: [2.514236, 0.32863677, 4.6998353] | Test Loss: [2.6588528, 0.42079508, 4.8969107]\n",
      "83: Train Loss: [2.139401, 0.36132133, 3.9174805] | Test Loss: [2.723354, 0.37712273, 5.0695853]\n",
      "84: Train Loss: [2.5713823, 0.32647666, 4.816288] | Test Loss: [2.3636668, 0.33410266, 4.393231]\n",
      "85: Train Loss: [2.3262255, 0.35095236, 4.301499] | Test Loss: [2.4689634, 0.36905164, 4.5688753]\n",
      "86: Train Loss: [2.4126165, 0.37874657, 4.4464865] | Test Loss: [2.5523992, 0.36468992, 4.7401085]\n",
      "87: Train Loss: [2.430655, 0.31504664, 4.546263] | Test Loss: [2.5301032, 0.3360666, 4.7241397]\n",
      "88: Train Loss: [2.154637, 0.30575418, 4.00352] | Test Loss: [2.7746727, 0.48725268, 5.062093]\n",
      "89: Train Loss: [2.4264655, 0.43134782, 4.421583] | Test Loss: [2.6470094, 0.43234387, 4.861675]\n",
      "90: Train Loss: [2.4125533, 0.33290172, 4.4922047] | Test Loss: [2.5857036, 0.34566766, 4.8257394]\n",
      "91: Train Loss: [2.5072634, 0.35004082, 4.664486] | Test Loss: [2.8235362, 0.4332917, 5.2137804]\n",
      "92: Train Loss: [2.361915, 0.33748794, 4.3863425] | Test Loss: [2.6963732, 0.32492903, 5.067817]\n",
      "93: Train Loss: [2.376606, 0.3597584, 4.3934536] | Test Loss: [2.5959477, 0.34989715, 4.8419986]\n",
      "94: Train Loss: [2.4921396, 0.29433388, 4.689945] | Test Loss: [2.691245, 0.34813163, 5.0343585]\n",
      "95: Train Loss: [2.3475604, 0.34599432, 4.3491263] | Test Loss: [2.4526455, 0.32511923, 4.580172]\n",
      "96: Train Loss: [2.4870932, 0.38920182, 4.584985] | Test Loss: [2.5925589, 0.3307265, 4.854391]\n",
      "97: Train Loss: [2.377337, 0.34376067, 4.4109135] | Test Loss: [2.3907914, 0.36590138, 4.4156814]\n",
      "98: Train Loss: [2.2962587, 0.3980189, 4.1944985] | Test Loss: [2.6734626, 0.49089733, 4.856028]\n",
      "99: Train Loss: [2.2422352, 0.33695674, 4.1475134] | Test Loss: [2.6174638, 0.35834438, 4.876583]\n",
      "100: Train Loss: [2.4025996, 0.3660225, 4.4391766] | Test Loss: [2.5977175, 0.33394322, 4.8614917]\n",
      "101: Train Loss: [2.2851, 0.36602584, 4.204174] | Test Loss: [2.5165386, 0.35454324, 4.678534]\n",
      "102: Train Loss: [2.1502285, 0.3518222, 3.9486346] | Test Loss: [2.209693, 0.34119916, 4.078187]\n",
      "103: Train Loss: [2.4531977, 0.45765993, 4.4487357] | Test Loss: [2.7474022, 0.37516627, 5.119638]\n",
      "104: Train Loss: [2.3138487, 0.34120837, 4.286489] | Test Loss: [2.8137782, 0.38268915, 5.2448673]\n",
      "105: Train Loss: [2.4644282, 0.3785678, 4.5502887] | Test Loss: [2.5034635, 0.3435786, 4.663348]\n",
      "106: Train Loss: [2.4447937, 0.3755816, 4.5140057] | Test Loss: [2.570243, 0.3492424, 4.7912436]\n",
      "107: Train Loss: [2.3790495, 0.37960976, 4.3784895] | Test Loss: [2.4457588, 0.31211245, 4.5794053]\n",
      "108: Train Loss: [2.5526385, 0.38453612, 4.720741] | Test Loss: [2.5304315, 0.44898817, 4.611875]\n",
      "109: Train Loss: [2.3431466, 0.34440202, 4.3418913] | Test Loss: [2.5664825, 0.34716552, 4.7857995]\n",
      "110: Train Loss: [2.3671503, 0.35275102, 4.3815494] | Test Loss: [2.6186712, 0.36542007, 4.8719225]\n",
      "111: Train Loss: [2.4409535, 0.35648727, 4.5254197] | Test Loss: [2.6103098, 0.36956677, 4.8510528]\n",
      "112: Train Loss: [2.4703462, 0.30065686, 4.6400356] | Test Loss: [2.676531, 0.4116923, 4.94137]\n",
      "113: Train Loss: [2.340174, 0.32106915, 4.3592787] | Test Loss: [2.6104393, 0.34596464, 4.874914]\n",
      "114: Train Loss: [2.4540994, 0.39481822, 4.5133805] | Test Loss: [2.5863752, 0.4242292, 4.7485213]\n",
      "115: Train Loss: [2.2790756, 0.32460856, 4.233543] | Test Loss: [2.6944444, 0.29434124, 5.0945477]\n",
      "116: Train Loss: [2.348246, 0.37194845, 4.324544] | Test Loss: [2.4489036, 0.40945137, 4.4883556]\n",
      "117: Train Loss: [2.3640537, 0.51336986, 4.2147374] | Test Loss: [2.57429, 0.3638641, 4.784716]\n",
      "118: Train Loss: [2.4284174, 0.34088483, 4.51595] | Test Loss: [2.7249916, 0.3529125, 5.0970707]\n",
      "119: Train Loss: [2.391737, 0.3782392, 4.405235] | Test Loss: [2.7872229, 0.4720029, 5.1024427]\n",
      "120: Train Loss: [2.4492927, 0.37207285, 4.5265126] | Test Loss: [2.732141, 0.38053223, 5.08375]\n",
      "121: Train Loss: [2.461051, 0.3463403, 4.575762] | Test Loss: [2.634768, 0.33447635, 4.9350595]\n",
      "122: Train Loss: [2.3386984, 0.34878188, 4.3286147] | Test Loss: [2.4463391, 0.3574783, 4.5352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123: Train Loss: [2.5171487, 0.4182545, 4.616043] | Test Loss: [2.6012774, 0.3324897, 4.870065]\n",
      "124: Train Loss: [2.3678226, 0.3679072, 4.3677382] | Test Loss: [2.5085618, 0.32345977, 4.693664]\n",
      "125: Train Loss: [2.4274657, 0.40955442, 4.445377] | Test Loss: [2.5800333, 0.44736338, 4.712703]\n",
      "126: Train Loss: [2.3810406, 0.3832485, 4.378833] | Test Loss: [2.5911791, 0.32854694, 4.8538113]\n",
      "127: Train Loss: [2.4687452, 0.3503055, 4.587185] | Test Loss: [2.771054, 0.35571116, 5.186397]\n",
      "128: Train Loss: [2.3537126, 0.36046746, 4.3469577] | Test Loss: [2.4620817, 0.31681645, 4.607347]\n",
      "129: Train Loss: [2.3651571, 0.3596204, 4.3706937] | Test Loss: [2.6008565, 0.37409005, 4.827623]\n",
      "130: Train Loss: [2.2569506, 0.3223417, 4.1915593] | Test Loss: [2.3560205, 0.38626584, 4.325775]\n",
      "131: Train Loss: [2.3746948, 0.39489803, 4.3544917] | Test Loss: [2.4546683, 0.34985045, 4.559486]\n",
      "132: Train Loss: [2.4456775, 0.3540257, 4.537329] | Test Loss: [2.822037, 0.3897857, 5.254288]\n",
      "133: Train Loss: [2.5076406, 0.3445124, 4.6707687] | Test Loss: [2.4092367, 0.3941173, 4.424356]\n",
      "134: Train Loss: [2.3884182, 0.36403325, 4.412803] | Test Loss: [2.4513624, 0.30209556, 4.6006293]\n",
      "135: Train Loss: [2.3584912, 0.31547976, 4.4015026] | Test Loss: [2.611293, 0.40510765, 4.8174787]\n",
      "136: Train Loss: [2.5859966, 0.37990922, 4.792084] | Test Loss: [2.5857024, 0.38173574, 4.789669]\n",
      "137: Train Loss: [2.408245, 0.3809135, 4.435577] | Test Loss: [2.58486, 0.31481987, 4.8549004]\n",
      "138: Train Loss: [2.5018535, 0.43451175, 4.5691953] | Test Loss: [2.579024, 0.37093914, 4.787109]\n",
      "139: Train Loss: [2.2944558, 0.3023958, 4.2865157] | Test Loss: [2.5224156, 0.3581124, 4.686719]\n",
      "140: Train Loss: [2.305206, 0.32416016, 4.286252] | Test Loss: [2.6265934, 0.38712996, 4.866057]\n",
      "141: Train Loss: [2.4178169, 0.3368251, 4.498809] | Test Loss: [2.6457684, 0.36012354, 4.931413]\n",
      "142: Train Loss: [2.4851043, 0.33336672, 4.636842] | Test Loss: [2.2186677, 0.34199658, 4.095339]\n",
      "143: Train Loss: [2.3425138, 0.38746193, 4.2975655] | Test Loss: [2.619744, 0.48474434, 4.7547436]\n",
      "144: Train Loss: [2.1161385, 0.33470583, 3.8975708] | Test Loss: [2.619535, 0.35382703, 4.885243]\n",
      "145: Train Loss: [2.3524113, 0.34293795, 4.3618846] | Test Loss: [2.6379113, 0.35323447, 4.9225883]\n",
      "146: Train Loss: [2.4235365, 0.31615558, 4.5309176] | Test Loss: [2.4130628, 0.33764032, 4.4884853]\n",
      "147: Train Loss: [2.352189, 0.36489686, 4.3394814] | Test Loss: [2.5318625, 0.37493473, 4.6887903]\n",
      "148: Train Loss: [2.3501754, 0.3702029, 4.3301477] | Test Loss: [2.6874301, 0.4392241, 4.935636]\n",
      "149: Train Loss: [2.3623624, 0.40009406, 4.3246307] | Test Loss: [2.607785, 0.39809763, 4.8174725]\n",
      "150: Train Loss: [2.4295826, 0.32513443, 4.534031] | Test Loss: [2.6101153, 0.3294977, 4.890733]\n",
      "151: Train Loss: [2.5704606, 0.3487639, 4.792157] | Test Loss: [2.688247, 0.3267995, 5.0496945]\n",
      "152: Train Loss: [2.335873, 0.39677313, 4.2749724] | Test Loss: [2.5564795, 0.36210153, 4.7508574]\n",
      "153: Train Loss: [2.3277621, 0.32558948, 4.3299346] | Test Loss: [2.5419514, 0.36139467, 4.722508]\n",
      "154: Train Loss: [2.3522108, 0.36801717, 4.3364043] | Test Loss: [2.3884258, 0.3453326, 4.431519]\n",
      "155: Train Loss: [2.420155, 0.3539374, 4.486373] | Test Loss: [2.5995219, 0.3066382, 4.8924055]\n",
      "156: Train Loss: [2.452447, 0.32735908, 4.5775347] | Test Loss: [2.6322823, 0.488089, 4.7764754]\n",
      "157: Train Loss: [2.3931909, 0.36796784, 4.4184136] | Test Loss: [2.7783763, 0.3590656, 5.197687]\n",
      "158: Train Loss: [2.5576618, 0.2973646, 4.817959] | Test Loss: [2.5483592, 0.3839475, 4.712771]\n",
      "159: Train Loss: [2.3892527, 0.31671536, 4.46179] | Test Loss: [2.5741215, 0.36962557, 4.7786174]\n",
      "160: Train Loss: [2.3313463, 0.31037906, 4.3523135] | Test Loss: [2.7354004, 0.33260378, 5.138197]\n",
      "161: Train Loss: [2.4012828, 0.38785395, 4.4147115] | Test Loss: [2.6870627, 0.35452932, 5.019596]\n",
      "162: Train Loss: [2.4183903, 0.34186518, 4.4949155] | Test Loss: [2.7644672, 0.3677915, 5.161143]\n",
      "163: Train Loss: [2.4238245, 0.38486895, 4.46278] | Test Loss: [2.6502376, 0.32696986, 4.973505]\n",
      "164: Train Loss: [2.4161942, 0.38300574, 4.449383] | Test Loss: [2.5711238, 0.3583663, 4.783881]\n",
      "165: Train Loss: [2.3395803, 0.37916738, 4.299993] | Test Loss: [2.5874155, 0.40970144, 4.7651296]\n",
      "166: Train Loss: [2.3667536, 0.3529467, 4.3805604] | Test Loss: [2.519349, 0.3888743, 4.649824]\n",
      "167: Train Loss: [2.413968, 0.32061622, 4.50732] | Test Loss: [2.5191479, 0.47638696, 4.5619087]\n",
      "168: Train Loss: [2.464958, 0.3757283, 4.554188] | Test Loss: [2.647863, 0.33312452, 4.962601]\n",
      "169: Train Loss: [2.297636, 0.3716295, 4.2236423] | Test Loss: [2.6752076, 0.309763, 5.0406523]\n",
      "170: Train Loss: [2.5610812, 0.32604402, 4.7961183] | Test Loss: [2.5558875, 0.3595243, 4.7522507]\n",
      "171: Train Loss: [2.515998, 0.394545, 4.6374507] | Test Loss: [2.5409074, 0.35811734, 4.7236977]\n",
      "172: Train Loss: [2.4019487, 0.37217474, 4.4317226] | Test Loss: [2.617576, 0.38017428, 4.8549776]\n",
      "173: Train Loss: [2.5371308, 0.38245744, 4.6918044] | Test Loss: [2.4206243, 0.4032645, 4.437984]\n",
      "174: Train Loss: [2.2586231, 0.30488336, 4.212363] | Test Loss: [2.6841867, 0.33515882, 5.0332146]\n",
      "175: Train Loss: [2.4352016, 0.39511624, 4.475287] | Test Loss: [2.5240324, 0.4540253, 4.5940394]\n",
      "176: Train Loss: [2.4334233, 0.36163944, 4.505207] | Test Loss: [2.5293655, 0.36640945, 4.692322]\n",
      "177: Train Loss: [2.466569, 0.35496783, 4.57817] | Test Loss: [2.6798642, 0.36130446, 4.998424]\n",
      "178: Train Loss: [2.432279, 0.35033643, 4.5142217] | Test Loss: [2.5071464, 0.35455555, 4.659737]\n",
      "179: Train Loss: [2.3124812, 0.34450993, 4.2804523] | Test Loss: [2.632542, 0.3946239, 4.87046]\n",
      "180: Train Loss: [2.5332513, 0.337589, 4.728914] | Test Loss: [2.6300685, 0.34185237, 4.918285]\n",
      "181: Train Loss: [2.378609, 0.35270283, 4.4045153] | Test Loss: [2.5521407, 0.41210556, 4.692176]\n",
      "182: Train Loss: [2.5569448, 0.34813863, 4.765751] | Test Loss: [2.6439054, 0.31011522, 4.9776955]\n",
      "183: Train Loss: [2.451836, 0.40429664, 4.499376] | Test Loss: [2.790431, 0.40664235, 5.1742196]\n",
      "184: Train Loss: [2.4142718, 0.35337913, 4.4751644] | Test Loss: [2.5304618, 0.30969453, 4.751229]\n",
      "185: Train Loss: [2.3291197, 0.3223082, 4.3359313] | Test Loss: [2.8105237, 0.45730677, 5.1637406]\n",
      "186: Train Loss: [2.3865025, 0.34574136, 4.4272637] | Test Loss: [2.634952, 0.39081627, 4.879088]\n",
      "187: Train Loss: [2.580447, 0.33327472, 4.827619] | Test Loss: [2.7320573, 0.5333268, 4.930788]\n",
      "188: Train Loss: [2.398933, 0.4305004, 4.3673654] | Test Loss: [2.5777402, 0.38854396, 4.7669363]\n",
      "189: Train Loss: [2.481869, 0.35386115, 4.6098766] | Test Loss: [2.6236138, 0.3658168, 4.881411]\n",
      "190: Train Loss: [2.29581, 0.3220198, 4.2696004] | Test Loss: [2.5660992, 0.32820296, 4.803995]\n",
      "191: Train Loss: [2.3671043, 0.3951829, 4.3390255] | Test Loss: [2.500657, 0.35584486, 4.645469]\n",
      "192: Train Loss: [2.303798, 0.30353123, 4.3040648] | Test Loss: [2.6072972, 0.38320586, 4.8313885]\n",
      "193: Train Loss: [2.3007166, 0.34163472, 4.2597985] | Test Loss: [2.7307198, 0.3765824, 5.084857]\n",
      "194: Train Loss: [2.390164, 0.37762713, 4.402701] | Test Loss: [2.7024918, 0.34122524, 5.0637584]\n",
      "195: Train Loss: [2.485105, 0.35203147, 4.618179] | Test Loss: [2.584759, 0.37331074, 4.7962074]\n",
      "196: Train Loss: [2.315207, 0.3789322, 4.251482] | Test Loss: [2.4205143, 0.3228949, 4.5181336]\n",
      "197: Train Loss: [2.480512, 0.4069064, 4.554117] | Test Loss: [2.694153, 0.3569294, 5.031377]\n",
      "198: Train Loss: [2.4994776, 0.4366301, 4.562325] | Test Loss: [2.6206207, 0.39502528, 4.846216]\n",
      "199: Train Loss: [2.4371085, 0.31168437, 4.5625324] | Test Loss: [2.5096273, 0.35513678, 4.664118]\n",
      "200: Train Loss: [2.3847184, 0.3019252, 4.4675117] | Test Loss: [2.5458195, 0.35896522, 4.7326736]\n",
      "201: Train Loss: [2.490839, 0.35677838, 4.6249] | Test Loss: [2.5253735, 0.31935525, 4.731392]\n",
      "202: Train Loss: [2.4395287, 0.3809267, 4.498131] | Test Loss: [2.5259886, 0.3784748, 4.6735024]\n",
      "203: Train Loss: [2.373783, 0.43234524, 4.315221] | Test Loss: [2.433547, 0.3617743, 4.5053196]\n",
      "204: Train Loss: [2.571942, 0.360293, 4.7835913] | Test Loss: [2.4712512, 0.35808882, 4.5844135]\n",
      "205: Train Loss: [2.4110842, 0.27891415, 4.5432544] | Test Loss: [2.6529024, 0.33762386, 4.9681807]\n",
      "206: Train Loss: [2.1769757, 0.2882811, 4.0656705] | Test Loss: [2.7351704, 0.37397644, 5.0963645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207: Train Loss: [2.2329445, 0.3450745, 4.1208143] | Test Loss: [2.6914217, 0.3349021, 5.047941]\n",
      "208: Train Loss: [2.2859056, 0.33132118, 4.24049] | Test Loss: [2.6706715, 0.3571216, 4.9842215]\n",
      "209: Train Loss: [2.506317, 0.34422594, 4.668408] | Test Loss: [2.579943, 0.3823632, 4.7775226]\n",
      "210: Train Loss: [2.504921, 0.39088923, 4.6189528] | Test Loss: [2.7601461, 0.40648428, 5.113808]\n",
      "211: Train Loss: [2.4416375, 0.32606146, 4.557214] | Test Loss: [2.5805383, 0.3931542, 4.7679224]\n",
      "212: Train Loss: [2.506521, 0.37912357, 4.6339183] | Test Loss: [2.6187987, 0.39925236, 4.838345]\n",
      "213: Train Loss: [2.3766336, 0.37698928, 4.376278] | Test Loss: [2.5613487, 0.34796253, 4.774735]\n",
      "214: Train Loss: [2.4873507, 0.37329334, 4.601408] | Test Loss: [2.6101637, 0.39123893, 4.829088]\n",
      "215: Train Loss: [2.3481362, 0.33914694, 4.3571253] | Test Loss: [2.5731914, 0.35337743, 4.7930055]\n",
      "216: Train Loss: [2.477926, 0.428051, 4.527801] | Test Loss: [2.493097, 0.3597174, 4.626477]\n",
      "217: Train Loss: [2.2905507, 0.32362786, 4.2574735] | Test Loss: [2.6705546, 0.42023763, 4.9208717]\n",
      "218: Train Loss: [2.2895262, 0.33360064, 4.245452] | Test Loss: [2.5614727, 0.41880506, 4.70414]\n",
      "219: Train Loss: [2.4973211, 0.37038657, 4.6242557] | Test Loss: [2.8140795, 0.4317601, 5.1963987]\n",
      "220: Train Loss: [2.4626353, 0.3461606, 4.57911] | Test Loss: [2.6204684, 0.32052785, 4.9204087]\n",
      "221: Train Loss: [2.4457648, 0.36594155, 4.525588] | Test Loss: [2.5283022, 0.3836403, 4.672964]\n",
      "222: Train Loss: [2.412885, 0.34252894, 4.483241] | Test Loss: [2.8375106, 0.39195132, 5.28307]\n",
      "223: Train Loss: [2.4750493, 0.35529786, 4.5948005] | Test Loss: [2.4639504, 0.36020723, 4.5676937]\n",
      "224: Train Loss: [2.4389567, 0.35861796, 4.5192957] | Test Loss: [2.6703773, 0.37429166, 4.966463]\n",
      "225: Train Loss: [2.4603834, 0.3869647, 4.533802] | Test Loss: [2.7609692, 0.3376653, 5.1842732]\n",
      "226: Train Loss: [2.4075007, 0.33804283, 4.4769588] | Test Loss: [2.6135454, 0.40675092, 4.82034]\n",
      "227: Train Loss: [2.3810523, 0.3925912, 4.3695135] | Test Loss: [2.3926296, 0.23212779, 4.5531316]\n",
      "228: Train Loss: [2.596494, 0.35361224, 4.8393755] | Test Loss: [2.8091295, 0.48521245, 5.1330466]\n",
      "229: Train Loss: [2.3248014, 0.2992328, 4.35037] | Test Loss: [2.4772477, 0.36776358, 4.586732]\n",
      "230: Train Loss: [2.380634, 0.3962752, 4.364993] | Test Loss: [2.5461416, 0.39723992, 4.695043]\n",
      "231: Train Loss: [2.4012802, 0.30313748, 4.499423] | Test Loss: [2.5442967, 0.36555457, 4.723039]\n",
      "232: Train Loss: [2.4833884, 0.333228, 4.6335487] | Test Loss: [2.6006038, 0.42423636, 4.7769713]\n",
      "233: Train Loss: [2.3126562, 0.2707358, 4.3545766] | Test Loss: [2.6191242, 0.41525036, 4.822998]\n",
      "234: Train Loss: [2.4706442, 0.3534091, 4.587879] | Test Loss: [2.542157, 0.3599869, 4.724327]\n",
      "235: Train Loss: [2.3477383, 0.4185258, 4.276951] | Test Loss: [2.6039536, 0.30786842, 4.9000387]\n",
      "236: Train Loss: [2.6027577, 0.34484905, 4.8606663] | Test Loss: [2.4178064, 0.31618336, 4.519429]\n",
      "237: Train Loss: [2.5072865, 0.29523274, 4.7193403] | Test Loss: [2.5910664, 0.36876178, 4.813371]\n",
      "238: Train Loss: [2.5713322, 0.3525345, 4.79013] | Test Loss: [2.4069915, 0.32929516, 4.484688]\n",
      "239: Train Loss: [2.360994, 0.3670672, 4.354921] | Test Loss: [2.6576145, 0.3223218, 4.992907]\n",
      "240: Train Loss: [2.5521476, 0.33701307, 4.767282] | Test Loss: [2.6567502, 0.38435107, 4.929149]\n",
      "241: Train Loss: [2.3802876, 0.2953619, 4.4652133] | Test Loss: [2.7254932, 0.31128433, 5.139702]\n",
      "242: Train Loss: [2.436938, 0.3681433, 4.505733] | Test Loss: [2.6812098, 0.41858414, 4.9438353]\n",
      "243: Train Loss: [2.4339917, 0.29770082, 4.5702825] | Test Loss: [2.78764, 0.4001016, 5.1751785]\n",
      "244: Train Loss: [2.4022582, 0.2950179, 4.5094986] | Test Loss: [2.5430388, 0.45017737, 4.6359005]\n",
      "245: Train Loss: [2.5093453, 0.37847537, 4.6402154] | Test Loss: [2.6492682, 0.36222067, 4.9363155]\n",
      "246: Train Loss: [2.4187374, 0.38097054, 4.4565043] | Test Loss: [2.599383, 0.3388964, 4.85987]\n",
      "247: Train Loss: [2.2669177, 0.34249198, 4.1913433] | Test Loss: [2.6106734, 0.3922715, 4.8290753]\n",
      "248: Train Loss: [2.4263194, 0.29856583, 4.554073] | Test Loss: [2.6634831, 0.36404416, 4.962922]\n",
      "249: Train Loss: [2.3712707, 0.30033186, 4.4422092] | Test Loss: [2.5290601, 0.326266, 4.7318544]\n",
      "250: Train Loss: [2.4638803, 0.44534275, 4.482418] | Test Loss: [2.6149561, 0.3575529, 4.8723593]\n",
      "251: Train Loss: [2.3734984, 0.39245242, 4.3545446] | Test Loss: [2.644075, 0.34750623, 4.940644]\n",
      "252: Train Loss: [2.5772398, 0.34261036, 4.811869] | Test Loss: [2.7670307, 0.3835976, 5.150464]\n",
      "253: Train Loss: [2.4633708, 0.41065508, 4.5160866] | Test Loss: [2.6051645, 0.4338231, 4.776506]\n",
      "254: Train Loss: [2.4774725, 0.31237563, 4.6425695] | Test Loss: [2.5297987, 0.36050084, 4.6990967]\n",
      "255: Train Loss: [2.2948277, 0.38717932, 4.202476] | Test Loss: [2.6635795, 0.4589003, 4.8682585]\n",
      "256: Train Loss: [2.539058, 0.347242, 4.730874] | Test Loss: [2.5837018, 0.33303785, 4.834366]\n",
      "257: Train Loss: [2.4245102, 0.35149968, 4.497521] | Test Loss: [2.6778412, 0.3226622, 5.03302]\n",
      "258: Train Loss: [2.458655, 0.356672, 4.5606384] | Test Loss: [2.7367764, 0.43379962, 5.039753]\n",
      "259: Train Loss: [2.2775779, 0.35834908, 4.196807] | Test Loss: [2.6160312, 0.3331397, 4.8989224]\n",
      "260: Train Loss: [2.4922705, 0.4215082, 4.5630326] | Test Loss: [2.8855212, 0.35730702, 5.4137354]\n",
      "261: Train Loss: [2.540293, 0.38036618, 4.7002196] | Test Loss: [2.7292082, 0.3648427, 5.0935736]\n",
      "262: Train Loss: [2.3959713, 0.38224247, 4.4097] | Test Loss: [2.6763666, 0.35401678, 4.9987164]\n",
      "263: Train Loss: [2.3237987, 0.40705645, 4.240541] | Test Loss: [2.7410886, 0.4092387, 5.0729384]\n",
      "264: Train Loss: [2.3212981, 0.35072514, 4.291871] | Test Loss: [2.5028334, 0.3461882, 4.6594787]\n",
      "265: Train Loss: [2.5660028, 0.37774584, 4.75426] | Test Loss: [2.6328921, 0.39923134, 4.866553]\n",
      "266: Train Loss: [2.5679507, 0.3389528, 4.7969484] | Test Loss: [2.653177, 0.4125226, 4.8938313]\n",
      "267: Train Loss: [2.4770017, 0.355502, 4.598501] | Test Loss: [2.6092348, 0.3524962, 4.8659735]\n",
      "268: Train Loss: [2.4563358, 0.35450298, 4.5581684] | Test Loss: [2.3864992, 0.3384537, 4.4345446]\n",
      "269: Train Loss: [2.597635, 0.2922947, 4.9029756] | Test Loss: [2.6280644, 0.42164505, 4.8344836]\n",
      "270: Train Loss: [2.4161003, 0.3689069, 4.4632936] | Test Loss: [2.5611396, 0.33218807, 4.790091]\n",
      "271: Train Loss: [2.4954824, 0.36509213, 4.6258726] | Test Loss: [2.5336096, 0.42386073, 4.6433587]\n",
      "272: Train Loss: [2.522841, 0.45069692, 4.594985] | Test Loss: [2.5611362, 0.34441337, 4.777859]\n",
      "273: Train Loss: [2.4470007, 0.37019125, 4.5238104] | Test Loss: [2.5666287, 0.35198227, 4.7812753]\n",
      "274: Train Loss: [2.536563, 0.42970902, 4.643417] | Test Loss: [2.6228526, 0.3488837, 4.8968215]\n",
      "275: Train Loss: [2.5189579, 0.38474452, 4.653171] | Test Loss: [2.622386, 0.3577458, 4.8870263]\n",
      "276: Train Loss: [2.6781015, 0.4864238, 4.869779] | Test Loss: [2.8405762, 0.42813024, 5.253022]\n",
      "277: Train Loss: [2.5089014, 0.39015776, 4.627645] | Test Loss: [2.5002267, 0.36285284, 4.6376004]\n",
      "278: Train Loss: [2.744597, 0.4804573, 5.0087366] | Test Loss: [2.5592453, 0.338932, 4.7795587]\n",
      "279: Train Loss: [2.5634713, 0.32209316, 4.8048496] | Test Loss: [2.5208983, 0.37243712, 4.6693597]\n",
      "280: Train Loss: [2.5523856, 0.30099902, 4.803772] | Test Loss: [2.6226003, 0.32781255, 4.917388]\n",
      "281: Train Loss: [2.4842076, 0.35273004, 4.615685] | Test Loss: [2.470323, 0.40316355, 4.5374827]\n",
      "282: Train Loss: [2.4852438, 0.4567095, 4.513778] | Test Loss: [2.5663855, 0.4360891, 4.696682]\n",
      "283: Train Loss: [2.389724, 0.45672166, 4.3227262] | Test Loss: [2.6709445, 0.43116447, 4.9107246]\n",
      "284: Train Loss: [2.4831543, 0.41008645, 4.556222] | Test Loss: [2.6640453, 0.36883822, 4.9592524]\n",
      "285: Train Loss: [2.3849607, 0.32051262, 4.4494085] | Test Loss: [2.7194777, 0.35651678, 5.0824385]\n",
      "286: Train Loss: [2.5934396, 0.36205393, 4.8248253] | Test Loss: [2.6281776, 0.3357222, 4.9206333]\n",
      "287: Train Loss: [2.4560976, 0.37094262, 4.5412526] | Test Loss: [2.9755616, 0.6784832, 5.27264]\n",
      "288: Train Loss: [2.430677, 0.39440045, 4.4669533] | Test Loss: [2.7190366, 0.3617689, 5.0763044]\n",
      "289: Train Loss: [2.3952167, 0.3525191, 4.4379144] | Test Loss: [2.9020588, 0.43434778, 5.36977]\n",
      "290: Train Loss: [2.5512354, 0.36996812, 4.732503] | Test Loss: [2.582019, 0.31421635, 4.849822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291: Train Loss: [2.483759, 0.35577682, 4.611741] | Test Loss: [2.677318, 0.41179544, 4.9428406]\n",
      "292: Train Loss: [2.5235546, 0.3694843, 4.6776247] | Test Loss: [2.8132617, 0.3959769, 5.2305465]\n",
      "293: Train Loss: [2.6149232, 0.4515138, 4.7783327] | Test Loss: [2.6246424, 0.36326692, 4.886018]\n",
      "294: Train Loss: [2.5385735, 0.37868968, 4.6984572] | Test Loss: [2.6006627, 0.41612738, 4.785198]\n",
      "295: Train Loss: [2.536281, 0.41303828, 4.659524] | Test Loss: [2.7562065, 0.37890595, 5.1335073]\n",
      "296: Train Loss: [2.5714645, 0.35004398, 4.7928853] | Test Loss: [2.551165, 0.37536666, 4.7269635]\n",
      "297: Train Loss: [2.4148955, 0.36713263, 4.4626584] | Test Loss: [2.7073262, 0.30802032, 5.106632]\n",
      "298: Train Loss: [2.643507, 0.3488798, 4.938134] | Test Loss: [2.6133208, 0.3527922, 4.8738494]\n",
      "299: Train Loss: [2.3970857, 0.3784922, 4.415679] | Test Loss: [2.6460829, 0.39325845, 4.898907]\n",
      "300: Train Loss: [2.4324825, 0.31086078, 4.5541043] | Test Loss: [2.4864886, 0.33685318, 4.636124]\n",
      "301: Train Loss: [2.4698248, 0.34357777, 4.5960717] | Test Loss: [2.5973213, 0.3179073, 4.876735]\n",
      "302: Train Loss: [2.5455909, 0.4175348, 4.673647] | Test Loss: [2.461456, 0.40580574, 4.5171065]\n",
      "303: Train Loss: [2.5614707, 0.3884841, 4.7344575] | Test Loss: [2.9442735, 0.4317703, 5.4567766]\n",
      "304: Train Loss: [2.560759, 0.29577628, 4.825742] | Test Loss: [2.5962243, 0.42373204, 4.7687163]\n",
      "305: Train Loss: [2.534501, 0.37581685, 4.6931853] | Test Loss: [2.7527304, 0.30280814, 5.2026525]\n",
      "306: Train Loss: [2.4390376, 0.42259994, 4.4554753] | Test Loss: [2.6131358, 0.37220892, 4.8540626]\n",
      "307: Train Loss: [2.4370756, 0.34496382, 4.529187] | Test Loss: [2.7049112, 0.3872397, 5.0225825]\n",
      "308: Train Loss: [2.4322298, 0.3481692, 4.51629] | Test Loss: [2.6951153, 0.36268032, 5.02755]\n",
      "309: Train Loss: [2.3870826, 0.34180793, 4.4323573] | Test Loss: [2.4240124, 0.3688832, 4.4791417]\n",
      "310: Train Loss: [2.3545806, 0.31111962, 4.3980417] | Test Loss: [2.6889713, 0.35120058, 5.026742]\n",
      "311: Train Loss: [2.4363189, 0.336778, 4.5358596] | Test Loss: [2.3970873, 0.32009417, 4.4740806]\n",
      "312: Train Loss: [2.5217857, 0.35169414, 4.6918774] | Test Loss: [2.4985816, 0.38533977, 4.6118236]\n",
      "313: Train Loss: [2.5555198, 0.42958602, 4.6814537] | Test Loss: [2.6408257, 0.40198675, 4.879665]\n",
      "314: Train Loss: [2.5289555, 0.3725548, 4.685356] | Test Loss: [2.6998186, 0.40746266, 4.9921746]\n",
      "315: Train Loss: [2.4864879, 0.3314737, 4.641502] | Test Loss: [2.5555365, 0.36457008, 4.746503]\n",
      "316: Train Loss: [2.5271573, 0.35501125, 4.699303] | Test Loss: [2.7384245, 0.38972253, 5.0871267]\n",
      "317: Train Loss: [2.5437188, 0.31824604, 4.7691917] | Test Loss: [2.627361, 0.37489334, 4.879829]\n",
      "318: Train Loss: [2.590486, 0.3529751, 4.827997] | Test Loss: [2.6922343, 0.33539695, 5.049072]\n",
      "319: Train Loss: [2.4936876, 0.3546001, 4.6327753] | Test Loss: [2.7137594, 0.30509844, 5.1224203]\n",
      "320: Train Loss: [2.7394273, 0.3446271, 5.1342278] | Test Loss: [2.6370678, 0.32923594, 4.9448996]\n",
      "321: Train Loss: [2.5205321, 0.37934408, 4.6617203] | Test Loss: [2.6160495, 0.31754792, 4.9145513]\n",
      "322: Train Loss: [2.4699838, 0.36467674, 4.5752907] | Test Loss: [2.6970904, 0.47650716, 4.9176736]\n",
      "323: Train Loss: [2.4227815, 0.32179657, 4.5237665] | Test Loss: [2.7839496, 0.36352625, 5.204373]\n",
      "324: Train Loss: [2.4262316, 0.420889, 4.4315743] | Test Loss: [2.6691391, 0.34665188, 4.9916263]\n",
      "325: Train Loss: [2.4673026, 0.33633614, 4.598269] | Test Loss: [2.5591993, 0.42847693, 4.689922]\n",
      "326: Train Loss: [2.6944134, 0.320633, 5.068194] | Test Loss: [2.7676353, 0.32555535, 5.2097154]\n",
      "327: Train Loss: [2.5678766, 0.4039163, 4.731837] | Test Loss: [2.593539, 0.3983406, 4.7887373]\n",
      "328: Train Loss: [2.5324006, 0.49122214, 4.573579] | Test Loss: [2.7146723, 0.34485918, 5.0844855]\n",
      "329: Train Loss: [2.454374, 0.36867386, 4.5400743] | Test Loss: [2.5641527, 0.39296642, 4.735339]\n",
      "330: Train Loss: [2.530051, 0.35753962, 4.7025623] | Test Loss: [2.7067974, 0.35184306, 5.061752]\n",
      "331: Train Loss: [2.5407693, 0.40709844, 4.6744404] | Test Loss: [2.7614038, 0.3492352, 5.1735725]\n",
      "332: Train Loss: [2.6839814, 0.48530096, 4.882662] | Test Loss: [2.589702, 0.34978187, 4.829622]\n",
      "333: Train Loss: [2.3594365, 0.36983415, 4.349039] | Test Loss: [2.568988, 0.42032194, 4.717654]\n",
      "334: Train Loss: [2.3021257, 0.32355297, 4.2806983] | Test Loss: [2.5932355, 0.47619382, 4.710277]\n",
      "335: Train Loss: [2.6073935, 0.36869913, 4.846088] | Test Loss: [2.6845615, 0.3846937, 4.9844294]\n",
      "336: Train Loss: [2.4249113, 0.3138664, 4.535956] | Test Loss: [2.6822202, 0.35299885, 5.0114417]\n",
      "337: Train Loss: [2.3550978, 0.33721852, 4.372977] | Test Loss: [2.6429093, 0.34671354, 4.939105]\n",
      "338: Train Loss: [2.5511773, 0.3684803, 4.7338743] | Test Loss: [2.5193112, 0.5251741, 4.513448]\n",
      "339: Train Loss: [2.5225492, 0.4267214, 4.6183767] | Test Loss: [2.6202736, 0.4044644, 4.836083]\n",
      "340: Train Loss: [2.7155664, 0.37625483, 5.0548778] | Test Loss: [2.6668603, 0.33223847, 5.001482]\n",
      "341: Train Loss: [2.7161546, 0.38437706, 5.047932] | Test Loss: [2.7114344, 0.3354683, 5.0874004]\n",
      "342: Train Loss: [2.5444117, 0.39668852, 4.692135] | Test Loss: [2.4713683, 0.37957722, 4.5631595]\n",
      "343: Train Loss: [2.4968169, 0.3853084, 4.6083255] | Test Loss: [2.6954045, 0.38409594, 5.006713]\n",
      "344: Train Loss: [2.445729, 0.39193305, 4.499525] | Test Loss: [2.677838, 0.3697073, 4.985969]\n",
      "345: Train Loss: [2.557454, 0.35124105, 4.763667] | Test Loss: [2.5730793, 0.35478076, 4.791378]\n",
      "346: Train Loss: [2.448303, 0.3494685, 4.5471373] | Test Loss: [2.2115757, 0.23483478, 4.188317]\n",
      "347: Train Loss: [2.4623575, 0.3410045, 4.5837107] | Test Loss: [2.605784, 0.36018264, 4.851385]\n",
      "348: Train Loss: [2.5137222, 0.28525764, 4.7421865] | Test Loss: [2.4997206, 0.43916067, 4.5602803]\n",
      "349: Train Loss: [2.5199633, 0.4697509, 4.5701756] | Test Loss: [2.694498, 0.36250672, 5.0264893]\n",
      "350: Train Loss: [2.4448032, 0.30258536, 4.587021] | Test Loss: [2.81531, 0.36571336, 5.264907]\n",
      "351: Train Loss: [2.4897091, 0.37241042, 4.607008] | Test Loss: [2.5005589, 0.41306862, 4.588049]\n",
      "352: Train Loss: [2.570564, 0.5284233, 4.6127048] | Test Loss: [2.6679373, 0.36724532, 4.9686294]\n",
      "353: Train Loss: [2.3332138, 0.33224696, 4.334181] | Test Loss: [2.409134, 0.3842311, 4.4340367]\n",
      "354: Train Loss: [2.417746, 0.42897826, 4.4065137] | Test Loss: [2.6812158, 0.3669291, 4.9955025]\n",
      "355: Train Loss: [2.4290373, 0.40844393, 4.4496307] | Test Loss: [2.6714797, 0.38616702, 4.9567924]\n",
      "356: Train Loss: [2.6083164, 0.40213832, 4.8144946] | Test Loss: [2.7712438, 0.36930797, 5.1731796]\n",
      "357: Train Loss: [2.426148, 0.38304707, 4.469249] | Test Loss: [2.6086016, 0.36145863, 4.8557444]\n",
      "358: Train Loss: [2.472309, 0.38595998, 4.558658] | Test Loss: [2.5545297, 0.39013717, 4.718922]\n",
      "359: Train Loss: [2.419363, 0.34219283, 4.4965334] | Test Loss: [2.4440248, 0.40489438, 4.4831553]\n",
      "360: Train Loss: [2.6484113, 0.4398524, 4.8569703] | Test Loss: [2.492155, 0.3665245, 4.6177855]\n",
      "361: Train Loss: [2.4894032, 0.38255134, 4.5962553] | Test Loss: [2.886011, 0.3782091, 5.3938127]\n",
      "362: Train Loss: [2.43578, 0.37226403, 4.499296] | Test Loss: [2.5838184, 0.45235395, 4.715283]\n",
      "363: Train Loss: [2.5082636, 0.35463822, 4.661889] | Test Loss: [2.599168, 0.34742242, 4.8509135]\n",
      "364: Train Loss: [2.4868248, 0.33923358, 4.634416] | Test Loss: [2.6927657, 0.37478855, 5.0107427]\n",
      "365: Train Loss: [2.472537, 0.39418635, 4.5508876] | Test Loss: [2.580276, 0.36236393, 4.798188]\n",
      "366: Train Loss: [2.4187052, 0.31195584, 4.5254545] | Test Loss: [2.7423222, 0.3431979, 5.1414466]\n",
      "367: Train Loss: [2.5700004, 0.37085304, 4.769148] | Test Loss: [2.5839791, 0.32018566, 4.8477726]\n",
      "368: Train Loss: [2.5160992, 0.36165372, 4.6705446] | Test Loss: [2.7119608, 0.3705907, 5.053331]\n",
      "369: Train Loss: [2.5881584, 0.37315857, 4.8031583] | Test Loss: [2.6835408, 0.42410094, 4.942981]\n",
      "370: Train Loss: [2.4578776, 0.3546825, 4.561073] | Test Loss: [2.4426553, 0.36766884, 4.517642]\n",
      "371: Train Loss: [2.4815936, 0.26819342, 4.694994] | Test Loss: [2.437935, 0.41352645, 4.4623437]\n",
      "372: Train Loss: [2.498873, 0.3477481, 4.6499977] | Test Loss: [2.6258345, 0.31359285, 4.938076]\n",
      "373: Train Loss: [2.4341085, 0.37097722, 4.4972396] | Test Loss: [2.7437685, 0.3915819, 5.095955]\n",
      "374: Train Loss: [2.3667564, 0.351754, 4.3817587] | Test Loss: [2.6008852, 0.4450226, 4.7567477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375: Train Loss: [2.5513353, 0.32263425, 4.7800364] | Test Loss: [2.5466778, 0.31144306, 4.781913]\n",
      "376: Train Loss: [2.440362, 0.29102024, 4.5897036] | Test Loss: [2.4785986, 0.24609725, 4.7111]\n",
      "377: Train Loss: [2.5069034, 0.36431804, 4.649489] | Test Loss: [2.5126135, 0.3285875, 4.6966395]\n",
      "378: Train Loss: [2.4241464, 0.3309951, 4.5172977] | Test Loss: [2.5895205, 0.36573863, 4.813302]\n",
      "379: Train Loss: [2.6665804, 0.3611757, 4.9719853] | Test Loss: [2.7444649, 0.37689885, 5.112031]\n",
      "380: Train Loss: [2.579973, 0.3386777, 4.821268] | Test Loss: [2.6850882, 0.43928868, 4.9308877]\n",
      "381: Train Loss: [2.4847393, 0.35465872, 4.61482] | Test Loss: [2.549476, 0.33458006, 4.764372]\n",
      "382: Train Loss: [2.5002267, 0.32191566, 4.678538] | Test Loss: [2.6480193, 0.3715832, 4.9244556]\n",
      "383: Train Loss: [2.5734758, 0.3429138, 4.804038] | Test Loss: [2.6688092, 0.37084615, 4.966772]\n",
      "384: Train Loss: [2.4719398, 0.34159815, 4.6022816] | Test Loss: [2.6029868, 0.4094932, 4.79648]\n",
      "385: Train Loss: [2.4629176, 0.44045883, 4.4853764] | Test Loss: [2.6385944, 0.330298, 4.946891]\n",
      "386: Train Loss: [2.528368, 0.37849593, 4.67824] | Test Loss: [2.5600858, 0.3562298, 4.763942]\n",
      "387: Train Loss: [2.535564, 0.38473773, 4.6863904] | Test Loss: [2.4921927, 0.39916408, 4.5852213]\n",
      "388: Train Loss: [2.2656448, 0.3312339, 4.2000556] | Test Loss: [2.6801152, 0.3602035, 5.0000267]\n",
      "389: Train Loss: [2.6185124, 0.3377063, 4.8993187] | Test Loss: [2.5432, 0.35533893, 4.731061]\n",
      "390: Train Loss: [2.5628412, 0.3469265, 4.7787557] | Test Loss: [2.68159, 0.42351305, 4.939667]\n",
      "391: Train Loss: [2.4756782, 0.39818004, 4.5531764] | Test Loss: [2.602605, 0.35555926, 4.849651]\n",
      "392: Train Loss: [2.4280019, 0.35245186, 4.503552] | Test Loss: [2.2575996, 0.35033038, 4.164869]\n",
      "393: Train Loss: [2.4485593, 0.29567552, 4.6014433] | Test Loss: [2.5777578, 0.35513917, 4.8003764]\n",
      "394: Train Loss: [2.4778771, 0.34386343, 4.611891] | Test Loss: [2.6381814, 0.33844697, 4.937916]\n",
      "395: Train Loss: [2.5352352, 0.3211759, 4.7492943] | Test Loss: [2.7177646, 0.32545128, 5.110078]\n",
      "396: Train Loss: [2.4844177, 0.4005197, 4.5683155] | Test Loss: [2.713162, 0.38136876, 5.0449553]\n",
      "397: Train Loss: [2.4669333, 0.48182696, 4.4520397] | Test Loss: [2.5836456, 0.40859833, 4.7586927]\n",
      "398: Train Loss: [2.5508363, 0.35934395, 4.7423286] | Test Loss: [2.5013885, 0.35991988, 4.642857]\n",
      "399: Train Loss: [2.4257958, 0.35717353, 4.494418] | Test Loss: [2.6086764, 0.37146124, 4.8458915]\n",
      "400: Train Loss: [2.5590336, 0.35613135, 4.7619357] | Test Loss: [2.7291527, 0.42658538, 5.03172]\n",
      "401: Train Loss: [2.354383, 0.3675032, 4.341263] | Test Loss: [2.7723095, 0.33124465, 5.2133746]\n",
      "402: Train Loss: [2.5255394, 0.33428064, 4.7167983] | Test Loss: [2.4501894, 0.40691295, 4.493466]\n",
      "403: Train Loss: [2.3804564, 0.32152814, 4.439385] | Test Loss: [2.6440692, 0.37195864, 4.9161797]\n",
      "404: Train Loss: [2.3964972, 0.41128516, 4.3817096] | Test Loss: [2.817591, 0.39936692, 5.235815]\n",
      "405: Train Loss: [2.6840003, 0.41917175, 4.9488287] | Test Loss: [2.510606, 0.41508526, 4.606127]\n",
      "406: Train Loss: [2.3695455, 0.33502734, 4.4040637] | Test Loss: [2.3938758, 0.29257646, 4.4951754]\n",
      "407: Train Loss: [2.6477907, 0.40705222, 4.8885293] | Test Loss: [2.570639, 0.41419685, 4.727081]\n",
      "408: Train Loss: [2.315633, 0.32590696, 4.3053594] | Test Loss: [2.3801053, 0.39434323, 4.365867]\n",
      "409: Train Loss: [2.5455468, 0.3485919, 4.7425017] | Test Loss: [2.4994876, 0.39953575, 4.5994396]\n",
      "410: Train Loss: [2.508515, 0.3798862, 4.6371436] | Test Loss: [2.6279945, 0.33482826, 4.9211607]\n",
      "411: Train Loss: [2.6919334, 0.45711476, 4.926752] | Test Loss: [2.647352, 0.36497402, 4.92973]\n",
      "412: Train Loss: [2.4772565, 0.32935497, 4.6251583] | Test Loss: [2.7243862, 0.34590405, 5.1028686]\n",
      "413: Train Loss: [2.44475, 0.33544722, 4.554053] | Test Loss: [2.6178882, 0.34166005, 4.8941164]\n",
      "414: Train Loss: [2.507847, 0.327315, 4.6883793] | Test Loss: [2.4338129, 0.31958172, 4.548044]\n",
      "415: Train Loss: [2.5056317, 0.39852148, 4.612742] | Test Loss: [2.7293992, 0.4286072, 5.0301914]\n",
      "416: Train Loss: [2.4302726, 0.38048097, 4.4800644] | Test Loss: [2.7169423, 0.36316773, 5.070717]\n",
      "417: Train Loss: [2.5927775, 0.37491217, 4.8106427] | Test Loss: [2.5510502, 0.30603746, 4.796063]\n",
      "418: Train Loss: [2.378528, 0.32291242, 4.434144] | Test Loss: [2.5120926, 0.2757671, 4.748418]\n",
      "419: Train Loss: [2.680916, 0.3558306, 5.0060015] | Test Loss: [2.6339965, 0.35518873, 4.912804]\n",
      "420: Train Loss: [2.4265783, 0.2864603, 4.566696] | Test Loss: [2.5067213, 0.34386107, 4.6695814]\n",
      "421: Train Loss: [2.4899588, 0.340204, 4.639714] | Test Loss: [2.5316284, 0.33805978, 4.725197]\n",
      "422: Train Loss: [2.5682807, 0.38843396, 4.7481275] | Test Loss: [2.4931014, 0.31337798, 4.672825]\n",
      "423: Train Loss: [2.6388001, 0.37228894, 4.905311] | Test Loss: [2.4915218, 0.43888086, 4.5441628]\n",
      "424: Train Loss: [2.6964254, 0.44554, 4.947311] | Test Loss: [2.6425254, 0.45330054, 4.8317504]\n",
      "425: Train Loss: [2.3595443, 0.3824263, 4.3366623] | Test Loss: [2.6023722, 0.3588509, 4.8458934]\n",
      "426: Train Loss: [2.669755, 0.39310932, 4.9464006] | Test Loss: [2.624315, 0.38734287, 4.861287]\n",
      "427: Train Loss: [2.466137, 0.37609354, 4.5561805] | Test Loss: [2.6965728, 0.4005028, 4.992643]\n",
      "428: Train Loss: [2.452258, 0.32415318, 4.5803633] | Test Loss: [2.702167, 0.3763751, 5.027959]\n",
      "429: Train Loss: [2.613174, 0.40755558, 4.8187923] | Test Loss: [2.7267392, 0.3788833, 5.074595]\n",
      "430: Train Loss: [2.4847004, 0.3587629, 4.610638] | Test Loss: [2.6384923, 0.36007273, 4.916912]\n",
      "431: Train Loss: [2.4992056, 0.38160497, 4.616806] | Test Loss: [2.6142442, 0.40895116, 4.819537]\n",
      "432: Train Loss: [2.539572, 0.41034704, 4.668797] | Test Loss: [2.552629, 0.33498996, 4.770268]\n",
      "433: Train Loss: [2.5632648, 0.32768095, 4.7988486] | Test Loss: [2.840121, 0.3973385, 5.2829037]\n",
      "434: Train Loss: [2.3463035, 0.38375288, 4.308854] | Test Loss: [2.6745672, 0.40538603, 4.9437485]\n",
      "435: Train Loss: [2.436904, 0.36170208, 4.512106] | Test Loss: [2.6290293, 0.41918805, 4.8388705]\n",
      "436: Train Loss: [2.6064353, 0.4724554, 4.740415] | Test Loss: [2.5904908, 0.37866008, 4.8023214]\n",
      "437: Train Loss: [2.4087505, 0.36498863, 4.4525123] | Test Loss: [2.584162, 0.34592107, 4.822403]\n",
      "438: Train Loss: [2.5135884, 0.36355448, 4.6636224] | Test Loss: [2.5249565, 0.4468052, 4.603108]\n",
      "439: Train Loss: [2.530231, 0.3574798, 4.7029824] | Test Loss: [2.6047223, 0.347781, 4.8616633]\n",
      "440: Train Loss: [2.4803472, 0.37003905, 4.5906553] | Test Loss: [2.6457927, 0.32808667, 4.9634986]\n",
      "441: Train Loss: [2.5828197, 0.38856733, 4.777072] | Test Loss: [2.7354696, 0.35275647, 5.1181827]\n",
      "442: Train Loss: [2.3684866, 0.3956755, 4.3412976] | Test Loss: [2.762106, 0.3869722, 5.1372395]\n",
      "443: Train Loss: [2.3935215, 0.30884105, 4.478202] | Test Loss: [2.633098, 0.37594888, 4.890247]\n",
      "444: Train Loss: [2.4349148, 0.34228605, 4.5275435] | Test Loss: [2.710867, 0.34564558, 5.0760884]\n",
      "445: Train Loss: [2.5772629, 0.37263402, 4.781892] | Test Loss: [2.7639406, 0.43303746, 5.094844]\n",
      "446: Train Loss: [2.6434991, 0.45445442, 4.832544] | Test Loss: [2.5581517, 0.42728356, 4.6890197]\n",
      "447: Train Loss: [2.3257618, 0.31642413, 4.3350997] | Test Loss: [2.5086913, 0.36975658, 4.647626]\n",
      "448: Train Loss: [2.5678692, 0.35647684, 4.7792616] | Test Loss: [2.6636667, 0.37919322, 4.94814]\n",
      "449: Train Loss: [2.4713871, 0.2941157, 4.6486588] | Test Loss: [2.5638814, 0.347327, 4.780436]\n",
      "450: Train Loss: [2.5009575, 0.38977903, 4.612136] | Test Loss: [2.8429365, 0.33802426, 5.347849]\n",
      "451: Train Loss: [2.5587149, 0.32142714, 4.7960024] | Test Loss: [2.5464406, 0.35380238, 4.739079]\n",
      "452: Train Loss: [2.383869, 0.3422912, 4.4254465] | Test Loss: [2.4458494, 0.31695035, 4.5747485]\n",
      "453: Train Loss: [2.5793073, 0.34226826, 4.816346] | Test Loss: [2.7198079, 0.33946532, 5.1001506]\n",
      "454: Train Loss: [2.415953, 0.30852363, 4.523382] | Test Loss: [2.568025, 0.49765214, 4.638398]\n",
      "455: Train Loss: [2.4853473, 0.36874032, 4.6019545] | Test Loss: [2.601831, 0.3447384, 4.8589234]\n",
      "456: Train Loss: [2.3890676, 0.38659403, 4.3915415] | Test Loss: [2.6491897, 0.31571668, 4.9826627]\n",
      "457: Train Loss: [2.4358733, 0.30324656, 4.5685] | Test Loss: [2.5433152, 0.32528734, 4.761343]\n",
      "458: Train Loss: [2.4361668, 0.37891632, 4.4934173] | Test Loss: [2.6730683, 0.3113669, 5.0347695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459: Train Loss: [2.3999648, 0.33182505, 4.4681044] | Test Loss: [2.5047805, 0.44064924, 4.568912]\n",
      "460: Train Loss: [2.57791, 0.37733042, 4.7784896] | Test Loss: [2.6471877, 0.31512475, 4.979251]\n",
      "461: Train Loss: [2.5969641, 0.39276513, 4.801163] | Test Loss: [2.5199628, 0.32848787, 4.7114377]\n",
      "462: Train Loss: [2.3744502, 0.32436758, 4.424533] | Test Loss: [2.5123348, 0.39235365, 4.632316]\n",
      "463: Train Loss: [2.4638984, 0.3717702, 4.5560265] | Test Loss: [2.6367002, 0.432537, 4.840863]\n",
      "464: Train Loss: [2.3317688, 0.29129612, 4.3722415] | Test Loss: [2.559626, 0.35801962, 4.7612324]\n",
      "465: Train Loss: [2.292602, 0.28078085, 4.3044233] | Test Loss: [2.6985583, 0.4445808, 4.9525356]\n",
      "466: Train Loss: [2.5010154, 0.29899457, 4.7030363] | Test Loss: [2.3861687, 0.46372935, 4.308608]\n",
      "467: Train Loss: [2.549657, 0.34900373, 4.7503104] | Test Loss: [2.69726, 0.40216303, 4.992357]\n",
      "468: Train Loss: [2.4162138, 0.3314842, 4.500943] | Test Loss: [2.539679, 0.3629752, 4.716383]\n",
      "469: Train Loss: [2.5349839, 0.3237136, 4.746254] | Test Loss: [2.6062708, 0.42578715, 4.7867546]\n",
      "470: Train Loss: [2.4744453, 0.31358594, 4.635305] | Test Loss: [2.5258517, 0.32686934, 4.724834]\n",
      "471: Train Loss: [2.417041, 0.35681957, 4.4772625] | Test Loss: [2.523261, 0.32377252, 4.7227497]\n",
      "472: Train Loss: [2.4266183, 0.31768396, 4.5355525] | Test Loss: [2.541778, 0.41298997, 4.670566]\n",
      "473: Train Loss: [2.530508, 0.35704753, 4.7039685] | Test Loss: [2.641654, 0.34235674, 4.9409513]\n",
      "474: Train Loss: [2.4512122, 0.36671457, 4.53571] | Test Loss: [2.662709, 0.32932383, 4.996094]\n",
      "475: Train Loss: [2.4780846, 0.34396058, 4.6122084] | Test Loss: [2.5909047, 0.4060573, 4.775752]\n",
      "476: Train Loss: [2.4570148, 0.3099729, 4.604057] | Test Loss: [2.5398436, 0.3495988, 4.730088]\n",
      "477: Train Loss: [2.4011285, 0.37123013, 4.431027] | Test Loss: [2.7032316, 0.3151044, 5.0913587]\n",
      "478: Train Loss: [2.3669877, 0.3513221, 4.382653] | Test Loss: [2.5601568, 0.3765199, 4.743794]\n",
      "479: Train Loss: [2.5072234, 0.39889544, 4.6155515] | Test Loss: [2.5821874, 0.39228928, 4.7720857]\n",
      "480: Train Loss: [2.3745232, 0.40590698, 4.343139] | Test Loss: [2.5357845, 0.36868897, 4.70288]\n",
      "481: Train Loss: [2.5760903, 0.40252608, 4.749655] | Test Loss: [2.5921686, 0.3416571, 4.84268]\n",
      "482: Train Loss: [2.3415458, 0.2816187, 4.401473] | Test Loss: [2.649527, 0.4966503, 4.802404]\n",
      "483: Train Loss: [2.5365129, 0.32708132, 4.7459445] | Test Loss: [2.6855643, 0.36758807, 5.0035405]\n",
      "484: Train Loss: [2.346253, 0.33465523, 4.3578506] | Test Loss: [2.7093801, 0.36501533, 5.053745]\n",
      "485: Train Loss: [2.4669948, 0.3524383, 4.581551] | Test Loss: [2.7164748, 0.36739665, 5.0655527]\n",
      "486: Train Loss: [2.4316216, 0.38587317, 4.47737] | Test Loss: [2.5527277, 0.36818767, 4.737268]\n",
      "487: Train Loss: [2.4901347, 0.34681362, 4.6334558] | Test Loss: [2.7807677, 0.36279902, 5.198736]\n",
      "488: Train Loss: [2.5650659, 0.36427328, 4.7658587] | Test Loss: [2.5136907, 0.354336, 4.6730456]\n",
      "489: Train Loss: [2.3728938, 0.3442599, 4.401528] | Test Loss: [2.6737769, 0.42831507, 4.9192386]\n",
      "490: Train Loss: [2.4728534, 0.31758124, 4.6281257] | Test Loss: [2.6236272, 0.36121345, 4.8860407]\n",
      "491: Train Loss: [2.4876215, 0.4582631, 4.51698] | Test Loss: [2.635968, 0.34736407, 4.924572]\n",
      "492: Train Loss: [2.5062132, 0.31097323, 4.701453] | Test Loss: [2.5096416, 0.38406682, 4.6352167]\n",
      "493: Train Loss: [2.4870973, 0.34797773, 4.626217] | Test Loss: [2.4943402, 0.336271, 4.6524096]\n",
      "494: Train Loss: [2.5509756, 0.35741964, 4.7445316] | Test Loss: [2.6149344, 0.40244278, 4.827426]\n",
      "495: Train Loss: [2.434252, 0.2776624, 4.590842] | Test Loss: [2.4873452, 0.3803246, 4.594366]\n",
      "496: Train Loss: [2.4913256, 0.38042995, 4.6022215] | Test Loss: [2.5042791, 0.3082478, 4.70031]\n",
      "497: Train Loss: [2.4746697, 0.40709716, 4.542242] | Test Loss: [2.6678529, 0.32416224, 5.0115433]\n",
      "498: Train Loss: [2.3667717, 0.3049286, 4.4286146] | Test Loss: [2.4486394, 0.3217549, 4.575524]\n",
      "499: Train Loss: [2.3740776, 0.31891558, 4.4292397] | Test Loss: [2.7079947, 0.40873653, 5.0072527]\n",
      "500: Train Loss: [2.5751774, 0.42925957, 4.721095] | Test Loss: [2.5543604, 0.34637147, 4.762349]\n",
      "501: Train Loss: [2.5233727, 0.31992826, 4.726817] | Test Loss: [2.7038205, 0.40536496, 5.002276]\n",
      "502: Train Loss: [2.488462, 0.5706467, 4.406277] | Test Loss: [2.5601153, 0.37161845, 4.7486124]\n",
      "503: Train Loss: [2.4838939, 0.3353417, 4.632446] | Test Loss: [2.7645001, 0.3737147, 5.1552854]\n",
      "504: Train Loss: [2.649511, 0.37483662, 4.9241858] | Test Loss: [2.5402935, 0.39264193, 4.687945]\n",
      "505: Train Loss: [2.649401, 0.36592066, 4.9328814] | Test Loss: [2.5738099, 0.34405237, 4.8035674]\n",
      "506: Train Loss: [2.5838528, 0.38102233, 4.786683] | Test Loss: [2.6768687, 0.3591444, 4.994593]\n",
      "507: Train Loss: [2.4950657, 0.30627224, 4.6838593] | Test Loss: [2.6109817, 0.4117168, 4.8102465]\n",
      "508: Train Loss: [2.3945913, 0.31211635, 4.4770665] | Test Loss: [2.5179403, 0.5323019, 4.5035787]\n",
      "509: Train Loss: [2.3702722, 0.31184575, 4.4286985] | Test Loss: [2.491985, 0.42025647, 4.5637136]\n",
      "510: Train Loss: [2.3356702, 0.44532922, 4.2260113] | Test Loss: [2.5631855, 0.32552287, 4.800848]\n",
      "511: Train Loss: [2.304083, 0.3384325, 4.269734] | Test Loss: [2.578905, 0.44879603, 4.709014]\n",
      "512: Train Loss: [2.557114, 0.34921724, 4.7650104] | Test Loss: [2.5176778, 0.40731135, 4.628044]\n",
      "513: Train Loss: [2.4853618, 0.38206384, 4.58866] | Test Loss: [2.5917997, 0.3673784, 4.816221]\n",
      "514: Train Loss: [2.5233119, 0.4020582, 4.6445656] | Test Loss: [2.764166, 0.4148412, 5.113491]\n",
      "515: Train Loss: [2.527041, 0.3681805, 4.685901] | Test Loss: [2.553482, 0.3750229, 4.731941]\n",
      "516: Train Loss: [2.5997665, 0.33779758, 4.8617353] | Test Loss: [2.5382664, 0.35077953, 4.7257533]\n",
      "517: Train Loss: [2.6822903, 0.3431677, 5.021413] | Test Loss: [2.577868, 0.33771306, 4.8180227]\n",
      "518: Train Loss: [2.4353225, 0.32532683, 4.545318] | Test Loss: [2.683208, 0.37914148, 4.9872746]\n",
      "519: Train Loss: [2.4237754, 0.38674098, 4.4608097] | Test Loss: [2.5060945, 0.3859762, 4.6262126]\n",
      "520: Train Loss: [2.5207648, 0.39676595, 4.6447635] | Test Loss: [2.618582, 0.4060611, 4.831103]\n",
      "521: Train Loss: [2.3594978, 0.32492182, 4.394074] | Test Loss: [2.7039673, 0.34623107, 5.0617037]\n",
      "522: Train Loss: [2.3204784, 0.45361862, 4.1873384] | Test Loss: [2.5748878, 0.3309154, 4.81886]\n",
      "523: Train Loss: [2.6616352, 0.38857606, 4.9346943] | Test Loss: [2.6141915, 0.3526909, 4.8756924]\n",
      "Epoch 9\n",
      "0: Train Loss: [2.2760017, 0.3583722, 4.193631] | Test Loss: [2.6034987, 0.3594437, 4.8475537]\n",
      "1: Train Loss: [2.3675573, 0.47669327, 4.2584214] | Test Loss: [2.5842152, 0.35901293, 4.8094172]\n",
      "2: Train Loss: [2.218384, 0.37088197, 4.065886] | Test Loss: [2.3349037, 0.36783776, 4.3019695]\n",
      "3: Train Loss: [2.3252323, 0.36629948, 4.284165] | Test Loss: [2.5436935, 0.37583712, 4.7115498]\n",
      "4: Train Loss: [2.486337, 0.36851507, 4.604159] | Test Loss: [2.514715, 0.3457356, 4.6836944]\n",
      "5: Train Loss: [2.2343557, 0.39707044, 4.071641] | Test Loss: [2.5531728, 0.35791013, 4.7484355]\n",
      "6: Train Loss: [2.235464, 0.3622096, 4.1087184] | Test Loss: [2.5973673, 0.35581243, 4.838922]\n",
      "7: Train Loss: [2.288567, 0.31238076, 4.2647533] | Test Loss: [2.697937, 0.33930975, 5.0565643]\n",
      "8: Train Loss: [2.2173278, 0.4052346, 4.029421] | Test Loss: [2.5252173, 0.35485303, 4.6955814]\n",
      "9: Train Loss: [2.4374688, 0.33499762, 4.53994] | Test Loss: [2.5902696, 0.4317247, 4.7488146]\n",
      "10: Train Loss: [2.1932516, 0.3031012, 4.083402] | Test Loss: [2.6703386, 0.41011235, 4.930565]\n",
      "11: Train Loss: [2.3003552, 0.36103454, 4.239676] | Test Loss: [2.6239662, 0.3958277, 4.8521047]\n",
      "12: Train Loss: [2.2275002, 0.32916176, 4.1258388] | Test Loss: [2.6582358, 0.39475593, 4.9217157]\n",
      "13: Train Loss: [2.3375463, 0.29112405, 4.383969] | Test Loss: [2.41373, 0.3550316, 4.4724283]\n",
      "14: Train Loss: [2.2991724, 0.36958742, 4.2287574] | Test Loss: [2.5943038, 0.3612839, 4.827324]\n",
      "15: Train Loss: [2.1064415, 0.36833236, 3.8445508] | Test Loss: [2.6308568, 0.34008262, 4.921631]\n",
      "16: Train Loss: [2.3231099, 0.3888257, 4.257394] | Test Loss: [2.4984226, 0.3677812, 4.629064]\n",
      "17: Train Loss: [2.3132608, 0.3374854, 4.2890363] | Test Loss: [2.424225, 0.32643977, 4.5220103]\n",
      "18: Train Loss: [2.2426832, 0.37245566, 4.1129107] | Test Loss: [2.774002, 0.39716735, 5.150837]\n",
      "19: Train Loss: [2.2941482, 0.35018232, 4.238114] | Test Loss: [2.6089568, 0.31967175, 4.898242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: Train Loss: [2.3130462, 0.34144166, 4.284651] | Test Loss: [2.6522963, 0.37188524, 4.9327073]\n",
      "21: Train Loss: [2.3963952, 0.36868712, 4.4241033] | Test Loss: [2.6903923, 0.3232671, 5.0575175]\n",
      "22: Train Loss: [2.314448, 0.3777592, 4.2511373] | Test Loss: [2.669922, 0.46564037, 4.8742037]\n",
      "23: Train Loss: [2.347333, 0.33020908, 4.3644567] | Test Loss: [2.7358453, 0.37313992, 5.098551]\n",
      "24: Train Loss: [2.354555, 0.36562023, 4.3434896] | Test Loss: [2.650414, 0.3778961, 4.9229317]\n",
      "25: Train Loss: [2.444715, 0.39980412, 4.489626] | Test Loss: [2.5971062, 0.36555395, 4.8286586]\n",
      "26: Train Loss: [2.4558382, 0.4804001, 4.4312763] | Test Loss: [2.6730587, 0.4314541, 4.9146633]\n",
      "27: Train Loss: [2.3056757, 0.34395903, 4.2673926] | Test Loss: [2.4410992, 0.35286632, 4.529332]\n",
      "28: Train Loss: [2.3895605, 0.3382727, 4.4408484] | Test Loss: [2.7010353, 0.3990345, 5.003036]\n",
      "29: Train Loss: [2.3225532, 0.36426523, 4.280841] | Test Loss: [2.4588065, 0.33970332, 4.57791]\n",
      "30: Train Loss: [2.1975038, 0.33840308, 4.0566044] | Test Loss: [2.73844, 0.45425075, 5.0226293]\n",
      "31: Train Loss: [2.2284174, 0.3237316, 4.1331034] | Test Loss: [2.6454654, 0.3672674, 4.923663]\n",
      "32: Train Loss: [2.2534595, 0.43159148, 4.0753274] | Test Loss: [2.6345096, 0.31979495, 4.949224]\n",
      "33: Train Loss: [2.28523, 0.33382463, 4.236635] | Test Loss: [2.600339, 0.36149642, 4.8391814]\n",
      "34: Train Loss: [2.3767736, 0.38272366, 4.3708234] | Test Loss: [2.6252246, 0.50945467, 4.7409945]\n",
      "35: Train Loss: [2.4277182, 0.33044896, 4.524987] | Test Loss: [2.6485605, 0.36027074, 4.9368505]\n",
      "36: Train Loss: [2.2133377, 0.32721293, 4.0994625] | Test Loss: [2.5399761, 0.37375763, 4.7061944]\n",
      "37: Train Loss: [2.3145657, 0.3324994, 4.296632] | Test Loss: [2.5621812, 0.35866275, 4.7657]\n",
      "38: Train Loss: [2.1971846, 0.44229382, 3.9520755] | Test Loss: [2.5791318, 0.40436262, 4.753901]\n",
      "39: Train Loss: [2.435363, 0.3603594, 4.510367] | Test Loss: [2.7469733, 0.36463064, 5.129316]\n",
      "40: Train Loss: [2.2209954, 0.33332717, 4.1086636] | Test Loss: [2.5851934, 0.420914, 4.7494726]\n",
      "41: Train Loss: [2.1825833, 0.3667, 3.9984667] | Test Loss: [2.6297138, 0.37970915, 4.8797183]\n",
      "42: Train Loss: [2.3176506, 0.37368727, 4.261614] | Test Loss: [2.6442645, 0.36563668, 4.922892]\n",
      "43: Train Loss: [2.368325, 0.30038875, 4.436261] | Test Loss: [2.6557546, 0.32792905, 4.98358]\n",
      "44: Train Loss: [2.2685935, 0.3222257, 4.2149615] | Test Loss: [2.6041245, 0.35187826, 4.856371]\n",
      "45: Train Loss: [2.2041783, 0.36325985, 4.045097] | Test Loss: [2.5793786, 0.38003227, 4.778725]\n",
      "46: Train Loss: [2.407921, 0.42499912, 4.390843] | Test Loss: [2.7434797, 0.32461783, 5.1623416]\n",
      "47: Train Loss: [2.28531, 0.35404676, 4.216573] | Test Loss: [2.549443, 0.44180977, 4.6570764]\n",
      "48: Train Loss: [2.3248055, 0.30350536, 4.3461056] | Test Loss: [2.6561384, 0.3518731, 4.960404]\n",
      "49: Train Loss: [2.1821656, 0.35964206, 4.004689] | Test Loss: [2.6661553, 0.34462816, 4.9876823]\n",
      "50: Train Loss: [2.3621497, 0.37103158, 4.3532677] | Test Loss: [2.5465617, 0.34967792, 4.7434454]\n",
      "51: Train Loss: [2.3355055, 0.3304041, 4.3406067] | Test Loss: [2.566705, 0.34097272, 4.792437]\n",
      "52: Train Loss: [2.1573088, 0.42151752, 3.8931003] | Test Loss: [2.714285, 0.34035718, 5.0882125]\n",
      "53: Train Loss: [2.4274912, 0.31874594, 4.5362363] | Test Loss: [2.5904968, 0.4157254, 4.7652683]\n",
      "54: Train Loss: [2.1800194, 0.33549058, 4.024548] | Test Loss: [2.6815522, 0.3668998, 4.9962044]\n",
      "55: Train Loss: [2.2715786, 0.32095766, 4.2221994] | Test Loss: [2.7394578, 0.3705842, 5.1083317]\n",
      "56: Train Loss: [2.462035, 0.36381188, 4.560258] | Test Loss: [2.6487772, 0.427319, 4.8702354]\n",
      "57: Train Loss: [2.3926277, 0.2913084, 4.493947] | Test Loss: [2.6014867, 0.41409266, 4.788881]\n",
      "58: Train Loss: [2.4876354, 0.5566888, 4.418582] | Test Loss: [2.5818794, 0.36670932, 4.7970495]\n",
      "59: Train Loss: [2.4288, 0.3598678, 4.497732] | Test Loss: [2.5245185, 0.38735375, 4.661683]\n",
      "60: Train Loss: [2.3934019, 0.41377103, 4.3730326] | Test Loss: [2.7021308, 0.38795686, 5.016305]\n",
      "61: Train Loss: [2.4752686, 0.3761257, 4.5744114] | Test Loss: [2.391828, 0.3523039, 4.431352]\n",
      "62: Train Loss: [2.5697696, 0.3851339, 4.7544055] | Test Loss: [2.765637, 0.31226003, 5.2190137]\n",
      "63: Train Loss: [2.5140462, 0.39424238, 4.63385] | Test Loss: [2.7120514, 0.33230257, 5.0918]\n",
      "64: Train Loss: [2.3729289, 0.34883043, 4.3970275] | Test Loss: [2.5743344, 0.45895466, 4.689714]\n",
      "65: Train Loss: [2.4269302, 0.35161105, 4.5022492] | Test Loss: [2.8492777, 0.36432266, 5.334233]\n",
      "66: Train Loss: [2.4166148, 0.37542397, 4.4578056] | Test Loss: [2.5651684, 0.3630099, 4.767327]\n",
      "67: Train Loss: [2.3734558, 0.33876434, 4.4081473] | Test Loss: [2.6872756, 0.3801149, 4.9944363]\n",
      "68: Train Loss: [2.3214662, 0.40233085, 4.2406015] | Test Loss: [2.5709715, 0.28200617, 4.8599367]\n",
      "69: Train Loss: [2.4142964, 0.34809485, 4.480498] | Test Loss: [2.5816998, 0.48410445, 4.679295]\n",
      "70: Train Loss: [2.3440893, 0.3530417, 4.335137] | Test Loss: [2.5710933, 0.37365538, 4.7685313]\n",
      "71: Train Loss: [2.488133, 0.390016, 4.58625] | Test Loss: [2.7337213, 0.37594303, 5.0914993]\n",
      "72: Train Loss: [2.343491, 0.30964422, 4.377338] | Test Loss: [2.5637589, 0.33966643, 4.7878513]\n",
      "73: Train Loss: [2.45745, 0.4261024, 4.4887977] | Test Loss: [2.5174575, 0.41502905, 4.619886]\n",
      "74: Train Loss: [2.4540799, 0.42547524, 4.4826846] | Test Loss: [2.5461493, 0.3453901, 4.746908]\n",
      "75: Train Loss: [2.5098944, 0.4327891, 4.5869994] | Test Loss: [2.6015294, 0.39177233, 4.8112864]\n",
      "76: Train Loss: [2.5364938, 0.383219, 4.6897683] | Test Loss: [2.6353586, 0.39808413, 4.872633]\n",
      "77: Train Loss: [2.32378, 0.34429604, 4.303264] | Test Loss: [2.5447934, 0.4176347, 4.6719522]\n",
      "78: Train Loss: [2.3428488, 0.3344307, 4.351267] | Test Loss: [2.8458884, 0.3144492, 5.3773274]\n",
      "79: Train Loss: [2.2513797, 0.36361378, 4.139146] | Test Loss: [2.7505224, 0.53028953, 4.970755]\n",
      "80: Train Loss: [2.4555705, 0.42394245, 4.4871984] | Test Loss: [2.6278324, 0.33060673, 4.925058]\n",
      "81: Train Loss: [2.4087644, 0.30647743, 4.511051] | Test Loss: [2.4992318, 0.38551036, 4.612953]\n",
      "82: Train Loss: [2.578296, 0.3654229, 4.791169] | Test Loss: [2.7251172, 0.35834754, 5.091887]\n",
      "83: Train Loss: [2.2988644, 0.3347199, 4.263009] | Test Loss: [2.6547027, 0.4048627, 4.9045424]\n",
      "84: Train Loss: [2.294104, 0.344168, 4.24404] | Test Loss: [2.6827433, 0.37529203, 4.990195]\n",
      "85: Train Loss: [2.3442826, 0.4017568, 4.2868085] | Test Loss: [2.422608, 0.35610953, 4.489106]\n",
      "86: Train Loss: [2.526859, 0.40359178, 4.6501265] | Test Loss: [2.6505177, 0.32232222, 4.978713]\n",
      "87: Train Loss: [2.2927647, 0.32262874, 4.262901] | Test Loss: [2.7731228, 0.37648037, 5.169765]\n",
      "88: Train Loss: [2.4704056, 0.35605508, 4.584756] | Test Loss: [2.5000567, 0.3301087, 4.670005]\n",
      "89: Train Loss: [2.4098465, 0.3084064, 4.5112867] | Test Loss: [2.548413, 0.38169754, 4.7151284]\n",
      "90: Train Loss: [2.4264388, 0.3276112, 4.5252666] | Test Loss: [2.7331462, 0.44364566, 5.022647]\n",
      "91: Train Loss: [2.556989, 0.38979635, 4.7241817] | Test Loss: [2.5857997, 0.37790942, 4.7936897]\n",
      "92: Train Loss: [2.4665742, 0.31078315, 4.622365] | Test Loss: [2.6698296, 0.23978797, 5.099871]\n",
      "93: Train Loss: [2.377796, 0.40473378, 4.350858] | Test Loss: [2.5811498, 0.33432052, 4.827979]\n",
      "94: Train Loss: [2.5250432, 0.3636618, 4.6864247] | Test Loss: [2.5698385, 0.4015419, 4.7381353]\n",
      "95: Train Loss: [2.4553127, 0.38156396, 4.5290613] | Test Loss: [2.6422434, 0.3927251, 4.891762]\n",
      "96: Train Loss: [2.2755094, 0.31728995, 4.233729] | Test Loss: [2.470673, 0.34511796, 4.596228]\n",
      "97: Train Loss: [2.4119494, 0.3490575, 4.474841] | Test Loss: [2.803176, 0.45681474, 5.149537]\n",
      "98: Train Loss: [2.4453769, 0.34351242, 4.547241] | Test Loss: [2.463896, 0.38314214, 4.54465]\n",
      "99: Train Loss: [2.415625, 0.40550923, 4.4257407] | Test Loss: [2.5841966, 0.36927152, 4.7991214]\n",
      "100: Train Loss: [2.4142885, 0.44745314, 4.381124] | Test Loss: [2.7213323, 0.3537588, 5.088906]\n",
      "101: Train Loss: [2.4680514, 0.41713053, 4.5189724] | Test Loss: [2.601849, 0.39257675, 4.8111215]\n",
      "102: Train Loss: [2.3097446, 0.36973128, 4.249758] | Test Loss: [2.7734025, 0.40214947, 5.144655]\n",
      "103: Train Loss: [2.583728, 0.3791818, 4.7882743] | Test Loss: [2.6168091, 0.34875926, 4.884859]\n",
      "104: Train Loss: [2.3745713, 0.3179638, 4.431179] | Test Loss: [2.7426176, 0.3461224, 5.139113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105: Train Loss: [2.3264406, 0.31229174, 4.3405895] | Test Loss: [2.5087986, 0.36821294, 4.649384]\n",
      "106: Train Loss: [2.4135656, 0.37267485, 4.4544563] | Test Loss: [2.4580498, 0.37451744, 4.541582]\n",
      "107: Train Loss: [2.3317037, 0.40706673, 4.2563405] | Test Loss: [2.5177693, 0.3556806, 4.679858]\n",
      "108: Train Loss: [2.3550563, 0.40988573, 4.3002267] | Test Loss: [2.671534, 0.28755337, 5.055515]\n",
      "109: Train Loss: [2.426947, 0.4012246, 4.4526696] | Test Loss: [2.704101, 0.39444473, 5.013757]\n",
      "110: Train Loss: [2.282316, 0.35447627, 4.2101555] | Test Loss: [2.5471654, 0.3428022, 4.7515287]\n",
      "111: Train Loss: [2.3695111, 0.33754644, 4.401476] | Test Loss: [2.6483243, 0.3645779, 4.9320707]\n",
      "112: Train Loss: [2.40049, 0.38135788, 4.4196224] | Test Loss: [2.6964083, 0.4908749, 4.901942]\n",
      "113: Train Loss: [2.4075403, 0.3389199, 4.4761605] | Test Loss: [2.5865464, 0.36399412, 4.8090987]\n",
      "114: Train Loss: [2.3637419, 0.38233578, 4.345148] | Test Loss: [2.7559164, 0.36081606, 5.1510167]\n",
      "115: Train Loss: [2.430975, 0.42741615, 4.4345336] | Test Loss: [2.7709148, 0.41503698, 5.1267924]\n",
      "116: Train Loss: [2.4092305, 0.4414335, 4.3770275] | Test Loss: [2.5214107, 0.39207008, 4.650751]\n",
      "117: Train Loss: [2.503586, 0.33742478, 4.6697474] | Test Loss: [2.7237499, 0.365186, 5.0823135]\n",
      "118: Train Loss: [2.4941509, 0.38291994, 4.605382] | Test Loss: [2.7282934, 0.3604316, 5.096155]\n",
      "119: Train Loss: [2.488517, 0.33395517, 4.643079] | Test Loss: [2.599079, 0.35767832, 4.8404794]\n",
      "120: Train Loss: [2.2698183, 0.3253602, 4.2142763] | Test Loss: [2.5470917, 0.31931362, 4.77487]\n",
      "121: Train Loss: [2.4998748, 0.34661853, 4.653131] | Test Loss: [2.403709, 0.3462298, 4.461188]\n",
      "122: Train Loss: [2.3478422, 0.34301353, 4.3526707] | Test Loss: [2.7011137, 0.3225464, 5.079681]\n",
      "123: Train Loss: [2.391306, 0.38733158, 4.3952804] | Test Loss: [2.7150497, 0.36849284, 5.061607]\n",
      "124: Train Loss: [2.4367366, 0.32001227, 4.553461] | Test Loss: [2.3711603, 0.3385495, 4.403771]\n",
      "125: Train Loss: [2.4971445, 0.36416727, 4.6301217] | Test Loss: [2.534678, 0.38864043, 4.6807156]\n",
      "126: Train Loss: [2.3881216, 0.33257058, 4.4436727] | Test Loss: [2.7068343, 0.40612817, 5.00754]\n",
      "127: Train Loss: [2.4534051, 0.307388, 4.5994225] | Test Loss: [2.6333444, 0.4036312, 4.8630576]\n",
      "128: Train Loss: [2.4214349, 0.3046999, 4.53817] | Test Loss: [2.708503, 0.30247495, 5.114531]\n",
      "129: Train Loss: [2.45028, 0.3726478, 4.527912] | Test Loss: [2.6979444, 0.40731493, 4.988574]\n",
      "130: Train Loss: [2.5836833, 0.2647346, 4.9026318] | Test Loss: [2.5705009, 0.3295545, 4.811447]\n",
      "131: Train Loss: [2.3699608, 0.33475748, 4.4051642] | Test Loss: [2.6448534, 0.3450235, 4.944683]\n",
      "132: Train Loss: [2.5635219, 0.31852737, 4.8085165] | Test Loss: [2.6470346, 0.36592996, 4.928139]\n",
      "133: Train Loss: [2.7110431, 0.33964404, 5.0824423] | Test Loss: [2.555709, 0.3709939, 4.7404237]\n",
      "134: Train Loss: [2.4360533, 0.35050005, 4.5216064] | Test Loss: [2.5879626, 0.43906662, 4.736859]\n",
      "135: Train Loss: [2.4287395, 0.3355543, 4.521925] | Test Loss: [2.5090158, 0.42134666, 4.596685]\n",
      "136: Train Loss: [2.6822739, 0.35879618, 5.0057516] | Test Loss: [2.5647662, 0.32842323, 4.8011093]\n",
      "137: Train Loss: [2.4798732, 0.4614076, 4.4983387] | Test Loss: [2.978167, 0.351845, 5.6044893]\n",
      "138: Train Loss: [2.440742, 0.32306123, 4.5584226] | Test Loss: [2.5489223, 0.3879366, 4.709908]\n",
      "139: Train Loss: [2.528387, 0.3297452, 4.727029] | Test Loss: [2.6149073, 0.32898426, 4.9008303]\n",
      "140: Train Loss: [2.3958151, 0.4184535, 4.3731766] | Test Loss: [2.4417665, 0.3506535, 4.5328794]\n",
      "141: Train Loss: [2.4658673, 0.37951085, 4.5522237] | Test Loss: [2.641968, 0.43202832, 4.8519077]\n",
      "142: Train Loss: [2.4035525, 0.3602092, 4.446896] | Test Loss: [2.519774, 0.4213669, 4.618181]\n",
      "143: Train Loss: [2.3781104, 0.35716906, 4.3990517] | Test Loss: [2.5196037, 0.38919115, 4.6500163]\n",
      "144: Train Loss: [2.4521532, 0.35128382, 4.5530224] | Test Loss: [2.7757723, 0.31116968, 5.240375]\n",
      "145: Train Loss: [2.4615672, 0.4056639, 4.5174704] | Test Loss: [2.6581864, 0.36661184, 4.949761]\n",
      "146: Train Loss: [2.5768323, 0.33543086, 4.8182335] | Test Loss: [2.769284, 0.440877, 5.097691]\n",
      "147: Train Loss: [2.4456277, 0.35477898, 4.5364766] | Test Loss: [2.600798, 0.35578018, 4.8458157]\n",
      "148: Train Loss: [2.4734244, 0.33523935, 4.6116095] | Test Loss: [2.685604, 0.35732648, 5.0138817]\n",
      "149: Train Loss: [2.5057607, 0.3079539, 4.7035675] | Test Loss: [2.6936598, 0.34473404, 5.0425854]\n",
      "150: Train Loss: [2.4389796, 0.37841958, 4.49954] | Test Loss: [2.5372245, 0.33932716, 4.7351217]\n",
      "151: Train Loss: [2.3992326, 0.3729801, 4.425485] | Test Loss: [2.6492944, 0.38448215, 4.914107]\n",
      "152: Train Loss: [2.4632277, 0.30303517, 4.6234202] | Test Loss: [2.6312754, 0.2880544, 4.9744964]\n",
      "153: Train Loss: [2.318636, 0.33309758, 4.3041744] | Test Loss: [2.591939, 0.32446545, 4.8594127]\n",
      "154: Train Loss: [2.437613, 0.33398944, 4.5412364] | Test Loss: [2.6645672, 0.34984457, 4.97929]\n",
      "155: Train Loss: [2.3475509, 0.45892045, 4.2361813] | Test Loss: [2.58137, 0.33129758, 4.831443]\n",
      "156: Train Loss: [2.5197935, 0.31355733, 4.72603] | Test Loss: [2.5385928, 0.46502203, 4.6121635]\n",
      "157: Train Loss: [2.4144142, 0.32442984, 4.5043983] | Test Loss: [2.4009247, 0.35146263, 4.4503865]\n",
      "158: Train Loss: [2.4150307, 0.4336666, 4.3963947] | Test Loss: [2.4787598, 0.35359296, 4.6039267]\n",
      "159: Train Loss: [2.3426292, 0.42976636, 4.255492] | Test Loss: [2.6896448, 0.36776364, 5.011526]\n",
      "160: Train Loss: [2.3774881, 0.32412785, 4.4308486] | Test Loss: [2.4782066, 0.36651608, 4.589897]\n",
      "161: Train Loss: [2.4182365, 0.36982232, 4.4666505] | Test Loss: [2.7200358, 0.33668464, 5.103387]\n",
      "162: Train Loss: [2.5119793, 0.34283456, 4.681124] | Test Loss: [2.7235715, 0.29444045, 5.152703]\n",
      "163: Train Loss: [2.300309, 0.33463016, 4.265988] | Test Loss: [2.6103637, 0.37430367, 4.8464236]\n",
      "164: Train Loss: [2.353819, 0.3114679, 4.39617] | Test Loss: [2.6118133, 0.37392753, 4.849699]\n",
      "165: Train Loss: [2.4133694, 0.3899452, 4.436794] | Test Loss: [2.5870743, 0.29875624, 4.8753924]\n",
      "166: Train Loss: [2.3950028, 0.38967192, 4.400334] | Test Loss: [2.8693862, 0.45525643, 5.283516]\n",
      "167: Train Loss: [2.347356, 0.35448125, 4.340231] | Test Loss: [2.486399, 0.38599268, 4.5868053]\n",
      "168: Train Loss: [2.3099608, 0.3497355, 4.270186] | Test Loss: [2.5745544, 0.3561713, 4.7929378]\n",
      "169: Train Loss: [2.4231713, 0.37430468, 4.472038] | Test Loss: [2.6717565, 0.4151498, 4.9283633]\n",
      "170: Train Loss: [2.4083402, 0.32647768, 4.490203] | Test Loss: [2.6028073, 0.38061395, 4.825001]\n",
      "171: Train Loss: [2.4242532, 0.36971256, 4.478794] | Test Loss: [2.5324183, 0.35189077, 4.712946]\n",
      "172: Train Loss: [2.4551902, 0.3377757, 4.5726047] | Test Loss: [2.6147962, 0.35016683, 4.8794255]\n",
      "173: Train Loss: [2.3042097, 0.3847507, 4.2236686] | Test Loss: [2.6018248, 0.43831486, 4.7653346]\n",
      "174: Train Loss: [2.4163663, 0.33919448, 4.4935384] | Test Loss: [2.6435146, 0.32706872, 4.9599605]\n",
      "175: Train Loss: [2.5550585, 0.34374017, 4.766377] | Test Loss: [2.5533805, 0.39030755, 4.7164536]\n",
      "176: Train Loss: [2.4195764, 0.30454752, 4.5346055] | Test Loss: [2.4313006, 0.3330352, 4.5295663]\n",
      "177: Train Loss: [2.2515123, 0.32199392, 4.1810308] | Test Loss: [2.6624234, 0.368759, 4.9560876]\n",
      "178: Train Loss: [2.4468367, 0.3413394, 4.552334] | Test Loss: [2.5777156, 0.49077386, 4.6646576]\n",
      "179: Train Loss: [2.4045806, 0.2961501, 4.513011] | Test Loss: [2.7249112, 0.42198542, 5.027837]\n",
      "180: Train Loss: [2.4564416, 0.43888482, 4.4739985] | Test Loss: [2.7312837, 0.3471664, 5.115401]\n",
      "181: Train Loss: [2.5824172, 0.3295804, 4.835254] | Test Loss: [2.6568449, 0.35923597, 4.954454]\n",
      "182: Train Loss: [2.5545242, 0.35798386, 4.7510643] | Test Loss: [3.0354877, 0.3650553, 5.70592]\n",
      "183: Train Loss: [2.4934137, 0.38983032, 4.5969973] | Test Loss: [2.640595, 0.4029605, 4.8782296]\n",
      "184: Train Loss: [2.423258, 0.31802326, 4.528493] | Test Loss: [2.7559154, 0.35586658, 5.1559644]\n",
      "185: Train Loss: [2.4875805, 0.31757003, 4.657591] | Test Loss: [2.5076287, 0.3791521, 4.636105]\n",
      "186: Train Loss: [2.3749647, 0.32463086, 4.4252987] | Test Loss: [2.6550453, 0.34964722, 4.9604435]\n",
      "187: Train Loss: [2.3369434, 0.30963075, 4.364256] | Test Loss: [2.4974387, 0.36019048, 4.634687]\n",
      "188: Train Loss: [2.405436, 0.34193197, 4.4689403] | Test Loss: [2.7252197, 0.38745597, 5.0629835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189: Train Loss: [2.323971, 0.37807506, 4.269867] | Test Loss: [2.664551, 0.37757686, 4.951525]\n",
      "190: Train Loss: [2.336461, 0.31113175, 4.36179] | Test Loss: [2.626516, 0.35519934, 4.897833]\n",
      "191: Train Loss: [2.4776754, 0.36548305, 4.5898676] | Test Loss: [2.7663238, 0.34801596, 5.184632]\n",
      "192: Train Loss: [2.4865181, 0.36288974, 4.6101465] | Test Loss: [2.6321988, 0.33776268, 4.926635]\n",
      "193: Train Loss: [2.344856, 0.3229644, 4.366748] | Test Loss: [2.6463885, 0.3906158, 4.902161]\n",
      "194: Train Loss: [2.5368738, 0.46848023, 4.6052675] | Test Loss: [2.6491961, 0.34632537, 4.952067]\n",
      "195: Train Loss: [2.5550406, 0.3941306, 4.7159505] | Test Loss: [2.5231454, 0.39969772, 4.646593]\n",
      "196: Train Loss: [2.433302, 0.35532328, 4.5112805] | Test Loss: [2.5669432, 0.41320798, 4.7206783]\n",
      "197: Train Loss: [2.2428389, 0.33743978, 4.148238] | Test Loss: [2.5263722, 0.3520651, 4.7006793]\n",
      "198: Train Loss: [2.3925705, 0.33845133, 4.4466896] | Test Loss: [2.4881291, 0.34567052, 4.6305876]\n",
      "199: Train Loss: [2.5996828, 0.3664062, 4.8329597] | Test Loss: [2.7525306, 0.4208322, 5.084229]\n",
      "200: Train Loss: [2.4103165, 0.34189624, 4.478737] | Test Loss: [2.5220668, 0.3823604, 4.661773]\n",
      "201: Train Loss: [2.5747027, 0.3541365, 4.795269] | Test Loss: [2.5239832, 0.3311459, 4.7168207]\n",
      "202: Train Loss: [2.331973, 0.3860092, 4.277937] | Test Loss: [2.501752, 0.35954928, 4.6439548]\n",
      "203: Train Loss: [2.5487287, 0.3810744, 4.716383] | Test Loss: [2.6730309, 0.3228763, 5.0231853]\n",
      "204: Train Loss: [2.3300622, 0.36321303, 4.2969112] | Test Loss: [2.6020012, 0.4423701, 4.7616324]\n",
      "205: Train Loss: [2.3060715, 0.3880448, 4.224098] | Test Loss: [2.7588837, 0.4111336, 5.1066337]\n",
      "206: Train Loss: [2.3796184, 0.39885956, 4.3603773] | Test Loss: [2.5250096, 0.35092816, 4.699091]\n",
      "207: Train Loss: [2.3953238, 0.42101982, 4.3696275] | Test Loss: [2.8499236, 0.4398234, 5.260024]\n",
      "208: Train Loss: [2.4465704, 0.38577843, 4.5073624] | Test Loss: [2.5608084, 0.3399202, 4.781697]\n",
      "209: Train Loss: [2.4800956, 0.3800932, 4.580098] | Test Loss: [2.538508, 0.350174, 4.726842]\n",
      "210: Train Loss: [2.5013828, 0.33544508, 4.6673207] | Test Loss: [2.5865197, 0.3890183, 4.784021]\n",
      "211: Train Loss: [2.4804618, 0.35169718, 4.6092267] | Test Loss: [2.6095164, 0.3793828, 4.83965]\n",
      "212: Train Loss: [2.5845068, 0.3526888, 4.8163247] | Test Loss: [2.4267735, 0.29915875, 4.5543885]\n",
      "213: Train Loss: [2.624463, 0.3834103, 4.8655157] | Test Loss: [2.7354686, 0.36456615, 5.106371]\n",
      "214: Train Loss: [2.3441124, 0.29739976, 4.3908253] | Test Loss: [2.7157645, 0.4133919, 5.018137]\n",
      "215: Train Loss: [2.473976, 0.42851937, 4.5194325] | Test Loss: [2.509601, 0.30549622, 4.713706]\n",
      "216: Train Loss: [2.351022, 0.34685633, 4.355188] | Test Loss: [2.7423587, 0.30873668, 5.1759806]\n",
      "217: Train Loss: [2.5848298, 0.39989203, 4.7697678] | Test Loss: [2.612165, 0.38680148, 4.837528]\n",
      "218: Train Loss: [2.3512576, 0.35319465, 4.3493204] | Test Loss: [2.5818098, 0.32895878, 4.8346605]\n",
      "219: Train Loss: [2.470407, 0.38936013, 4.551454] | Test Loss: [2.7696688, 0.3381367, 5.201201]\n",
      "220: Train Loss: [2.4579167, 0.3901547, 4.5256786] | Test Loss: [2.626183, 0.41894716, 4.833419]\n",
      "221: Train Loss: [2.3881657, 0.3853698, 4.3909616] | Test Loss: [2.501796, 0.46743095, 4.536161]\n",
      "222: Train Loss: [2.4708972, 0.33422065, 4.6075735] | Test Loss: [2.5630262, 0.35476077, 4.7712917]\n",
      "223: Train Loss: [2.2432742, 0.37962034, 4.106928] | Test Loss: [2.5939882, 0.42020294, 4.7677736]\n",
      "224: Train Loss: [2.3205035, 0.3882853, 4.252722] | Test Loss: [2.643318, 0.34284508, 4.943791]\n",
      "225: Train Loss: [2.525152, 0.3778513, 4.6724524] | Test Loss: [2.5091643, 0.3558587, 4.66247]\n",
      "226: Train Loss: [2.4422915, 0.35029185, 4.5342913] | Test Loss: [2.5998857, 0.41858226, 4.781189]\n",
      "227: Train Loss: [2.459342, 0.36684105, 4.551843] | Test Loss: [2.5727096, 0.37279618, 4.772623]\n",
      "228: Train Loss: [2.4666295, 0.47991258, 4.4533463] | Test Loss: [2.7110379, 0.35469693, 5.067379]\n",
      "229: Train Loss: [2.505646, 0.3404052, 4.670887] | Test Loss: [2.4270797, 0.34739125, 4.506768]\n",
      "230: Train Loss: [2.412453, 0.37503195, 4.449874] | Test Loss: [2.5858502, 0.38221145, 4.7894893]\n",
      "231: Train Loss: [2.3985493, 0.349202, 4.4478965] | Test Loss: [2.5765982, 0.35983813, 4.7933583]\n",
      "232: Train Loss: [2.430062, 0.29536775, 4.5647564] | Test Loss: [2.5399325, 0.36975583, 4.710109]\n",
      "233: Train Loss: [2.461787, 0.32853743, 4.5950365] | Test Loss: [2.7061167, 0.3874991, 5.024734]\n",
      "234: Train Loss: [2.4755876, 0.35410812, 4.597067] | Test Loss: [2.8275247, 0.42490655, 5.2301426]\n",
      "235: Train Loss: [2.503199, 0.3728574, 4.6335406] | Test Loss: [2.540112, 0.32855752, 4.7516665]\n",
      "236: Train Loss: [2.3932521, 0.31043684, 4.4760675] | Test Loss: [2.523183, 0.37373814, 4.672628]\n",
      "237: Train Loss: [2.4333165, 0.25463915, 4.611994] | Test Loss: [2.641809, 0.36235574, 4.9212623]\n",
      "238: Train Loss: [2.534473, 0.5877999, 4.481146] | Test Loss: [2.7294807, 0.39565706, 5.0633044]\n",
      "239: Train Loss: [2.4591382, 0.35408586, 4.5641904] | Test Loss: [2.585855, 0.36521173, 4.8064985]\n",
      "240: Train Loss: [2.4410055, 0.33913434, 4.5428767] | Test Loss: [2.5332189, 0.39737844, 4.6690593]\n",
      "241: Train Loss: [2.4315643, 0.3932469, 4.469882] | Test Loss: [2.575225, 0.3469872, 4.803463]\n",
      "242: Train Loss: [2.2173793, 0.3292452, 4.1055136] | Test Loss: [2.606542, 0.48438373, 4.7287006]\n",
      "243: Train Loss: [2.4020865, 0.3545111, 4.4496617] | Test Loss: [2.6676571, 0.38534096, 4.949973]\n",
      "244: Train Loss: [2.369208, 0.35729182, 4.3811245] | Test Loss: [2.6535554, 0.33579028, 4.9713206]\n",
      "245: Train Loss: [2.4397845, 0.34458035, 4.534989] | Test Loss: [2.6360757, 0.359165, 4.9129863]\n",
      "246: Train Loss: [2.499466, 0.48352286, 4.515409] | Test Loss: [2.5331454, 0.41631877, 4.649972]\n",
      "247: Train Loss: [2.569208, 0.3363015, 4.8021145] | Test Loss: [2.5957544, 0.36981857, 4.82169]\n",
      "248: Train Loss: [2.2139165, 0.33090556, 4.0969276] | Test Loss: [2.6459727, 0.40621674, 4.885729]\n",
      "249: Train Loss: [2.3777702, 0.30824798, 4.4472923] | Test Loss: [2.5425575, 0.34278348, 4.7423315]\n",
      "250: Train Loss: [2.4256232, 0.379916, 4.47133] | Test Loss: [2.6236596, 0.34024957, 4.9070697]\n",
      "251: Train Loss: [2.3198361, 0.343777, 4.295895] | Test Loss: [2.5895364, 0.2850413, 4.8940315]\n",
      "252: Train Loss: [2.5285733, 0.32163966, 4.735507] | Test Loss: [2.5386477, 0.35579562, 4.7214994]\n",
      "253: Train Loss: [2.4147007, 0.330806, 4.4985957] | Test Loss: [2.6113288, 0.3851817, 4.837476]\n",
      "254: Train Loss: [2.3317056, 0.39822012, 4.265191] | Test Loss: [2.5602865, 0.3759022, 4.744671]\n",
      "255: Train Loss: [2.630877, 0.35554078, 4.9062133] | Test Loss: [2.6462443, 0.39995286, 4.8925357]\n",
      "256: Train Loss: [2.3443363, 0.32691252, 4.36176] | Test Loss: [2.5726123, 0.35212016, 4.793104]\n",
      "257: Train Loss: [2.333066, 0.36608455, 4.3000474] | Test Loss: [2.6331449, 0.39863473, 4.867655]\n",
      "258: Train Loss: [2.5007465, 0.36479324, 4.6366997] | Test Loss: [2.7086697, 0.31994227, 5.097397]\n",
      "259: Train Loss: [2.447214, 0.38429752, 4.5101304] | Test Loss: [2.5914443, 0.39928153, 4.783607]\n",
      "260: Train Loss: [2.501299, 0.36336493, 4.6392326] | Test Loss: [2.703283, 0.35034868, 5.0562177]\n",
      "261: Train Loss: [2.4848213, 0.45629033, 4.5133524] | Test Loss: [2.549508, 0.40555495, 4.6934614]\n",
      "262: Train Loss: [2.4373226, 0.40176204, 4.472883] | Test Loss: [2.7054946, 0.35906225, 5.051927]\n",
      "263: Train Loss: [2.4078934, 0.33240977, 4.483377] | Test Loss: [2.734876, 0.4148729, 5.0548787]\n",
      "264: Train Loss: [2.4367917, 0.36701614, 4.506567] | Test Loss: [2.687205, 0.3521501, 5.02226]\n",
      "265: Train Loss: [2.3022766, 0.36794803, 4.236605] | Test Loss: [2.6573563, 0.30769315, 5.0070195]\n",
      "266: Train Loss: [2.5112994, 0.30176792, 4.720831] | Test Loss: [2.546905, 0.33172092, 4.7620893]\n",
      "267: Train Loss: [2.3127768, 0.37195614, 4.2535973] | Test Loss: [2.6167173, 0.3355946, 4.89784]\n",
      "268: Train Loss: [2.422981, 0.29294744, 4.5530148] | Test Loss: [2.5054533, 0.36321038, 4.6476965]\n",
      "269: Train Loss: [2.4270873, 0.3387329, 4.515442] | Test Loss: [2.9180732, 0.49211043, 5.344036]\n",
      "270: Train Loss: [2.4532504, 0.3888097, 4.517691] | Test Loss: [2.3846352, 0.4640752, 4.3051953]\n",
      "271: Train Loss: [2.5073676, 0.31911948, 4.695616] | Test Loss: [2.5452876, 0.36454794, 4.7260275]\n",
      "272: Train Loss: [2.4910707, 0.3061696, 4.675972] | Test Loss: [2.5229871, 0.35562667, 4.6903477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273: Train Loss: [2.4889557, 0.48080057, 4.497111] | Test Loss: [2.674926, 0.43080682, 4.9190454]\n",
      "274: Train Loss: [2.6342494, 0.3768439, 4.891655] | Test Loss: [2.5234678, 0.34374782, 4.703188]\n",
      "275: Train Loss: [2.395596, 0.36274558, 4.4284463] | Test Loss: [2.866232, 0.37186217, 5.3606014]\n",
      "276: Train Loss: [2.5325484, 0.34733164, 4.7177653] | Test Loss: [2.62336, 0.3908365, 4.855883]\n",
      "277: Train Loss: [2.3522494, 0.35860834, 4.3458905] | Test Loss: [2.6127954, 0.35770547, 4.867885]\n",
      "278: Train Loss: [2.4851966, 0.36881253, 4.6015806] | Test Loss: [2.5542533, 0.32881382, 4.7796926]\n",
      "279: Train Loss: [2.5431738, 0.30778766, 4.77856] | Test Loss: [2.6681795, 0.41873425, 4.917625]\n",
      "280: Train Loss: [2.3382285, 0.339164, 4.337293] | Test Loss: [2.6920571, 0.33891204, 5.0452023]\n",
      "281: Train Loss: [2.344009, 0.33819392, 4.349824] | Test Loss: [2.4375107, 0.35368878, 4.5213327]\n",
      "282: Train Loss: [2.3331833, 0.41158146, 4.254785] | Test Loss: [2.5929918, 0.3097849, 4.876199]\n",
      "283: Train Loss: [2.3905118, 0.37925658, 4.401767] | Test Loss: [2.6144779, 0.43043628, 4.7985196]\n",
      "284: Train Loss: [2.6216514, 0.3161402, 4.9271626] | Test Loss: [2.5695026, 0.31033677, 4.8286686]\n",
      "285: Train Loss: [2.5018072, 0.3664824, 4.637132] | Test Loss: [2.7003307, 0.41442144, 4.98624]\n",
      "286: Train Loss: [2.5184152, 0.33964905, 4.697181] | Test Loss: [2.426719, 0.4419674, 4.4114704]\n",
      "287: Train Loss: [2.3474956, 0.31978166, 4.3752093] | Test Loss: [2.7524545, 0.34392175, 5.1609874]\n",
      "288: Train Loss: [2.3606393, 0.32399562, 4.397283] | Test Loss: [2.6148415, 0.38436645, 4.8453164]\n",
      "289: Train Loss: [2.3300798, 0.37202874, 4.2881308] | Test Loss: [2.6096144, 0.40711522, 4.8121133]\n",
      "290: Train Loss: [2.4372518, 0.43978855, 4.4347153] | Test Loss: [2.5671885, 0.41665602, 4.717721]\n",
      "291: Train Loss: [2.3791802, 0.3331464, 4.425214] | Test Loss: [2.533327, 0.34311432, 4.72354]\n",
      "292: Train Loss: [2.4274745, 0.33675817, 4.518191] | Test Loss: [2.769707, 0.39542052, 5.1439934]\n",
      "293: Train Loss: [2.4660347, 0.30762106, 4.6244483] | Test Loss: [2.5381858, 0.35422072, 4.722151]\n",
      "294: Train Loss: [2.5811172, 0.32393524, 4.8382993] | Test Loss: [2.6061978, 0.3276028, 4.884793]\n",
      "295: Train Loss: [2.435126, 0.37108272, 4.4991693] | Test Loss: [2.59931, 0.32644907, 4.872171]\n",
      "296: Train Loss: [2.3834171, 0.32606846, 4.440766] | Test Loss: [2.4501636, 0.33569798, 4.564629]\n",
      "297: Train Loss: [2.5055506, 0.31107798, 4.700023] | Test Loss: [2.5293262, 0.36149415, 4.6971583]\n",
      "298: Train Loss: [2.2506976, 0.3256005, 4.1757946] | Test Loss: [2.6595337, 0.3973881, 4.9216795]\n",
      "299: Train Loss: [2.5219102, 0.3132727, 4.7305474] | Test Loss: [2.5603445, 0.3522793, 4.7684097]\n",
      "300: Train Loss: [2.4691875, 0.31175473, 4.6266203] | Test Loss: [2.6295857, 0.39498806, 4.8641834]\n",
      "301: Train Loss: [2.4906585, 0.34363768, 4.6376796] | Test Loss: [2.6516783, 0.4101764, 4.8931804]\n",
      "302: Train Loss: [2.39952, 0.29794776, 4.501092] | Test Loss: [2.538927, 0.28664365, 4.7912107]\n",
      "303: Train Loss: [2.4243295, 0.34623495, 4.5024242] | Test Loss: [2.5194964, 0.3554228, 4.68357]\n",
      "304: Train Loss: [2.2715743, 0.33176267, 4.2113857] | Test Loss: [2.7562413, 0.29196706, 5.2205157]\n",
      "305: Train Loss: [2.4168227, 0.38629425, 4.447351] | Test Loss: [2.4290898, 0.36823466, 4.489945]\n",
      "306: Train Loss: [2.4545715, 0.42426616, 4.4848766] | Test Loss: [2.5157428, 0.3392982, 4.6921873]\n",
      "307: Train Loss: [2.3971515, 0.36701322, 4.4272895] | Test Loss: [2.6562722, 0.38199124, 4.930553]\n",
      "308: Train Loss: [2.3324735, 0.37056684, 4.29438] | Test Loss: [2.5245945, 0.34296638, 4.7062225]\n",
      "309: Train Loss: [2.6236417, 0.33847073, 4.9088125] | Test Loss: [2.6702952, 0.38203907, 4.9585514]\n",
      "310: Train Loss: [2.4257014, 0.37434614, 4.4770565] | Test Loss: [2.6398962, 0.3946071, 4.8851852]\n",
      "311: Train Loss: [2.3518806, 0.3653201, 4.338441] | Test Loss: [2.692585, 0.45617387, 4.928996]\n",
      "312: Train Loss: [2.5127308, 0.3516887, 4.673773] | Test Loss: [2.5645192, 0.43402722, 4.695011]\n",
      "313: Train Loss: [2.3336904, 0.40699968, 4.260381] | Test Loss: [2.6965048, 0.3389937, 5.054016]\n",
      "314: Train Loss: [2.628055, 0.5134224, 4.7426877] | Test Loss: [2.7062647, 0.37413862, 5.0383906]\n",
      "315: Train Loss: [2.349031, 0.34686184, 4.3512] | Test Loss: [2.5521648, 0.3709549, 4.7333746]\n",
      "316: Train Loss: [2.408816, 0.40332454, 4.4143076] | Test Loss: [2.5514195, 0.31432977, 4.7885094]\n",
      "317: Train Loss: [2.3330197, 0.37523723, 4.290802] | Test Loss: [2.7197478, 0.36689293, 5.0726027]\n",
      "318: Train Loss: [2.446344, 0.38225967, 4.510428] | Test Loss: [2.4979062, 0.3829021, 4.6129103]\n",
      "319: Train Loss: [2.5088902, 0.34364438, 4.674136] | Test Loss: [2.563822, 0.36225718, 4.765387]\n",
      "320: Train Loss: [2.5232432, 0.37914035, 4.667346] | Test Loss: [2.571906, 0.45395005, 4.6898623]\n",
      "321: Train Loss: [2.5208101, 0.3302545, 4.7113657] | Test Loss: [2.5912828, 0.38345307, 4.799113]\n",
      "322: Train Loss: [2.3674583, 0.36878935, 4.3661275] | Test Loss: [2.5345054, 0.45954433, 4.6094666]\n",
      "323: Train Loss: [2.4610493, 0.39938325, 4.5227156] | Test Loss: [2.5278862, 0.32316858, 4.7326035]\n",
      "324: Train Loss: [2.2799685, 0.31100824, 4.2489285] | Test Loss: [2.5295374, 0.35262877, 4.706446]\n",
      "325: Train Loss: [2.4409115, 0.3798177, 4.5020056] | Test Loss: [2.7198162, 0.33970797, 5.0999246]\n",
      "326: Train Loss: [2.5092247, 0.3242355, 4.694214] | Test Loss: [2.6640701, 0.35341823, 4.974722]\n",
      "327: Train Loss: [2.5272245, 0.34763217, 4.7068167] | Test Loss: [2.4520018, 0.34949672, 4.554507]\n",
      "328: Train Loss: [2.4744883, 0.32812008, 4.6208563] | Test Loss: [2.5137105, 0.3706452, 4.656776]\n",
      "329: Train Loss: [2.294893, 0.35242155, 4.2373643] | Test Loss: [2.7555432, 0.3740865, 5.137]\n",
      "330: Train Loss: [2.3992555, 0.33867323, 4.459838] | Test Loss: [2.7066104, 0.41239455, 5.0008264]\n",
      "331: Train Loss: [2.4208636, 0.35494447, 4.486783] | Test Loss: [2.6622894, 0.38363186, 4.940947]\n",
      "332: Train Loss: [2.3917868, 0.41024765, 4.373326] | Test Loss: [2.8058815, 0.2706891, 5.341074]\n",
      "333: Train Loss: [2.412882, 0.33164865, 4.4941154] | Test Loss: [2.4110897, 0.33814967, 4.48403]\n",
      "334: Train Loss: [2.3915124, 0.31762046, 4.4654045] | Test Loss: [2.6468463, 0.32631654, 4.967376]\n",
      "335: Train Loss: [2.4108224, 0.35586807, 4.465777] | Test Loss: [2.7016582, 0.4293719, 4.9739447]\n",
      "336: Train Loss: [2.4808223, 0.33732936, 4.6243153] | Test Loss: [2.6875074, 0.45602527, 4.9189897]\n",
      "337: Train Loss: [2.3937886, 0.41181648, 4.3757606] | Test Loss: [2.6579576, 0.41587454, 4.9000406]\n",
      "338: Train Loss: [2.3680134, 0.3371364, 4.3988905] | Test Loss: [2.698839, 0.44034067, 4.9573374]\n",
      "339: Train Loss: [2.4115725, 0.4203045, 4.4028406] | Test Loss: [2.5635817, 0.36913624, 4.758027]\n",
      "340: Train Loss: [2.4649217, 0.3449788, 4.5848646] | Test Loss: [2.6148074, 0.39760607, 4.832009]\n",
      "341: Train Loss: [2.3949006, 0.37402055, 4.4157805] | Test Loss: [2.523488, 0.31577033, 4.731206]\n",
      "342: Train Loss: [2.544191, 0.41771314, 4.6706686] | Test Loss: [2.4921434, 0.39387947, 4.5904074]\n",
      "343: Train Loss: [2.5307548, 0.33238897, 4.7291207] | Test Loss: [2.5080392, 0.3597908, 4.6562877]\n",
      "344: Train Loss: [2.5590186, 0.3856155, 4.732422] | Test Loss: [2.6474748, 0.35047027, 4.9444795]\n",
      "345: Train Loss: [2.1837373, 0.3673485, 4.000126] | Test Loss: [2.636976, 0.33257443, 4.9413776]\n",
      "346: Train Loss: [2.1559007, 0.32861274, 3.9831889] | Test Loss: [2.6024392, 0.3377509, 4.8671274]\n",
      "347: Train Loss: [2.4824846, 0.3307641, 4.634205] | Test Loss: [2.6887484, 0.3874788, 4.990018]\n",
      "348: Train Loss: [2.338379, 0.32692692, 4.349831] | Test Loss: [2.766942, 0.35264903, 5.181235]\n",
      "349: Train Loss: [2.4108539, 0.37441927, 4.4472885] | Test Loss: [2.5979962, 0.42043862, 4.7755537]\n",
      "350: Train Loss: [2.504428, 0.37766528, 4.6311903] | Test Loss: [2.6631198, 0.4444403, 4.881799]\n",
      "351: Train Loss: [2.3402395, 0.3728905, 4.3075886] | Test Loss: [2.6429431, 0.3733953, 4.912491]\n",
      "352: Train Loss: [2.1954134, 0.302206, 4.0886207] | Test Loss: [2.738793, 0.37844253, 5.0991435]\n",
      "353: Train Loss: [2.443585, 0.3640445, 4.523125] | Test Loss: [2.5999904, 0.3597262, 4.8402543]\n",
      "354: Train Loss: [2.5853999, 0.3301511, 4.8406487] | Test Loss: [2.6111586, 0.42485797, 4.797459]\n",
      "355: Train Loss: [2.5211189, 0.3895889, 4.652649] | Test Loss: [2.638245, 0.31737557, 4.9591146]\n",
      "356: Train Loss: [2.5460956, 0.36003315, 4.732158] | Test Loss: [2.6113029, 0.34535718, 4.8772483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357: Train Loss: [2.3880033, 0.31603613, 4.4599705] | Test Loss: [2.4466524, 0.33013192, 4.563173]\n",
      "358: Train Loss: [2.4066422, 0.37897983, 4.4343047] | Test Loss: [2.6458454, 0.32074618, 4.970945]\n",
      "359: Train Loss: [2.548655, 0.31011122, 4.787199] | Test Loss: [2.595898, 0.33907026, 4.8527255]\n",
      "360: Train Loss: [2.5799634, 0.401155, 4.758772] | Test Loss: [2.466838, 0.34002382, 4.593652]\n",
      "361: Train Loss: [2.352465, 0.32197225, 4.3829575] | Test Loss: [2.5446186, 0.39304397, 4.696193]\n",
      "362: Train Loss: [2.5199761, 0.41765845, 4.622294] | Test Loss: [2.8272986, 0.31640944, 5.3381877]\n",
      "363: Train Loss: [2.420692, 0.33234036, 4.5090437] | Test Loss: [2.566586, 0.3966948, 4.7364774]\n",
      "364: Train Loss: [2.5749764, 0.4291517, 4.7208014] | Test Loss: [2.629263, 0.37120128, 4.8873243]\n",
      "365: Train Loss: [2.5242746, 0.41164333, 4.6369057] | Test Loss: [2.5384603, 0.38764033, 4.68928]\n",
      "366: Train Loss: [2.490297, 0.40516603, 4.575428] | Test Loss: [2.52359, 0.35881728, 4.688363]\n",
      "367: Train Loss: [2.525036, 0.3679565, 4.6821156] | Test Loss: [2.400578, 0.36938867, 4.4317675]\n",
      "368: Train Loss: [2.4719505, 0.4153915, 4.5285096] | Test Loss: [2.6926217, 0.36033958, 5.024904]\n",
      "369: Train Loss: [2.4426453, 0.35348782, 4.5318027] | Test Loss: [2.560611, 0.44151986, 4.6797023]\n",
      "370: Train Loss: [2.5713658, 0.32538417, 4.8173475] | Test Loss: [2.568835, 0.4442839, 4.693386]\n",
      "371: Train Loss: [2.4638708, 0.35445693, 4.5732846] | Test Loss: [2.8339553, 0.41809288, 5.249818]\n",
      "372: Train Loss: [2.5346272, 0.3147598, 4.7544947] | Test Loss: [2.802351, 0.4172938, 5.187408]\n",
      "373: Train Loss: [2.4763873, 0.4931731, 4.4596014] | Test Loss: [2.5323882, 0.30908608, 4.7556906]\n",
      "374: Train Loss: [2.3344958, 0.43139476, 4.237597] | Test Loss: [2.5483859, 0.35248053, 4.7442913]\n",
      "375: Train Loss: [2.4901779, 0.3473426, 4.6330132] | Test Loss: [2.430981, 0.374554, 4.4874077]\n",
      "376: Train Loss: [2.2672114, 0.2868014, 4.2476215] | Test Loss: [2.6775594, 0.276949, 5.07817]\n",
      "377: Train Loss: [2.6400166, 0.36357778, 4.9164553] | Test Loss: [2.5424268, 0.35243008, 4.732424]\n",
      "378: Train Loss: [2.4773164, 0.35913813, 4.5954947] | Test Loss: [2.664083, 0.33240667, 4.9957595]\n",
      "379: Train Loss: [2.350106, 0.32732937, 4.372883] | Test Loss: [2.6312408, 0.36517984, 4.8973017]\n",
      "380: Train Loss: [2.3903239, 0.34321102, 4.4374366] | Test Loss: [2.7693477, 0.32817164, 5.2105236]\n",
      "381: Train Loss: [2.4666455, 0.37162367, 4.5616674] | Test Loss: [2.7500334, 0.39805344, 5.102013]\n",
      "382: Train Loss: [2.4733057, 0.33852598, 4.6080856] | Test Loss: [2.7246134, 0.42035925, 5.0288677]\n",
      "383: Train Loss: [2.4512358, 0.36200172, 4.5404696] | Test Loss: [2.7757537, 0.3824139, 5.1690936]\n",
      "384: Train Loss: [2.3661466, 0.30663404, 4.425659] | Test Loss: [2.631498, 0.37756222, 4.885434]\n",
      "385: Train Loss: [2.6249366, 0.34845778, 4.9014153] | Test Loss: [2.2712095, 0.3712501, 4.171169]\n",
      "386: Train Loss: [2.546734, 0.35444635, 4.739022] | Test Loss: [2.5974905, 0.3544242, 4.840557]\n",
      "387: Train Loss: [2.4349914, 0.38609737, 4.4838853] | Test Loss: [2.5385737, 0.3425716, 4.7345757]\n",
      "388: Train Loss: [2.4185176, 0.39939094, 4.4376445] | Test Loss: [2.707026, 0.35823148, 5.0558205]\n",
      "389: Train Loss: [2.3694077, 0.35506827, 4.383747] | Test Loss: [2.5808387, 0.36295074, 4.7987266]\n",
      "390: Train Loss: [2.4101098, 0.3517672, 4.4684525] | Test Loss: [2.5793393, 0.41992337, 4.738755]\n",
      "391: Train Loss: [2.35089, 0.4012484, 4.3005314] | Test Loss: [2.6043053, 0.3384163, 4.8701944]\n",
      "392: Train Loss: [2.5801156, 0.30290478, 4.8573265] | Test Loss: [2.712081, 0.35465682, 5.069505]\n",
      "393: Train Loss: [2.5567336, 0.41771618, 4.695751] | Test Loss: [2.6544173, 0.3659717, 4.942863]\n",
      "394: Train Loss: [2.549405, 0.3539828, 4.7448273] | Test Loss: [2.5977688, 0.37130725, 4.82423]\n",
      "395: Train Loss: [2.4705958, 0.38238478, 4.558807] | Test Loss: [2.6352732, 0.49737597, 4.7731705]\n",
      "396: Train Loss: [2.499389, 0.34184405, 4.656934] | Test Loss: [2.629542, 0.3695738, 4.8895106]\n",
      "397: Train Loss: [2.423638, 0.32876626, 4.51851] | Test Loss: [2.669284, 0.3657368, 4.9728312]\n",
      "398: Train Loss: [2.537106, 0.33741105, 4.736801] | Test Loss: [2.550457, 0.34297743, 4.7579365]\n",
      "399: Train Loss: [2.5637672, 0.4054652, 4.7220693] | Test Loss: [2.6854546, 0.31045958, 5.0604496]\n",
      "400: Train Loss: [2.472015, 0.33760384, 4.606426] | Test Loss: [2.6768584, 0.35127228, 5.0024447]\n",
      "401: Train Loss: [2.4231997, 0.36636564, 4.480034] | Test Loss: [2.5654666, 0.39346224, 4.737471]\n",
      "402: Train Loss: [2.4862335, 0.40049246, 4.5719743] | Test Loss: [2.7616303, 0.49750185, 5.0257587]\n",
      "403: Train Loss: [2.2611077, 0.39697453, 4.125241] | Test Loss: [2.6535225, 0.3922272, 4.914818]\n",
      "404: Train Loss: [2.4015977, 0.37997422, 4.423221] | Test Loss: [2.67596, 0.39787602, 4.9540443]\n",
      "405: Train Loss: [2.3891008, 0.3328357, 4.445366] | Test Loss: [2.5814562, 0.34591633, 4.816996]\n",
      "406: Train Loss: [2.3773038, 0.38736334, 4.3672442] | Test Loss: [2.4283025, 0.37794763, 4.4786572]\n",
      "407: Train Loss: [2.2990901, 0.37862265, 4.219558] | Test Loss: [2.571235, 0.34635642, 4.7961135]\n",
      "408: Train Loss: [2.444319, 0.33624965, 4.552388] | Test Loss: [2.391048, 0.34759828, 4.434498]\n",
      "409: Train Loss: [2.4918952, 0.39121118, 4.5925794] | Test Loss: [2.630578, 0.37813056, 4.8830256]\n",
      "410: Train Loss: [2.4325333, 0.40437144, 4.4606953] | Test Loss: [2.586423, 0.35625798, 4.816588]\n",
      "411: Train Loss: [2.4213052, 0.37582275, 4.466788] | Test Loss: [2.521276, 0.3247842, 4.7177677]\n",
      "412: Train Loss: [2.2601454, 0.3591837, 4.161107] | Test Loss: [2.5501919, 0.33058226, 4.7698016]\n",
      "413: Train Loss: [2.5062082, 0.3879997, 4.624417] | Test Loss: [2.564901, 0.40816116, 4.721641]\n",
      "414: Train Loss: [2.3431804, 0.28913847, 4.3972225] | Test Loss: [2.5963056, 0.3179116, 4.8746996]\n",
      "415: Train Loss: [2.4179544, 0.32560098, 4.510308] | Test Loss: [2.6373644, 0.31363967, 4.961089]\n",
      "416: Train Loss: [2.5893724, 0.3826392, 4.7961054] | Test Loss: [2.5459373, 0.32264388, 4.769231]\n",
      "417: Train Loss: [2.575446, 0.42550486, 4.725387] | Test Loss: [2.634246, 0.37968686, 4.8888054]\n",
      "418: Train Loss: [2.4460473, 0.33736214, 4.5547323] | Test Loss: [2.7426722, 0.35624564, 5.129099]\n",
      "419: Train Loss: [2.4698877, 0.33834395, 4.6014314] | Test Loss: [2.5634964, 0.30625787, 4.820735]\n",
      "420: Train Loss: [2.4279194, 0.34613076, 4.509708] | Test Loss: [2.5882375, 0.38976038, 4.7867146]\n",
      "421: Train Loss: [2.278117, 0.3401302, 4.2161036] | Test Loss: [2.5395565, 0.40164542, 4.677468]\n",
      "422: Train Loss: [2.535455, 0.37838975, 4.69252] | Test Loss: [2.626468, 0.3474046, 4.9055314]\n",
      "423: Train Loss: [2.4833722, 0.3355799, 4.6311646] | Test Loss: [2.4707706, 0.36103755, 4.5805035]\n",
      "424: Train Loss: [2.3435307, 0.34460586, 4.3424554] | Test Loss: [2.5897126, 0.39099815, 4.788427]\n",
      "425: Train Loss: [2.4845412, 0.38957825, 4.579504] | Test Loss: [2.6084495, 0.37346187, 4.843437]\n",
      "426: Train Loss: [2.5003862, 0.41415447, 4.586618] | Test Loss: [2.5181997, 0.3696069, 4.6667924]\n",
      "427: Train Loss: [2.4113157, 0.34027356, 4.482358] | Test Loss: [2.5035205, 0.32391608, 4.683125]\n",
      "428: Train Loss: [2.3956227, 0.36345196, 4.4277935] | Test Loss: [2.5181708, 0.35392728, 4.6824145]\n",
      "429: Train Loss: [2.3146963, 0.35298717, 4.2764053] | Test Loss: [2.5986316, 0.4175108, 4.7797523]\n",
      "430: Train Loss: [2.4698658, 0.29941708, 4.6403146] | Test Loss: [2.7076764, 0.4335751, 4.9817777]\n",
      "431: Train Loss: [2.556908, 0.33280334, 4.7810125] | Test Loss: [2.667326, 0.37225366, 4.9623985]\n",
      "432: Train Loss: [2.5090294, 0.38956532, 4.6284933] | Test Loss: [2.5803685, 0.33946642, 4.8212705]\n",
      "433: Train Loss: [2.4620743, 0.3320434, 4.5921054] | Test Loss: [2.6701646, 0.33902028, 5.001309]\n",
      "434: Train Loss: [2.4129856, 0.30241862, 4.5235524] | Test Loss: [2.5175564, 0.4961967, 4.538916]\n",
      "435: Train Loss: [2.4126558, 0.3786677, 4.446644] | Test Loss: [2.6116898, 0.39821002, 4.8251696]\n",
      "436: Train Loss: [2.5285435, 0.42035356, 4.6367335] | Test Loss: [2.8358731, 0.30928385, 5.3624625]\n",
      "437: Train Loss: [2.61259, 0.32059935, 4.9045806] | Test Loss: [2.538632, 0.40166172, 4.675602]\n",
      "438: Train Loss: [2.3587253, 0.40761057, 4.30984] | Test Loss: [2.419865, 0.37808466, 4.461645]\n",
      "439: Train Loss: [2.7206795, 0.4427224, 4.9986367] | Test Loss: [2.568779, 0.32081136, 4.8167467]\n",
      "440: Train Loss: [2.5697863, 0.39161676, 4.747956] | Test Loss: [2.4525816, 0.34362748, 4.561536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441: Train Loss: [2.4086246, 0.4147702, 4.402479] | Test Loss: [2.4848688, 0.3300298, 4.6397076]\n",
      "442: Train Loss: [2.505383, 0.33774406, 4.673022] | Test Loss: [2.641711, 0.36159787, 4.921824]\n",
      "443: Train Loss: [2.5288782, 0.37783173, 4.6799245] | Test Loss: [2.7270505, 0.44055575, 5.0135455]\n",
      "444: Train Loss: [2.4644017, 0.4447699, 4.4840336] | Test Loss: [2.5564852, 0.41156077, 4.7014093]\n",
      "445: Train Loss: [2.3405347, 0.3736594, 4.30741] | Test Loss: [2.4847555, 0.33730185, 4.6322093]\n",
      "446: Train Loss: [2.304771, 0.40016994, 4.209372] | Test Loss: [2.5320783, 0.29868665, 4.76547]\n",
      "447: Train Loss: [2.430425, 0.46681014, 4.3940396] | Test Loss: [2.5564039, 0.34828216, 4.7645254]\n",
      "448: Train Loss: [2.5687654, 0.34924188, 4.788289] | Test Loss: [2.679515, 0.39584368, 4.9631863]\n",
      "449: Train Loss: [2.40579, 0.34318158, 4.4683986] | Test Loss: [2.7245853, 0.37411988, 5.075051]\n",
      "450: Train Loss: [2.5080423, 0.3994941, 4.6165905] | Test Loss: [2.755382, 0.40607274, 5.1046915]\n",
      "451: Train Loss: [2.5666814, 0.36010134, 4.7732615] | Test Loss: [2.6855314, 0.32428163, 5.046781]\n",
      "452: Train Loss: [2.4347575, 0.39620844, 4.4733067] | Test Loss: [2.6758668, 0.6926144, 4.659119]\n",
      "453: Train Loss: [2.2845678, 0.34643146, 4.2227044] | Test Loss: [2.3904042, 0.3273037, 4.4535046]\n",
      "454: Train Loss: [2.4561603, 0.32624146, 4.586079] | Test Loss: [2.7069006, 0.4100589, 5.003742]\n",
      "455: Train Loss: [2.4198477, 0.35726643, 4.482429] | Test Loss: [2.665555, 0.41430423, 4.9168057]\n",
      "456: Train Loss: [2.2765706, 0.39286903, 4.160272] | Test Loss: [2.7357523, 0.3559197, 5.115585]\n",
      "457: Train Loss: [2.5469196, 0.5166616, 4.5771775] | Test Loss: [2.5484746, 0.3432598, 4.7536893]\n",
      "458: Train Loss: [2.5208201, 0.33489564, 4.7067447] | Test Loss: [2.491588, 0.38610116, 4.597075]\n",
      "459: Train Loss: [2.5875647, 0.3260335, 4.849096] | Test Loss: [2.4430377, 0.42393368, 4.462142]\n",
      "460: Train Loss: [2.3833113, 0.33478838, 4.431834] | Test Loss: [2.4293861, 0.35229006, 4.506482]\n",
      "461: Train Loss: [2.4415371, 0.35237592, 4.5306983] | Test Loss: [2.5802271, 0.4578678, 4.7025867]\n",
      "462: Train Loss: [2.4050803, 0.43801907, 4.3721414] | Test Loss: [2.5306942, 0.33915472, 4.722234]\n",
      "463: Train Loss: [2.3114386, 0.28152937, 4.3413477] | Test Loss: [2.5116785, 0.40437356, 4.6189833]\n",
      "464: Train Loss: [2.441713, 0.34320322, 4.540223] | Test Loss: [2.6538506, 0.3803491, 4.927352]\n",
      "465: Train Loss: [2.4820223, 0.32062846, 4.643416] | Test Loss: [2.6562154, 0.3254475, 4.9869833]\n",
      "466: Train Loss: [2.5139306, 0.31602904, 4.711832] | Test Loss: [2.6085784, 0.3492897, 4.867867]\n",
      "467: Train Loss: [2.4571319, 0.35739362, 4.55687] | Test Loss: [2.562067, 0.4066904, 4.7174435]\n",
      "468: Train Loss: [2.3966365, 0.34353858, 4.449734] | Test Loss: [2.6012754, 0.35725674, 4.845294]\n",
      "469: Train Loss: [2.2674491, 0.33714834, 4.19775] | Test Loss: [2.675854, 0.3615979, 4.99011]\n",
      "470: Train Loss: [2.4203796, 0.36509877, 4.4756603] | Test Loss: [2.7777193, 0.4339112, 5.121527]\n",
      "471: Train Loss: [2.4710946, 0.33868292, 4.603506] | Test Loss: [2.5888855, 0.33766448, 4.8401065]\n",
      "472: Train Loss: [2.3751054, 0.34524035, 4.4049706] | Test Loss: [2.6422908, 0.3964868, 4.888095]\n",
      "473: Train Loss: [2.4058464, 0.35367996, 4.4580126] | Test Loss: [2.5350037, 0.3278752, 4.742132]\n",
      "474: Train Loss: [2.3220968, 0.33944198, 4.304752] | Test Loss: [2.826613, 0.41516083, 5.2380652]\n",
      "475: Train Loss: [2.3739033, 0.35141826, 4.396388] | Test Loss: [2.5714757, 0.33042175, 4.8125296]\n",
      "476: Train Loss: [2.4945257, 0.34143203, 4.6476192] | Test Loss: [2.6025145, 0.36356723, 4.8414617]\n",
      "477: Train Loss: [2.3953526, 0.2857443, 4.504961] | Test Loss: [2.499837, 0.36736557, 4.6323085]\n",
      "478: Train Loss: [2.5883222, 0.3695882, 4.807056] | Test Loss: [2.6154876, 0.3794122, 4.851563]\n",
      "479: Train Loss: [2.3811917, 0.37479794, 4.3875856] | Test Loss: [2.528836, 0.31990176, 4.73777]\n",
      "480: Train Loss: [2.298514, 0.2555782, 4.3414497] | Test Loss: [2.5930128, 0.34172297, 4.8443027]\n",
      "481: Train Loss: [2.437171, 0.35709482, 4.517247] | Test Loss: [2.7421212, 0.3763702, 5.1078725]\n",
      "482: Train Loss: [2.4786022, 0.44315055, 4.514054] | Test Loss: [2.4711587, 0.28803158, 4.654286]\n",
      "483: Train Loss: [2.581541, 0.34169126, 4.821391] | Test Loss: [2.5484862, 0.34485364, 4.7521186]\n",
      "484: Train Loss: [2.4253693, 0.31005564, 4.540683] | Test Loss: [2.6262765, 0.45439294, 4.79816]\n",
      "485: Train Loss: [2.4746783, 0.33682173, 4.612535] | Test Loss: [2.6536653, 0.36038986, 4.946941]\n",
      "486: Train Loss: [2.494221, 0.35869157, 4.6297503] | Test Loss: [2.5288055, 0.39587313, 4.661738]\n",
      "487: Train Loss: [2.4054801, 0.36782244, 4.4431376] | Test Loss: [2.7013583, 0.36832568, 5.034391]\n",
      "488: Train Loss: [2.4995184, 0.42694837, 4.5720882] | Test Loss: [2.5702631, 0.3498579, 4.7906685]\n",
      "489: Train Loss: [2.4912517, 0.40445098, 4.5780525] | Test Loss: [2.5873592, 0.41194317, 4.7627754]\n",
      "490: Train Loss: [2.540157, 0.36073124, 4.719583] | Test Loss: [2.581635, 0.33365488, 4.829615]\n",
      "491: Train Loss: [2.4731677, 0.44376168, 4.5025735] | Test Loss: [2.7124465, 0.33567584, 5.089217]\n",
      "492: Train Loss: [2.5043602, 0.2944777, 4.714243] | Test Loss: [2.7320044, 0.40886346, 5.0551453]\n",
      "493: Train Loss: [2.563031, 0.35505894, 4.771003] | Test Loss: [2.5614476, 0.38559416, 4.737301]\n",
      "494: Train Loss: [2.3962095, 0.3350647, 4.457354] | Test Loss: [2.5627441, 0.33346614, 4.792022]\n",
      "495: Train Loss: [2.4169176, 0.40870818, 4.425127] | Test Loss: [2.583752, 0.32445952, 4.8430443]\n",
      "496: Train Loss: [2.3851178, 0.3411964, 4.429039] | Test Loss: [2.5690541, 0.41840896, 4.7196994]\n",
      "497: Train Loss: [2.5305574, 0.34392208, 4.7171926] | Test Loss: [2.657559, 0.38076043, 4.934357]\n",
      "498: Train Loss: [2.3791227, 0.3374409, 4.4208045] | Test Loss: [2.4696507, 0.35617843, 4.583123]\n",
      "499: Train Loss: [2.313419, 0.39716592, 4.2296724] | Test Loss: [2.4369285, 0.3140405, 4.5598164]\n",
      "500: Train Loss: [2.409032, 0.3074199, 4.5106444] | Test Loss: [2.7765672, 0.3774597, 5.175675]\n",
      "501: Train Loss: [2.4633522, 0.3377235, 4.5889807] | Test Loss: [2.6153162, 0.34097248, 4.88966]\n",
      "502: Train Loss: [2.4386787, 0.35470703, 4.5226502] | Test Loss: [2.416511, 0.34959453, 4.4834275]\n",
      "503: Train Loss: [2.5182812, 0.3568882, 4.679674] | Test Loss: [2.662833, 0.40323836, 4.9224277]\n",
      "504: Train Loss: [2.442087, 0.39767182, 4.486502] | Test Loss: [2.5054936, 0.36696893, 4.644018]\n",
      "505: Train Loss: [2.3847022, 0.31778273, 4.4516215] | Test Loss: [2.6429925, 0.39439848, 4.8915863]\n",
      "506: Train Loss: [2.4096713, 0.4904558, 4.328887] | Test Loss: [2.694553, 0.445035, 4.944071]\n",
      "507: Train Loss: [2.3789017, 0.3405922, 4.417211] | Test Loss: [2.5695934, 0.36634308, 4.772844]\n",
      "508: Train Loss: [2.610367, 0.36365488, 4.857079] | Test Loss: [2.4990413, 0.35384017, 4.6442423]\n",
      "509: Train Loss: [2.5587523, 0.37183735, 4.7456675] | Test Loss: [2.7054152, 0.391464, 5.0193667]\n",
      "510: Train Loss: [2.429521, 0.3698653, 4.4891768] | Test Loss: [2.4423344, 0.3166211, 4.5680475]\n",
      "511: Train Loss: [2.4334753, 0.37227708, 4.4946733] | Test Loss: [2.6644409, 0.3468869, 4.9819946]\n",
      "512: Train Loss: [2.5339875, 0.3686218, 4.699353] | Test Loss: [1.9364196, 0.2184508, 3.6543884]\n",
      "513: Train Loss: [2.529526, 0.3119229, 4.747129] | Test Loss: [2.425349, 0.34233543, 4.508363]\n",
      "514: Train Loss: [2.4903646, 0.35086304, 4.629866] | Test Loss: [2.6236494, 0.30249974, 4.944799]\n",
      "515: Train Loss: [2.5781803, 0.36852324, 4.7878375] | Test Loss: [2.760107, 0.3831786, 5.1370354]\n",
      "516: Train Loss: [2.495439, 0.31685156, 4.6740265] | Test Loss: [2.576291, 0.44316968, 4.7094126]\n",
      "517: Train Loss: [2.3493376, 0.3228453, 4.3758297] | Test Loss: [2.565523, 0.3223705, 4.8086753]\n",
      "518: Train Loss: [2.5507116, 0.30193278, 4.7994905] | Test Loss: [2.4801633, 0.3336796, 4.626647]\n",
      "519: Train Loss: [2.4774318, 0.427074, 4.5277896] | Test Loss: [2.6080148, 0.3636516, 4.852378]\n",
      "520: Train Loss: [2.471531, 0.34718984, 4.595872] | Test Loss: [2.47294, 0.41780648, 4.5280733]\n",
      "521: Train Loss: [2.5888772, 0.33221135, 4.845543] | Test Loss: [2.4034772, 0.40729058, 4.399664]\n",
      "522: Train Loss: [2.488699, 0.3398542, 4.6375437] | Test Loss: [2.5964084, 0.44764903, 4.7451677]\n",
      "523: Train Loss: [2.3275518, 0.32635826, 4.3287454] | Test Loss: [2.289767, 0.29832828, 4.2812057]\n",
      "Epoch 10\n",
      "0: Train Loss: [2.2365358, 0.3137455, 4.159326] | Test Loss: [2.595875, 0.42093667, 4.7708135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Train Loss: [2.3524017, 0.34131315, 4.36349] | Test Loss: [2.6906586, 0.31836373, 5.0629535]\n",
      "2: Train Loss: [2.3271518, 0.34492797, 4.309376] | Test Loss: [2.7595556, 0.35022458, 5.1688867]\n",
      "3: Train Loss: [2.146562, 0.36057425, 3.93255] | Test Loss: [2.6549182, 0.3519267, 4.9579096]\n",
      "4: Train Loss: [2.3297944, 0.316684, 4.342905] | Test Loss: [2.8338318, 0.3434049, 5.324259]\n",
      "5: Train Loss: [2.1410837, 0.40429366, 3.877874] | Test Loss: [2.663093, 0.3500142, 4.976172]\n",
      "6: Train Loss: [2.366209, 0.48481536, 4.2476025] | Test Loss: [2.496388, 0.41561666, 4.5771594]\n",
      "7: Train Loss: [2.4509575, 0.4494318, 4.452483] | Test Loss: [2.3195357, 0.38851303, 4.2505584]\n",
      "8: Train Loss: [2.2358053, 0.33636186, 4.1352487] | Test Loss: [2.5260832, 0.37185633, 4.6803102]\n",
      "9: Train Loss: [2.2105534, 0.43869403, 3.9824128] | Test Loss: [2.7200885, 0.42758247, 5.0125947]\n",
      "10: Train Loss: [2.2396655, 0.41099662, 4.0683346] | Test Loss: [2.7356033, 0.35584286, 5.1153636]\n",
      "11: Train Loss: [2.0978663, 0.38457444, 3.811158] | Test Loss: [2.3586752, 0.3603248, 4.3570256]\n",
      "12: Train Loss: [2.2144341, 0.3222238, 4.1066446] | Test Loss: [2.615345, 0.43207222, 4.798618]\n",
      "13: Train Loss: [2.2419016, 0.32413313, 4.1596704] | Test Loss: [2.631229, 0.45238867, 4.810069]\n",
      "14: Train Loss: [2.235221, 0.3685719, 4.10187] | Test Loss: [2.7052212, 0.31681418, 5.093628]\n",
      "15: Train Loss: [2.1563747, 0.3194806, 3.993269] | Test Loss: [2.7552207, 0.36100933, 5.149432]\n",
      "16: Train Loss: [2.1978545, 0.31793842, 4.0777707] | Test Loss: [2.7417789, 0.40104228, 5.0825152]\n",
      "17: Train Loss: [2.2415757, 0.40793538, 4.075216] | Test Loss: [2.5823858, 0.3326692, 4.8321023]\n",
      "18: Train Loss: [2.2361827, 0.3326824, 4.139683] | Test Loss: [2.7042043, 0.33438438, 5.074024]\n",
      "19: Train Loss: [2.1492426, 0.41602927, 3.8824558] | Test Loss: [2.5216959, 0.32683843, 4.716553]\n",
      "20: Train Loss: [2.2831376, 0.3587366, 4.2075386] | Test Loss: [2.4639418, 0.42459455, 4.503289]\n",
      "21: Train Loss: [2.2150757, 0.35992318, 4.070228] | Test Loss: [2.7473953, 0.3540116, 5.140779]\n",
      "22: Train Loss: [2.4767706, 0.3951265, 4.558415] | Test Loss: [2.6962843, 0.4041934, 4.988375]\n",
      "23: Train Loss: [2.280783, 0.3856596, 4.175906] | Test Loss: [2.4314127, 0.39474458, 4.468081]\n",
      "24: Train Loss: [2.2642403, 0.33707204, 4.1914086] | Test Loss: [2.590991, 0.30984572, 4.872136]\n",
      "25: Train Loss: [2.3302524, 0.37311146, 4.2873936] | Test Loss: [2.45426, 0.34243393, 4.5660863]\n",
      "26: Train Loss: [2.3333483, 0.4385978, 4.228099] | Test Loss: [2.660886, 0.3907434, 4.931029]\n",
      "27: Train Loss: [2.2446356, 0.39739776, 4.091873] | Test Loss: [2.6663156, 0.3296414, 5.00299]\n",
      "28: Train Loss: [2.3474305, 0.29933006, 4.3955307] | Test Loss: [2.654356, 0.38414264, 4.9245696]\n",
      "29: Train Loss: [2.3377016, 0.47108397, 4.204319] | Test Loss: [2.453784, 0.38271058, 4.5248575]\n",
      "30: Train Loss: [2.2780263, 0.3382589, 4.217794] | Test Loss: [2.7119343, 0.43557984, 4.988289]\n",
      "31: Train Loss: [2.1976352, 0.31459683, 4.0806737] | Test Loss: [2.6408358, 0.41661465, 4.865057]\n",
      "32: Train Loss: [2.310167, 0.3642916, 4.2560425] | Test Loss: [2.4009216, 0.35537767, 4.4464655]\n",
      "33: Train Loss: [2.2139857, 0.35983926, 4.068132] | Test Loss: [2.5280602, 0.3758961, 4.6802244]\n",
      "34: Train Loss: [2.4177692, 0.38467067, 4.4508677] | Test Loss: [2.6396074, 0.32039687, 4.958818]\n",
      "35: Train Loss: [2.2971268, 0.42689127, 4.167362] | Test Loss: [2.7355719, 0.3763519, 5.094792]\n",
      "36: Train Loss: [2.4191318, 0.36806837, 4.4701953] | Test Loss: [2.7486372, 0.3527729, 5.1445017]\n",
      "37: Train Loss: [2.3569062, 0.38600618, 4.327806] | Test Loss: [2.5768878, 0.43456632, 4.719209]\n",
      "38: Train Loss: [2.318108, 0.34639207, 4.289824] | Test Loss: [2.693725, 0.36741552, 5.020035]\n",
      "39: Train Loss: [2.151565, 0.3949075, 3.9082227] | Test Loss: [2.7590036, 0.42787644, 5.090131]\n",
      "40: Train Loss: [2.2340467, 0.37761897, 4.0904746] | Test Loss: [2.830927, 0.386083, 5.2757707]\n",
      "41: Train Loss: [2.2740378, 0.32153934, 4.2265363] | Test Loss: [2.4850044, 0.3581906, 4.6118183]\n",
      "42: Train Loss: [2.2861872, 0.33926022, 4.2331142] | Test Loss: [2.608433, 0.36290875, 4.853957]\n",
      "43: Train Loss: [2.244552, 0.34797022, 4.141134] | Test Loss: [2.628231, 0.33176684, 4.924695]\n",
      "44: Train Loss: [2.1789906, 0.328635, 4.029346] | Test Loss: [2.6333914, 0.34889185, 4.917891]\n",
      "45: Train Loss: [2.3082047, 0.31957373, 4.2968354] | Test Loss: [2.425576, 0.3258453, 4.5253067]\n",
      "46: Train Loss: [2.1516716, 0.36507592, 3.9382672] | Test Loss: [2.5077193, 0.32214957, 4.693289]\n",
      "47: Train Loss: [2.4801672, 0.3324357, 4.6278987] | Test Loss: [2.5138655, 0.34954113, 4.6781898]\n",
      "48: Train Loss: [2.2334633, 0.36795175, 4.0989747] | Test Loss: [2.1678052, 0.61848927, 3.7171211]\n",
      "49: Train Loss: [2.3443296, 0.3420619, 4.346597] | Test Loss: [2.5617645, 0.3711422, 4.7523866]\n",
      "50: Train Loss: [2.265094, 0.35199934, 4.178189] | Test Loss: [2.557632, 0.3599074, 4.7553563]\n",
      "51: Train Loss: [2.2072692, 0.3405966, 4.0739417] | Test Loss: [2.6752205, 0.3108059, 5.039635]\n",
      "52: Train Loss: [2.2800033, 0.35455832, 4.205448] | Test Loss: [2.8842635, 0.43875545, 5.3297715]\n",
      "53: Train Loss: [2.352694, 0.32254308, 4.382845] | Test Loss: [2.4883077, 0.37280598, 4.6038094]\n",
      "54: Train Loss: [2.319999, 0.3196348, 4.320363] | Test Loss: [2.5205739, 0.40908825, 4.6320596]\n",
      "55: Train Loss: [2.4237745, 0.38626727, 4.461282] | Test Loss: [2.627588, 0.32570332, 4.929473]\n",
      "56: Train Loss: [2.4146926, 0.34241703, 4.486968] | Test Loss: [2.7030392, 0.38957235, 5.016506]\n",
      "57: Train Loss: [2.3451455, 0.35682842, 4.3334627] | Test Loss: [2.5570998, 0.35126325, 4.7629366]\n",
      "58: Train Loss: [2.242008, 0.39451763, 4.0894985] | Test Loss: [2.6730506, 0.36122647, 4.9848747]\n",
      "59: Train Loss: [2.3088696, 0.33552334, 4.282216] | Test Loss: [2.5971282, 0.36375386, 4.8305025]\n",
      "60: Train Loss: [2.321617, 0.37469992, 4.2685337] | Test Loss: [2.5584898, 0.35688502, 4.7600946]\n",
      "61: Train Loss: [2.2624784, 0.32652172, 4.198435] | Test Loss: [2.3495367, 0.35465702, 4.344416]\n",
      "62: Train Loss: [2.225216, 0.30567798, 4.144754] | Test Loss: [2.8972554, 0.35547224, 5.4390388]\n",
      "63: Train Loss: [2.4789503, 0.37497526, 4.5829253] | Test Loss: [2.6411602, 0.36866066, 4.91366]\n",
      "64: Train Loss: [2.2719011, 0.31213585, 4.2316666] | Test Loss: [2.4863484, 0.33379987, 4.638897]\n",
      "65: Train Loss: [2.2872276, 0.35795426, 4.2165008] | Test Loss: [2.560826, 0.41582578, 4.7058263]\n",
      "66: Train Loss: [2.2011564, 0.38450572, 4.017807] | Test Loss: [2.5323832, 0.3894333, 4.675333]\n",
      "67: Train Loss: [2.2629173, 0.30902508, 4.2168093] | Test Loss: [2.671505, 0.32845554, 5.0145545]\n",
      "68: Train Loss: [2.242849, 0.27957764, 4.2061205] | Test Loss: [2.501312, 0.32281074, 4.6798134]\n",
      "69: Train Loss: [2.1660354, 0.3696663, 3.9624045] | Test Loss: [2.6023276, 0.3873583, 4.817297]\n",
      "70: Train Loss: [2.3993375, 0.3852849, 4.41339] | Test Loss: [2.5690582, 0.4233072, 4.714809]\n",
      "71: Train Loss: [2.4201663, 0.34043998, 4.4998927] | Test Loss: [2.5966415, 0.4587613, 4.734522]\n",
      "72: Train Loss: [2.38374, 0.42232436, 4.3451557] | Test Loss: [2.5018501, 0.34200636, 4.661694]\n",
      "73: Train Loss: [2.4203746, 0.33102638, 4.5097227] | Test Loss: [2.5250685, 0.33371305, 4.716424]\n",
      "74: Train Loss: [2.2912736, 0.35408854, 4.2284584] | Test Loss: [2.786254, 0.37744117, 5.195067]\n",
      "75: Train Loss: [2.372259, 0.37217095, 4.372347] | Test Loss: [2.589779, 0.38943037, 4.7901273]\n",
      "76: Train Loss: [2.4698298, 0.36513343, 4.5745263] | Test Loss: [2.768773, 0.35288325, 5.184663]\n",
      "77: Train Loss: [2.3497891, 0.3828302, 4.316748] | Test Loss: [2.4098456, 0.378972, 4.440719]\n",
      "78: Train Loss: [2.3766115, 0.30422595, 4.448997] | Test Loss: [2.5123208, 0.37125063, 4.653391]\n",
      "79: Train Loss: [2.2356575, 0.39139786, 4.079917] | Test Loss: [2.845052, 0.35105753, 5.3390465]\n",
      "80: Train Loss: [2.2058964, 0.30983907, 4.1019535] | Test Loss: [2.461065, 0.31856474, 4.603565]\n",
      "81: Train Loss: [2.4198222, 0.3157771, 4.523867] | Test Loss: [2.5631645, 0.43620887, 4.69012]\n",
      "82: Train Loss: [2.5079215, 0.41142112, 4.6044216] | Test Loss: [2.6114552, 0.25898167, 4.9639287]\n",
      "83: Train Loss: [2.4398847, 0.395262, 4.4845076] | Test Loss: [2.7305, 0.34974653, 5.1112533]\n",
      "84: Train Loss: [2.346328, 0.5215612, 4.171095] | Test Loss: [2.7055469, 0.34747967, 5.063614]\n",
      "85: Train Loss: [2.406606, 0.40751225, 4.4056997] | Test Loss: [2.5329127, 0.37331843, 4.692507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86: Train Loss: [2.3079722, 0.38180894, 4.2341356] | Test Loss: [2.5893383, 0.41321602, 4.7654605]\n",
      "87: Train Loss: [2.3230236, 0.37478846, 4.271259] | Test Loss: [2.6957788, 0.3840309, 5.007527]\n",
      "88: Train Loss: [2.4637454, 0.33934164, 4.588149] | Test Loss: [2.7102983, 0.3777901, 5.0428066]\n",
      "89: Train Loss: [2.3985755, 0.3778024, 4.4193487] | Test Loss: [2.6881335, 0.40451995, 4.971747]\n",
      "90: Train Loss: [2.5289466, 0.5035387, 4.5543547] | Test Loss: [2.5702536, 0.36073273, 4.7797747]\n",
      "91: Train Loss: [2.5024319, 0.39482227, 4.6100416] | Test Loss: [2.6225636, 0.3806352, 4.864492]\n",
      "92: Train Loss: [2.4788136, 0.38093084, 4.5766964] | Test Loss: [2.6981876, 0.35668552, 5.0396895]\n",
      "93: Train Loss: [2.3011742, 0.33954304, 4.2628055] | Test Loss: [2.4924233, 0.36227745, 4.622569]\n",
      "94: Train Loss: [2.518073, 0.38311264, 4.6530337] | Test Loss: [2.5767522, 0.31577116, 4.8377333]\n",
      "95: Train Loss: [2.3268094, 0.3417218, 4.311897] | Test Loss: [2.6624913, 0.3665123, 4.9584703]\n",
      "96: Train Loss: [2.4052336, 0.35185614, 4.458611] | Test Loss: [2.6564825, 0.34331843, 4.9696465]\n",
      "97: Train Loss: [2.4367392, 0.36340722, 4.5100713] | Test Loss: [2.6760693, 0.35346788, 4.9986706]\n",
      "98: Train Loss: [2.4947355, 0.32903653, 4.6604342] | Test Loss: [2.686621, 0.39358157, 4.9796605]\n",
      "99: Train Loss: [2.522892, 0.35765502, 4.688129] | Test Loss: [2.5242069, 0.3688859, 4.6795278]\n",
      "100: Train Loss: [2.5352879, 0.50001365, 4.570562] | Test Loss: [2.7885025, 0.40755066, 5.169454]\n",
      "101: Train Loss: [2.4813864, 0.34463263, 4.61814] | Test Loss: [2.740599, 0.3991639, 5.082034]\n",
      "102: Train Loss: [2.3537304, 0.3237499, 4.383711] | Test Loss: [2.6053762, 0.3459168, 4.8648357]\n",
      "103: Train Loss: [2.4131386, 0.41552475, 4.4107523] | Test Loss: [2.5239604, 0.32477942, 4.723141]\n",
      "104: Train Loss: [2.380652, 0.31486654, 4.4464374] | Test Loss: [2.8437796, 0.45039645, 5.2371626]\n",
      "105: Train Loss: [2.2910292, 0.35912007, 4.2229385] | Test Loss: [2.6246467, 0.5006664, 4.748627]\n",
      "106: Train Loss: [2.3108368, 0.3665874, 4.2550864] | Test Loss: [2.5493, 0.40508762, 4.6935124]\n",
      "107: Train Loss: [2.5055633, 0.3277962, 4.6833305] | Test Loss: [2.4107947, 0.3486823, 4.472907]\n",
      "108: Train Loss: [2.31971, 0.29591545, 4.3435044] | Test Loss: [2.53282, 0.34288022, 4.7227597]\n",
      "109: Train Loss: [2.3744447, 0.31278744, 4.436102] | Test Loss: [2.6913753, 0.37203297, 5.0107174]\n",
      "110: Train Loss: [2.2477531, 0.42285395, 4.0726523] | Test Loss: [2.681217, 0.34888634, 5.0135474]\n",
      "111: Train Loss: [2.3671093, 0.3578557, 4.376363] | Test Loss: [2.542381, 0.42695346, 4.657809]\n",
      "112: Train Loss: [2.434256, 0.293087, 4.575425] | Test Loss: [2.4420774, 0.3263957, 4.5577593]\n",
      "113: Train Loss: [2.346073, 0.30928576, 4.38286] | Test Loss: [2.8015683, 0.3907381, 5.2123985]\n",
      "114: Train Loss: [2.4427829, 0.34788844, 4.5376773] | Test Loss: [2.596385, 0.40976787, 4.783002]\n",
      "115: Train Loss: [2.3222728, 0.34370303, 4.300843] | Test Loss: [2.629513, 0.35763118, 4.901395]\n",
      "116: Train Loss: [2.35002, 0.29963237, 4.4004073] | Test Loss: [2.7192655, 0.38005027, 5.0584807]\n",
      "117: Train Loss: [2.4786763, 0.38140598, 4.575947] | Test Loss: [2.4682565, 0.3523062, 4.5842066]\n",
      "118: Train Loss: [2.437797, 0.32253817, 4.553056] | Test Loss: [2.7757988, 0.37255624, 5.1790414]\n",
      "119: Train Loss: [2.3184712, 0.3965594, 4.240383] | Test Loss: [2.6040535, 0.3142822, 4.8938246]\n",
      "120: Train Loss: [2.4113626, 0.36833113, 4.4543943] | Test Loss: [2.4966626, 0.42330498, 4.57002]\n",
      "121: Train Loss: [2.4969604, 0.32018182, 4.673739] | Test Loss: [2.6631896, 0.35995355, 4.966426]\n",
      "122: Train Loss: [2.2977319, 0.36342892, 4.2320347] | Test Loss: [2.596715, 0.37641066, 4.8170195]\n",
      "123: Train Loss: [2.4827754, 0.3495989, 4.615952] | Test Loss: [2.6239402, 0.39797723, 4.849903]\n",
      "124: Train Loss: [2.2497501, 0.30574498, 4.193755] | Test Loss: [2.678876, 0.35271093, 5.005041]\n",
      "125: Train Loss: [2.4649465, 0.30669615, 4.623197] | Test Loss: [2.6348112, 0.5082521, 4.76137]\n",
      "126: Train Loss: [2.2866254, 0.34061742, 4.232633] | Test Loss: [2.7885878, 0.4242063, 5.1529694]\n",
      "127: Train Loss: [2.3828979, 0.31628317, 4.4495125] | Test Loss: [2.6990032, 0.36859658, 5.02941]\n",
      "128: Train Loss: [2.4243572, 0.32810333, 4.520611] | Test Loss: [2.492512, 0.43075186, 4.554272]\n",
      "129: Train Loss: [2.4030688, 0.32900706, 4.4771304] | Test Loss: [2.622643, 0.36120722, 4.884079]\n",
      "130: Train Loss: [2.2445009, 0.48481303, 4.0041885] | Test Loss: [2.6823514, 0.3265404, 5.038162]\n",
      "131: Train Loss: [2.5621126, 0.33066097, 4.7935643] | Test Loss: [2.6632645, 0.36354247, 4.9629865]\n",
      "132: Train Loss: [2.4116192, 0.419856, 4.4033823] | Test Loss: [2.509496, 0.2890878, 4.729904]\n",
      "133: Train Loss: [2.381685, 0.4060686, 4.357301] | Test Loss: [2.7326922, 0.39607286, 5.0693116]\n",
      "134: Train Loss: [2.3654008, 0.3403022, 4.3904996] | Test Loss: [2.800716, 0.35439965, 5.247032]\n",
      "135: Train Loss: [2.443474, 0.415369, 4.471579] | Test Loss: [2.6186042, 0.34706655, 4.890142]\n",
      "136: Train Loss: [2.3556004, 0.4003182, 4.3108826] | Test Loss: [2.417229, 0.36577016, 4.4686875]\n",
      "137: Train Loss: [2.4250946, 0.40955785, 4.4406314] | Test Loss: [2.602446, 0.3239342, 4.880958]\n",
      "138: Train Loss: [2.3059103, 0.35791245, 4.253908] | Test Loss: [2.622518, 0.28349677, 4.9615393]\n",
      "139: Train Loss: [2.3495898, 0.39930364, 4.299876] | Test Loss: [2.7154963, 0.36869717, 5.0622954]\n",
      "140: Train Loss: [2.3874311, 0.32640433, 4.4484577] | Test Loss: [2.575704, 0.4500625, 4.7013454]\n",
      "141: Train Loss: [2.6335986, 0.35062933, 4.916568] | Test Loss: [2.4580078, 0.3697633, 4.5462523]\n",
      "142: Train Loss: [2.3251812, 0.37071785, 4.2796445] | Test Loss: [2.499245, 0.30880806, 4.689682]\n",
      "143: Train Loss: [2.3466806, 0.33627567, 4.3570857] | Test Loss: [2.636105, 0.33032086, 4.9418893]\n",
      "144: Train Loss: [2.4610174, 0.3678211, 4.5542135] | Test Loss: [2.7069209, 0.38540858, 5.0284333]\n",
      "145: Train Loss: [2.3706236, 0.31418055, 4.427067] | Test Loss: [2.6932173, 0.36650604, 5.0199285]\n",
      "146: Train Loss: [2.4496775, 0.35696054, 4.542394] | Test Loss: [2.6941137, 0.41967618, 4.968551]\n",
      "147: Train Loss: [2.4245355, 0.34489334, 4.5041776] | Test Loss: [2.5843325, 0.3711087, 4.7975564]\n",
      "148: Train Loss: [2.2242594, 0.3448744, 4.1036444] | Test Loss: [2.6424673, 0.3773595, 4.907575]\n",
      "149: Train Loss: [2.39192, 0.33043477, 4.4534054] | Test Loss: [2.6316931, 0.35011837, 4.913268]\n",
      "150: Train Loss: [2.513583, 0.37935528, 4.6478105] | Test Loss: [2.534982, 0.3270237, 4.7429404]\n",
      "151: Train Loss: [2.413537, 0.33258396, 4.49449] | Test Loss: [2.7137995, 0.44544894, 4.98215]\n",
      "152: Train Loss: [2.4039516, 0.31180283, 4.4961004] | Test Loss: [2.7387445, 0.36373788, 5.113751]\n",
      "153: Train Loss: [2.4402785, 0.32376164, 4.5567956] | Test Loss: [2.7158465, 0.43202224, 4.999671]\n",
      "154: Train Loss: [2.390418, 0.30558428, 4.4752517] | Test Loss: [2.57088, 0.46742323, 4.6743364]\n",
      "155: Train Loss: [2.4704003, 0.361845, 4.5789557] | Test Loss: [2.6154604, 0.35862237, 4.8722982]\n",
      "156: Train Loss: [2.4177566, 0.45272395, 4.382789] | Test Loss: [2.404856, 0.31517845, 4.4945335]\n",
      "157: Train Loss: [2.324561, 0.34453285, 4.3045893] | Test Loss: [2.889417, 0.40442166, 5.374412]\n",
      "158: Train Loss: [2.3749435, 0.33416218, 4.4157248] | Test Loss: [2.4886134, 0.35672274, 4.620504]\n",
      "159: Train Loss: [2.2025309, 0.39748025, 4.0075817] | Test Loss: [2.479268, 0.33246413, 4.626072]\n",
      "160: Train Loss: [2.3690593, 0.36431086, 4.373808] | Test Loss: [2.6047986, 0.36736855, 4.8422284]\n",
      "161: Train Loss: [2.266709, 0.37996504, 4.1534534] | Test Loss: [2.661859, 0.36530954, 4.9584084]\n",
      "162: Train Loss: [2.4232335, 0.35626483, 4.4902024] | Test Loss: [2.4683442, 0.30822057, 4.628468]\n",
      "163: Train Loss: [2.4126966, 0.34633547, 4.479058] | Test Loss: [2.5592902, 0.33745617, 4.781124]\n",
      "164: Train Loss: [2.4436285, 0.339109, 4.548148] | Test Loss: [2.6628985, 0.4489957, 4.8768015]\n",
      "165: Train Loss: [2.187656, 0.37296557, 4.002346] | Test Loss: [2.713236, 0.3065106, 5.1199617]\n",
      "166: Train Loss: [2.4228253, 0.28518975, 4.560461] | Test Loss: [2.697632, 0.42189276, 4.9733715]\n",
      "167: Train Loss: [2.4980164, 0.38397992, 4.612053] | Test Loss: [2.688648, 0.32627362, 5.0510225]\n",
      "168: Train Loss: [2.471071, 0.2900626, 4.6520796] | Test Loss: [2.7320094, 0.3336724, 5.1303463]\n",
      "169: Train Loss: [2.5643368, 0.37069392, 4.75798] | Test Loss: [2.5974808, 0.44783482, 4.7471266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170: Train Loss: [2.4861307, 0.33788794, 4.6343737] | Test Loss: [2.510138, 0.31285807, 4.707418]\n",
      "171: Train Loss: [2.5234256, 0.33271313, 4.714138] | Test Loss: [2.4900873, 0.35059798, 4.6295767]\n",
      "172: Train Loss: [2.2912467, 0.34363684, 4.2388563] | Test Loss: [2.6868992, 0.32959935, 5.044199]\n",
      "173: Train Loss: [2.516138, 0.3991603, 4.633116] | Test Loss: [2.5763762, 0.3542262, 4.7985263]\n",
      "174: Train Loss: [2.423786, 0.30838, 4.5391917] | Test Loss: [2.5330946, 0.3418904, 4.724299]\n",
      "175: Train Loss: [2.3477943, 0.3500668, 4.345522] | Test Loss: [2.7722905, 0.4523446, 5.0922365]\n",
      "176: Train Loss: [2.4541392, 0.3936332, 4.514645] | Test Loss: [2.7061746, 0.33065742, 5.0816917]\n",
      "177: Train Loss: [2.517375, 0.3943609, 4.640389] | Test Loss: [2.5588078, 0.33943516, 4.7781806]\n",
      "178: Train Loss: [2.4732394, 0.31953332, 4.6269455] | Test Loss: [2.701655, 0.42066118, 4.982649]\n",
      "179: Train Loss: [2.3334837, 0.478405, 4.1885624] | Test Loss: [2.7277818, 0.3607371, 5.0948267]\n",
      "180: Train Loss: [2.3497317, 0.3125707, 4.386893] | Test Loss: [2.450303, 0.34040794, 4.5601983]\n",
      "181: Train Loss: [2.4336832, 0.42195806, 4.4454083] | Test Loss: [2.7839937, 0.3327785, 5.235209]\n",
      "182: Train Loss: [2.3325717, 0.34300122, 4.322142] | Test Loss: [2.7646296, 0.40523106, 5.124028]\n",
      "183: Train Loss: [2.4091368, 0.37709498, 4.441179] | Test Loss: [2.6493015, 0.44315815, 4.855445]\n",
      "184: Train Loss: [2.3755434, 0.40815547, 4.3429313] | Test Loss: [2.6779456, 0.36660823, 4.989283]\n",
      "185: Train Loss: [2.496598, 0.39228532, 4.6009107] | Test Loss: [2.71607, 0.39690602, 5.035234]\n",
      "186: Train Loss: [2.3614452, 0.35052764, 4.3723626] | Test Loss: [2.833147, 0.3745358, 5.291758]\n",
      "187: Train Loss: [2.3622618, 0.33693388, 4.3875895] | Test Loss: [2.618209, 0.41266927, 4.8237486]\n",
      "188: Train Loss: [2.3919039, 0.43558025, 4.3482275] | Test Loss: [2.436378, 0.33974254, 4.5330133]\n",
      "189: Train Loss: [2.3510485, 0.3426478, 4.3594494] | Test Loss: [2.361798, 0.3449052, 4.3786907]\n",
      "190: Train Loss: [2.2635784, 0.35057306, 4.176584] | Test Loss: [2.6802227, 0.38374218, 4.976703]\n",
      "191: Train Loss: [2.5800462, 0.39920074, 4.7608914] | Test Loss: [2.7152967, 0.30211934, 5.128474]\n",
      "192: Train Loss: [2.4363368, 0.34218103, 4.5304923] | Test Loss: [2.734153, 0.30912033, 5.159186]\n",
      "193: Train Loss: [2.4651213, 0.38608536, 4.544157] | Test Loss: [2.6397235, 0.3741941, 4.905253]\n",
      "194: Train Loss: [2.550179, 0.3540604, 4.746298] | Test Loss: [2.7072387, 0.3978163, 5.016661]\n",
      "195: Train Loss: [2.3535924, 0.30932143, 4.3978634] | Test Loss: [2.4239717, 0.36136124, 4.4865823]\n",
      "196: Train Loss: [2.4182947, 0.35543546, 4.481154] | Test Loss: [2.4746141, 0.33818763, 4.6110406]\n",
      "197: Train Loss: [2.439118, 0.34253064, 4.535705] | Test Loss: [2.898195, 0.46110123, 5.335289]\n",
      "198: Train Loss: [2.3257895, 0.41117442, 4.2404046] | Test Loss: [2.1407697, 0.31582037, 3.9657192]\n",
      "199: Train Loss: [2.4019315, 0.3193846, 4.4844785] | Test Loss: [2.347311, 0.317589, 4.377033]\n",
      "200: Train Loss: [2.3511415, 0.422532, 4.279751] | Test Loss: [2.7767775, 0.33687022, 5.216685]\n",
      "201: Train Loss: [2.4088762, 0.29085776, 4.5268946] | Test Loss: [2.6285539, 0.3156707, 4.9414372]\n",
      "202: Train Loss: [2.4415753, 0.36589324, 4.517257] | Test Loss: [2.6543858, 0.329217, 4.9795547]\n",
      "203: Train Loss: [2.3595843, 0.31665608, 4.4025126] | Test Loss: [2.729216, 0.3836579, 5.0747743]\n",
      "204: Train Loss: [2.2982416, 0.34240472, 4.2540784] | Test Loss: [2.7305756, 0.38204405, 5.0791073]\n",
      "205: Train Loss: [2.5050488, 0.41276926, 4.597328] | Test Loss: [2.6170118, 0.36310256, 4.870921]\n",
      "206: Train Loss: [2.4981976, 0.38909623, 4.607299] | Test Loss: [2.6145132, 0.35977823, 4.869248]\n",
      "207: Train Loss: [2.4491997, 0.3868189, 4.5115805] | Test Loss: [2.5374558, 0.36413348, 4.710778]\n",
      "208: Train Loss: [2.4393451, 0.30940062, 4.5692897] | Test Loss: [2.6825993, 0.37357098, 4.9916277]\n",
      "209: Train Loss: [2.494536, 0.31622955, 4.6728425] | Test Loss: [2.4637697, 0.43377763, 4.4937615]\n",
      "210: Train Loss: [2.5701435, 0.5008643, 4.6394224] | Test Loss: [2.4976614, 0.36513087, 4.630192]\n",
      "211: Train Loss: [2.3156493, 0.3352943, 4.2960043] | Test Loss: [2.728232, 0.34670493, 5.109759]\n",
      "212: Train Loss: [2.3553526, 0.32104942, 4.389656] | Test Loss: [2.6181085, 0.3672298, 4.868987]\n",
      "213: Train Loss: [2.3943481, 0.3585177, 4.4301786] | Test Loss: [2.5793438, 0.37568942, 4.782998]\n",
      "214: Train Loss: [2.4336867, 0.36976045, 4.497613] | Test Loss: [2.6185956, 0.3602369, 4.876954]\n",
      "215: Train Loss: [2.3357806, 0.2970459, 4.3745155] | Test Loss: [2.6052055, 0.32091796, 4.889493]\n",
      "216: Train Loss: [2.5620148, 0.35406864, 4.769961] | Test Loss: [2.8001957, 0.4030164, 5.197375]\n",
      "217: Train Loss: [2.4415684, 0.36442298, 4.518714] | Test Loss: [2.575312, 0.43646896, 4.7141547]\n",
      "218: Train Loss: [2.343755, 0.37415966, 4.31335] | Test Loss: [2.4696636, 0.36197835, 4.5773487]\n",
      "219: Train Loss: [2.4201689, 0.31172588, 4.5286117] | Test Loss: [2.576332, 0.36536717, 4.787297]\n",
      "220: Train Loss: [2.3900087, 0.37857965, 4.4014378] | Test Loss: [2.659346, 0.43204293, 4.886649]\n",
      "221: Train Loss: [2.3514888, 0.3169846, 4.385993] | Test Loss: [2.6377084, 0.40321305, 4.872204]\n",
      "222: Train Loss: [2.4102912, 0.35810712, 4.4624753] | Test Loss: [2.6726027, 0.34103125, 5.004174]\n",
      "223: Train Loss: [2.4288864, 0.4026002, 4.4551725] | Test Loss: [2.8547254, 0.43131533, 5.2781353]\n",
      "224: Train Loss: [2.346693, 0.3398238, 4.3535624] | Test Loss: [2.4683135, 0.33830258, 4.5983243]\n",
      "225: Train Loss: [2.4521842, 0.37255576, 4.5318127] | Test Loss: [2.5943108, 0.33479905, 4.8538227]\n",
      "226: Train Loss: [2.2334292, 0.3350772, 4.131781] | Test Loss: [2.6947417, 0.38459876, 5.0048847]\n",
      "227: Train Loss: [2.3490798, 0.3129048, 4.385255] | Test Loss: [2.637821, 0.34666282, 4.928979]\n",
      "228: Train Loss: [2.4839883, 0.40702015, 4.5609565] | Test Loss: [2.6774583, 0.627812, 4.7271047]\n",
      "229: Train Loss: [2.2851639, 0.3228432, 4.2474847] | Test Loss: [2.5461361, 0.421045, 4.6712275]\n",
      "230: Train Loss: [2.372287, 0.38340843, 4.3611655] | Test Loss: [2.6093664, 0.3275984, 4.8911343]\n",
      "231: Train Loss: [2.4969764, 0.42610574, 4.567847] | Test Loss: [2.5940793, 0.36097208, 4.8271866]\n",
      "232: Train Loss: [2.4441857, 0.3664839, 4.521888] | Test Loss: [2.5685737, 0.38212162, 4.755026]\n",
      "233: Train Loss: [2.4109054, 0.3082269, 4.5135837] | Test Loss: [2.632392, 0.38670316, 4.878081]\n",
      "234: Train Loss: [2.5239847, 0.3046208, 4.7433486] | Test Loss: [2.670849, 0.39403048, 4.9476676]\n",
      "235: Train Loss: [2.2721627, 0.33942538, 4.2049] | Test Loss: [2.4686215, 0.29703134, 4.6402116]\n",
      "236: Train Loss: [2.3929865, 0.3536392, 4.432334] | Test Loss: [2.7965126, 0.3288414, 5.264184]\n",
      "237: Train Loss: [2.4670923, 0.37476927, 4.5594153] | Test Loss: [2.652894, 0.34780625, 4.9579816]\n",
      "238: Train Loss: [2.3686128, 0.46818337, 4.269042] | Test Loss: [2.8925433, 0.40164885, 5.3834376]\n",
      "239: Train Loss: [2.3632543, 0.346549, 4.3799596] | Test Loss: [2.635938, 0.35547072, 4.916405]\n",
      "240: Train Loss: [2.500869, 0.36493376, 4.636804] | Test Loss: [2.6164703, 0.34471616, 4.8882246]\n",
      "241: Train Loss: [2.4865687, 0.41437775, 4.5587597] | Test Loss: [2.6732783, 0.38941348, 4.9571433]\n",
      "242: Train Loss: [2.448355, 0.36043885, 4.536271] | Test Loss: [2.6351554, 0.36923283, 4.901078]\n",
      "243: Train Loss: [2.3247008, 0.35577723, 4.2936244] | Test Loss: [2.620363, 0.41808146, 4.8226447]\n",
      "244: Train Loss: [2.4950764, 0.32383332, 4.6663194] | Test Loss: [2.5294266, 0.3699515, 4.6889014]\n",
      "245: Train Loss: [2.4957497, 0.3695955, 4.621904] | Test Loss: [2.4594414, 0.42647025, 4.4924126]\n",
      "246: Train Loss: [2.4467099, 0.31913206, 4.574288] | Test Loss: [2.6286685, 0.43077093, 4.826566]\n",
      "247: Train Loss: [2.278558, 0.33076712, 4.226349] | Test Loss: [2.5736918, 0.42778224, 4.7196016]\n",
      "248: Train Loss: [2.494752, 0.39472783, 4.594776] | Test Loss: [2.629287, 0.39125288, 4.867321]\n",
      "249: Train Loss: [2.2826996, 0.45544794, 4.109951] | Test Loss: [2.5987856, 0.33841354, 4.8591576]\n",
      "250: Train Loss: [2.5191977, 0.44172058, 4.596675] | Test Loss: [2.6764908, 0.35920852, 4.993773]\n",
      "251: Train Loss: [2.4696105, 0.36777285, 4.571448] | Test Loss: [2.5976565, 0.39690068, 4.7984123]\n",
      "252: Train Loss: [2.360361, 0.39595634, 4.3247657] | Test Loss: [2.572889, 0.38868302, 4.7570953]\n",
      "253: Train Loss: [2.1813645, 0.35942236, 4.003307] | Test Loss: [2.7389593, 0.42637372, 5.0515447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254: Train Loss: [2.4700234, 0.41228873, 4.527758] | Test Loss: [2.7370763, 0.34111607, 5.1330366]\n",
      "255: Train Loss: [2.4991562, 0.3526333, 4.645679] | Test Loss: [2.670281, 0.3211174, 5.0194445]\n",
      "256: Train Loss: [2.468894, 0.33495122, 4.6028366] | Test Loss: [2.5706487, 0.359117, 4.7821803]\n",
      "257: Train Loss: [2.4027762, 0.44340172, 4.3621507] | Test Loss: [2.4413238, 0.37865472, 4.503993]\n",
      "258: Train Loss: [2.4163132, 0.32125986, 4.5113664] | Test Loss: [2.6887026, 0.33974826, 5.037657]\n",
      "259: Train Loss: [2.4300091, 0.4169548, 4.4430633] | Test Loss: [2.5124366, 0.36956072, 4.6553125]\n",
      "260: Train Loss: [2.3756201, 0.40011126, 4.351129] | Test Loss: [2.4513617, 0.37076592, 4.5319576]\n",
      "261: Train Loss: [2.4645226, 0.3129569, 4.6160884] | Test Loss: [2.5328186, 0.4009377, 4.6646996]\n",
      "262: Train Loss: [2.5008667, 0.28938758, 4.7123456] | Test Loss: [2.6106355, 0.3407255, 4.8805456]\n",
      "263: Train Loss: [2.4571943, 0.35278973, 4.561599] | Test Loss: [2.7766721, 0.39277938, 5.160565]\n",
      "264: Train Loss: [2.5157425, 0.32981685, 4.7016683] | Test Loss: [2.7147174, 0.377486, 5.051949]\n",
      "265: Train Loss: [2.5705216, 0.36418432, 4.776859] | Test Loss: [2.6251416, 0.32701707, 4.923266]\n",
      "266: Train Loss: [2.501422, 0.3238974, 4.6789465] | Test Loss: [2.708159, 0.37335083, 5.0429673]\n",
      "267: Train Loss: [2.3694746, 0.29129463, 4.4476547] | Test Loss: [2.620794, 0.30674446, 4.9348435]\n",
      "268: Train Loss: [2.4467447, 0.33995506, 4.5535345] | Test Loss: [2.419667, 0.3738227, 4.4655113]\n",
      "269: Train Loss: [2.4614136, 0.30654356, 4.616284] | Test Loss: [2.7840972, 0.35105985, 5.2171345]\n",
      "270: Train Loss: [2.5777776, 0.46287292, 4.6926823] | Test Loss: [2.8280163, 0.46524927, 5.1907835]\n",
      "271: Train Loss: [2.5898876, 0.34544757, 4.8343277] | Test Loss: [2.8266006, 0.3773886, 5.2758126]\n",
      "272: Train Loss: [2.3525147, 0.32875973, 4.37627] | Test Loss: [2.5937939, 0.33606446, 4.8515234]\n",
      "273: Train Loss: [2.5148864, 0.34059504, 4.6891775] | Test Loss: [2.6396174, 0.38866794, 4.890567]\n",
      "274: Train Loss: [2.3202884, 0.36066103, 4.279916] | Test Loss: [2.576139, 0.31491554, 4.8373623]\n",
      "275: Train Loss: [2.491474, 0.35109848, 4.6318493] | Test Loss: [2.6362326, 0.37973106, 4.892734]\n",
      "276: Train Loss: [2.3767939, 0.37730592, 4.3762817] | Test Loss: [2.83272, 0.37088376, 5.294556]\n",
      "277: Train Loss: [2.5260644, 0.3110513, 4.7410774] | Test Loss: [2.4901123, 0.41814992, 4.5620747]\n",
      "278: Train Loss: [2.4276597, 0.33574718, 4.5195723] | Test Loss: [2.5968301, 0.32540837, 4.868252]\n",
      "279: Train Loss: [2.4574435, 0.3547773, 4.5601096] | Test Loss: [2.6810956, 0.45691407, 4.9052773]\n",
      "280: Train Loss: [2.288477, 0.3450175, 4.2319365] | Test Loss: [2.6303294, 0.36338204, 4.897277]\n",
      "281: Train Loss: [2.4648535, 0.32423365, 4.6054735] | Test Loss: [2.7678564, 0.3667016, 5.169011]\n",
      "282: Train Loss: [2.5048134, 0.49316514, 4.516462] | Test Loss: [2.4688277, 0.42314345, 4.514512]\n",
      "283: Train Loss: [2.4751606, 0.35156345, 4.5987577] | Test Loss: [2.6611147, 0.36463845, 4.957591]\n",
      "284: Train Loss: [2.4817407, 0.41728067, 4.5462008] | Test Loss: [2.762788, 0.36114594, 5.16443]\n",
      "285: Train Loss: [2.3908215, 0.40347755, 4.3781652] | Test Loss: [2.7997115, 0.357582, 5.241841]\n",
      "286: Train Loss: [2.4344585, 0.3344565, 4.5344605] | Test Loss: [2.6979802, 0.43098396, 4.9649763]\n",
      "287: Train Loss: [2.5141513, 0.39525694, 4.6330457] | Test Loss: [2.605651, 0.3710291, 4.840273]\n",
      "288: Train Loss: [2.4510512, 0.38356167, 4.518541] | Test Loss: [2.410739, 0.38223258, 4.439245]\n",
      "289: Train Loss: [2.5288675, 0.35015523, 4.7075796] | Test Loss: [2.6282341, 0.3255192, 4.930949]\n",
      "290: Train Loss: [2.5001307, 0.29500043, 4.7052608] | Test Loss: [2.822624, 0.51255953, 5.1326885]\n",
      "291: Train Loss: [2.4466443, 0.3513014, 4.5419874] | Test Loss: [2.5741382, 0.42484722, 4.723429]\n",
      "292: Train Loss: [2.5457265, 0.40767816, 4.683775] | Test Loss: [2.70462, 0.31137505, 5.0978646]\n",
      "293: Train Loss: [2.462726, 0.3308741, 4.5945783] | Test Loss: [2.7671907, 0.33618367, 5.198198]\n",
      "294: Train Loss: [2.5565834, 0.42223445, 4.6909323] | Test Loss: [2.4915252, 0.44545957, 4.537591]\n",
      "295: Train Loss: [2.4245293, 0.37533492, 4.473724] | Test Loss: [2.6175566, 0.3619231, 4.87319]\n",
      "296: Train Loss: [2.4320683, 0.32301542, 4.5411215] | Test Loss: [2.730358, 0.38735119, 5.0733647]\n",
      "297: Train Loss: [2.3610225, 0.3458089, 4.376236] | Test Loss: [2.5160975, 0.31962824, 4.712567]\n",
      "298: Train Loss: [2.4620473, 0.3947032, 4.5293913] | Test Loss: [2.6418536, 0.37311485, 4.910592]\n",
      "299: Train Loss: [2.46956, 0.36318234, 4.5759373] | Test Loss: [2.5270915, 0.3386925, 4.7154903]\n",
      "300: Train Loss: [2.2421296, 0.31995207, 4.164307] | Test Loss: [2.4738026, 0.43170547, 4.5158997]\n",
      "301: Train Loss: [2.5557642, 0.38431957, 4.7272086] | Test Loss: [2.5427961, 0.31521556, 4.7703767]\n",
      "302: Train Loss: [2.5229616, 0.33185315, 4.7140703] | Test Loss: [2.628188, 0.33131227, 4.9250636]\n",
      "303: Train Loss: [2.5303485, 0.30801478, 4.752682] | Test Loss: [2.777993, 0.3857826, 5.170203]\n",
      "304: Train Loss: [2.5167258, 0.48846766, 4.544984] | Test Loss: [2.6720839, 0.37362558, 4.970542]\n",
      "305: Train Loss: [2.4774356, 0.36541983, 4.5894513] | Test Loss: [2.6899443, 0.35942373, 5.020465]\n",
      "306: Train Loss: [2.4879267, 0.30835083, 4.6675024] | Test Loss: [2.7387779, 0.43925485, 5.038301]\n",
      "307: Train Loss: [2.3607788, 0.34743002, 4.3741274] | Test Loss: [2.7553458, 0.4418983, 5.0687933]\n",
      "308: Train Loss: [2.467515, 0.4516877, 4.483342] | Test Loss: [2.695971, 0.3476921, 5.04425]\n",
      "309: Train Loss: [2.5184956, 0.32777557, 4.7092156] | Test Loss: [2.5514116, 0.36143976, 4.7413836]\n",
      "310: Train Loss: [2.4038672, 0.3100508, 4.4976835] | Test Loss: [2.566375, 0.4101527, 4.722597]\n",
      "311: Train Loss: [2.5388987, 0.36788777, 4.7099094] | Test Loss: [2.744942, 0.3417028, 5.148181]\n",
      "312: Train Loss: [2.4672692, 0.3164504, 4.618088] | Test Loss: [2.6879706, 0.33351833, 5.042423]\n",
      "313: Train Loss: [2.515384, 0.31839886, 4.712369] | Test Loss: [2.5658734, 0.34399363, 4.787753]\n",
      "314: Train Loss: [2.5503917, 0.3817675, 4.7190156] | Test Loss: [2.547692, 0.35618633, 4.7391977]\n",
      "315: Train Loss: [2.286827, 0.39442125, 4.179233] | Test Loss: [2.638315, 0.3361634, 4.9404664]\n",
      "316: Train Loss: [2.4760618, 0.41251543, 4.539608] | Test Loss: [2.6283457, 0.3666844, 4.890007]\n",
      "317: Train Loss: [2.3417883, 0.3867457, 4.2968307] | Test Loss: [2.6386817, 0.37210333, 4.90526]\n",
      "318: Train Loss: [2.2227151, 0.34424314, 4.101187] | Test Loss: [2.4745426, 0.67897993, 4.2701054]\n",
      "319: Train Loss: [2.4290566, 0.34253842, 4.515575] | Test Loss: [2.7609649, 0.38488647, 5.1370435]\n",
      "320: Train Loss: [2.4624588, 0.3672655, 4.557652] | Test Loss: [2.6381392, 0.38607413, 4.8902044]\n",
      "321: Train Loss: [2.4130583, 0.3218808, 4.5042357] | Test Loss: [2.4589813, 0.3551237, 4.562839]\n",
      "322: Train Loss: [2.3678498, 0.36323982, 4.37246] | Test Loss: [2.4345353, 0.3736538, 4.4954166]\n",
      "323: Train Loss: [2.4898381, 0.31779754, 4.6618786] | Test Loss: [2.6608384, 0.33416826, 4.9875083]\n",
      "324: Train Loss: [2.5007753, 0.35684243, 4.644708] | Test Loss: [2.652771, 0.43622515, 4.869317]\n",
      "325: Train Loss: [2.5290453, 0.51598406, 4.5421066] | Test Loss: [2.4315445, 0.37008035, 4.4930086]\n",
      "326: Train Loss: [2.419652, 0.37696594, 4.462338] | Test Loss: [2.5281105, 0.33826524, 4.7179556]\n",
      "327: Train Loss: [2.5384429, 0.33865345, 4.738232] | Test Loss: [2.901988, 0.36018234, 5.443794]\n",
      "328: Train Loss: [2.3523366, 0.32440272, 4.3802705] | Test Loss: [2.5827315, 0.4448314, 4.7206316]\n",
      "329: Train Loss: [2.539818, 0.33814624, 4.74149] | Test Loss: [2.6150174, 0.33684754, 4.8931875]\n",
      "330: Train Loss: [2.4481914, 0.31763667, 4.5787463] | Test Loss: [2.5802968, 0.30564168, 4.854952]\n",
      "331: Train Loss: [2.5126407, 0.40990576, 4.6153755] | Test Loss: [2.613422, 0.40282938, 4.8240147]\n",
      "332: Train Loss: [2.443913, 0.38722104, 4.500605] | Test Loss: [2.6115193, 0.41549486, 4.8075438]\n",
      "333: Train Loss: [2.5340967, 0.3193171, 4.7488766] | Test Loss: [2.700368, 0.43491563, 4.9658203]\n",
      "334: Train Loss: [2.5819597, 0.39774907, 4.7661705] | Test Loss: [2.7487884, 0.3982881, 5.0992885]\n",
      "335: Train Loss: [2.645972, 0.42239177, 4.869552] | Test Loss: [2.699666, 0.35144687, 5.047885]\n",
      "336: Train Loss: [2.3315754, 0.34946755, 4.313683] | Test Loss: [2.5380092, 0.39821002, 4.6778083]\n",
      "337: Train Loss: [2.3791885, 0.37317798, 4.385199] | Test Loss: [2.6681159, 0.393225, 4.9430065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338: Train Loss: [2.4319916, 0.37539265, 4.4885907] | Test Loss: [2.6616838, 0.30821255, 5.015155]\n",
      "339: Train Loss: [2.4802985, 0.3971069, 4.5634904] | Test Loss: [2.745505, 0.3320113, 5.158999]\n",
      "340: Train Loss: [2.5829487, 0.33175287, 4.8341446] | Test Loss: [2.4852386, 0.37626088, 4.5942163]\n",
      "341: Train Loss: [2.5205133, 0.39188072, 4.649146] | Test Loss: [2.5665395, 0.35109204, 4.781987]\n",
      "342: Train Loss: [2.4724898, 0.3634161, 4.5815635] | Test Loss: [2.9521875, 0.44287145, 5.4615035]\n",
      "343: Train Loss: [2.3821528, 0.322648, 4.4416575] | Test Loss: [2.681668, 0.3751031, 4.988233]\n",
      "344: Train Loss: [2.4918528, 0.39964157, 4.584064] | Test Loss: [2.658628, 0.3525118, 4.964744]\n",
      "345: Train Loss: [2.309828, 0.37731248, 4.2423434] | Test Loss: [2.7483966, 0.31255853, 5.1842346]\n",
      "346: Train Loss: [2.4152749, 0.38065118, 4.4498987] | Test Loss: [2.573887, 0.35211504, 4.795659]\n",
      "347: Train Loss: [2.3813055, 0.33663782, 4.425973] | Test Loss: [2.6080487, 0.34695673, 4.8691406]\n",
      "348: Train Loss: [2.4640858, 0.4316085, 4.496563] | Test Loss: [2.592988, 0.34554887, 4.8404274]\n",
      "349: Train Loss: [2.4414327, 0.32176888, 4.5610967] | Test Loss: [2.6692767, 0.3231813, 5.0153723]\n",
      "350: Train Loss: [2.450023, 0.35723212, 4.542814] | Test Loss: [2.4774392, 0.39127305, 4.5636053]\n",
      "351: Train Loss: [2.4098182, 0.3844028, 4.4352336] | Test Loss: [2.5756404, 0.44122428, 4.710057]\n",
      "352: Train Loss: [2.3689868, 0.36041632, 4.3775573] | Test Loss: [2.6036203, 0.40020955, 4.807031]\n",
      "353: Train Loss: [2.500978, 0.43844804, 4.563508] | Test Loss: [2.6001072, 0.48336232, 4.716852]\n",
      "354: Train Loss: [2.4513583, 0.40347248, 4.499244] | Test Loss: [2.7540286, 0.39590827, 5.112149]\n",
      "355: Train Loss: [2.5324738, 0.3777505, 4.687197] | Test Loss: [2.6215026, 0.37369028, 4.869315]\n",
      "356: Train Loss: [2.4009638, 0.32040125, 4.4815264] | Test Loss: [2.676184, 0.34769678, 5.004671]\n",
      "357: Train Loss: [2.3035073, 0.3642642, 4.2427506] | Test Loss: [2.5023925, 0.34081447, 4.6639705]\n",
      "358: Train Loss: [2.50007, 0.48103404, 4.519106] | Test Loss: [2.5614226, 0.3142912, 4.808554]\n",
      "359: Train Loss: [2.40625, 0.3155818, 4.496918] | Test Loss: [2.713503, 0.3413943, 5.0856113]\n",
      "360: Train Loss: [2.4540966, 0.32852215, 4.579671] | Test Loss: [2.63754, 0.36507142, 4.910009]\n",
      "361: Train Loss: [2.5037105, 0.38458017, 4.622841] | Test Loss: [2.83506, 0.3586007, 5.311519]\n",
      "362: Train Loss: [2.5584524, 0.37568903, 4.7412157] | Test Loss: [2.5762079, 0.29917204, 4.853244]\n",
      "363: Train Loss: [2.3662536, 0.35188538, 4.380622] | Test Loss: [2.6984985, 0.319557, 5.07744]\n",
      "364: Train Loss: [2.3877573, 0.41198465, 4.36353] | Test Loss: [2.5590851, 0.36688602, 4.751284]\n",
      "365: Train Loss: [2.5805259, 0.32224247, 4.8388095] | Test Loss: [2.7725468, 0.4359749, 5.1091185]\n",
      "366: Train Loss: [2.521688, 0.33263493, 4.710741] | Test Loss: [2.5740466, 0.37529412, 4.772799]\n",
      "367: Train Loss: [2.6933246, 0.3677445, 5.0189047] | Test Loss: [2.5417864, 0.3455437, 4.738029]\n",
      "368: Train Loss: [2.4806082, 0.3393036, 4.621913] | Test Loss: [2.5938394, 0.3278845, 4.859794]\n",
      "369: Train Loss: [2.3852, 0.32595593, 4.444444] | Test Loss: [2.4357095, 0.39604074, 4.475378]\n",
      "370: Train Loss: [2.5548239, 0.38320863, 4.726439] | Test Loss: [2.6009982, 0.3535904, 4.848406]\n",
      "371: Train Loss: [2.3392885, 0.36878636, 4.3097906] | Test Loss: [2.6398623, 0.36313412, 4.9165907]\n",
      "372: Train Loss: [2.518762, 0.4119843, 4.62554] | Test Loss: [2.7103202, 0.35724992, 5.0633907]\n",
      "373: Train Loss: [2.482525, 0.38631952, 4.5787306] | Test Loss: [2.8016484, 0.33887175, 5.264425]\n",
      "374: Train Loss: [2.355944, 0.3255939, 4.386294] | Test Loss: [2.347764, 0.3971746, 4.298353]\n",
      "375: Train Loss: [2.5205746, 0.33099818, 4.7101507] | Test Loss: [2.7003438, 0.4066407, 4.994047]\n",
      "376: Train Loss: [2.553828, 0.38601342, 4.7216425] | Test Loss: [2.5894125, 0.36077216, 4.818053]\n",
      "377: Train Loss: [2.4690049, 0.37192816, 4.5660815] | Test Loss: [2.8670237, 0.41287553, 5.3211718]\n",
      "378: Train Loss: [2.5433788, 0.3703452, 4.7164125] | Test Loss: [2.2645388, 0.3080797, 4.220998]\n",
      "379: Train Loss: [2.5680087, 0.4659643, 4.670053] | Test Loss: [2.720424, 0.3707098, 5.070138]\n",
      "380: Train Loss: [2.5031888, 0.39967114, 4.6067066] | Test Loss: [2.697968, 0.33866325, 5.057273]\n",
      "381: Train Loss: [2.4734366, 0.33530277, 4.6115704] | Test Loss: [2.555779, 0.36816096, 4.7433968]\n",
      "382: Train Loss: [2.500317, 0.39835694, 4.6022773] | Test Loss: [2.619267, 0.33107874, 4.9074554]\n",
      "383: Train Loss: [2.5503025, 0.3456854, 4.7549195] | Test Loss: [2.6704822, 0.32339087, 5.0175734]\n",
      "384: Train Loss: [2.4625795, 0.33812577, 4.5870333] | Test Loss: [2.7222548, 0.33349246, 5.111017]\n",
      "385: Train Loss: [2.5359566, 0.34073228, 4.731181] | Test Loss: [2.6250284, 0.4136751, 4.8363814]\n",
      "386: Train Loss: [2.5779629, 0.38969398, 4.7662315] | Test Loss: [2.6521416, 0.304307, 4.999976]\n",
      "387: Train Loss: [2.3532426, 0.39722398, 4.3092613] | Test Loss: [2.588602, 0.3123057, 4.864898]\n",
      "388: Train Loss: [2.3887298, 0.3699929, 4.407467] | Test Loss: [2.7126725, 0.4239331, 5.001412]\n",
      "389: Train Loss: [2.3947768, 0.34917608, 4.4403777] | Test Loss: [2.6602535, 0.37779742, 4.9427094]\n",
      "390: Train Loss: [2.390601, 0.35992715, 4.4212747] | Test Loss: [2.588849, 0.33627832, 4.8414197]\n",
      "391: Train Loss: [2.6719046, 0.37638763, 4.9674215] | Test Loss: [2.5957606, 0.35669413, 4.834827]\n",
      "392: Train Loss: [2.4102383, 0.38383782, 4.436639] | Test Loss: [2.544094, 0.42098692, 4.667201]\n",
      "393: Train Loss: [2.4687376, 0.36336032, 4.574115] | Test Loss: [2.5092556, 0.3593805, 4.6591306]\n",
      "394: Train Loss: [2.2540474, 0.31661683, 4.191478] | Test Loss: [2.5036764, 0.3472507, 4.660102]\n",
      "395: Train Loss: [2.309524, 0.37018427, 4.2488637] | Test Loss: [2.8019416, 0.34783643, 5.256047]\n",
      "396: Train Loss: [2.5560775, 0.41920924, 4.692946] | Test Loss: [2.5571585, 0.38864946, 4.7256675]\n",
      "397: Train Loss: [2.56301, 0.3528766, 4.7731433] | Test Loss: [2.5566883, 0.32755747, 4.785819]\n",
      "398: Train Loss: [2.5229738, 0.3074676, 4.73848] | Test Loss: [2.556742, 0.33267286, 4.780811]\n",
      "399: Train Loss: [2.3588374, 0.31684875, 4.400826] | Test Loss: [2.7840483, 0.44686374, 5.121233]\n",
      "400: Train Loss: [2.5975828, 0.35325035, 4.841915] | Test Loss: [2.7421246, 0.44191813, 5.042331]\n",
      "401: Train Loss: [2.5079145, 0.30191407, 4.713915] | Test Loss: [2.4429426, 0.39036998, 4.4955153]\n",
      "402: Train Loss: [2.458769, 0.34812397, 4.569414] | Test Loss: [2.4912736, 0.39226744, 4.59028]\n",
      "403: Train Loss: [2.5041382, 0.36315852, 4.6451178] | Test Loss: [2.7303686, 0.37478778, 5.0859494]\n",
      "404: Train Loss: [2.5308244, 0.386669, 4.6749797] | Test Loss: [2.6570477, 0.3998525, 4.9142427]\n",
      "405: Train Loss: [2.4981525, 0.36859703, 4.627708] | Test Loss: [2.6204789, 0.41340682, 4.827551]\n",
      "406: Train Loss: [2.4106386, 0.3272036, 4.4940734] | Test Loss: [2.7359362, 0.41579938, 5.056073]\n",
      "407: Train Loss: [2.3761847, 0.34069782, 4.4116716] | Test Loss: [2.5348835, 0.34699517, 4.7227716]\n",
      "408: Train Loss: [2.5463734, 0.3662516, 4.7264953] | Test Loss: [2.242975, 0.32042396, 4.165526]\n",
      "409: Train Loss: [2.5967786, 0.31750345, 4.876054] | Test Loss: [2.4768922, 0.34013927, 4.613645]\n",
      "410: Train Loss: [2.4985456, 0.36097217, 4.636119] | Test Loss: [2.6718493, 0.33598003, 5.0077186]\n",
      "411: Train Loss: [2.408013, 0.39159507, 4.4244313] | Test Loss: [2.6150792, 0.31860495, 4.9115534]\n",
      "412: Train Loss: [2.51223, 0.3338579, 4.690602] | Test Loss: [2.6352146, 0.3654741, 4.904955]\n",
      "413: Train Loss: [2.4036832, 0.3414918, 4.4658747] | Test Loss: [2.6876576, 0.34209734, 5.033218]\n",
      "414: Train Loss: [2.4409342, 0.34145167, 4.5404167] | Test Loss: [2.5140593, 0.3474049, 4.6807137]\n",
      "415: Train Loss: [2.3597636, 0.2920781, 4.427449] | Test Loss: [2.5752738, 0.38239515, 4.768152]\n",
      "416: Train Loss: [2.6196938, 0.44828314, 4.7911043] | Test Loss: [2.534844, 0.34543347, 4.7242546]\n",
      "417: Train Loss: [2.503551, 0.3084848, 4.698617] | Test Loss: [2.6556835, 0.33305347, 4.9783134]\n",
      "418: Train Loss: [2.3920367, 0.38316748, 4.400906] | Test Loss: [2.7048705, 0.40397522, 5.005766]\n",
      "419: Train Loss: [2.4980013, 0.33934453, 4.656658] | Test Loss: [2.606458, 0.3937784, 4.8191376]\n",
      "420: Train Loss: [2.594298, 0.36983514, 4.818761] | Test Loss: [2.5721078, 0.34049782, 4.8037176]\n",
      "421: Train Loss: [2.3995843, 0.4057939, 4.3933744] | Test Loss: [2.509987, 0.36570203, 4.654272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422: Train Loss: [2.3814895, 0.34183484, 4.421144] | Test Loss: [2.564889, 0.34533906, 4.7844386]\n",
      "423: Train Loss: [2.4062924, 0.3525543, 4.4600306] | Test Loss: [2.6118562, 0.354781, 4.8689313]\n",
      "424: Train Loss: [2.351824, 0.2861595, 4.4174886] | Test Loss: [2.702952, 0.45293674, 4.952967]\n",
      "425: Train Loss: [2.3578894, 0.38046357, 4.335315] | Test Loss: [2.6217146, 0.3159986, 4.9274306]\n",
      "426: Train Loss: [2.4342535, 0.30179074, 4.566716] | Test Loss: [2.7633529, 0.442086, 5.0846195]\n",
      "427: Train Loss: [2.4805899, 0.4375902, 4.5235896] | Test Loss: [2.5020816, 0.36163607, 4.642527]\n",
      "428: Train Loss: [2.486728, 0.34704113, 4.626415] | Test Loss: [2.5267353, 0.3333624, 4.720108]\n",
      "429: Train Loss: [2.4725912, 0.38664976, 4.5585327] | Test Loss: [2.5155125, 0.34037614, 4.690649]\n",
      "430: Train Loss: [2.4977307, 0.2747578, 4.7207036] | Test Loss: [2.6645713, 0.421137, 4.9080057]\n",
      "431: Train Loss: [2.5744987, 0.34881887, 4.8001785] | Test Loss: [2.5537028, 0.38225597, 4.7251496]\n",
      "432: Train Loss: [2.511523, 0.32894722, 4.694099] | Test Loss: [2.575051, 0.3776004, 4.772502]\n",
      "433: Train Loss: [2.425318, 0.31402788, 4.536608] | Test Loss: [2.6484792, 0.37584803, 4.9211106]\n",
      "434: Train Loss: [2.2947996, 0.29400447, 4.2955947] | Test Loss: [2.7710185, 0.41464806, 5.127389]\n",
      "435: Train Loss: [2.437857, 0.384603, 4.491111] | Test Loss: [2.5928442, 0.35801974, 4.8276687]\n",
      "436: Train Loss: [2.4113274, 0.3382751, 4.48438] | Test Loss: [2.807285, 0.5219253, 5.0926447]\n",
      "437: Train Loss: [2.4322968, 0.3798825, 4.484711] | Test Loss: [2.4467845, 0.39693773, 4.496631]\n",
      "438: Train Loss: [2.5924253, 0.3996675, 4.7851834] | Test Loss: [2.992443, 0.344375, 5.640511]\n",
      "439: Train Loss: [2.565412, 0.36335343, 4.767471] | Test Loss: [2.5336287, 0.36040196, 4.7068553]\n",
      "440: Train Loss: [2.4072895, 0.41635597, 4.398223] | Test Loss: [2.7431405, 0.3412096, 5.1450715]\n",
      "441: Train Loss: [2.4848466, 0.38854215, 4.581151] | Test Loss: [2.7453165, 0.40587115, 5.084762]\n",
      "442: Train Loss: [2.2227464, 0.31346935, 4.1320233] | Test Loss: [2.587426, 0.4137363, 4.7611156]\n",
      "443: Train Loss: [2.3812463, 0.35936972, 4.403123] | Test Loss: [2.5478141, 0.41146272, 4.6841655]\n",
      "444: Train Loss: [2.531818, 0.45224214, 4.611394] | Test Loss: [2.6972392, 0.32797578, 5.0665026]\n",
      "445: Train Loss: [2.524274, 0.2975496, 4.7509985] | Test Loss: [2.528552, 0.30547792, 4.751626]\n",
      "446: Train Loss: [2.5056932, 0.33188304, 4.6795034] | Test Loss: [2.6800847, 0.3738075, 4.986362]\n",
      "447: Train Loss: [2.4526572, 0.38502863, 4.5202856] | Test Loss: [2.657512, 0.3323537, 4.9826703]\n",
      "448: Train Loss: [2.5410857, 0.38183093, 4.7003403] | Test Loss: [2.507392, 0.34560657, 4.6691775]\n",
      "449: Train Loss: [2.3215113, 0.36260343, 4.280419] | Test Loss: [2.5217605, 0.27681568, 4.766705]\n",
      "450: Train Loss: [2.6647565, 0.37476775, 4.9547453] | Test Loss: [2.5845792, 0.36675283, 4.802406]\n",
      "451: Train Loss: [2.5542526, 0.351751, 4.7567544] | Test Loss: [2.5386033, 0.4247299, 4.652477]\n",
      "452: Train Loss: [2.4146545, 0.36316878, 4.4661403] | Test Loss: [2.626513, 0.3714432, 4.8815827]\n",
      "453: Train Loss: [2.4212694, 0.4132311, 4.429308] | Test Loss: [2.7267544, 0.41557056, 5.037938]\n",
      "454: Train Loss: [2.602323, 0.41128182, 4.793364] | Test Loss: [2.5767024, 0.36671093, 4.7866936]\n",
      "455: Train Loss: [2.4635053, 0.36816734, 4.558843] | Test Loss: [2.6793826, 0.3504317, 5.008333]\n",
      "456: Train Loss: [2.54214, 0.42736593, 4.656914] | Test Loss: [2.6028042, 0.32279998, 4.882808]\n",
      "457: Train Loss: [2.4973946, 0.38582623, 4.608963] | Test Loss: [2.7848303, 0.37345988, 5.196201]\n",
      "458: Train Loss: [2.3913457, 0.2983533, 4.4843383] | Test Loss: [2.7272, 0.39159688, 5.0628033]\n",
      "459: Train Loss: [2.538264, 0.3642765, 4.7122517] | Test Loss: [2.597419, 0.41591442, 4.7789235]\n",
      "460: Train Loss: [2.4161117, 0.34967378, 4.4825497] | Test Loss: [2.7819061, 0.4763935, 5.0874186]\n",
      "461: Train Loss: [2.4017186, 0.33722606, 4.4662113] | Test Loss: [2.5280285, 0.30574453, 4.7503123]\n",
      "462: Train Loss: [2.450511, 0.29167658, 4.6093454] | Test Loss: [2.7148156, 0.4045173, 5.025114]\n",
      "463: Train Loss: [2.4443364, 0.37677, 4.511903] | Test Loss: [2.53608, 0.38483372, 4.687326]\n",
      "464: Train Loss: [2.5322793, 0.3851172, 4.6794415] | Test Loss: [2.8600647, 0.4203051, 5.299824]\n",
      "465: Train Loss: [2.4299698, 0.36958838, 4.490351] | Test Loss: [2.4290576, 0.32701978, 4.5310955]\n",
      "466: Train Loss: [2.523494, 0.3492049, 4.697783] | Test Loss: [2.4682071, 0.3837941, 4.55262]\n",
      "467: Train Loss: [2.5271518, 0.40519688, 4.649107] | Test Loss: [2.6587088, 0.4091359, 4.908282]\n",
      "468: Train Loss: [2.3243513, 0.3682058, 4.2804966] | Test Loss: [2.7313144, 0.34019038, 5.1224384]\n",
      "469: Train Loss: [2.2514365, 0.32781845, 4.1750546] | Test Loss: [2.6765506, 0.3666219, 4.9864793]\n",
      "470: Train Loss: [2.4294925, 0.3121561, 4.5468287] | Test Loss: [2.6417787, 0.37425664, 4.909301]\n",
      "471: Train Loss: [2.5485375, 0.3963747, 4.7007003] | Test Loss: [2.6270483, 0.2991467, 4.95495]\n",
      "472: Train Loss: [2.4644985, 0.40114188, 4.527855] | Test Loss: [2.755206, 0.3627652, 5.147647]\n",
      "473: Train Loss: [2.3035116, 0.32076812, 4.2862554] | Test Loss: [2.5957727, 0.35105592, 4.8404894]\n",
      "474: Train Loss: [2.5651364, 0.39313334, 4.7371397] | Test Loss: [2.6615639, 0.30540377, 5.017724]\n",
      "475: Train Loss: [2.4698658, 0.3311714, 4.60856] | Test Loss: [2.5284853, 0.33682358, 4.720147]\n",
      "476: Train Loss: [2.375079, 0.37900788, 4.37115] | Test Loss: [2.5531635, 0.35635766, 4.7499695]\n",
      "477: Train Loss: [2.5874727, 0.34255114, 4.832394] | Test Loss: [2.5496202, 0.38636878, 4.7128716]\n",
      "478: Train Loss: [2.40705, 0.35474297, 4.459357] | Test Loss: [2.48178, 0.39686933, 4.566691]\n",
      "479: Train Loss: [2.3353987, 0.3474085, 4.323389] | Test Loss: [2.6578333, 0.3185704, 4.997096]\n",
      "480: Train Loss: [2.49131, 0.31880048, 4.6638193] | Test Loss: [2.67543, 0.3533905, 4.9974694]\n",
      "481: Train Loss: [2.4588735, 0.33398837, 4.583759] | Test Loss: [2.5635824, 0.38701832, 4.7401466]\n",
      "482: Train Loss: [2.3397624, 0.33312672, 4.3463984] | Test Loss: [2.6683536, 0.36840504, 4.9683022]\n",
      "483: Train Loss: [2.524896, 0.42778188, 4.6220098] | Test Loss: [2.5734315, 0.35621676, 4.790646]\n",
      "484: Train Loss: [2.5208948, 0.314021, 4.7277684] | Test Loss: [2.4320464, 0.34701943, 4.5170736]\n",
      "485: Train Loss: [2.4839745, 0.40219617, 4.5657525] | Test Loss: [2.638011, 0.34587157, 4.9301505]\n",
      "486: Train Loss: [2.3934765, 0.41816607, 4.368787] | Test Loss: [2.6203642, 0.35572267, 4.8850055]\n",
      "487: Train Loss: [2.5031304, 0.41474453, 4.5915165] | Test Loss: [2.7868338, 0.3611309, 5.212537]\n",
      "488: Train Loss: [2.363636, 0.33514553, 4.3921266] | Test Loss: [2.6202192, 0.38165525, 4.8587832]\n",
      "489: Train Loss: [2.5324414, 0.34088668, 4.723996] | Test Loss: [2.4466193, 0.3934972, 4.4997416]\n",
      "490: Train Loss: [2.450194, 0.30426782, 4.59612] | Test Loss: [2.6751175, 0.44282582, 4.907409]\n",
      "491: Train Loss: [2.5936124, 0.33465275, 4.852572] | Test Loss: [2.6334279, 0.3744879, 4.892368]\n",
      "492: Train Loss: [2.3553417, 0.3525528, 4.3581305] | Test Loss: [2.7353697, 0.48588964, 4.98485]\n",
      "493: Train Loss: [2.4253464, 0.3520406, 4.498652] | Test Loss: [2.491818, 0.39702556, 4.5866103]\n",
      "494: Train Loss: [2.4626193, 0.30088133, 4.624357] | Test Loss: [2.606334, 0.34589317, 4.8667746]\n",
      "495: Train Loss: [2.3743348, 0.3424698, 4.4062] | Test Loss: [2.6150403, 0.45985675, 4.770224]\n",
      "496: Train Loss: [2.493106, 0.34871164, 4.6375003] | Test Loss: [2.5766273, 0.41144428, 4.7418103]\n",
      "497: Train Loss: [2.5251873, 0.38815492, 4.6622195] | Test Loss: [2.7897303, 0.4226348, 5.156826]\n",
      "498: Train Loss: [2.4381812, 0.38056377, 4.4957986] | Test Loss: [2.693292, 0.4187478, 4.967836]\n",
      "499: Train Loss: [2.5950387, 0.3227404, 4.8673368] | Test Loss: [2.6011677, 0.3345654, 4.8677697]\n",
      "500: Train Loss: [2.3530643, 0.3243659, 4.3817625] | Test Loss: [2.722648, 0.38759992, 5.057696]\n",
      "501: Train Loss: [2.4259474, 0.3164021, 4.535493] | Test Loss: [2.650869, 0.36628562, 4.935452]\n",
      "502: Train Loss: [2.3843536, 0.34110668, 4.4276004] | Test Loss: [2.5567, 0.3557291, 4.757671]\n",
      "503: Train Loss: [2.456576, 0.32094976, 4.5922027] | Test Loss: [2.4841752, 0.4402866, 4.528064]\n",
      "504: Train Loss: [2.3839238, 0.27291784, 4.49493] | Test Loss: [2.4144924, 0.4014908, 4.427494]\n",
      "505: Train Loss: [2.4444952, 0.34249875, 4.5464916] | Test Loss: [2.6534624, 0.3794063, 4.9275184]\n",
      "506: Train Loss: [2.5853672, 0.30254596, 4.8681884] | Test Loss: [2.610787, 0.37518126, 4.8463926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507: Train Loss: [2.6491566, 0.4135167, 4.8847966] | Test Loss: [2.4826722, 0.3326336, 4.632711]\n",
      "508: Train Loss: [2.4536033, 0.34179002, 4.5654163] | Test Loss: [2.5078607, 0.37113726, 4.644584]\n",
      "509: Train Loss: [2.4601946, 0.39047012, 4.529919] | Test Loss: [2.5499578, 0.3567426, 4.743173]\n",
      "510: Train Loss: [2.6790216, 0.33354393, 5.0244994] | Test Loss: [2.696973, 0.3675445, 5.0264015]\n",
      "511: Train Loss: [2.5137327, 0.33901036, 4.688455] | Test Loss: [2.7138016, 0.45100346, 4.9765997]\n",
      "512: Train Loss: [2.5133727, 0.3153442, 4.711401] | Test Loss: [2.5578973, 0.3511326, 4.7646623]\n",
      "513: Train Loss: [2.547748, 0.5089938, 4.5865026] | Test Loss: [2.5607266, 0.38618478, 4.7352686]\n",
      "514: Train Loss: [2.4843743, 0.33832037, 4.6304283] | Test Loss: [2.5138276, 0.3286671, 4.698988]\n",
      "515: Train Loss: [2.5755203, 0.31989944, 4.831141] | Test Loss: [2.4824216, 0.33960664, 4.6252365]\n",
      "516: Train Loss: [2.3797596, 0.3366389, 4.42288] | Test Loss: [2.7981963, 0.45881513, 5.1375775]\n",
      "517: Train Loss: [2.5385642, 0.43464, 4.6424885] | Test Loss: [2.6820319, 0.40128937, 4.9627743]\n",
      "518: Train Loss: [2.472601, 0.31126833, 4.6339335] | Test Loss: [2.7135856, 0.4555015, 4.9716697]\n",
      "519: Train Loss: [2.5087538, 0.36051196, 4.656996] | Test Loss: [2.487004, 0.39875588, 4.575252]\n",
      "520: Train Loss: [2.4335766, 0.2991281, 4.568025] | Test Loss: [2.64521, 0.32991752, 4.9605026]\n",
      "521: Train Loss: [2.384996, 0.35606378, 4.413928] | Test Loss: [2.5319786, 0.3665164, 4.6974406]\n",
      "522: Train Loss: [2.3561985, 0.37412378, 4.3382735] | Test Loss: [2.709215, 0.3258233, 5.0926065]\n",
      "523: Train Loss: [2.6651175, 0.38521856, 4.9450164] | Test Loss: [2.7834435, 0.3356378, 5.231249]\n",
      "Epoch 11\n",
      "0: Train Loss: [2.4178095, 0.39006335, 4.4455557] | Test Loss: [2.6099641, 0.31907356, 4.9008546]\n",
      "1: Train Loss: [2.2522879, 0.31534794, 4.1892276] | Test Loss: [2.4846263, 0.33771518, 4.6315374]\n",
      "2: Train Loss: [2.3284202, 0.37865895, 4.2781816] | Test Loss: [2.5480385, 0.40858954, 4.6874876]\n",
      "3: Train Loss: [2.2244582, 0.39852357, 4.050393] | Test Loss: [2.839383, 0.38007027, 5.2986956]\n",
      "4: Train Loss: [2.278049, 0.33275104, 4.2233467] | Test Loss: [2.462491, 0.18791477, 4.737067]\n",
      "5: Train Loss: [2.2820961, 0.32932022, 4.234872] | Test Loss: [2.7050776, 0.38488114, 5.0252743]\n",
      "6: Train Loss: [2.3469844, 0.32187995, 4.372089] | Test Loss: [2.4463542, 0.3678302, 4.524878]\n",
      "7: Train Loss: [2.2546737, 0.34637567, 4.162972] | Test Loss: [2.8011854, 0.4727461, 5.129625]\n",
      "8: Train Loss: [2.317665, 0.3137126, 4.3216176] | Test Loss: [2.601773, 0.36152717, 4.842019]\n",
      "9: Train Loss: [2.4393396, 0.31898987, 4.5596895] | Test Loss: [2.5946436, 0.31128973, 4.8779974]\n",
      "10: Train Loss: [2.368153, 0.41790298, 4.3184032] | Test Loss: [2.6417687, 0.39552096, 4.888016]\n",
      "11: Train Loss: [2.3091092, 0.3394487, 4.2787695] | Test Loss: [2.7762575, 0.41333428, 5.1391807]\n",
      "12: Train Loss: [2.3555684, 0.29980928, 4.4113274] | Test Loss: [2.548827, 0.3181987, 4.779455]\n",
      "13: Train Loss: [2.2818084, 0.32031432, 4.2433023] | Test Loss: [2.7953382, 0.3753531, 5.2153234]\n",
      "14: Train Loss: [2.3275213, 0.37305108, 4.2819915] | Test Loss: [2.6881402, 0.34551957, 5.030761]\n",
      "15: Train Loss: [2.270231, 0.4140845, 4.1263776] | Test Loss: [2.5190868, 0.38885617, 4.6493177]\n",
      "16: Train Loss: [2.3124654, 0.37375844, 4.2511725] | Test Loss: [2.5729742, 0.34330836, 4.80264]\n",
      "17: Train Loss: [2.2066422, 0.3783565, 4.034928] | Test Loss: [2.5520399, 0.3905595, 4.71352]\n",
      "18: Train Loss: [2.4142299, 0.37020564, 4.4582543] | Test Loss: [2.5679915, 0.3247651, 4.811218]\n",
      "19: Train Loss: [2.3343666, 0.3223947, 4.3463383] | Test Loss: [2.4744358, 0.40811837, 4.5407534]\n",
      "20: Train Loss: [2.3989835, 0.3999513, 4.3980155] | Test Loss: [2.5153162, 0.3126916, 4.717941]\n",
      "21: Train Loss: [2.303835, 0.41649213, 4.191178] | Test Loss: [2.7689161, 0.41222277, 5.1256094]\n",
      "22: Train Loss: [2.3483772, 0.34139627, 4.355358] | Test Loss: [2.6107416, 0.37917602, 4.842307]\n",
      "23: Train Loss: [2.1721292, 0.30076194, 4.0434966] | Test Loss: [2.7025645, 0.4259945, 4.9791346]\n",
      "24: Train Loss: [2.414318, 0.368996, 4.45964] | Test Loss: [2.619097, 0.35748187, 4.880712]\n",
      "25: Train Loss: [2.2576876, 0.3560951, 4.15928] | Test Loss: [2.4573905, 0.32886073, 4.5859203]\n",
      "26: Train Loss: [2.2263682, 0.36184874, 4.0908875] | Test Loss: [2.582881, 0.38763997, 4.778122]\n",
      "27: Train Loss: [2.3677537, 0.3844035, 4.351104] | Test Loss: [2.6911125, 0.39032528, 4.9919]\n",
      "28: Train Loss: [2.3784964, 0.29933473, 4.4576583] | Test Loss: [2.5983565, 0.32108644, 4.8756266]\n",
      "29: Train Loss: [2.2138696, 0.36563385, 4.062105] | Test Loss: [2.5726619, 0.40549538, 4.7398286]\n",
      "30: Train Loss: [2.1343708, 0.37356278, 3.8951788] | Test Loss: [2.5158732, 0.3576587, 4.6740875]\n",
      "31: Train Loss: [2.2681, 0.32778826, 4.2084117] | Test Loss: [2.7748845, 0.3799755, 5.1697936]\n",
      "32: Train Loss: [2.2647607, 0.32212126, 4.2074003] | Test Loss: [2.5892558, 0.3688414, 4.8096704]\n",
      "33: Train Loss: [2.3334002, 0.3540189, 4.312782] | Test Loss: [2.5187607, 0.36628643, 4.671235]\n",
      "34: Train Loss: [2.269707, 0.34711567, 4.1922984] | Test Loss: [2.2352674, 0.35122675, 4.119308]\n",
      "35: Train Loss: [2.2834873, 0.30391768, 4.2630568] | Test Loss: [2.4914017, 0.36748615, 4.6153173]\n",
      "36: Train Loss: [2.2791023, 0.3341571, 4.2240477] | Test Loss: [2.5366833, 0.3659978, 4.707369]\n",
      "37: Train Loss: [2.4326968, 0.36866266, 4.496731] | Test Loss: [2.6884675, 0.36377302, 5.013162]\n",
      "38: Train Loss: [2.2562065, 0.34306988, 4.169343] | Test Loss: [2.5901835, 0.39992887, 4.780438]\n",
      "39: Train Loss: [2.3497086, 0.42841923, 4.270998] | Test Loss: [2.656756, 0.33949113, 4.974021]\n",
      "40: Train Loss: [2.228647, 0.3917766, 4.0655174] | Test Loss: [2.644587, 0.34455305, 4.944621]\n",
      "41: Train Loss: [2.4023461, 0.3225711, 4.482121] | Test Loss: [2.5569787, 0.37853998, 4.7354174]\n",
      "42: Train Loss: [2.4278138, 0.4315852, 4.424042] | Test Loss: [2.8055885, 0.40696627, 5.2042108]\n",
      "43: Train Loss: [2.3435225, 0.36656094, 4.320484] | Test Loss: [2.5932412, 0.34634423, 4.840138]\n",
      "44: Train Loss: [2.245314, 0.32354885, 4.167079] | Test Loss: [2.4669955, 0.3209286, 4.6130624]\n",
      "45: Train Loss: [2.3219688, 0.3354121, 4.3085256] | Test Loss: [2.6498847, 0.38813218, 4.9116373]\n",
      "46: Train Loss: [2.2457376, 0.31893313, 4.172542] | Test Loss: [2.5710769, 0.3985076, 4.743646]\n",
      "47: Train Loss: [2.336113, 0.38868126, 4.2835445] | Test Loss: [2.582911, 0.37428287, 4.791539]\n",
      "48: Train Loss: [2.3003628, 0.36035877, 4.240367] | Test Loss: [2.5743146, 0.4333058, 4.7153234]\n",
      "49: Train Loss: [2.473482, 0.38661873, 4.560345] | Test Loss: [2.721331, 0.33270243, 5.109959]\n",
      "50: Train Loss: [2.4173522, 0.38902682, 4.4456778] | Test Loss: [2.6698334, 0.35487515, 4.9847918]\n",
      "51: Train Loss: [2.421825, 0.3072009, 4.536449] | Test Loss: [2.6353643, 0.42645878, 4.8442698]\n",
      "52: Train Loss: [2.2221875, 0.36449125, 4.0798836] | Test Loss: [2.4522598, 0.40084574, 4.503674]\n",
      "53: Train Loss: [2.2472253, 0.3370052, 4.1574454] | Test Loss: [2.5210395, 0.32908037, 4.7129984]\n",
      "54: Train Loss: [2.197272, 0.42943144, 3.9651127] | Test Loss: [2.6981556, 0.39547172, 5.0008397]\n",
      "55: Train Loss: [2.2940018, 0.38859493, 4.1994085] | Test Loss: [2.4168103, 0.35532123, 4.478299]\n",
      "56: Train Loss: [2.317396, 0.3240329, 4.310759] | Test Loss: [2.6353242, 0.35306853, 4.91758]\n",
      "57: Train Loss: [2.3462627, 0.36645582, 4.3260694] | Test Loss: [2.635473, 0.40750006, 4.8634458]\n",
      "58: Train Loss: [2.2938867, 0.3191905, 4.268583] | Test Loss: [2.6763575, 0.32831794, 5.024397]\n",
      "59: Train Loss: [2.4247506, 0.39755362, 4.4519477] | Test Loss: [2.609147, 0.36489925, 4.853395]\n",
      "60: Train Loss: [2.4157913, 0.3660678, 4.4655147] | Test Loss: [2.6235836, 0.4112798, 4.8358874]\n",
      "61: Train Loss: [2.5411873, 0.37754735, 4.7048273] | Test Loss: [2.559575, 0.38832867, 4.7308216]\n",
      "62: Train Loss: [2.2592432, 0.35745054, 4.161036] | Test Loss: [2.7875137, 0.40029886, 5.1747284]\n",
      "63: Train Loss: [2.3414369, 0.35685134, 4.326022] | Test Loss: [2.6598854, 0.364864, 4.954907]\n",
      "64: Train Loss: [2.4118342, 0.3134644, 4.510204] | Test Loss: [2.3348577, 0.2504628, 4.4192524]\n",
      "65: Train Loss: [2.2786849, 0.32327756, 4.234092] | Test Loss: [2.61429, 0.4131338, 4.8154464]\n",
      "66: Train Loss: [2.282733, 0.35198984, 4.213476] | Test Loss: [2.5199928, 0.35988986, 4.6800957]\n",
      "67: Train Loss: [2.2855892, 0.32280114, 4.2483773] | Test Loss: [2.5164754, 0.3433399, 4.689611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68: Train Loss: [2.390489, 0.41657737, 4.364401] | Test Loss: [2.730714, 0.32538977, 5.1360383]\n",
      "69: Train Loss: [2.3151405, 0.36043116, 4.26985] | Test Loss: [2.6664476, 0.40677568, 4.92612]\n",
      "70: Train Loss: [2.266759, 0.3431661, 4.190352] | Test Loss: [2.7142563, 0.35861197, 5.0699005]\n",
      "71: Train Loss: [2.3098197, 0.39852688, 4.2211127] | Test Loss: [2.4741223, 0.3556036, 4.592641]\n",
      "72: Train Loss: [2.279088, 0.33481276, 4.2233634] | Test Loss: [2.5067716, 0.34097838, 4.6725645]\n",
      "73: Train Loss: [2.256178, 0.32721525, 4.1851406] | Test Loss: [2.753631, 0.3461195, 5.161143]\n",
      "74: Train Loss: [2.344897, 0.31323606, 4.376558] | Test Loss: [2.6675656, 0.4640131, 4.871118]\n",
      "75: Train Loss: [2.2949777, 0.35263124, 4.237324] | Test Loss: [2.574884, 0.359565, 4.790203]\n",
      "76: Train Loss: [2.3378422, 0.36490116, 4.3107834] | Test Loss: [2.4263194, 0.37107167, 4.481567]\n",
      "77: Train Loss: [2.2850978, 0.30667007, 4.2635255] | Test Loss: [2.4062603, 0.35629815, 4.4562225]\n",
      "78: Train Loss: [2.4241624, 0.38970515, 4.4586196] | Test Loss: [2.7144787, 0.35802698, 5.0709305]\n",
      "79: Train Loss: [2.4161055, 0.35775083, 4.47446] | Test Loss: [2.518132, 0.3817554, 4.6545086]\n",
      "80: Train Loss: [2.2891862, 0.32370824, 4.2546644] | Test Loss: [2.6856337, 0.39240044, 4.978867]\n",
      "81: Train Loss: [2.326415, 0.35801205, 4.294818] | Test Loss: [2.5794783, 0.3464069, 4.8125496]\n",
      "82: Train Loss: [2.3272104, 0.36067778, 4.293743] | Test Loss: [2.7369576, 0.3392354, 5.13468]\n",
      "83: Train Loss: [2.3724709, 0.3573308, 4.387611] | Test Loss: [2.5733266, 0.39131564, 4.7553377]\n",
      "84: Train Loss: [2.357271, 0.3414688, 4.373073] | Test Loss: [2.5405383, 0.34708065, 4.733996]\n",
      "85: Train Loss: [2.3708625, 0.2963092, 4.445416] | Test Loss: [2.5056036, 0.3517552, 4.659452]\n",
      "86: Train Loss: [2.389081, 0.4333909, 4.344771] | Test Loss: [2.6800869, 0.34770468, 5.012469]\n",
      "87: Train Loss: [2.4252274, 0.4249878, 4.425467] | Test Loss: [2.57863, 0.34955266, 4.8077073]\n",
      "88: Train Loss: [2.437829, 0.39741063, 4.4782476] | Test Loss: [2.6726916, 0.48072085, 4.864662]\n",
      "89: Train Loss: [2.5496259, 0.34606126, 4.7531905] | Test Loss: [2.6489506, 0.32013625, 4.977765]\n",
      "90: Train Loss: [2.2722054, 0.40423632, 4.1401744] | Test Loss: [2.5720441, 0.41556746, 4.728521]\n",
      "91: Train Loss: [2.303797, 0.35472935, 4.252865] | Test Loss: [2.721175, 0.37604418, 5.0663056]\n",
      "92: Train Loss: [2.3644528, 0.30329403, 4.4256115] | Test Loss: [2.7878256, 0.37608942, 5.1995616]\n",
      "93: Train Loss: [2.3228343, 0.42758986, 4.2180786] | Test Loss: [2.6362078, 0.34296674, 4.929449]\n",
      "94: Train Loss: [2.532156, 0.40604395, 4.658268] | Test Loss: [2.7471726, 0.32153666, 5.1728086]\n",
      "95: Train Loss: [2.3209903, 0.3683563, 4.2736244] | Test Loss: [2.65781, 0.32154384, 4.9940763]\n",
      "96: Train Loss: [2.33878, 0.38742903, 4.2901306] | Test Loss: [2.6450737, 0.32687834, 4.9632688]\n",
      "97: Train Loss: [2.3375804, 0.36335585, 4.3118052] | Test Loss: [2.5723677, 0.3657599, 4.7789755]\n",
      "98: Train Loss: [2.274421, 0.32270586, 4.226136] | Test Loss: [2.769299, 0.48888493, 5.049713]\n",
      "99: Train Loss: [2.5523822, 0.3313211, 4.773443] | Test Loss: [2.7193549, 0.3828397, 5.05587]\n",
      "100: Train Loss: [2.274189, 0.3295576, 4.2188206] | Test Loss: [2.9190898, 0.37151682, 5.466663]\n",
      "101: Train Loss: [2.3752525, 0.36593547, 4.3845696] | Test Loss: [2.6001084, 0.38253087, 4.817686]\n",
      "102: Train Loss: [2.5180883, 0.47449583, 4.561681] | Test Loss: [2.4928033, 0.382392, 4.6032147]\n",
      "103: Train Loss: [2.3942246, 0.3418282, 4.446621] | Test Loss: [2.528388, 0.3526274, 4.704149]\n",
      "104: Train Loss: [2.4773657, 0.3316246, 4.623107] | Test Loss: [2.5569794, 0.34215486, 4.771804]\n",
      "105: Train Loss: [2.4398868, 0.34158796, 4.5381856] | Test Loss: [2.6852236, 0.34361336, 5.026834]\n",
      "106: Train Loss: [2.3075275, 0.42433488, 4.19072] | Test Loss: [2.7269096, 0.36347657, 5.0903425]\n",
      "107: Train Loss: [2.263001, 0.3157491, 4.210253] | Test Loss: [2.621317, 0.39978614, 4.842848]\n",
      "108: Train Loss: [2.330937, 0.38547507, 4.2763987] | Test Loss: [2.7021036, 0.30611244, 5.098095]\n",
      "109: Train Loss: [2.193242, 0.41111687, 3.975367] | Test Loss: [2.5759902, 0.32467222, 4.827308]\n",
      "110: Train Loss: [2.411695, 0.41360563, 4.4097843] | Test Loss: [2.4970798, 0.3921004, 4.6020594]\n",
      "111: Train Loss: [2.285039, 0.40199897, 4.168079] | Test Loss: [2.6919582, 0.46535647, 4.91856]\n",
      "112: Train Loss: [2.3280244, 0.42719924, 4.2288494] | Test Loss: [2.4393523, 0.34232247, 4.536382]\n",
      "113: Train Loss: [2.3897536, 0.3481509, 4.4313564] | Test Loss: [2.56243, 0.32597867, 4.798881]\n",
      "114: Train Loss: [2.2419896, 0.3710191, 4.1129603] | Test Loss: [2.5493076, 0.36512572, 4.7334895]\n",
      "115: Train Loss: [2.481122, 0.33814555, 4.6240983] | Test Loss: [2.570631, 0.34652868, 4.7947335]\n",
      "116: Train Loss: [2.3037767, 0.32173657, 4.285817] | Test Loss: [2.5788684, 0.38945225, 4.7682843]\n",
      "117: Train Loss: [2.3226635, 0.31361327, 4.3317137] | Test Loss: [2.6350117, 0.40113032, 4.868893]\n",
      "118: Train Loss: [2.13755, 0.2672845, 4.007816] | Test Loss: [2.6031559, 0.3843915, 4.8219204]\n",
      "119: Train Loss: [2.41779, 0.44150722, 4.3940725] | Test Loss: [2.5632987, 0.39305475, 4.7335424]\n",
      "120: Train Loss: [2.1114686, 0.27496603, 3.947971] | Test Loss: [2.5084136, 0.45307276, 4.5637546]\n",
      "121: Train Loss: [2.4137664, 0.48979607, 4.3377366] | Test Loss: [2.5135539, 0.45428878, 4.5728188]\n",
      "122: Train Loss: [2.337647, 0.32769296, 4.347601] | Test Loss: [2.6538582, 0.42506707, 4.8826494]\n",
      "123: Train Loss: [2.351125, 0.3889937, 4.3132563] | Test Loss: [2.5848224, 0.30858552, 4.861059]\n",
      "124: Train Loss: [2.37343, 0.37118992, 4.37567] | Test Loss: [2.9019496, 0.35542187, 5.4484773]\n",
      "125: Train Loss: [2.3354976, 0.37090847, 4.300087] | Test Loss: [2.716738, 0.37256187, 5.060914]\n",
      "126: Train Loss: [2.4074426, 0.35748515, 4.4574] | Test Loss: [2.6345596, 0.35922077, 4.9098983]\n",
      "127: Train Loss: [2.5357969, 0.3825842, 4.6890097] | Test Loss: [2.5819738, 0.39018193, 4.7737656]\n",
      "128: Train Loss: [2.3211553, 0.307637, 4.3346734] | Test Loss: [2.6306942, 0.36607543, 4.895313]\n",
      "129: Train Loss: [2.3958194, 0.39471656, 4.396922] | Test Loss: [2.6423478, 0.31275967, 4.9719357]\n",
      "130: Train Loss: [2.359522, 0.40880257, 4.3102417] | Test Loss: [2.549526, 0.34009984, 4.758952]\n",
      "131: Train Loss: [2.3230722, 0.4105009, 4.2356434] | Test Loss: [2.8720672, 0.4902336, 5.253901]\n",
      "132: Train Loss: [2.3859031, 0.29744673, 4.4743595] | Test Loss: [2.524694, 0.3845594, 4.664829]\n",
      "133: Train Loss: [2.384366, 0.3255803, 4.443152] | Test Loss: [2.7176738, 0.36074454, 5.074603]\n",
      "134: Train Loss: [2.3868706, 0.47569117, 4.29805] | Test Loss: [2.457443, 0.4170069, 4.497879]\n",
      "135: Train Loss: [2.2579284, 0.33275783, 4.183099] | Test Loss: [2.5301085, 0.42257854, 4.6376386]\n",
      "136: Train Loss: [2.419446, 0.4247471, 4.414145] | Test Loss: [2.517186, 0.3615275, 4.6728444]\n",
      "137: Train Loss: [2.4054244, 0.35626623, 4.4545827] | Test Loss: [2.7466755, 0.36964393, 5.123707]\n",
      "138: Train Loss: [2.235291, 0.35127473, 4.1193075] | Test Loss: [2.696944, 0.35141653, 5.0424714]\n",
      "139: Train Loss: [2.3614979, 0.3716619, 4.351334] | Test Loss: [2.5402215, 0.28788367, 4.792559]\n",
      "140: Train Loss: [2.284877, 0.30259898, 4.267155] | Test Loss: [2.6881278, 0.3705021, 5.0057535]\n",
      "141: Train Loss: [2.3323462, 0.43219718, 4.2324953] | Test Loss: [2.4811997, 0.42641196, 4.5359874]\n",
      "142: Train Loss: [2.3731425, 0.3953892, 4.350896] | Test Loss: [2.5471628, 0.36433575, 4.72999]\n",
      "143: Train Loss: [2.407154, 0.33320403, 4.4811044] | Test Loss: [2.5645394, 0.37881443, 4.7502646]\n",
      "144: Train Loss: [2.304923, 0.37938818, 4.230458] | Test Loss: [2.727183, 0.46394268, 4.9904237]\n",
      "145: Train Loss: [2.311421, 0.3478343, 4.2750077] | Test Loss: [2.6018844, 0.35003525, 4.8537335]\n",
      "146: Train Loss: [2.4570591, 0.36661988, 4.547498] | Test Loss: [2.6409955, 0.36860132, 4.9133897]\n",
      "147: Train Loss: [2.2712028, 0.37165335, 4.170752] | Test Loss: [2.7726312, 0.39397225, 5.15129]\n",
      "148: Train Loss: [2.4606423, 0.33999145, 4.581293] | Test Loss: [2.584489, 0.29423, 4.874748]\n",
      "149: Train Loss: [2.5178602, 0.4996508, 4.5360694] | Test Loss: [2.566893, 0.37082633, 4.76296]\n",
      "150: Train Loss: [2.3303306, 0.37569174, 4.2849693] | Test Loss: [2.5051708, 0.35534722, 4.6549945]\n",
      "151: Train Loss: [2.3153818, 0.28863046, 4.342133] | Test Loss: [2.7101884, 0.39189222, 5.0284843]\n",
      "152: Train Loss: [2.4493442, 0.37864536, 4.520043] | Test Loss: [2.610874, 0.39800283, 4.8237453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153: Train Loss: [2.2481315, 0.29572016, 4.200543] | Test Loss: [2.5830142, 0.32204345, 4.843985]\n",
      "154: Train Loss: [2.4125195, 0.35673875, 4.4683003] | Test Loss: [2.4131856, 0.44050434, 4.3858666]\n",
      "155: Train Loss: [2.3268492, 0.3212121, 4.332486] | Test Loss: [2.4541366, 0.380235, 4.528038]\n",
      "156: Train Loss: [2.3759437, 0.3483599, 4.4035273] | Test Loss: [2.6543589, 0.3502338, 4.958484]\n",
      "157: Train Loss: [2.3086402, 0.3318238, 4.2854567] | Test Loss: [2.5322504, 0.32150006, 4.7430005]\n",
      "158: Train Loss: [2.3264313, 0.3476997, 4.305163] | Test Loss: [2.7400217, 0.37844896, 5.1015944]\n",
      "159: Train Loss: [2.2826517, 0.41117117, 4.1541324] | Test Loss: [2.7897656, 0.43428296, 5.1452484]\n",
      "160: Train Loss: [2.313292, 0.32706589, 4.299518] | Test Loss: [2.5426471, 0.3524791, 4.7328153]\n",
      "161: Train Loss: [2.4696681, 0.3283917, 4.6109447] | Test Loss: [2.594909, 0.3148031, 4.875015]\n",
      "162: Train Loss: [2.3595963, 0.38728988, 4.3319025] | Test Loss: [2.4593122, 0.3978385, 4.520786]\n",
      "163: Train Loss: [2.482125, 0.38138825, 4.582862] | Test Loss: [2.5978696, 0.3603112, 4.835428]\n",
      "164: Train Loss: [2.3070555, 0.3435309, 4.27058] | Test Loss: [2.7437258, 0.4351752, 5.052276]\n",
      "165: Train Loss: [2.4732316, 0.321458, 4.6250052] | Test Loss: [2.5911121, 0.3595448, 4.8226795]\n",
      "166: Train Loss: [2.5053926, 0.35192934, 4.658856] | Test Loss: [2.5476356, 0.36169565, 4.7335753]\n",
      "167: Train Loss: [2.3261867, 0.31491363, 4.3374596] | Test Loss: [2.6761668, 0.37746727, 4.9748664]\n",
      "168: Train Loss: [2.4877932, 0.3618724, 4.613714] | Test Loss: [2.7064173, 0.4911309, 4.921704]\n",
      "169: Train Loss: [2.2918727, 0.3552305, 4.228515] | Test Loss: [2.5266776, 0.36182776, 4.6915274]\n",
      "170: Train Loss: [2.417843, 0.3913068, 4.4443793] | Test Loss: [2.635312, 0.37070966, 4.8999147]\n",
      "171: Train Loss: [2.3834834, 0.44053313, 4.3264337] | Test Loss: [2.7290168, 0.37473226, 5.083301]\n",
      "172: Train Loss: [2.178943, 0.35012925, 4.0077567] | Test Loss: [2.6356535, 0.34901512, 4.9222918]\n",
      "173: Train Loss: [2.303398, 0.32785746, 4.2789383] | Test Loss: [2.4823792, 0.3427456, 4.6220126]\n",
      "174: Train Loss: [2.549544, 0.3300646, 4.7690234] | Test Loss: [2.5569782, 0.36921024, 4.744746]\n",
      "175: Train Loss: [2.3682482, 0.4004442, 4.3360524] | Test Loss: [2.5042658, 0.37814105, 4.6303906]\n",
      "176: Train Loss: [2.5016916, 0.32347763, 4.6799054] | Test Loss: [2.4295242, 0.4122617, 4.446787]\n",
      "177: Train Loss: [2.351613, 0.35481444, 4.3484116] | Test Loss: [2.5750031, 0.32325703, 4.8267493]\n",
      "178: Train Loss: [2.277476, 0.3331384, 4.2218137] | Test Loss: [2.8063757, 0.444411, 5.1683407]\n",
      "179: Train Loss: [2.2009072, 0.3130586, 4.0887556] | Test Loss: [2.6471748, 0.30936956, 4.98498]\n",
      "180: Train Loss: [2.3090785, 0.4809008, 4.137256] | Test Loss: [2.585679, 0.40786356, 4.7634945]\n",
      "181: Train Loss: [2.3363361, 0.35780168, 4.3148704] | Test Loss: [2.6884115, 0.35858878, 5.0182343]\n",
      "182: Train Loss: [2.4669018, 0.37116235, 4.562641] | Test Loss: [2.6053724, 0.39025024, 4.8204947]\n",
      "183: Train Loss: [2.4192567, 0.41876626, 4.419747] | Test Loss: [2.7261038, 0.34170222, 5.110505]\n",
      "184: Train Loss: [2.363228, 0.41527504, 4.311181] | Test Loss: [2.709609, 0.3858356, 5.0333824]\n",
      "185: Train Loss: [2.3708909, 0.41255397, 4.329228] | Test Loss: [2.655399, 0.39581063, 4.9149876]\n",
      "186: Train Loss: [2.5002215, 0.410018, 4.590425] | Test Loss: [2.6460736, 0.39428, 4.897867]\n",
      "187: Train Loss: [2.3538039, 0.37347418, 4.3341336] | Test Loss: [2.481593, 0.32295048, 4.6402354]\n",
      "188: Train Loss: [2.2879267, 0.30548072, 4.270373] | Test Loss: [2.4678624, 0.4016096, 4.5341153]\n",
      "189: Train Loss: [2.0928888, 0.32904947, 3.856728] | Test Loss: [2.5171998, 0.31670496, 4.7176948]\n",
      "190: Train Loss: [2.3773143, 0.35969508, 4.3949337] | Test Loss: [2.6001318, 0.33062837, 4.869635]\n",
      "191: Train Loss: [2.2972887, 0.33547506, 4.2591023] | Test Loss: [2.677239, 0.33876044, 5.0157175]\n",
      "192: Train Loss: [2.4311416, 0.43804395, 4.424239] | Test Loss: [2.7571337, 0.4534088, 5.0608587]\n",
      "193: Train Loss: [2.3927891, 0.422859, 4.362719] | Test Loss: [2.515968, 0.30880398, 4.723132]\n",
      "194: Train Loss: [2.4090402, 0.30603692, 4.5120435] | Test Loss: [2.7111478, 0.3480683, 5.0742273]\n",
      "195: Train Loss: [2.465819, 0.39859006, 4.5330477] | Test Loss: [2.6694517, 0.35349315, 4.98541]\n",
      "196: Train Loss: [2.3829565, 0.3177942, 4.4481187] | Test Loss: [2.6219087, 0.3547431, 4.8890743]\n",
      "197: Train Loss: [2.4582345, 0.49170405, 4.424765] | Test Loss: [2.620677, 0.39701173, 4.844342]\n",
      "198: Train Loss: [2.3991685, 0.39173025, 4.4066067] | Test Loss: [2.5850687, 0.43794107, 4.7321963]\n",
      "199: Train Loss: [2.406539, 0.3752028, 4.4378753] | Test Loss: [2.5132494, 0.41557845, 4.6109204]\n",
      "200: Train Loss: [2.2734919, 0.3905005, 4.156483] | Test Loss: [2.7705026, 0.34171677, 5.1992884]\n",
      "201: Train Loss: [2.590067, 0.42398256, 4.756151] | Test Loss: [2.707819, 0.3331253, 5.082513]\n",
      "202: Train Loss: [2.2107153, 0.3437672, 4.0776634] | Test Loss: [2.733923, 0.45341554, 5.0144305]\n",
      "203: Train Loss: [2.365258, 0.3342617, 4.396254] | Test Loss: [2.6585257, 0.3906972, 4.9263544]\n",
      "204: Train Loss: [2.3081088, 0.30135813, 4.3148594] | Test Loss: [2.614506, 0.3647713, 4.8642406]\n",
      "205: Train Loss: [2.3207026, 0.32821614, 4.313189] | Test Loss: [2.5001853, 0.472976, 4.5273943]\n",
      "206: Train Loss: [2.2604625, 0.30735177, 4.2135735] | Test Loss: [2.7705975, 0.3777323, 5.1634626]\n",
      "207: Train Loss: [2.3420696, 0.32156497, 4.362574] | Test Loss: [2.7044313, 0.40216383, 5.0066986]\n",
      "208: Train Loss: [2.2858005, 0.42733124, 4.1442695] | Test Loss: [2.5197642, 0.38542584, 4.6541023]\n",
      "209: Train Loss: [2.1969914, 0.3535419, 4.040441] | Test Loss: [2.6697357, 0.36150107, 4.97797]\n",
      "210: Train Loss: [2.5270207, 0.3339414, 4.7201] | Test Loss: [2.572372, 0.40243724, 4.7423067]\n",
      "211: Train Loss: [2.1867616, 0.34395427, 4.029569] | Test Loss: [2.6921544, 0.39311588, 4.991193]\n",
      "212: Train Loss: [2.1780052, 0.34234747, 4.013663] | Test Loss: [2.689922, 0.3638541, 5.0159903]\n",
      "213: Train Loss: [2.2856238, 0.36272338, 4.208524] | Test Loss: [2.5159843, 0.39338169, 4.638587]\n",
      "214: Train Loss: [2.4908512, 0.30612567, 4.6755767] | Test Loss: [2.4309595, 0.34198198, 4.519937]\n",
      "215: Train Loss: [2.5022414, 0.33568415, 4.6687984] | Test Loss: [2.7799032, 0.36259964, 5.1972065]\n",
      "216: Train Loss: [2.3852136, 0.3670374, 4.40339] | Test Loss: [2.6931715, 0.33699894, 5.049344]\n",
      "217: Train Loss: [2.412224, 0.40515265, 4.4192953] | Test Loss: [2.511132, 0.3294595, 4.6928043]\n",
      "218: Train Loss: [2.4436586, 0.36421973, 4.5230975] | Test Loss: [2.7020245, 0.4350438, 4.969005]\n",
      "219: Train Loss: [2.41043, 0.33372867, 4.487131] | Test Loss: [2.7063422, 0.3370073, 5.075677]\n",
      "220: Train Loss: [2.3183115, 0.2981945, 4.3384285] | Test Loss: [2.7762463, 0.44546705, 5.1070256]\n",
      "221: Train Loss: [2.4622836, 0.43128213, 4.493285] | Test Loss: [2.40568, 0.39663407, 4.414726]\n",
      "222: Train Loss: [2.3770032, 0.34795693, 4.4060493] | Test Loss: [2.763321, 0.3871735, 5.139468]\n",
      "223: Train Loss: [2.3530374, 0.38066283, 4.325412] | Test Loss: [2.6469364, 0.33625093, 4.957622]\n",
      "224: Train Loss: [2.4452446, 0.37561697, 4.514872] | Test Loss: [2.6830616, 0.37291595, 4.9932075]\n",
      "225: Train Loss: [2.3306553, 0.36808726, 4.2932234] | Test Loss: [2.53111, 0.45660216, 4.605618]\n",
      "226: Train Loss: [2.3178751, 0.37387636, 4.2618737] | Test Loss: [2.781826, 0.39366555, 5.1699862]\n",
      "227: Train Loss: [2.370331, 0.30840644, 4.4322557] | Test Loss: [2.6930594, 0.33705094, 5.049068]\n",
      "228: Train Loss: [2.3458984, 0.37148085, 4.320316] | Test Loss: [2.3834713, 0.3719917, 4.394951]\n",
      "229: Train Loss: [2.4053664, 0.38169372, 4.429039] | Test Loss: [2.8425853, 0.45232356, 5.232847]\n",
      "230: Train Loss: [2.3319218, 0.38537663, 4.278467] | Test Loss: [2.6288104, 0.32399327, 4.9336276]\n",
      "231: Train Loss: [2.3874264, 0.29182002, 4.4830327] | Test Loss: [2.5834615, 0.42464197, 4.742281]\n",
      "232: Train Loss: [2.3501472, 0.33648637, 4.363808] | Test Loss: [2.4355004, 0.3717308, 4.49927]\n",
      "233: Train Loss: [2.378835, 0.3121122, 4.4455576] | Test Loss: [2.706089, 0.3483727, 5.0638056]\n",
      "234: Train Loss: [2.4356935, 0.4155106, 4.4558764] | Test Loss: [2.6679962, 0.371718, 4.9642744]\n",
      "235: Train Loss: [2.27759, 0.30285844, 4.2523217] | Test Loss: [2.6365175, 0.3112567, 4.961778]\n",
      "236: Train Loss: [2.427279, 0.40053183, 4.454026] | Test Loss: [2.4209177, 0.35294697, 4.4888887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237: Train Loss: [2.258963, 0.35461712, 4.163309] | Test Loss: [2.6560571, 0.33883578, 4.9732785]\n",
      "238: Train Loss: [2.383738, 0.40832195, 4.359154] | Test Loss: [2.5555792, 0.412477, 4.6986814]\n",
      "239: Train Loss: [2.3587427, 0.40834388, 4.3091416] | Test Loss: [2.6042483, 0.36784992, 4.8406467]\n",
      "240: Train Loss: [2.3838682, 0.34436867, 4.423368] | Test Loss: [2.5468867, 0.37288967, 4.720884]\n",
      "241: Train Loss: [2.3488748, 0.3820805, 4.315669] | Test Loss: [2.5117009, 0.3438985, 4.6795034]\n",
      "242: Train Loss: [2.4733076, 0.45106912, 4.495546] | Test Loss: [2.455378, 0.3454058, 4.5653505]\n",
      "243: Train Loss: [2.3921177, 0.35456973, 4.4296656] | Test Loss: [2.606913, 0.33743858, 4.8763876]\n",
      "244: Train Loss: [2.355637, 0.34180522, 4.3694687] | Test Loss: [2.592497, 0.2908234, 4.8941708]\n",
      "245: Train Loss: [2.3228626, 0.36443684, 4.2812886] | Test Loss: [2.5584075, 0.34829283, 4.7685223]\n",
      "246: Train Loss: [2.5208902, 0.3180148, 4.723766] | Test Loss: [2.534697, 0.34415552, 4.725239]\n",
      "247: Train Loss: [2.3112535, 0.3246479, 4.297859] | Test Loss: [2.7828324, 0.4233298, 5.142335]\n",
      "248: Train Loss: [2.3511314, 0.42712626, 4.2751365] | Test Loss: [2.6179438, 0.2876627, 4.948225]\n",
      "249: Train Loss: [2.4783385, 0.35651976, 4.6001573] | Test Loss: [2.535361, 0.36354288, 4.707179]\n",
      "250: Train Loss: [2.3611746, 0.39231372, 4.330035] | Test Loss: [2.8187957, 0.37757078, 5.2600207]\n",
      "251: Train Loss: [2.365083, 0.36237997, 4.367786] | Test Loss: [2.5158536, 0.41881192, 4.6128955]\n",
      "252: Train Loss: [2.5476003, 0.33134812, 4.7638526] | Test Loss: [2.6318533, 0.33654144, 4.927165]\n",
      "253: Train Loss: [2.3132336, 0.32483613, 4.301631] | Test Loss: [2.694198, 0.38557762, 5.002818]\n",
      "254: Train Loss: [2.5164785, 0.33742815, 4.695529] | Test Loss: [2.615807, 0.38116443, 4.8504496]\n",
      "255: Train Loss: [2.2835329, 0.34319443, 4.223871] | Test Loss: [2.705462, 0.316342, 5.094582]\n",
      "256: Train Loss: [2.4026985, 0.39458, 4.410817] | Test Loss: [2.386394, 0.30102655, 4.4717617]\n",
      "257: Train Loss: [2.334043, 0.3404597, 4.327626] | Test Loss: [2.4368021, 0.5470187, 4.326586]\n",
      "258: Train Loss: [2.475014, 0.36741334, 4.5826144] | Test Loss: [2.5322156, 0.34785998, 4.7165713]\n",
      "259: Train Loss: [2.4016752, 0.31993023, 4.4834204] | Test Loss: [2.7317045, 0.3871549, 5.076254]\n",
      "260: Train Loss: [2.398263, 0.35410276, 4.4424233] | Test Loss: [2.6863537, 0.4207754, 4.951932]\n",
      "261: Train Loss: [2.4772942, 0.42623338, 4.528355] | Test Loss: [2.5668774, 0.36700764, 4.766747]\n",
      "262: Train Loss: [2.4244282, 0.41131344, 4.437543] | Test Loss: [2.5098648, 0.34332547, 4.676404]\n",
      "263: Train Loss: [2.2507436, 0.35363626, 4.147851] | Test Loss: [2.6327631, 0.39062345, 4.8749027]\n",
      "264: Train Loss: [2.4623122, 0.39429533, 4.530329] | Test Loss: [2.5200558, 0.33407924, 4.7060323]\n",
      "265: Train Loss: [2.457146, 0.34540978, 4.568882] | Test Loss: [2.5680728, 0.38347048, 4.752675]\n",
      "266: Train Loss: [2.533166, 0.3704061, 4.6959257] | Test Loss: [2.7111564, 0.38746068, 5.034852]\n",
      "267: Train Loss: [2.3758273, 0.33513305, 4.4165215] | Test Loss: [2.7354412, 0.39428654, 5.076596]\n",
      "268: Train Loss: [2.3418758, 0.36451256, 4.319239] | Test Loss: [2.6745915, 0.37410048, 4.9750824]\n",
      "269: Train Loss: [2.4064806, 0.41415223, 4.398809] | Test Loss: [2.5575957, 0.3289945, 4.7861967]\n",
      "270: Train Loss: [2.4867823, 0.35593176, 4.617633] | Test Loss: [2.7092586, 0.3383, 5.0802174]\n",
      "271: Train Loss: [2.3540604, 0.33690792, 4.371213] | Test Loss: [2.6877468, 0.37848538, 4.9970083]\n",
      "272: Train Loss: [2.397095, 0.39326927, 4.400921] | Test Loss: [2.5080469, 0.38331935, 4.6327744]\n",
      "273: Train Loss: [2.3935082, 0.31257394, 4.4744425] | Test Loss: [2.6029766, 0.43882713, 4.767126]\n",
      "274: Train Loss: [2.3369656, 0.30659586, 4.3673353] | Test Loss: [2.5722015, 0.38740835, 4.7569947]\n",
      "275: Train Loss: [2.407761, 0.34299064, 4.472532] | Test Loss: [2.5636108, 0.32926288, 4.797959]\n",
      "276: Train Loss: [2.2793643, 0.2994479, 4.2592807] | Test Loss: [2.6114666, 0.3754986, 4.8474345]\n",
      "277: Train Loss: [2.427774, 0.30824316, 4.5473046] | Test Loss: [2.406425, 0.39893317, 4.413917]\n",
      "278: Train Loss: [2.632557, 0.36623836, 4.8988757] | Test Loss: [2.5457547, 0.39903277, 4.6924767]\n",
      "279: Train Loss: [2.4385154, 0.32370523, 4.5533257] | Test Loss: [2.6476738, 0.3437007, 4.951647]\n",
      "280: Train Loss: [2.24541, 0.33187544, 4.1589446] | Test Loss: [2.7425394, 0.39101905, 5.09406]\n",
      "281: Train Loss: [2.415187, 0.29848439, 4.5318894] | Test Loss: [2.7832904, 0.34860763, 5.217973]\n",
      "282: Train Loss: [2.342639, 0.29740408, 4.3878736] | Test Loss: [2.4772236, 0.36805522, 4.586392]\n",
      "283: Train Loss: [2.4877458, 0.30297923, 4.6725125] | Test Loss: [2.4375277, 0.39122123, 4.4838343]\n",
      "284: Train Loss: [2.1902063, 0.35336405, 4.0270486] | Test Loss: [2.5011427, 0.34852234, 4.6537633]\n",
      "285: Train Loss: [2.4819696, 0.3811781, 4.5827613] | Test Loss: [2.58752, 0.36962458, 4.805415]\n",
      "286: Train Loss: [2.3121872, 0.3275252, 4.2968493] | Test Loss: [2.6638784, 0.3650539, 4.962703]\n",
      "287: Train Loss: [2.360557, 0.33615854, 4.3849554] | Test Loss: [2.7206402, 0.42372185, 5.0175586]\n",
      "288: Train Loss: [2.3920715, 0.37822285, 4.40592] | Test Loss: [2.6935904, 0.385716, 5.001465]\n",
      "289: Train Loss: [2.5477223, 0.36981702, 4.7256274] | Test Loss: [2.6815426, 0.31480658, 5.048279]\n",
      "290: Train Loss: [2.476302, 0.34529364, 4.6073103] | Test Loss: [2.4912052, 0.36511552, 4.617295]\n",
      "291: Train Loss: [2.4325042, 0.36025134, 4.504757] | Test Loss: [2.6676674, 0.43451816, 4.9008164]\n",
      "292: Train Loss: [2.5410852, 0.3650051, 4.7171655] | Test Loss: [2.5973687, 0.36191303, 4.832824]\n",
      "293: Train Loss: [2.4748483, 0.31802425, 4.6316724] | Test Loss: [2.6495545, 0.33226755, 4.966841]\n",
      "294: Train Loss: [2.2647483, 0.30259058, 4.2269063] | Test Loss: [2.5442407, 0.29560387, 4.7928777]\n",
      "295: Train Loss: [2.3718596, 0.29368857, 4.4500303] | Test Loss: [2.7640126, 0.35579416, 5.172231]\n",
      "296: Train Loss: [2.428316, 0.5160906, 4.340542] | Test Loss: [2.689658, 0.37378925, 5.0055265]\n",
      "297: Train Loss: [2.440575, 0.45891225, 4.4222374] | Test Loss: [2.6005192, 0.36113733, 4.839901]\n",
      "298: Train Loss: [2.3652105, 0.37284184, 4.357579] | Test Loss: [2.6131606, 0.43924227, 4.787079]\n",
      "299: Train Loss: [2.347426, 0.35976896, 4.335083] | Test Loss: [2.7556164, 0.41834936, 5.0928836]\n",
      "300: Train Loss: [2.517906, 0.354679, 4.681133] | Test Loss: [2.6800623, 0.3912034, 4.968921]\n",
      "301: Train Loss: [2.5069547, 0.3861969, 4.6277122] | Test Loss: [2.641718, 0.3233397, 4.960096]\n",
      "302: Train Loss: [2.4189403, 0.37623796, 4.4616427] | Test Loss: [2.6705806, 0.39988205, 4.9412794]\n",
      "303: Train Loss: [2.4262028, 0.3485575, 4.503848] | Test Loss: [2.6036057, 0.4226472, 4.7845645]\n",
      "304: Train Loss: [2.3924322, 0.3318027, 4.4530616] | Test Loss: [2.5785847, 0.40987268, 4.747297]\n",
      "305: Train Loss: [2.510433, 0.3271796, 4.6936865] | Test Loss: [2.6983225, 0.36805776, 5.0285873]\n",
      "306: Train Loss: [2.3540673, 0.32506222, 4.3830724] | Test Loss: [2.5325582, 0.44331044, 4.621806]\n",
      "307: Train Loss: [2.3805256, 0.31546864, 4.4455824] | Test Loss: [2.6063023, 0.3849237, 4.8276806]\n",
      "308: Train Loss: [2.3311536, 0.42124417, 4.241063] | Test Loss: [2.4682083, 0.3887369, 4.54768]\n",
      "309: Train Loss: [2.312783, 0.2926287, 4.3329372] | Test Loss: [2.674687, 0.39310345, 4.95627]\n",
      "310: Train Loss: [2.2375045, 0.30600622, 4.1690025] | Test Loss: [2.5913694, 0.35093698, 4.831802]\n",
      "311: Train Loss: [2.5212781, 0.3959632, 4.646593] | Test Loss: [2.582211, 0.32020175, 4.84422]\n",
      "312: Train Loss: [2.5070307, 0.5056209, 4.5084405] | Test Loss: [2.8127785, 0.49751472, 5.128042]\n",
      "313: Train Loss: [2.3827758, 0.37085176, 4.3946996] | Test Loss: [2.5830083, 0.44329035, 4.7227263]\n",
      "314: Train Loss: [2.3504982, 0.3943249, 4.3066716] | Test Loss: [2.5931394, 0.3712146, 4.8150644]\n",
      "315: Train Loss: [2.4633374, 0.37692466, 4.5497503] | Test Loss: [2.5108464, 0.34786224, 4.6738305]\n",
      "316: Train Loss: [2.4809816, 0.34861773, 4.6133456] | Test Loss: [2.690585, 0.33037552, 5.050794]\n",
      "317: Train Loss: [2.2206967, 0.31335768, 4.1280355] | Test Loss: [2.5684676, 0.30610168, 4.8308334]\n",
      "318: Train Loss: [2.382622, 0.31343555, 4.4518085] | Test Loss: [2.7585142, 0.38199806, 5.1350303]\n",
      "319: Train Loss: [2.4795845, 0.44807985, 4.511089] | Test Loss: [2.7613192, 0.39817587, 5.1244626]\n",
      "320: Train Loss: [2.4921622, 0.36776662, 4.616558] | Test Loss: [2.8308733, 0.4206124, 5.241134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321: Train Loss: [2.4216335, 0.34791717, 4.49535] | Test Loss: [2.5703125, 0.33592623, 4.804699]\n",
      "322: Train Loss: [2.2310922, 0.3270863, 4.135098] | Test Loss: [2.641181, 0.3374622, 4.9448996]\n",
      "323: Train Loss: [2.333301, 0.35655573, 4.310046] | Test Loss: [2.4262743, 0.3433372, 4.5092115]\n",
      "324: Train Loss: [2.3713262, 0.38931808, 4.3533344] | Test Loss: [2.8077376, 0.36789247, 5.247583]\n",
      "325: Train Loss: [2.345152, 0.3596368, 4.330667] | Test Loss: [2.745331, 0.44226202, 5.0484]\n",
      "326: Train Loss: [2.5627952, 0.39823067, 4.72736] | Test Loss: [2.4970648, 0.39476076, 4.599369]\n",
      "327: Train Loss: [2.437505, 0.37987345, 4.4951367] | Test Loss: [2.409058, 0.33848098, 4.4796352]\n",
      "328: Train Loss: [2.5628042, 0.4194623, 4.7061462] | Test Loss: [2.7025518, 0.3750797, 5.030024]\n",
      "329: Train Loss: [2.479296, 0.3925909, 4.566001] | Test Loss: [2.557189, 0.33506727, 4.7793107]\n",
      "330: Train Loss: [2.643878, 0.3701755, 4.9175806] | Test Loss: [2.629452, 0.3601797, 4.898724]\n",
      "331: Train Loss: [2.5210435, 0.32396224, 4.718125] | Test Loss: [2.6618016, 0.35546568, 4.9681373]\n",
      "332: Train Loss: [2.3213863, 0.3490541, 4.2937183] | Test Loss: [2.568109, 0.43154797, 4.70467]\n",
      "333: Train Loss: [2.5071788, 0.35233712, 4.6620207] | Test Loss: [2.537858, 0.30996856, 4.7657475]\n",
      "334: Train Loss: [2.4576585, 0.4109282, 4.504389] | Test Loss: [2.758768, 0.33545625, 5.18208]\n",
      "335: Train Loss: [2.3107383, 0.51663405, 4.1048427] | Test Loss: [2.6135902, 0.45682597, 4.7703543]\n",
      "336: Train Loss: [2.3483553, 0.41073704, 4.2859735] | Test Loss: [2.6077106, 0.37578866, 4.8396325]\n",
      "337: Train Loss: [2.5306072, 0.30385587, 4.7573586] | Test Loss: [2.4715545, 0.39299393, 4.550115]\n",
      "338: Train Loss: [2.4835067, 0.41634256, 4.5506706] | Test Loss: [2.6067991, 0.37384132, 4.839757]\n",
      "339: Train Loss: [2.4280818, 0.38261467, 4.473549] | Test Loss: [2.5262132, 0.32963592, 4.7227902]\n",
      "340: Train Loss: [2.274469, 0.35863748, 4.1903005] | Test Loss: [2.701567, 0.33091888, 5.072215]\n",
      "341: Train Loss: [2.2771142, 0.36706343, 4.187165] | Test Loss: [2.720447, 0.33011815, 5.110776]\n",
      "342: Train Loss: [2.1993005, 0.34722227, 4.0513787] | Test Loss: [2.7588327, 0.35423785, 5.1634274]\n",
      "343: Train Loss: [2.4733071, 0.40165126, 4.544963] | Test Loss: [2.5928516, 0.3618177, 4.8238854]\n",
      "344: Train Loss: [2.3823116, 0.35831836, 4.406305] | Test Loss: [2.7724707, 0.40208298, 5.1428585]\n",
      "345: Train Loss: [2.3245695, 0.31772473, 4.331414] | Test Loss: [2.7005928, 0.4312643, 4.969921]\n",
      "346: Train Loss: [2.3231258, 0.46590492, 4.180347] | Test Loss: [2.5904768, 0.37608492, 4.8048687]\n",
      "347: Train Loss: [2.4022737, 0.37439376, 4.4301534] | Test Loss: [2.6211438, 0.35962698, 4.882661]\n",
      "348: Train Loss: [2.3479471, 0.33159077, 4.3643036] | Test Loss: [2.6570203, 0.33895195, 4.9750886]\n",
      "349: Train Loss: [2.3223498, 0.36228144, 4.2824183] | Test Loss: [2.679695, 0.42526442, 4.9341254]\n",
      "350: Train Loss: [2.572254, 0.39343455, 4.7510734] | Test Loss: [2.6151345, 0.36759654, 4.8626723]\n",
      "351: Train Loss: [2.2814956, 0.30312836, 4.259863] | Test Loss: [2.4966667, 0.36838296, 4.6249504]\n",
      "352: Train Loss: [2.3484962, 0.3163615, 4.380631] | Test Loss: [2.5095227, 0.30727673, 4.7117686]\n",
      "353: Train Loss: [2.4127584, 0.31629485, 4.509222] | Test Loss: [2.6578252, 0.39963067, 4.91602]\n",
      "354: Train Loss: [2.4039357, 0.44838512, 4.359486] | Test Loss: [2.6330402, 0.5566205, 4.70946]\n",
      "355: Train Loss: [2.4515693, 0.3685115, 4.534627] | Test Loss: [2.5599897, 0.3804964, 4.739483]\n",
      "356: Train Loss: [2.6529472, 0.34153828, 4.964356] | Test Loss: [2.6278589, 0.42665002, 4.8290677]\n",
      "357: Train Loss: [2.3879724, 0.34135208, 4.4345927] | Test Loss: [2.6207557, 0.3596717, 4.8818398]\n",
      "358: Train Loss: [2.3420575, 0.3988069, 4.285308] | Test Loss: [2.6246839, 0.29339015, 4.9559774]\n",
      "359: Train Loss: [2.4095783, 0.41528454, 4.403872] | Test Loss: [2.7696755, 0.3523263, 5.1870246]\n",
      "360: Train Loss: [2.3479023, 0.43344417, 4.2623606] | Test Loss: [2.7044406, 0.3433994, 5.0654817]\n",
      "361: Train Loss: [2.534429, 0.30840674, 4.7604513] | Test Loss: [2.4438775, 0.39249182, 4.495263]\n",
      "362: Train Loss: [2.5542197, 0.33826318, 4.7701764] | Test Loss: [2.5448835, 0.39452595, 4.695241]\n",
      "363: Train Loss: [2.5096173, 0.4271995, 4.5920353] | Test Loss: [2.8367753, 0.36409575, 5.309455]\n",
      "364: Train Loss: [2.4353776, 0.3872395, 4.4835157] | Test Loss: [2.6340413, 0.43548486, 4.8325977]\n",
      "365: Train Loss: [2.4747176, 0.35502934, 4.5944057] | Test Loss: [2.672301, 0.35564184, 4.9889603]\n",
      "366: Train Loss: [2.5314784, 0.36356825, 4.6993885] | Test Loss: [2.6946049, 0.3714689, 5.0177407]\n",
      "367: Train Loss: [2.366491, 0.31544602, 4.4175363] | Test Loss: [2.5674407, 0.33610168, 4.79878]\n",
      "368: Train Loss: [2.3447697, 0.33917364, 4.3503656] | Test Loss: [2.5566573, 0.3895101, 4.7238045]\n",
      "369: Train Loss: [2.5820541, 0.30301777, 4.8610907] | Test Loss: [2.6088588, 0.3571656, 4.860552]\n",
      "370: Train Loss: [2.2676063, 0.3315962, 4.203616] | Test Loss: [2.5183806, 0.31225467, 4.7245064]\n",
      "371: Train Loss: [2.4550185, 0.32644024, 4.5835967] | Test Loss: [2.695886, 0.47884175, 4.91293]\n",
      "372: Train Loss: [2.5639658, 0.3614399, 4.766492] | Test Loss: [2.671685, 0.39440683, 4.948963]\n",
      "373: Train Loss: [2.480255, 0.39655504, 4.563955] | Test Loss: [2.7781982, 0.5129254, 5.0434713]\n",
      "374: Train Loss: [2.4535298, 0.3882314, 4.5188284] | Test Loss: [2.7328658, 0.36252782, 5.103204]\n",
      "375: Train Loss: [2.4858675, 0.34274876, 4.6289864] | Test Loss: [2.453606, 0.33418438, 4.5730276]\n",
      "376: Train Loss: [2.380757, 0.32375735, 4.437757] | Test Loss: [2.399691, 0.31409502, 4.485287]\n",
      "377: Train Loss: [2.4752932, 0.3972965, 4.55329] | Test Loss: [2.781142, 0.36034605, 5.201938]\n",
      "378: Train Loss: [2.429025, 0.31427166, 4.5437784] | Test Loss: [2.630595, 0.34738404, 4.913806]\n",
      "379: Train Loss: [2.4368536, 0.38156986, 4.4921374] | Test Loss: [2.5711834, 0.35869172, 4.783675]\n",
      "380: Train Loss: [2.2925766, 0.3488719, 4.2362814] | Test Loss: [2.7960317, 0.39192232, 5.200141]\n",
      "381: Train Loss: [2.4495356, 0.2837779, 4.6152935] | Test Loss: [2.575614, 0.37628487, 4.774943]\n",
      "382: Train Loss: [2.5797231, 0.35768375, 4.8017626] | Test Loss: [2.4497993, 0.41956976, 4.4800286]\n",
      "383: Train Loss: [2.2712848, 0.30592784, 4.236642] | Test Loss: [2.5533826, 0.35731775, 4.7494473]\n",
      "384: Train Loss: [2.3501751, 0.42140737, 4.278943] | Test Loss: [2.6228018, 0.32017806, 4.9254255]\n",
      "385: Train Loss: [2.3593545, 0.2818438, 4.4368653] | Test Loss: [2.596693, 0.41461787, 4.778768]\n",
      "386: Train Loss: [2.3853896, 0.35697535, 4.4138036] | Test Loss: [2.7832031, 0.38745162, 5.1789546]\n",
      "387: Train Loss: [2.4126701, 0.35634297, 4.4689975] | Test Loss: [2.6001468, 0.32294303, 4.8773503]\n",
      "388: Train Loss: [2.445682, 0.30012512, 4.591239] | Test Loss: [2.6536114, 0.37585217, 4.9313707]\n",
      "389: Train Loss: [2.4659133, 0.40887007, 4.5229564] | Test Loss: [2.643376, 0.33348462, 4.9532676]\n",
      "390: Train Loss: [2.502562, 0.37815276, 4.6269712] | Test Loss: [2.7728548, 0.3398915, 5.205818]\n",
      "391: Train Loss: [2.5501745, 0.3561634, 4.7441854] | Test Loss: [2.3987281, 0.39031735, 4.407139]\n",
      "392: Train Loss: [2.3235486, 0.36969915, 4.277398] | Test Loss: [2.5987978, 0.3562931, 4.8413024]\n",
      "393: Train Loss: [2.4243445, 0.3657811, 4.482908] | Test Loss: [2.1727693, 0.34639594, 3.999143]\n",
      "394: Train Loss: [2.4422684, 0.41739762, 4.4671392] | Test Loss: [2.7490115, 0.34878334, 5.1492395]\n",
      "395: Train Loss: [2.5124612, 0.4441923, 4.58073] | Test Loss: [2.3831842, 0.41846228, 4.347906]\n",
      "396: Train Loss: [2.5136333, 0.39405698, 4.6332097] | Test Loss: [2.6995866, 0.33962214, 5.0595512]\n",
      "397: Train Loss: [2.3429508, 0.33791533, 4.347986] | Test Loss: [2.5117338, 0.33955377, 4.6839137]\n",
      "398: Train Loss: [2.4537714, 0.4123043, 4.4952383] | Test Loss: [2.657051, 0.3776551, 4.936447]\n",
      "399: Train Loss: [2.4266543, 0.3324797, 4.520829] | Test Loss: [2.4786787, 0.35610008, 4.6012573]\n",
      "400: Train Loss: [2.4243767, 0.4106217, 4.438132] | Test Loss: [2.8040571, 0.33270264, 5.2754116]\n",
      "401: Train Loss: [2.3860831, 0.3759441, 4.396222] | Test Loss: [2.8788705, 0.45288885, 5.304852]\n",
      "402: Train Loss: [2.4426072, 0.33845478, 4.5467596] | Test Loss: [2.5860853, 0.3571458, 4.815025]\n",
      "403: Train Loss: [2.4261184, 0.43059707, 4.4216394] | Test Loss: [2.5188036, 0.33188513, 4.705722]\n",
      "404: Train Loss: [2.3988833, 0.41563073, 4.382136] | Test Loss: [2.6707625, 0.40863848, 4.9328866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405: Train Loss: [2.2792265, 0.30329934, 4.2551537] | Test Loss: [2.4782403, 0.32764766, 4.628833]\n",
      "406: Train Loss: [2.3472133, 0.29287517, 4.4015512] | Test Loss: [2.710809, 0.3477755, 5.0738425]\n",
      "407: Train Loss: [2.5682735, 0.37273842, 4.7638087] | Test Loss: [2.6161911, 0.37414587, 4.8582363]\n",
      "408: Train Loss: [2.4884138, 0.37969947, 4.597128] | Test Loss: [2.63306, 0.4344598, 4.8316603]\n",
      "409: Train Loss: [2.423215, 0.285864, 4.560566] | Test Loss: [2.6896584, 0.36249906, 5.0168176]\n",
      "410: Train Loss: [2.395392, 0.34947473, 4.441309] | Test Loss: [2.7394698, 0.43775943, 5.04118]\n",
      "411: Train Loss: [2.5060964, 0.36710545, 4.6450872] | Test Loss: [2.575065, 0.4449659, 4.705164]\n",
      "412: Train Loss: [2.4839232, 0.36112612, 4.6067204] | Test Loss: [2.5706487, 0.35269105, 4.788606]\n",
      "413: Train Loss: [2.524961, 0.42556074, 4.624361] | Test Loss: [2.727838, 0.3233769, 5.1322994]\n",
      "414: Train Loss: [2.344109, 0.35269827, 4.33552] | Test Loss: [2.6192715, 0.35164893, 4.886894]\n",
      "415: Train Loss: [2.430365, 0.3380927, 4.5226374] | Test Loss: [2.6687706, 0.4119671, 4.925574]\n",
      "416: Train Loss: [2.4548151, 0.39240772, 4.5172224] | Test Loss: [2.5294952, 0.45107552, 4.607915]\n",
      "417: Train Loss: [2.4837244, 0.34755033, 4.6198983] | Test Loss: [2.5433695, 0.3521555, 4.7345834]\n",
      "418: Train Loss: [2.3739533, 0.3531185, 4.3947883] | Test Loss: [2.5385094, 0.3534312, 4.7235875]\n",
      "419: Train Loss: [2.4515753, 0.31913155, 4.584019] | Test Loss: [2.684686, 0.34275252, 5.0266194]\n",
      "420: Train Loss: [2.2510576, 0.3566013, 4.145514] | Test Loss: [2.7457232, 0.31132215, 5.1801243]\n",
      "421: Train Loss: [2.5121772, 0.33280784, 4.6915464] | Test Loss: [2.4534888, 0.34332523, 4.5636525]\n",
      "422: Train Loss: [2.303789, 0.36337325, 4.2442045] | Test Loss: [2.6492531, 0.4197917, 4.8787146]\n",
      "423: Train Loss: [2.4607909, 0.3455214, 4.5760603] | Test Loss: [2.9101567, 0.56080395, 5.2595096]\n",
      "424: Train Loss: [2.4909015, 0.3010475, 4.6807556] | Test Loss: [2.5900898, 0.33528048, 4.844899]\n",
      "425: Train Loss: [2.3476818, 0.35632432, 4.3390393] | Test Loss: [2.6719387, 0.3746739, 4.9692035]\n",
      "426: Train Loss: [2.5374823, 0.3289779, 4.7459865] | Test Loss: [2.5777335, 0.2969125, 4.8585544]\n",
      "427: Train Loss: [2.37918, 0.37867957, 4.37968] | Test Loss: [2.4225218, 0.3652962, 4.4797473]\n",
      "428: Train Loss: [2.2402349, 0.35988575, 4.120584] | Test Loss: [2.399191, 0.4314173, 4.3669643]\n",
      "429: Train Loss: [2.4878888, 0.44752344, 4.528254] | Test Loss: [2.7821097, 0.44845045, 5.115769]\n",
      "430: Train Loss: [2.346713, 0.38115272, 4.3122735] | Test Loss: [2.681179, 0.40716952, 4.9551888]\n",
      "431: Train Loss: [2.307016, 0.44716224, 4.1668696] | Test Loss: [2.6503918, 0.3570892, 4.9436946]\n",
      "432: Train Loss: [2.620243, 0.3331933, 4.907293] | Test Loss: [2.6493964, 0.3781031, 4.9206896]\n",
      "433: Train Loss: [2.6112118, 0.35025495, 4.8721685] | Test Loss: [2.689224, 0.3622079, 5.01624]\n",
      "434: Train Loss: [2.6142442, 0.34837753, 4.8801107] | Test Loss: [2.5940826, 0.42332914, 4.764836]\n",
      "435: Train Loss: [2.391073, 0.33388013, 4.448266] | Test Loss: [2.558829, 0.44612172, 4.6715364]\n",
      "436: Train Loss: [2.4198265, 0.44111866, 4.3985343] | Test Loss: [2.71584, 0.40680054, 5.0248795]\n",
      "437: Train Loss: [2.40837, 0.37667912, 4.440061] | Test Loss: [2.8599477, 0.32028913, 5.399606]\n",
      "438: Train Loss: [2.5030384, 0.33144483, 4.674632] | Test Loss: [2.4596744, 0.34221065, 4.577138]\n",
      "439: Train Loss: [2.374948, 0.39117292, 4.358723] | Test Loss: [2.5620987, 0.32574394, 4.7984533]\n",
      "440: Train Loss: [2.574153, 0.38663226, 4.7616735] | Test Loss: [2.626226, 0.3613952, 4.8910565]\n",
      "441: Train Loss: [2.4242227, 0.40439448, 4.444051] | Test Loss: [2.5997279, 0.3637323, 4.8357234]\n",
      "442: Train Loss: [2.6844647, 0.3408224, 5.028107] | Test Loss: [2.6471038, 0.37332222, 4.9208856]\n",
      "443: Train Loss: [2.3724077, 0.34675723, 4.398058] | Test Loss: [2.6252606, 0.42515904, 4.825362]\n",
      "444: Train Loss: [2.4722629, 0.31388, 4.6306458] | Test Loss: [2.6207743, 0.3808281, 4.8607206]\n",
      "445: Train Loss: [2.543381, 0.3714601, 4.715302] | Test Loss: [2.411426, 0.42001566, 4.4028363]\n",
      "446: Train Loss: [2.5366185, 0.38180563, 4.6914315] | Test Loss: [2.642445, 0.37547365, 4.9094167]\n",
      "447: Train Loss: [2.5559347, 0.3416203, 4.770249] | Test Loss: [2.6393766, 0.4207917, 4.8579617]\n",
      "448: Train Loss: [2.3555317, 0.46487293, 4.2461905] | Test Loss: [2.7230964, 0.39111778, 5.055075]\n",
      "449: Train Loss: [2.3797255, 0.33098966, 4.428461] | Test Loss: [2.5188308, 0.32263976, 4.7150216]\n",
      "450: Train Loss: [2.4165292, 0.33234027, 4.500718] | Test Loss: [2.6172426, 0.3091956, 4.9252896]\n",
      "451: Train Loss: [2.4853435, 0.45201334, 4.5186734] | Test Loss: [2.7127748, 0.37464586, 5.050904]\n",
      "452: Train Loss: [2.5024986, 0.4272049, 4.577792] | Test Loss: [2.6807408, 0.35921612, 5.0022655]\n",
      "453: Train Loss: [2.5251954, 0.38296854, 4.6674223] | Test Loss: [2.5330033, 0.38420835, 4.6817985]\n",
      "454: Train Loss: [2.3880036, 0.33694988, 4.4390574] | Test Loss: [2.2945435, 0.35345453, 4.2356324]\n",
      "455: Train Loss: [2.3749175, 0.39675942, 4.3530755] | Test Loss: [2.619811, 0.34631038, 4.893312]\n",
      "456: Train Loss: [2.3963704, 0.37048432, 4.4222565] | Test Loss: [2.5578809, 0.3984163, 4.717345]\n",
      "457: Train Loss: [2.3201683, 0.3246913, 4.315645] | Test Loss: [2.5777025, 0.38325423, 4.772151]\n",
      "458: Train Loss: [2.1829665, 0.36381647, 4.0021167] | Test Loss: [2.632104, 0.34692314, 4.9172845]\n",
      "459: Train Loss: [2.340354, 0.38420707, 4.2965007] | Test Loss: [2.7308826, 0.35290593, 5.1088595]\n",
      "460: Train Loss: [2.4113665, 0.33729476, 4.4854383] | Test Loss: [2.6276133, 0.37115195, 4.8840747]\n",
      "461: Train Loss: [2.4315562, 0.27436948, 4.5887427] | Test Loss: [2.6866412, 0.3369003, 5.036382]\n",
      "462: Train Loss: [2.2178931, 0.34395593, 4.0918303] | Test Loss: [2.713, 0.3791419, 5.0468583]\n",
      "463: Train Loss: [2.4461358, 0.31186524, 4.580406] | Test Loss: [2.7114663, 0.40398717, 5.0189457]\n",
      "464: Train Loss: [2.5061061, 0.46156564, 4.550647] | Test Loss: [2.596101, 0.384457, 4.807745]\n",
      "465: Train Loss: [2.4758744, 0.31682757, 4.634921] | Test Loss: [2.5171409, 0.39686042, 4.637421]\n",
      "466: Train Loss: [2.4803746, 0.35309595, 4.607653] | Test Loss: [2.657549, 0.47582006, 4.8392777]\n",
      "467: Train Loss: [2.1721683, 0.34756184, 3.9967747] | Test Loss: [2.587354, 0.4050737, 4.7696342]\n",
      "468: Train Loss: [2.390499, 0.34555212, 4.4354463] | Test Loss: [2.6773105, 0.4025504, 4.9520707]\n",
      "469: Train Loss: [2.527126, 0.33283547, 4.7214165] | Test Loss: [2.5677197, 0.32353914, 4.8119]\n",
      "470: Train Loss: [2.4613497, 0.40265745, 4.520042] | Test Loss: [2.6933725, 0.31444803, 5.072297]\n",
      "471: Train Loss: [2.352442, 0.3441137, 4.36077] | Test Loss: [2.567937, 0.37631464, 4.759559]\n",
      "472: Train Loss: [2.440271, 0.3205939, 4.559948] | Test Loss: [2.8005435, 0.39835435, 5.2027326]\n",
      "473: Train Loss: [2.1024008, 0.25512302, 3.9496784] | Test Loss: [2.5940924, 0.30014822, 4.8880367]\n",
      "474: Train Loss: [2.3947814, 0.4005254, 4.389037] | Test Loss: [2.7727737, 0.40830258, 5.1372447]\n",
      "475: Train Loss: [2.5716314, 0.39017013, 4.753093] | Test Loss: [2.502165, 0.2819132, 4.722417]\n",
      "476: Train Loss: [2.34157, 0.3339705, 4.3491693] | Test Loss: [2.633321, 0.34844628, 4.9181957]\n",
      "477: Train Loss: [2.3251677, 0.31346264, 4.3368726] | Test Loss: [2.6720133, 0.46499515, 4.879031]\n",
      "478: Train Loss: [2.4013, 0.3317599, 4.47084] | Test Loss: [2.6231198, 0.46501985, 4.78122]\n",
      "479: Train Loss: [2.4186406, 0.4441981, 4.393083] | Test Loss: [2.4200778, 0.31339526, 4.52676]\n",
      "480: Train Loss: [2.4403286, 0.34913266, 4.5315247] | Test Loss: [2.622306, 0.33614638, 4.908466]\n",
      "481: Train Loss: [2.5201259, 0.37878147, 4.6614704] | Test Loss: [2.655528, 0.33060294, 4.980453]\n",
      "482: Train Loss: [2.455132, 0.3313599, 4.578904] | Test Loss: [2.714973, 0.3457987, 5.0841475]\n",
      "483: Train Loss: [2.4409413, 0.36227334, 4.5196095] | Test Loss: [2.876358, 0.6656213, 5.087095]\n",
      "484: Train Loss: [2.2768636, 0.285711, 4.2680163] | Test Loss: [2.7802289, 0.3565103, 5.2039475]\n",
      "485: Train Loss: [2.4603746, 0.34274855, 4.5780005] | Test Loss: [2.6686873, 0.387468, 4.949907]\n",
      "486: Train Loss: [2.410631, 0.41712722, 4.4041348] | Test Loss: [2.4406009, 0.28506568, 4.596136]\n",
      "487: Train Loss: [2.384813, 0.380318, 4.389308] | Test Loss: [2.6316216, 0.3908473, 4.872396]\n",
      "488: Train Loss: [2.2442129, 0.31282365, 4.175602] | Test Loss: [2.5513165, 0.35541075, 4.7472224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489: Train Loss: [2.7363918, 0.5519959, 4.920788] | Test Loss: [2.5268888, 0.38145238, 4.672325]\n",
      "490: Train Loss: [2.4736223, 0.43020117, 4.5170436] | Test Loss: [2.6650417, 0.38230935, 4.947774]\n",
      "491: Train Loss: [2.513603, 0.37399995, 4.653206] | Test Loss: [2.6535969, 0.39617696, 4.911017]\n",
      "492: Train Loss: [2.3905983, 0.36443365, 4.416763] | Test Loss: [2.709479, 0.3626838, 5.0562744]\n",
      "493: Train Loss: [2.5062156, 0.3994499, 4.6129813] | Test Loss: [2.6079042, 0.33188117, 4.8839273]\n",
      "494: Train Loss: [2.39291, 0.37334552, 4.4124746] | Test Loss: [2.7494144, 0.35633796, 5.142491]\n",
      "495: Train Loss: [2.3109229, 0.42665848, 4.195187] | Test Loss: [2.5296483, 0.38752484, 4.671772]\n",
      "496: Train Loss: [2.25539, 0.38007766, 4.130702] | Test Loss: [2.5917532, 0.4026598, 4.7808466]\n",
      "497: Train Loss: [2.4981856, 0.33752194, 4.6588492] | Test Loss: [2.6775057, 0.38210544, 4.972906]\n",
      "498: Train Loss: [2.517301, 0.35206154, 4.6825404] | Test Loss: [2.5446882, 0.39106762, 4.698309]\n",
      "499: Train Loss: [2.3773508, 0.3489808, 4.4057207] | Test Loss: [2.7610893, 0.5420176, 4.980161]\n",
      "500: Train Loss: [2.5318596, 0.37058002, 4.693139] | Test Loss: [2.6515627, 0.31345904, 4.9896665]\n",
      "501: Train Loss: [2.3984702, 0.33739218, 4.459548] | Test Loss: [2.527722, 0.3308266, 4.724617]\n",
      "502: Train Loss: [2.3484406, 0.32520097, 4.3716803] | Test Loss: [2.6971562, 0.47471443, 4.919598]\n",
      "503: Train Loss: [2.3522642, 0.35010502, 4.3544235] | Test Loss: [2.5309741, 0.4048019, 4.6571465]\n",
      "504: Train Loss: [2.4491782, 0.4017834, 4.496573] | Test Loss: [2.52772, 0.35016564, 4.705274]\n",
      "505: Train Loss: [2.543381, 0.33680508, 4.749957] | Test Loss: [2.606816, 0.39315382, 4.8204784]\n",
      "506: Train Loss: [2.4060006, 0.3173462, 4.494655] | Test Loss: [2.576323, 0.3592772, 4.793369]\n",
      "507: Train Loss: [2.3881783, 0.39346993, 4.382887] | Test Loss: [2.6300669, 0.36674148, 4.893392]\n",
      "508: Train Loss: [2.4265046, 0.3468123, 4.506197] | Test Loss: [2.7508311, 0.37444958, 5.1272125]\n",
      "509: Train Loss: [2.5204601, 0.3762048, 4.6647153] | Test Loss: [2.506409, 0.40636575, 4.606452]\n",
      "510: Train Loss: [2.5237079, 0.42327568, 4.6241403] | Test Loss: [2.6317523, 0.31253698, 4.9509673]\n",
      "511: Train Loss: [2.374944, 0.35562035, 4.3942676] | Test Loss: [2.4388726, 0.34413856, 4.5336065]\n",
      "512: Train Loss: [2.4192004, 0.33201754, 4.5063834] | Test Loss: [2.7906578, 0.34050384, 5.240812]\n",
      "513: Train Loss: [2.4218044, 0.44090647, 4.4027023] | Test Loss: [2.7100334, 0.41494834, 5.0051184]\n",
      "514: Train Loss: [2.461204, 0.33572856, 4.5866795] | Test Loss: [2.72697, 0.39783096, 5.056109]\n",
      "515: Train Loss: [2.4587307, 0.3454893, 4.571972] | Test Loss: [2.6172378, 0.3054836, 4.928992]\n",
      "516: Train Loss: [2.3063354, 0.41027653, 4.2023945] | Test Loss: [2.7014625, 0.42232856, 4.9805965]\n",
      "517: Train Loss: [2.5512438, 0.3837163, 4.7187715] | Test Loss: [2.5393405, 0.33403942, 4.744642]\n",
      "518: Train Loss: [2.5154467, 0.3384006, 4.692493] | Test Loss: [2.6910763, 0.3707735, 5.0113792]\n",
      "519: Train Loss: [2.4372845, 0.33603725, 4.538532] | Test Loss: [2.4321048, 0.3355191, 4.5286903]\n",
      "520: Train Loss: [2.4715905, 0.32944834, 4.613733] | Test Loss: [2.45429, 0.40621355, 4.502366]\n",
      "521: Train Loss: [2.5795355, 0.31990692, 4.8391643] | Test Loss: [2.5397923, 0.31175116, 4.767833]\n",
      "522: Train Loss: [2.3078094, 0.34798902, 4.2676296] | Test Loss: [2.519105, 0.38612702, 4.652083]\n",
      "523: Train Loss: [2.5736587, 0.42396894, 4.7233486] | Test Loss: [2.5421112, 0.38436574, 4.6998568]\n",
      "Epoch 12\n",
      "0: Train Loss: [2.339706, 0.39728543, 4.2821264] | Test Loss: [2.6681561, 0.40727735, 4.929035]\n",
      "1: Train Loss: [2.2604203, 0.33518222, 4.1856585] | Test Loss: [2.800819, 0.38925672, 5.212381]\n",
      "2: Train Loss: [2.2495, 0.40652996, 4.09247] | Test Loss: [2.6093822, 0.3807443, 4.83802]\n",
      "3: Train Loss: [2.2868695, 0.31833485, 4.255404] | Test Loss: [2.5188665, 0.3324425, 4.705291]\n",
      "4: Train Loss: [2.2743726, 0.36581087, 4.1829343] | Test Loss: [2.8080184, 0.39479208, 5.221245]\n",
      "5: Train Loss: [2.253919, 0.41683537, 4.0910025] | Test Loss: [2.6079836, 0.39769936, 4.818268]\n",
      "6: Train Loss: [2.2998254, 0.40316954, 4.196481] | Test Loss: [2.5210767, 0.35610244, 4.686051]\n",
      "7: Train Loss: [2.2941175, 0.3737162, 4.2145185] | Test Loss: [2.6487186, 0.3828327, 4.9146047]\n",
      "8: Train Loss: [2.1777074, 0.3574257, 3.9979892] | Test Loss: [2.4414842, 0.32673925, 4.556229]\n",
      "9: Train Loss: [2.2878928, 0.3481159, 4.2276697] | Test Loss: [2.8710954, 0.47342908, 5.2687616]\n",
      "10: Train Loss: [2.253469, 0.34541637, 4.1615214] | Test Loss: [2.694851, 0.35993963, 5.0297623]\n",
      "11: Train Loss: [2.1956956, 0.3716319, 4.019759] | Test Loss: [2.5821743, 0.43301496, 4.7313337]\n",
      "12: Train Loss: [2.11407, 0.35851634, 3.8696237] | Test Loss: [2.539818, 0.3854772, 4.694159]\n",
      "13: Train Loss: [2.262114, 0.32229686, 4.2019315] | Test Loss: [2.5595567, 0.32368937, 4.795424]\n",
      "14: Train Loss: [2.2194326, 0.33634606, 4.102519] | Test Loss: [2.4901588, 0.37254873, 4.607769]\n",
      "15: Train Loss: [2.4042785, 0.45666635, 4.3518906] | Test Loss: [2.7197847, 0.35365778, 5.0859118]\n",
      "16: Train Loss: [2.2010899, 0.33105135, 4.0711284] | Test Loss: [2.6346443, 0.39888126, 4.870407]\n",
      "17: Train Loss: [2.2272778, 0.3665229, 4.0880327] | Test Loss: [2.748403, 0.3398975, 5.1569085]\n",
      "18: Train Loss: [2.3325732, 0.33047396, 4.3346725] | Test Loss: [2.5081978, 0.33294097, 4.6834545]\n",
      "19: Train Loss: [2.3928237, 0.3533993, 4.432248] | Test Loss: [2.9676416, 0.34173408, 5.5935493]\n",
      "20: Train Loss: [2.258201, 0.324824, 4.191578] | Test Loss: [2.5576842, 0.4067244, 4.708644]\n",
      "21: Train Loss: [2.4570732, 0.3480407, 4.566106] | Test Loss: [2.7529945, 0.47525012, 5.030739]\n",
      "22: Train Loss: [2.232968, 0.32507572, 4.1408606] | Test Loss: [2.6873457, 0.35756305, 5.0171285]\n",
      "23: Train Loss: [2.3288293, 0.33111352, 4.3265452] | Test Loss: [2.606531, 0.39669588, 4.8163657]\n",
      "24: Train Loss: [2.4135723, 0.3993173, 4.4278274] | Test Loss: [2.516126, 0.3629964, 4.6692553]\n",
      "25: Train Loss: [2.4523256, 0.4568641, 4.4477873] | Test Loss: [2.62472, 0.37572172, 4.8737183]\n",
      "26: Train Loss: [2.2964668, 0.3218833, 4.2710505] | Test Loss: [2.4893465, 0.30192325, 4.6767697]\n",
      "27: Train Loss: [2.1381056, 0.35050958, 3.9257019] | Test Loss: [2.6939409, 0.3929036, 4.994978]\n",
      "28: Train Loss: [2.2482526, 0.35831034, 4.138195] | Test Loss: [2.6102362, 0.36992574, 4.850547]\n",
      "29: Train Loss: [2.2641728, 0.31556723, 4.2127786] | Test Loss: [2.7320862, 0.39192152, 5.072251]\n",
      "30: Train Loss: [2.2776349, 0.34676626, 4.2085032] | Test Loss: [2.6834185, 0.36242166, 5.0044155]\n",
      "31: Train Loss: [2.1729538, 0.42537242, 3.920535] | Test Loss: [2.7765112, 0.43823564, 5.1147866]\n",
      "32: Train Loss: [2.342173, 0.38725966, 4.2970867] | Test Loss: [2.5762496, 0.2763038, 4.8761954]\n",
      "33: Train Loss: [2.2475693, 0.34801835, 4.1471205] | Test Loss: [2.6379335, 0.3710138, 4.9048533]\n",
      "34: Train Loss: [2.2662725, 0.32512248, 4.2074227] | Test Loss: [2.5311666, 0.39367676, 4.6686563]\n",
      "35: Train Loss: [2.4205127, 0.35438797, 4.4866376] | Test Loss: [2.6873121, 0.36359835, 5.011026]\n",
      "36: Train Loss: [2.25433, 0.35401484, 4.154645] | Test Loss: [2.6090698, 0.33298644, 4.8851533]\n",
      "37: Train Loss: [2.3475866, 0.3099441, 4.385229] | Test Loss: [2.6816876, 0.39582142, 4.9675536]\n",
      "38: Train Loss: [2.340196, 0.3224251, 4.357967] | Test Loss: [2.5026941, 0.37340966, 4.6319785]\n",
      "39: Train Loss: [2.1650653, 0.3641774, 3.965953] | Test Loss: [2.7515447, 0.39318758, 5.109902]\n",
      "40: Train Loss: [2.141677, 0.34636146, 3.9369924] | Test Loss: [2.5117378, 0.32908124, 4.6943946]\n",
      "41: Train Loss: [2.3440459, 0.4236596, 4.264432] | Test Loss: [2.6828957, 0.3462494, 5.0195417]\n",
      "42: Train Loss: [2.139221, 0.3344655, 3.9439764] | Test Loss: [2.6125107, 0.35439754, 4.8706236]\n",
      "43: Train Loss: [2.302262, 0.43226638, 4.172258] | Test Loss: [2.5562425, 0.36811104, 4.744374]\n",
      "44: Train Loss: [2.4062874, 0.32841426, 4.4841604] | Test Loss: [2.5512035, 0.34751284, 4.7548943]\n",
      "45: Train Loss: [2.353155, 0.36174256, 4.3445673] | Test Loss: [2.7634351, 0.3667825, 5.1600876]\n",
      "46: Train Loss: [2.2513657, 0.40429068, 4.0984406] | Test Loss: [2.4956589, 0.40651923, 4.5847983]\n",
      "47: Train Loss: [2.3654242, 0.32729238, 4.403556] | Test Loss: [2.4658248, 0.41972634, 4.5119233]\n",
      "48: Train Loss: [2.3801372, 0.37822798, 4.382046] | Test Loss: [2.6068077, 0.36307222, 4.850543]\n",
      "49: Train Loss: [2.3522518, 0.4380249, 4.2664785] | Test Loss: [2.5563617, 0.3157404, 4.796983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: Train Loss: [2.1561525, 0.31500632, 3.9972987] | Test Loss: [2.5918672, 0.40993932, 4.773795]\n",
      "51: Train Loss: [2.4633203, 0.48831117, 4.438329] | Test Loss: [2.5269403, 0.33723602, 4.716645]\n",
      "52: Train Loss: [2.3197012, 0.35261557, 4.286787] | Test Loss: [2.5602372, 0.36835065, 4.752124]\n",
      "53: Train Loss: [2.458287, 0.35676622, 4.559808] | Test Loss: [2.6194034, 0.33614647, 4.9026604]\n",
      "54: Train Loss: [2.2344112, 0.315536, 4.1532865] | Test Loss: [2.6680257, 0.38604218, 4.9500093]\n",
      "55: Train Loss: [2.3050642, 0.34343752, 4.2666907] | Test Loss: [2.5674932, 0.35369128, 4.7812953]\n",
      "56: Train Loss: [2.4566479, 0.33018187, 4.5831137] | Test Loss: [2.4969435, 0.35375997, 4.640127]\n",
      "57: Train Loss: [2.246433, 0.35754195, 4.135324] | Test Loss: [2.7942429, 0.31594348, 5.272542]\n",
      "58: Train Loss: [2.2940545, 0.35056245, 4.2375464] | Test Loss: [2.5831275, 0.43742913, 4.728826]\n",
      "59: Train Loss: [2.2265642, 0.3212641, 4.131864] | Test Loss: [2.5906932, 0.3471802, 4.834206]\n",
      "60: Train Loss: [2.2141871, 0.3643925, 4.063982] | Test Loss: [2.5330486, 0.39454883, 4.6715484]\n",
      "61: Train Loss: [2.2902176, 0.33534968, 4.2450857] | Test Loss: [2.6993973, 0.40399963, 4.994795]\n",
      "62: Train Loss: [2.2240267, 0.4874694, 3.9605842] | Test Loss: [2.5523763, 0.40740168, 4.697351]\n",
      "63: Train Loss: [2.3619313, 0.35116556, 4.372697] | Test Loss: [2.6487632, 0.37830538, 4.919221]\n",
      "64: Train Loss: [2.3534005, 0.34359378, 4.3632073] | Test Loss: [2.633129, 0.44253957, 4.823718]\n",
      "65: Train Loss: [2.385183, 0.39137572, 4.3789907] | Test Loss: [2.7654037, 0.38235307, 5.1484547]\n",
      "66: Train Loss: [2.2611647, 0.3567295, 4.1656] | Test Loss: [2.5562396, 0.3347052, 4.777774]\n",
      "67: Train Loss: [2.3832755, 0.37924546, 4.3873057] | Test Loss: [2.5859616, 0.39155895, 4.780364]\n",
      "68: Train Loss: [2.2744067, 0.40133142, 4.147482] | Test Loss: [2.534007, 0.3983891, 4.6696253]\n",
      "69: Train Loss: [2.3434315, 0.33139172, 4.355471] | Test Loss: [2.7078156, 0.38858363, 5.0270476]\n",
      "70: Train Loss: [2.391226, 0.361446, 4.421006] | Test Loss: [2.5930808, 0.4109498, 4.775212]\n",
      "71: Train Loss: [2.1263597, 0.35465214, 3.8980675] | Test Loss: [2.51996, 0.38156348, 4.658356]\n",
      "72: Train Loss: [2.3465014, 0.3267112, 4.3662915] | Test Loss: [2.6167967, 0.3403861, 4.8932076]\n",
      "73: Train Loss: [2.3503737, 0.38094276, 4.3198047] | Test Loss: [2.5997941, 0.33586553, 4.863723]\n",
      "74: Train Loss: [2.3331654, 0.3560356, 4.310295] | Test Loss: [2.8335629, 0.38185096, 5.285275]\n",
      "75: Train Loss: [2.2592492, 0.3577957, 4.1607027] | Test Loss: [2.6962261, 0.31813186, 5.0743203]\n",
      "76: Train Loss: [2.3628259, 0.3081534, 4.4174986] | Test Loss: [2.6674368, 0.4337271, 4.9011464]\n",
      "77: Train Loss: [2.1542506, 0.34151065, 3.9669907] | Test Loss: [2.5048943, 0.3531034, 4.6566854]\n",
      "78: Train Loss: [2.3054338, 0.41255474, 4.1983128] | Test Loss: [2.503042, 0.3552862, 4.650798]\n",
      "79: Train Loss: [2.296431, 0.36306983, 4.229792] | Test Loss: [2.3814075, 0.33269656, 4.4301186]\n",
      "80: Train Loss: [2.3141415, 0.36182716, 4.2664557] | Test Loss: [2.436972, 0.3959195, 4.4780245]\n",
      "81: Train Loss: [2.5518522, 0.40574983, 4.6979547] | Test Loss: [2.4771225, 0.36584643, 4.5883985]\n",
      "82: Train Loss: [2.3586345, 0.31963995, 4.397629] | Test Loss: [2.7269478, 0.3531308, 5.1007648]\n",
      "83: Train Loss: [2.3608804, 0.34460792, 4.377153] | Test Loss: [2.7371109, 0.33609858, 5.138123]\n",
      "84: Train Loss: [2.2925751, 0.3281514, 4.256999] | Test Loss: [2.5254421, 0.3463581, 4.704526]\n",
      "85: Train Loss: [2.2332492, 0.40442938, 4.062069] | Test Loss: [2.774629, 0.44784507, 5.1014132]\n",
      "86: Train Loss: [2.2640502, 0.395017, 4.1330833] | Test Loss: [2.635242, 0.392524, 4.8779597]\n",
      "87: Train Loss: [2.3643005, 0.3598412, 4.3687596] | Test Loss: [2.5300865, 0.37226722, 4.687906]\n",
      "88: Train Loss: [2.3452508, 0.35190183, 4.3385997] | Test Loss: [2.6290233, 0.38477144, 4.8732753]\n",
      "89: Train Loss: [2.3854504, 0.4662354, 4.304665] | Test Loss: [2.6597095, 0.38044447, 4.9389744]\n",
      "90: Train Loss: [2.2766948, 0.37419975, 4.1791897] | Test Loss: [2.7537768, 0.4258385, 5.081715]\n",
      "91: Train Loss: [2.3380785, 0.34945026, 4.326707] | Test Loss: [2.7203968, 0.37886903, 5.0619245]\n",
      "92: Train Loss: [2.299769, 0.34065118, 4.258887] | Test Loss: [2.6902575, 0.36830807, 5.012207]\n",
      "93: Train Loss: [2.280715, 0.303651, 4.257779] | Test Loss: [2.6543872, 0.4309241, 4.8778505]\n",
      "94: Train Loss: [2.3987656, 0.389929, 4.4076023] | Test Loss: [2.5699983, 0.49138328, 4.6486135]\n",
      "95: Train Loss: [2.3432684, 0.29674572, 4.389791] | Test Loss: [2.3934386, 0.27581075, 4.5110664]\n",
      "96: Train Loss: [2.4541612, 0.37210673, 4.536216] | Test Loss: [2.7129886, 0.29560512, 5.130372]\n",
      "97: Train Loss: [2.3691683, 0.3319312, 4.4064054] | Test Loss: [2.429088, 0.2990344, 4.5591416]\n",
      "98: Train Loss: [2.2442162, 0.44998756, 4.038445] | Test Loss: [2.6166244, 0.3484722, 4.8847766]\n",
      "99: Train Loss: [2.317984, 0.40122485, 4.234743] | Test Loss: [2.5758824, 0.40021256, 4.751552]\n",
      "100: Train Loss: [2.4297526, 0.34268263, 4.5168223] | Test Loss: [2.5979056, 0.3856029, 4.8102083]\n",
      "101: Train Loss: [2.386155, 0.34665143, 4.425658] | Test Loss: [2.5421062, 0.367814, 4.7163982]\n",
      "102: Train Loss: [2.3090403, 0.3338687, 4.284212] | Test Loss: [2.7052312, 0.34213695, 5.0683255]\n",
      "103: Train Loss: [2.4278543, 0.3412742, 4.5144343] | Test Loss: [2.599284, 0.4012988, 4.797269]\n",
      "104: Train Loss: [2.1035643, 0.341723, 3.8654053] | Test Loss: [2.6087577, 0.34345675, 4.8740587]\n",
      "105: Train Loss: [2.2427752, 0.3003411, 4.1852093] | Test Loss: [2.6557014, 0.40101132, 4.9103913]\n",
      "106: Train Loss: [2.1881213, 0.421875, 3.9543674] | Test Loss: [2.5660732, 0.41181523, 4.720331]\n",
      "107: Train Loss: [2.2368574, 0.31686184, 4.156853] | Test Loss: [2.55271, 0.36082202, 4.744598]\n",
      "108: Train Loss: [2.4087481, 0.3082108, 4.5092854] | Test Loss: [2.6202416, 0.32792044, 4.912563]\n",
      "109: Train Loss: [2.2164114, 0.34488815, 4.0879345] | Test Loss: [2.4716227, 0.2757258, 4.6675196]\n",
      "110: Train Loss: [2.421698, 0.325632, 4.517764] | Test Loss: [2.9119637, 0.46183315, 5.3620944]\n",
      "111: Train Loss: [2.3378255, 0.32805336, 4.3475976] | Test Loss: [2.4947405, 0.3244295, 4.6650515]\n",
      "112: Train Loss: [2.3944335, 0.35491842, 4.4339485] | Test Loss: [2.7049837, 0.4663844, 4.943583]\n",
      "113: Train Loss: [2.507509, 0.3582123, 4.6568055] | Test Loss: [2.6383262, 0.36571378, 4.9109387]\n",
      "114: Train Loss: [2.2686868, 0.386048, 4.1513257] | Test Loss: [2.604291, 0.35659558, 4.8519864]\n",
      "115: Train Loss: [2.364206, 0.33243924, 4.3959727] | Test Loss: [2.4962234, 0.38536525, 4.607082]\n",
      "116: Train Loss: [2.294373, 0.40594566, 4.1828003] | Test Loss: [2.6078844, 0.37687132, 4.8388977]\n",
      "117: Train Loss: [2.3860133, 0.33412322, 4.4379034] | Test Loss: [2.67233, 0.34545487, 4.999205]\n",
      "118: Train Loss: [2.4266694, 0.4653979, 4.387941] | Test Loss: [2.6912131, 0.37379783, 5.0086284]\n",
      "119: Train Loss: [2.2794387, 0.3095177, 4.2493596] | Test Loss: [2.6450253, 0.3806401, 4.9094105]\n",
      "120: Train Loss: [2.311732, 0.2882881, 4.335176] | Test Loss: [2.454591, 0.3307506, 4.5784316]\n",
      "121: Train Loss: [2.3214426, 0.3244088, 4.318476] | Test Loss: [2.8695679, 0.39310908, 5.346027]\n",
      "122: Train Loss: [2.4528592, 0.4048595, 4.500859] | Test Loss: [2.5540648, 0.31474134, 4.7933884]\n",
      "123: Train Loss: [2.277076, 0.46904626, 4.085106] | Test Loss: [2.6375265, 0.34540898, 4.929644]\n",
      "124: Train Loss: [2.2873573, 0.42283875, 4.151876] | Test Loss: [2.4635046, 0.38696888, 4.54004]\n",
      "125: Train Loss: [2.3403537, 0.35467827, 4.3260293] | Test Loss: [2.5379407, 0.38253042, 4.6933513]\n",
      "126: Train Loss: [2.2688742, 0.30496135, 4.232787] | Test Loss: [2.5012596, 0.35045418, 4.652065]\n",
      "127: Train Loss: [2.343902, 0.32175168, 4.3660526] | Test Loss: [2.75422, 0.33744138, 5.1709986]\n",
      "128: Train Loss: [2.3680232, 0.32391068, 4.4121356] | Test Loss: [2.5038342, 0.37499622, 4.6326723]\n",
      "129: Train Loss: [2.3921232, 0.30376145, 4.480485] | Test Loss: [2.5951498, 0.28076562, 4.909534]\n",
      "130: Train Loss: [2.312358, 0.37648365, 4.2482324] | Test Loss: [2.609138, 0.36608586, 4.85219]\n",
      "131: Train Loss: [2.3358042, 0.40006775, 4.2715406] | Test Loss: [2.4156651, 0.42187864, 4.4094515]\n",
      "132: Train Loss: [2.2470405, 0.38199216, 4.1120887] | Test Loss: [2.5395794, 0.42241883, 4.65674]\n",
      "133: Train Loss: [2.325237, 0.32455364, 4.3259206] | Test Loss: [2.6325336, 0.4151113, 4.8499556]\n",
      "134: Train Loss: [2.214038, 0.3274367, 4.100639] | Test Loss: [2.6764653, 0.41008103, 4.9428496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135: Train Loss: [2.4127827, 0.36105445, 4.464511] | Test Loss: [2.7114797, 0.37823775, 5.0447216]\n",
      "136: Train Loss: [2.2750406, 0.34609237, 4.203989] | Test Loss: [2.6653671, 0.33415297, 4.996581]\n",
      "137: Train Loss: [2.4112594, 0.35485038, 4.4676685] | Test Loss: [2.6149867, 0.35213903, 4.8778343]\n",
      "138: Train Loss: [2.2871552, 0.37988266, 4.1944275] | Test Loss: [2.765943, 0.36687344, 5.165013]\n",
      "139: Train Loss: [2.5135396, 0.40822047, 4.618859] | Test Loss: [2.6665852, 0.39520296, 4.9379673]\n",
      "140: Train Loss: [2.396632, 0.3720976, 4.4211664] | Test Loss: [2.4453702, 0.32052347, 4.570217]\n",
      "141: Train Loss: [2.496372, 0.33843136, 4.6543126] | Test Loss: [2.5172315, 0.3019537, 4.732509]\n",
      "142: Train Loss: [2.235393, 0.37343344, 4.0973525] | Test Loss: [2.6248002, 0.3350163, 4.914584]\n",
      "143: Train Loss: [2.1390357, 0.34985313, 3.9282181] | Test Loss: [2.5914335, 0.40994456, 4.7729225]\n",
      "144: Train Loss: [2.2937107, 0.3352963, 4.2521253] | Test Loss: [2.6763616, 0.39026687, 4.962456]\n",
      "145: Train Loss: [2.3534455, 0.31081852, 4.3960724] | Test Loss: [2.5162656, 0.36119938, 4.671332]\n",
      "146: Train Loss: [2.3481884, 0.44418454, 4.2521925] | Test Loss: [2.6402318, 0.29173386, 4.98873]\n",
      "147: Train Loss: [2.5277624, 0.4189539, 4.636571] | Test Loss: [2.628331, 0.34736022, 4.9093018]\n",
      "148: Train Loss: [2.331838, 0.3445589, 4.319117] | Test Loss: [2.526791, 0.36038598, 4.6931963]\n",
      "149: Train Loss: [2.3715448, 0.3822181, 4.360872] | Test Loss: [2.707652, 0.46364078, 4.9516635]\n",
      "150: Train Loss: [2.292725, 0.32968414, 4.255766] | Test Loss: [2.5297267, 0.36513287, 4.6943207]\n",
      "151: Train Loss: [2.4388006, 0.34260756, 4.5349936] | Test Loss: [2.864135, 0.3416657, 5.3866043]\n",
      "152: Train Loss: [2.370435, 0.31527764, 4.4255924] | Test Loss: [2.5347764, 0.3343746, 4.7351785]\n",
      "153: Train Loss: [2.3807964, 0.36774465, 4.3938484] | Test Loss: [2.7418654, 0.37094447, 5.1127863]\n",
      "154: Train Loss: [2.3981647, 0.3564205, 4.439909] | Test Loss: [2.540304, 0.3220104, 4.7585974]\n",
      "155: Train Loss: [2.4507957, 0.36848667, 4.5331044] | Test Loss: [2.7793787, 0.3753185, 5.183439]\n",
      "156: Train Loss: [2.4042246, 0.35782441, 4.450625] | Test Loss: [2.6536493, 0.41316748, 4.894131]\n",
      "157: Train Loss: [2.1999872, 0.3027445, 4.09723] | Test Loss: [2.7397213, 0.44161344, 5.0378294]\n",
      "158: Train Loss: [2.2595525, 0.30553508, 4.2135696] | Test Loss: [2.8012242, 0.4776082, 5.1248403]\n",
      "159: Train Loss: [2.454776, 0.32792073, 4.581631] | Test Loss: [2.5845308, 0.39643365, 4.772628]\n",
      "160: Train Loss: [2.4499755, 0.3843292, 4.5156217] | Test Loss: [2.5203795, 0.3329136, 4.7078457]\n",
      "161: Train Loss: [2.330149, 0.27701947, 4.3832784] | Test Loss: [2.491095, 0.36215922, 4.620031]\n",
      "162: Train Loss: [2.4638002, 0.30937472, 4.6182256] | Test Loss: [2.694228, 0.34792954, 5.0405264]\n",
      "163: Train Loss: [2.4218938, 0.4042503, 4.4395375] | Test Loss: [2.726658, 0.4210447, 5.0322714]\n",
      "164: Train Loss: [2.470457, 0.32396713, 4.616947] | Test Loss: [2.742077, 0.39466426, 5.08949]\n",
      "165: Train Loss: [2.3426194, 0.37058362, 4.3146553] | Test Loss: [2.513949, 0.3961202, 4.631778]\n",
      "166: Train Loss: [2.2257414, 0.33754307, 4.11394] | Test Loss: [2.5664847, 0.32336798, 4.8096013]\n",
      "167: Train Loss: [2.4579005, 0.41381893, 4.501982] | Test Loss: [2.387639, 0.38819265, 4.3870854]\n",
      "168: Train Loss: [2.338962, 0.3425152, 4.335409] | Test Loss: [2.6922874, 0.3862579, 4.9983172]\n",
      "169: Train Loss: [2.3719354, 0.37873715, 4.365134] | Test Loss: [2.2516038, 0.30565324, 4.1975546]\n",
      "170: Train Loss: [2.464631, 0.36194015, 4.567322] | Test Loss: [2.8266923, 0.40481126, 5.2485733]\n",
      "171: Train Loss: [2.2967656, 0.34129345, 4.252238] | Test Loss: [2.469413, 0.32973173, 4.609094]\n",
      "172: Train Loss: [2.3842373, 0.37972808, 4.3887463] | Test Loss: [2.5685446, 0.29471043, 4.8423786]\n",
      "173: Train Loss: [2.330644, 0.34679526, 4.3144927] | Test Loss: [2.3848033, 0.37974235, 4.3898644]\n",
      "174: Train Loss: [2.33375, 0.37755516, 4.2899446] | Test Loss: [2.776186, 0.43458098, 5.117791]\n",
      "175: Train Loss: [2.3312109, 0.4051526, 4.257269] | Test Loss: [2.5501885, 0.3446482, 4.7557287]\n",
      "176: Train Loss: [2.2751591, 0.36111778, 4.1892004] | Test Loss: [2.5620801, 0.44623792, 4.6779222]\n",
      "177: Train Loss: [2.479687, 0.39102304, 4.568351] | Test Loss: [2.6497984, 0.34674665, 4.9528503]\n",
      "178: Train Loss: [2.3477478, 0.3703835, 4.3251123] | Test Loss: [2.8283098, 0.4141167, 5.2425027]\n",
      "179: Train Loss: [2.261091, 0.36085507, 4.161327] | Test Loss: [2.6605659, 0.33600903, 4.9851227]\n",
      "180: Train Loss: [2.2418814, 0.31035846, 4.173404] | Test Loss: [2.638461, 0.40804797, 4.868874]\n",
      "181: Train Loss: [2.3269682, 0.27994782, 4.3739886] | Test Loss: [2.6223245, 0.42868042, 4.8159685]\n",
      "182: Train Loss: [2.3807597, 0.31628007, 4.4452395] | Test Loss: [2.6424975, 0.383206, 4.901789]\n",
      "183: Train Loss: [2.3876114, 0.33985215, 4.4353704] | Test Loss: [2.5031755, 0.28988504, 4.716466]\n",
      "184: Train Loss: [2.4170809, 0.33502746, 4.4991345] | Test Loss: [2.7779768, 0.3484003, 5.2075534]\n",
      "185: Train Loss: [2.36732, 0.2922034, 4.4424367] | Test Loss: [2.5958457, 0.31003493, 4.8816566]\n",
      "186: Train Loss: [2.300294, 0.34950212, 4.2510858] | Test Loss: [2.5937188, 0.39423865, 4.793199]\n",
      "187: Train Loss: [2.4635491, 0.34601974, 4.5810785] | Test Loss: [2.5700643, 0.38386312, 4.7562656]\n",
      "188: Train Loss: [2.5013628, 0.44404572, 4.55868] | Test Loss: [2.6027005, 0.3954219, 4.809979]\n",
      "189: Train Loss: [2.4564812, 0.413923, 4.4990396] | Test Loss: [2.605409, 0.35500222, 4.8558154]\n",
      "190: Train Loss: [2.4425435, 0.37422404, 4.510863] | Test Loss: [2.696494, 0.423467, 4.969521]\n",
      "191: Train Loss: [2.4031324, 0.36924973, 4.437015] | Test Loss: [2.7915764, 0.4637547, 5.119398]\n",
      "192: Train Loss: [2.3895507, 0.36207289, 4.4170284] | Test Loss: [2.63392, 0.36542302, 4.9024167]\n",
      "193: Train Loss: [2.4178712, 0.33049795, 4.5052447] | Test Loss: [2.560994, 0.29579532, 4.8261924]\n",
      "194: Train Loss: [2.2919235, 0.36104518, 4.2228017] | Test Loss: [2.5190353, 0.34019738, 4.697873]\n",
      "195: Train Loss: [2.4749658, 0.34455517, 4.6053762] | Test Loss: [2.6437023, 0.4298763, 4.857528]\n",
      "196: Train Loss: [2.383903, 0.31877944, 4.4490266] | Test Loss: [2.61374, 0.34325638, 4.8842235]\n",
      "197: Train Loss: [2.3086593, 0.3319998, 4.285319] | Test Loss: [2.5445485, 0.36061406, 4.728483]\n",
      "198: Train Loss: [2.2533867, 0.28375277, 4.2230206] | Test Loss: [2.5075405, 0.38040212, 4.634679]\n",
      "199: Train Loss: [2.418516, 0.37653455, 4.4604974] | Test Loss: [2.5697618, 0.36749694, 4.7720265]\n",
      "200: Train Loss: [2.4140253, 0.56092507, 4.2671256] | Test Loss: [2.4263885, 0.4090641, 4.4437127]\n",
      "201: Train Loss: [2.3155262, 0.38258618, 4.2484665] | Test Loss: [2.6418765, 0.3678867, 4.9158664]\n",
      "202: Train Loss: [2.4724343, 0.3700497, 4.574819] | Test Loss: [2.518405, 0.3346208, 4.702189]\n",
      "203: Train Loss: [2.4045932, 0.35716256, 4.452024] | Test Loss: [2.544488, 0.38999087, 4.698985]\n",
      "204: Train Loss: [2.3853323, 0.41163158, 4.359033] | Test Loss: [2.5627391, 0.38598576, 4.7394924]\n",
      "205: Train Loss: [2.4304414, 0.46258888, 4.398294] | Test Loss: [2.4797535, 0.421849, 4.537658]\n",
      "206: Train Loss: [2.1547475, 0.42388168, 3.8856132] | Test Loss: [2.5808702, 0.36105517, 4.800685]\n",
      "207: Train Loss: [2.3954573, 0.32362953, 4.467285] | Test Loss: [2.7712467, 0.4193826, 5.123111]\n",
      "208: Train Loss: [2.4617474, 0.3557073, 4.5677876] | Test Loss: [2.7591424, 0.3726624, 5.1456223]\n",
      "209: Train Loss: [2.47235, 0.46901762, 4.4756823] | Test Loss: [2.580541, 0.3146314, 4.8464503]\n",
      "210: Train Loss: [2.3733997, 0.29825616, 4.4485435] | Test Loss: [2.6586773, 0.39838934, 4.9189653]\n",
      "211: Train Loss: [2.341884, 0.43904147, 4.244726] | Test Loss: [2.7785423, 0.41597387, 5.141111]\n",
      "212: Train Loss: [2.360088, 0.4314684, 4.2887077] | Test Loss: [2.6662726, 0.3511173, 4.981428]\n",
      "213: Train Loss: [2.4652424, 0.33490172, 4.595583] | Test Loss: [2.5931911, 0.40099603, 4.785386]\n",
      "214: Train Loss: [2.417987, 0.35178983, 4.4841843] | Test Loss: [2.6621983, 0.40421394, 4.9201827]\n",
      "215: Train Loss: [2.3220434, 0.37689254, 4.2671943] | Test Loss: [2.5452158, 0.37533304, 4.715099]\n",
      "216: Train Loss: [2.4455862, 0.34430242, 4.54687] | Test Loss: [2.6732063, 0.3425271, 5.0038857]\n",
      "217: Train Loss: [2.4638612, 0.3696242, 4.5580983] | Test Loss: [2.6559958, 0.40924445, 4.902747]\n",
      "218: Train Loss: [2.419632, 0.40387964, 4.4353843] | Test Loss: [2.5050907, 0.36493784, 4.6452436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219: Train Loss: [2.3370626, 0.41558817, 4.258537] | Test Loss: [2.649535, 0.33225462, 4.9668155]\n",
      "220: Train Loss: [2.287036, 0.2828145, 4.2912574] | Test Loss: [2.7838645, 0.40873656, 5.1589923]\n",
      "221: Train Loss: [2.4302356, 0.37244228, 4.488029] | Test Loss: [2.8602772, 0.5056401, 5.2149143]\n",
      "222: Train Loss: [2.4457085, 0.32617205, 4.565245] | Test Loss: [2.5564878, 0.33827302, 4.7747025]\n",
      "223: Train Loss: [2.3747635, 0.33963698, 4.40989] | Test Loss: [2.6053586, 0.36061412, 4.850103]\n",
      "224: Train Loss: [2.3911242, 0.31863225, 4.4636164] | Test Loss: [2.7633576, 0.32766545, 5.19905]\n",
      "225: Train Loss: [2.3214364, 0.41020426, 4.2326684] | Test Loss: [2.5269747, 0.4053458, 4.6486034]\n",
      "226: Train Loss: [2.3412364, 0.3402807, 4.342192] | Test Loss: [2.502129, 0.35337603, 4.6508822]\n",
      "227: Train Loss: [2.4956243, 0.32701653, 4.6642323] | Test Loss: [2.6582875, 0.33512813, 4.9814467]\n",
      "228: Train Loss: [2.2320225, 0.3965544, 4.0674906] | Test Loss: [2.616159, 0.31130248, 4.9210153]\n",
      "229: Train Loss: [2.6118908, 0.4385473, 4.7852345] | Test Loss: [2.5174773, 0.3155561, 4.7193985]\n",
      "230: Train Loss: [2.3465025, 0.35526672, 4.3377385] | Test Loss: [2.639698, 0.36852384, 4.9108725]\n",
      "231: Train Loss: [2.3754256, 0.3601039, 4.390747] | Test Loss: [2.4827175, 0.32598138, 4.639454]\n",
      "232: Train Loss: [2.4208553, 0.38531324, 4.4563975] | Test Loss: [2.7079837, 0.33220336, 5.083764]\n",
      "233: Train Loss: [2.3449259, 0.37941962, 4.310432] | Test Loss: [2.6100872, 0.35383844, 4.866336]\n",
      "234: Train Loss: [2.4010773, 0.31444705, 4.4877076] | Test Loss: [2.6620674, 0.34239432, 4.9817405]\n",
      "235: Train Loss: [2.3632843, 0.33020306, 4.3963656] | Test Loss: [2.4714546, 0.440564, 4.502345]\n",
      "236: Train Loss: [2.2565892, 0.3673024, 4.145876] | Test Loss: [2.7710664, 0.38546932, 5.1566634]\n",
      "237: Train Loss: [2.298556, 0.31090668, 4.2862053] | Test Loss: [2.5856688, 0.42104223, 4.750295]\n",
      "238: Train Loss: [2.3541288, 0.3808241, 4.3274336] | Test Loss: [2.5451086, 0.39047605, 4.699741]\n",
      "239: Train Loss: [2.4126694, 0.36230466, 4.463034] | Test Loss: [2.7219818, 0.4413486, 5.002615]\n",
      "240: Train Loss: [2.4875946, 0.42607355, 4.5491157] | Test Loss: [2.560576, 0.35908866, 4.762063]\n",
      "241: Train Loss: [2.1805441, 0.30545852, 4.0556297] | Test Loss: [2.656196, 0.374389, 4.938003]\n",
      "242: Train Loss: [2.1869366, 0.36045808, 4.0134153] | Test Loss: [2.6501222, 0.3591953, 4.941049]\n",
      "243: Train Loss: [2.3598764, 0.3228432, 4.3969097] | Test Loss: [2.7737963, 0.31510803, 5.232485]\n",
      "244: Train Loss: [2.4477978, 0.38603702, 4.5095587] | Test Loss: [2.687482, 0.36384317, 5.0111213]\n",
      "245: Train Loss: [2.3669052, 0.34434786, 4.3894625] | Test Loss: [2.8008254, 0.39531952, 5.2063313]\n",
      "246: Train Loss: [2.2029896, 0.28521731, 4.120762] | Test Loss: [2.5279982, 0.32900485, 4.7269917]\n",
      "247: Train Loss: [2.2978961, 0.36302915, 4.2327633] | Test Loss: [2.7237017, 0.45436737, 4.9930363]\n",
      "248: Train Loss: [2.3505085, 0.38980022, 4.311217] | Test Loss: [2.643596, 0.37050712, 4.9166846]\n",
      "249: Train Loss: [2.5354528, 0.40930992, 4.661596] | Test Loss: [2.5629265, 0.3816165, 4.7442365]\n",
      "250: Train Loss: [2.5709736, 0.39127868, 4.7506685] | Test Loss: [2.6574433, 0.38669485, 4.9281917]\n",
      "251: Train Loss: [2.4619937, 0.3677859, 4.5562015] | Test Loss: [2.4947894, 0.34517616, 4.6444025]\n",
      "252: Train Loss: [2.352893, 0.38095495, 4.3248315] | Test Loss: [2.5749402, 0.3513384, 4.798542]\n",
      "253: Train Loss: [2.3602717, 0.36385024, 4.3566933] | Test Loss: [2.610438, 0.43308988, 4.7877865]\n",
      "254: Train Loss: [2.314782, 0.31470397, 4.31486] | Test Loss: [2.6230946, 0.41111392, 4.8350754]\n",
      "255: Train Loss: [2.1189225, 0.4060507, 3.831794] | Test Loss: [2.616922, 0.37500012, 4.858844]\n",
      "256: Train Loss: [2.4471269, 0.31760487, 4.5766487] | Test Loss: [2.578286, 0.36862904, 4.787943]\n",
      "257: Train Loss: [2.3549192, 0.35880858, 4.35103] | Test Loss: [2.6748261, 0.3163955, 5.033257]\n",
      "258: Train Loss: [2.325103, 0.38376048, 4.2664456] | Test Loss: [2.5471, 0.32553366, 4.7686663]\n",
      "259: Train Loss: [2.3129601, 0.3752273, 4.250693] | Test Loss: [2.8415647, 0.38331088, 5.2998185]\n",
      "260: Train Loss: [2.1997685, 0.40327382, 3.996263] | Test Loss: [2.6195138, 0.34555557, 4.8934717]\n",
      "261: Train Loss: [2.380186, 0.3305909, 4.4297814] | Test Loss: [2.3950353, 0.3921272, 4.3979435]\n",
      "262: Train Loss: [2.3500571, 0.3969322, 4.303182] | Test Loss: [2.6723156, 0.39440846, 4.950223]\n",
      "263: Train Loss: [2.51885, 0.451783, 4.585917] | Test Loss: [2.7399492, 0.3970043, 5.0828943]\n",
      "264: Train Loss: [2.515775, 0.34532797, 4.686222] | Test Loss: [2.5397005, 0.34359607, 4.735805]\n",
      "265: Train Loss: [2.5869634, 0.39385796, 4.780069] | Test Loss: [2.5408325, 0.42252076, 4.6591444]\n",
      "266: Train Loss: [2.3876548, 0.37167013, 4.4036393] | Test Loss: [2.6431282, 0.3471485, 4.939108]\n",
      "267: Train Loss: [2.3845735, 0.33253413, 4.4366126] | Test Loss: [2.5794437, 0.36563367, 4.793254]\n",
      "268: Train Loss: [2.319785, 0.28856707, 4.351003] | Test Loss: [2.5323315, 0.32953814, 4.7351246]\n",
      "269: Train Loss: [2.424332, 0.3134864, 4.535177] | Test Loss: [2.6815417, 0.38371095, 4.9793725]\n",
      "270: Train Loss: [2.406755, 0.38984707, 4.4236627] | Test Loss: [2.674439, 0.43924162, 4.9096365]\n",
      "271: Train Loss: [2.4412892, 0.36108467, 4.521494] | Test Loss: [2.6393652, 0.41019046, 4.86854]\n",
      "272: Train Loss: [2.472825, 0.33990812, 4.605742] | Test Loss: [2.8262136, 0.40026155, 5.252166]\n",
      "273: Train Loss: [2.3258462, 0.33840582, 4.313287] | Test Loss: [2.5902839, 0.355088, 4.8254795]\n",
      "274: Train Loss: [2.480646, 0.32019028, 4.6411014] | Test Loss: [2.6452594, 0.3354813, 4.9550376]\n",
      "275: Train Loss: [2.3217006, 0.3661931, 4.277208] | Test Loss: [2.5750344, 0.29878977, 4.851279]\n",
      "276: Train Loss: [2.3143198, 0.28381798, 4.344822] | Test Loss: [2.6647387, 0.36951762, 4.9599595]\n",
      "277: Train Loss: [2.4351068, 0.3599204, 4.510293] | Test Loss: [2.6763077, 0.32874808, 5.023867]\n",
      "278: Train Loss: [2.4372435, 0.3591014, 4.5153856] | Test Loss: [2.860571, 0.41047847, 5.310663]\n",
      "279: Train Loss: [2.404679, 0.3222145, 4.4871435] | Test Loss: [2.7438047, 0.34012574, 5.147484]\n",
      "280: Train Loss: [2.386899, 0.33785713, 4.4359407] | Test Loss: [2.6131692, 0.28826436, 4.938074]\n",
      "281: Train Loss: [2.383606, 0.38054785, 4.386664] | Test Loss: [2.6201143, 0.33524957, 4.904979]\n",
      "282: Train Loss: [2.2991745, 0.2934195, 4.3049297] | Test Loss: [2.38304, 0.35714585, 4.408934]\n",
      "283: Train Loss: [2.4602947, 0.36380926, 4.5567803] | Test Loss: [2.6395156, 0.43166956, 4.8473616]\n",
      "284: Train Loss: [2.422552, 0.29844233, 4.546662] | Test Loss: [2.626515, 0.46098757, 4.7920423]\n",
      "285: Train Loss: [2.410284, 0.35227972, 4.4682884] | Test Loss: [2.678675, 0.36170116, 4.995649]\n",
      "286: Train Loss: [2.4167552, 0.339445, 4.4940653] | Test Loss: [2.4946694, 0.41476065, 4.5745783]\n",
      "287: Train Loss: [2.5003347, 0.35918063, 4.641489] | Test Loss: [2.5486023, 0.33706218, 4.7601423]\n",
      "288: Train Loss: [2.3444734, 0.4454758, 4.243471] | Test Loss: [2.7264025, 0.44248092, 5.010324]\n",
      "289: Train Loss: [2.3738136, 0.44867796, 4.2989492] | Test Loss: [2.1218028, 0.3649821, 3.8786237]\n",
      "290: Train Loss: [2.3988419, 0.3236361, 4.4740477] | Test Loss: [2.559291, 0.32947627, 4.7891054]\n",
      "291: Train Loss: [2.3577619, 0.29966697, 4.415857] | Test Loss: [2.4411643, 0.40168333, 4.480645]\n",
      "292: Train Loss: [2.4454174, 0.36349916, 4.5273356] | Test Loss: [2.6055598, 0.38378853, 4.827331]\n",
      "293: Train Loss: [2.3384602, 0.36968315, 4.307237] | Test Loss: [2.6252456, 0.3816037, 4.8688874]\n",
      "294: Train Loss: [2.3800302, 0.33139843, 4.428662] | Test Loss: [2.606447, 0.33951196, 4.873382]\n",
      "295: Train Loss: [2.1597466, 0.35612276, 3.9633703] | Test Loss: [2.510056, 0.31590915, 4.7042027]\n",
      "296: Train Loss: [2.4379811, 0.43391407, 4.442048] | Test Loss: [2.7719364, 0.3282373, 5.215636]\n",
      "297: Train Loss: [2.3915606, 0.39744574, 4.3856754] | Test Loss: [2.5379727, 0.43431446, 4.641631]\n",
      "298: Train Loss: [2.3313525, 0.40595162, 4.2567534] | Test Loss: [2.5540698, 0.3694781, 4.7386613]\n",
      "299: Train Loss: [2.3647203, 0.3950973, 4.3343434] | Test Loss: [2.6417432, 0.34710675, 4.9363794]\n",
      "300: Train Loss: [2.515853, 0.34368366, 4.688022] | Test Loss: [2.5230122, 0.34653324, 4.699491]\n",
      "301: Train Loss: [2.0955591, 0.3651919, 3.8259263] | Test Loss: [2.7838457, 0.38410455, 5.1835866]\n",
      "302: Train Loss: [2.367591, 0.39855403, 4.336628] | Test Loss: [2.6177845, 0.36503094, 4.870538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303: Train Loss: [2.4488003, 0.36050037, 4.5371003] | Test Loss: [2.6993895, 0.3885774, 5.0102015]\n",
      "304: Train Loss: [2.4786963, 0.36176187, 4.5956306] | Test Loss: [2.5015132, 0.34308642, 4.6599402]\n",
      "305: Train Loss: [2.5390704, 0.49768046, 4.58046] | Test Loss: [2.9033957, 0.46250677, 5.3442845]\n",
      "306: Train Loss: [2.2948096, 0.27101696, 4.318602] | Test Loss: [2.5203738, 0.41478434, 4.625963]\n",
      "307: Train Loss: [2.4987833, 0.47088337, 4.5266833] | Test Loss: [2.701877, 0.36696786, 5.0367866]\n",
      "308: Train Loss: [2.4862218, 0.5184301, 4.4540133] | Test Loss: [2.7090898, 0.39019153, 5.027988]\n",
      "309: Train Loss: [2.3193, 0.39866847, 4.2399316] | Test Loss: [2.589547, 0.43183205, 4.747262]\n",
      "310: Train Loss: [2.2466252, 0.3386505, 4.1545997] | Test Loss: [2.6225922, 0.36993447, 4.87525]\n",
      "311: Train Loss: [2.4915679, 0.3723305, 4.610805] | Test Loss: [2.810695, 0.4186068, 5.202783]\n",
      "312: Train Loss: [2.3402286, 0.3292716, 4.3511853] | Test Loss: [2.5514958, 0.39617628, 4.7068152]\n",
      "313: Train Loss: [2.5224545, 0.39919594, 4.645713] | Test Loss: [2.512212, 0.36196935, 4.6624546]\n",
      "314: Train Loss: [2.5890357, 0.4186513, 4.7594204] | Test Loss: [2.5067751, 0.36512932, 4.648421]\n",
      "315: Train Loss: [2.4725168, 0.338561, 4.6064725] | Test Loss: [2.7228816, 0.38326603, 5.062497]\n",
      "316: Train Loss: [2.406996, 0.34539634, 4.4685955] | Test Loss: [2.7519302, 0.321666, 5.1821947]\n",
      "317: Train Loss: [2.263658, 0.36007556, 4.1672406] | Test Loss: [2.5050828, 0.34395403, 4.6662116]\n",
      "318: Train Loss: [2.4995177, 0.38921556, 4.60982] | Test Loss: [2.5696335, 0.37590957, 4.7633576]\n",
      "319: Train Loss: [2.4455364, 0.35402012, 4.5370526] | Test Loss: [2.6718206, 0.3264602, 5.017181]\n",
      "320: Train Loss: [2.4446359, 0.31934592, 4.569926] | Test Loss: [2.5053847, 0.3499079, 4.6608615]\n",
      "321: Train Loss: [2.4629276, 0.38972595, 4.536129] | Test Loss: [2.6275566, 0.4384817, 4.8166313]\n",
      "322: Train Loss: [2.4095817, 0.36921266, 4.4499507] | Test Loss: [2.7933407, 0.3958103, 5.1908712]\n",
      "323: Train Loss: [2.407268, 0.336213, 4.478323] | Test Loss: [2.651147, 0.39292076, 4.909373]\n",
      "324: Train Loss: [2.435608, 0.3333385, 4.537877] | Test Loss: [2.7918565, 0.36839727, 5.215316]\n",
      "325: Train Loss: [2.5254009, 0.4111006, 4.6397014] | Test Loss: [2.8260102, 0.2981485, 5.353872]\n",
      "326: Train Loss: [2.3451877, 0.35906222, 4.331313] | Test Loss: [2.7530098, 0.38280943, 5.12321]\n",
      "327: Train Loss: [2.2763982, 0.34322506, 4.2095714] | Test Loss: [2.575454, 0.33413824, 4.8167696]\n",
      "328: Train Loss: [2.4979787, 0.38576207, 4.610195] | Test Loss: [2.6272979, 0.43060678, 4.823989]\n",
      "329: Train Loss: [2.3696158, 0.2922323, 4.446999] | Test Loss: [2.7018733, 0.33387083, 5.0698757]\n",
      "330: Train Loss: [2.231205, 0.339041, 4.123369] | Test Loss: [2.550978, 0.36882308, 4.733133]\n",
      "331: Train Loss: [2.4648268, 0.5189128, 4.410741] | Test Loss: [2.738694, 0.37638512, 5.1010027]\n",
      "332: Train Loss: [2.2637582, 0.3151416, 4.2123747] | Test Loss: [2.7730403, 0.36190882, 5.1841717]\n",
      "333: Train Loss: [2.5616717, 0.38964954, 4.733694] | Test Loss: [2.4216952, 0.40219975, 4.4411907]\n",
      "334: Train Loss: [2.3834994, 0.40811846, 4.3588805] | Test Loss: [2.6917493, 0.38627613, 4.9972224]\n",
      "335: Train Loss: [2.3736181, 0.3616856, 4.3855505] | Test Loss: [2.397033, 0.35065907, 4.443407]\n",
      "336: Train Loss: [2.371613, 0.3571799, 4.386046] | Test Loss: [2.7223501, 0.34453273, 5.1001678]\n",
      "337: Train Loss: [2.4886453, 0.3322528, 4.6450377] | Test Loss: [2.4768162, 0.3259934, 4.627639]\n",
      "338: Train Loss: [2.3272843, 0.36283693, 4.291732] | Test Loss: [2.4472816, 0.38157716, 4.512986]\n",
      "339: Train Loss: [2.4795074, 0.3954321, 4.563583] | Test Loss: [2.69986, 0.3804023, 5.019318]\n",
      "340: Train Loss: [2.2631307, 0.30837882, 4.2178826] | Test Loss: [2.564368, 0.37374714, 4.7549887]\n",
      "341: Train Loss: [2.5642128, 0.38413215, 4.744293] | Test Loss: [2.6146352, 0.3542237, 4.8750467]\n",
      "342: Train Loss: [2.3375895, 0.38521445, 4.2899647] | Test Loss: [2.6005414, 0.40428, 4.7968025]\n",
      "343: Train Loss: [2.3556137, 0.36221305, 4.3490143] | Test Loss: [2.637185, 0.4003645, 4.874006]\n",
      "344: Train Loss: [2.516698, 0.3192283, 4.7141676] | Test Loss: [2.6124794, 0.32606852, 4.8988905]\n",
      "345: Train Loss: [2.509805, 0.40007147, 4.6195383] | Test Loss: [2.408695, 0.32470846, 4.4926815]\n",
      "346: Train Loss: [2.4085376, 0.3158169, 4.5012584] | Test Loss: [2.3958237, 0.44154075, 4.3501067]\n",
      "347: Train Loss: [2.288761, 0.3234411, 4.254081] | Test Loss: [2.777526, 0.42921218, 5.1258397]\n",
      "348: Train Loss: [2.3121364, 0.31574976, 4.308523] | Test Loss: [2.485153, 0.3549806, 4.6153255]\n",
      "349: Train Loss: [2.4691677, 0.30783516, 4.6305003] | Test Loss: [2.8789165, 0.21769632, 5.540137]\n",
      "350: Train Loss: [2.503792, 0.4064594, 4.601125] | Test Loss: [2.7495375, 0.37058803, 5.128487]\n",
      "351: Train Loss: [2.4376516, 0.31089026, 4.564413] | Test Loss: [2.7429717, 0.47491172, 5.0110316]\n",
      "352: Train Loss: [2.4409263, 0.38551548, 4.496337] | Test Loss: [2.5389113, 0.39438966, 4.683433]\n",
      "353: Train Loss: [2.4586868, 0.3243727, 4.593001] | Test Loss: [2.6824045, 0.36502752, 4.9997816]\n",
      "354: Train Loss: [2.4222307, 0.33227307, 4.5121884] | Test Loss: [2.644518, 0.32944953, 4.959586]\n",
      "355: Train Loss: [2.496093, 0.34427255, 4.6479135] | Test Loss: [2.6452825, 0.3966809, 4.893884]\n",
      "356: Train Loss: [2.5665188, 0.41340402, 4.7196336] | Test Loss: [2.5330553, 0.3053989, 4.7607117]\n",
      "357: Train Loss: [2.571771, 0.3941706, 4.749371] | Test Loss: [2.4043384, 0.36872658, 4.43995]\n",
      "358: Train Loss: [2.4259143, 0.38234204, 4.4694867] | Test Loss: [2.659885, 0.3230311, 4.996739]\n",
      "359: Train Loss: [2.4936798, 0.3253741, 4.6619854] | Test Loss: [2.6486502, 0.435445, 4.8618555]\n",
      "360: Train Loss: [2.5835488, 0.3668199, 4.8002777] | Test Loss: [2.507808, 0.4053149, 4.610301]\n",
      "361: Train Loss: [2.3605325, 0.3418041, 4.379261] | Test Loss: [2.6183176, 0.39239284, 4.8442426]\n",
      "362: Train Loss: [2.3939366, 0.3446171, 4.4432564] | Test Loss: [2.5480676, 0.29253381, 4.8036013]\n",
      "363: Train Loss: [2.3931186, 0.3519655, 4.434272] | Test Loss: [2.643327, 0.38021952, 4.9064345]\n",
      "364: Train Loss: [2.3682482, 0.36685959, 4.369637] | Test Loss: [2.7137527, 0.37199914, 5.055506]\n",
      "365: Train Loss: [2.2543757, 0.31078196, 4.1979694] | Test Loss: [2.6538937, 0.37548366, 4.932304]\n",
      "366: Train Loss: [2.330393, 0.33311385, 4.3276725] | Test Loss: [2.7250264, 0.39277038, 5.0572824]\n",
      "367: Train Loss: [2.4525685, 0.29324135, 4.6118956] | Test Loss: [2.712429, 0.36717945, 5.0576787]\n",
      "368: Train Loss: [2.5097146, 0.504932, 4.5144973] | Test Loss: [2.5147078, 0.35081047, 4.678605]\n",
      "369: Train Loss: [2.4105616, 0.4231774, 4.397946] | Test Loss: [2.6582987, 0.3556434, 4.960954]\n",
      "370: Train Loss: [2.3347242, 0.3873746, 4.282074] | Test Loss: [2.6479695, 0.3862562, 4.9096828]\n",
      "371: Train Loss: [2.2597954, 0.32308397, 4.196507] | Test Loss: [2.4724743, 0.38110447, 4.563844]\n",
      "372: Train Loss: [2.5079408, 0.3354164, 4.680465] | Test Loss: [2.7488508, 0.38883647, 5.1088653]\n",
      "373: Train Loss: [2.339278, 0.3304834, 4.3480725] | Test Loss: [2.673081, 0.38974446, 4.9564176]\n",
      "374: Train Loss: [2.562708, 0.34748718, 4.777929] | Test Loss: [2.706949, 0.37548366, 5.0384145]\n",
      "375: Train Loss: [2.4672909, 0.40208238, 4.5324993] | Test Loss: [2.5792298, 0.2947143, 4.863745]\n",
      "376: Train Loss: [2.3995137, 0.38553715, 4.4134903] | Test Loss: [2.5533082, 0.44863385, 4.657983]\n",
      "377: Train Loss: [2.4852505, 0.44498527, 4.5255156] | Test Loss: [2.734339, 0.42507, 5.043608]\n",
      "378: Train Loss: [2.4386091, 0.38909385, 4.4881244] | Test Loss: [2.529831, 0.32402173, 4.73564]\n",
      "379: Train Loss: [2.4081318, 0.33779413, 4.4784694] | Test Loss: [2.5358212, 0.3232956, 4.748347]\n",
      "380: Train Loss: [2.557554, 0.32862565, 4.7864823] | Test Loss: [2.3196933, 0.34448102, 4.2949057]\n",
      "381: Train Loss: [2.4147615, 0.3316826, 4.4978404] | Test Loss: [2.574032, 0.37516922, 4.772895]\n",
      "382: Train Loss: [2.427297, 0.37945998, 4.4751344] | Test Loss: [2.7803898, 0.3878311, 5.1729484]\n",
      "383: Train Loss: [2.4419093, 0.35003406, 4.5337844] | Test Loss: [2.481599, 0.32741198, 4.635786]\n",
      "384: Train Loss: [2.3683765, 0.37418926, 4.3625636] | Test Loss: [2.5876763, 0.32635355, 4.848999]\n",
      "385: Train Loss: [2.4237127, 0.3331293, 4.514296] | Test Loss: [2.5919778, 0.3319146, 4.8520412]\n",
      "386: Train Loss: [2.4857922, 0.4080875, 4.563497] | Test Loss: [2.695724, 0.34661376, 5.044834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387: Train Loss: [2.4020877, 0.32706526, 4.47711] | Test Loss: [2.6185305, 0.40326872, 4.833792]\n",
      "388: Train Loss: [2.4237707, 0.299897, 4.547644] | Test Loss: [2.565639, 0.37175867, 4.7595196]\n",
      "389: Train Loss: [2.407326, 0.3295857, 4.4850664] | Test Loss: [2.6341205, 0.44763255, 4.820608]\n",
      "390: Train Loss: [2.2978241, 0.33048594, 4.2651625] | Test Loss: [2.6844974, 0.3510654, 5.017929]\n",
      "391: Train Loss: [2.553182, 0.38215202, 4.7242117] | Test Loss: [2.7281911, 0.34882897, 5.1075535]\n",
      "392: Train Loss: [2.4293222, 0.31798935, 4.540655] | Test Loss: [2.4820778, 0.484242, 4.4799137]\n",
      "393: Train Loss: [2.2950935, 0.33097646, 4.2592106] | Test Loss: [2.6917338, 0.27923647, 5.1042314]\n",
      "394: Train Loss: [2.3657148, 0.32071778, 4.410712] | Test Loss: [2.603022, 0.40203658, 4.8040075]\n",
      "395: Train Loss: [2.384354, 0.36519668, 4.4035115] | Test Loss: [2.7852674, 0.43228173, 5.138253]\n",
      "396: Train Loss: [2.4721408, 0.4208391, 4.5234423] | Test Loss: [2.6090105, 0.37966815, 4.8383527]\n",
      "397: Train Loss: [2.213432, 0.3269975, 4.099867] | Test Loss: [2.3946784, 0.34634045, 4.443016]\n",
      "398: Train Loss: [2.411357, 0.4168808, 4.4058332] | Test Loss: [2.652676, 0.38248602, 4.9228663]\n",
      "399: Train Loss: [2.4812734, 0.34001562, 4.6225314] | Test Loss: [2.7327256, 0.33054444, 5.134907]\n",
      "400: Train Loss: [2.4600239, 0.3836676, 4.5363803] | Test Loss: [2.5953774, 0.3572724, 4.8334827]\n",
      "401: Train Loss: [2.6276128, 0.3949087, 4.8603168] | Test Loss: [2.665014, 0.40055415, 4.929474]\n",
      "402: Train Loss: [2.2775488, 0.39616477, 4.1589327] | Test Loss: [2.8904924, 0.34982455, 5.4311604]\n",
      "403: Train Loss: [2.47015, 0.35896078, 4.5813394] | Test Loss: [2.6469262, 0.34666413, 4.9471884]\n",
      "404: Train Loss: [2.3882616, 0.29908168, 4.4774413] | Test Loss: [2.7376444, 0.44180867, 5.03348]\n",
      "405: Train Loss: [2.5222218, 0.32828495, 4.716159] | Test Loss: [2.5265613, 0.32822248, 4.7249002]\n",
      "406: Train Loss: [2.5301797, 0.3539592, 4.7064004] | Test Loss: [2.72088, 0.38545772, 5.0563025]\n",
      "407: Train Loss: [2.367899, 0.42243686, 4.313361] | Test Loss: [2.537476, 0.3710637, 4.7038884]\n",
      "408: Train Loss: [2.3335729, 0.38459226, 4.2825537] | Test Loss: [2.492423, 0.43236732, 4.552479]\n",
      "409: Train Loss: [2.297486, 0.39690176, 4.1980705] | Test Loss: [2.5321517, 0.3420354, 4.722268]\n",
      "410: Train Loss: [2.3742967, 0.38011497, 4.3684783] | Test Loss: [2.5843358, 0.37739137, 4.7912803]\n",
      "411: Train Loss: [2.510188, 0.4133818, 4.6069946] | Test Loss: [2.5003817, 0.35103062, 4.6497326]\n",
      "412: Train Loss: [2.492982, 0.34723958, 4.6387243] | Test Loss: [2.607422, 0.37541077, 4.8394337]\n",
      "413: Train Loss: [2.5046322, 0.31832093, 4.6909437] | Test Loss: [2.7824128, 0.4253684, 5.139457]\n",
      "414: Train Loss: [2.4253087, 0.3729878, 4.4776297] | Test Loss: [2.4144893, 0.3630704, 4.465908]\n",
      "415: Train Loss: [2.2886744, 0.36683518, 4.2105136] | Test Loss: [2.6559165, 0.41703433, 4.8947988]\n",
      "416: Train Loss: [2.4119744, 0.36917743, 4.4547715] | Test Loss: [2.6708403, 0.3785318, 4.9631486]\n",
      "417: Train Loss: [2.3417022, 0.37116396, 4.3122406] | Test Loss: [2.6585987, 0.30544984, 5.0117474]\n",
      "418: Train Loss: [2.387303, 0.37025625, 4.40435] | Test Loss: [2.536531, 0.33479097, 4.7382708]\n",
      "419: Train Loss: [2.496357, 0.34023985, 4.652474] | Test Loss: [2.7060728, 0.3263172, 5.0858283]\n",
      "420: Train Loss: [2.3977737, 0.4082156, 4.387332] | Test Loss: [2.707352, 0.33752102, 5.077183]\n",
      "421: Train Loss: [2.4623294, 0.34905362, 4.575605] | Test Loss: [2.8776407, 0.3430005, 5.412281]\n",
      "422: Train Loss: [2.3572454, 0.35508758, 4.359403] | Test Loss: [2.4700644, 0.40123603, 4.5388927]\n",
      "423: Train Loss: [2.4420087, 0.34923875, 4.5347786] | Test Loss: [2.5416276, 0.36998913, 4.7132664]\n",
      "424: Train Loss: [2.4193776, 0.35007674, 4.4886785] | Test Loss: [2.598223, 0.33453935, 4.8619065]\n",
      "425: Train Loss: [2.4938102, 0.37140644, 4.616214] | Test Loss: [2.5540905, 0.37412444, 4.7340565]\n",
      "426: Train Loss: [2.4349458, 0.33720908, 4.5326824] | Test Loss: [2.7117, 0.35163823, 5.0717616]\n",
      "427: Train Loss: [2.4590652, 0.37948167, 4.5386486] | Test Loss: [2.597932, 0.3630238, 4.8328404]\n",
      "428: Train Loss: [2.5279489, 0.31163067, 4.744267] | Test Loss: [2.702953, 0.34355268, 5.0623536]\n",
      "429: Train Loss: [2.3940518, 0.42674404, 4.3613596] | Test Loss: [2.659849, 0.38243237, 4.9372654]\n",
      "430: Train Loss: [2.4146807, 0.42647547, 4.402886] | Test Loss: [2.584702, 0.397115, 4.7722893]\n",
      "431: Train Loss: [2.4208586, 0.36444375, 4.4772735] | Test Loss: [2.5578744, 0.39254528, 4.7232037]\n",
      "432: Train Loss: [2.536217, 0.34325558, 4.7291784] | Test Loss: [2.5012193, 0.33582246, 4.666616]\n",
      "433: Train Loss: [2.4048164, 0.3599812, 4.4496517] | Test Loss: [2.6987507, 0.40666956, 4.990832]\n",
      "434: Train Loss: [2.3881965, 0.32816014, 4.4482327] | Test Loss: [2.4845946, 0.35018188, 4.619007]\n",
      "435: Train Loss: [2.427016, 0.3586618, 4.4953704] | Test Loss: [2.6320717, 0.3987573, 4.865386]\n",
      "436: Train Loss: [2.3010924, 0.33968866, 4.262496] | Test Loss: [2.459318, 0.3194694, 4.5991664]\n",
      "437: Train Loss: [2.4515433, 0.37515113, 4.5279355] | Test Loss: [2.769274, 0.49259254, 5.0459557]\n",
      "438: Train Loss: [2.4438453, 0.36072838, 4.5269623] | Test Loss: [2.4536211, 0.42241248, 4.48483]\n",
      "439: Train Loss: [2.3144546, 0.3777723, 4.251137] | Test Loss: [2.5519269, 0.5376291, 4.5662246]\n",
      "440: Train Loss: [2.5131886, 0.5187074, 4.50767] | Test Loss: [2.4543672, 0.38480487, 4.5239296]\n",
      "441: Train Loss: [2.3122127, 0.38163295, 4.2427926] | Test Loss: [2.6571007, 0.40900522, 4.905196]\n",
      "442: Train Loss: [2.4139972, 0.37396896, 4.4540253] | Test Loss: [2.6919725, 0.38307923, 5.000866]\n",
      "443: Train Loss: [2.4057746, 0.37931907, 4.43223] | Test Loss: [2.5415597, 0.34475744, 4.738362]\n",
      "444: Train Loss: [2.3852406, 0.3888542, 4.381627] | Test Loss: [2.6409204, 0.36186734, 4.9199734]\n",
      "445: Train Loss: [2.301857, 0.36910897, 4.234605] | Test Loss: [2.38938, 0.3807952, 4.397965]\n",
      "446: Train Loss: [2.591809, 0.3665977, 4.8170204] | Test Loss: [2.5433264, 0.3789291, 4.7077236]\n",
      "447: Train Loss: [2.2829127, 0.2921356, 4.2736897] | Test Loss: [2.6055272, 0.34697944, 4.8640747]\n",
      "448: Train Loss: [2.4166734, 0.30912733, 4.5242195] | Test Loss: [2.5516505, 0.35383868, 4.749462]\n",
      "449: Train Loss: [2.5334716, 0.35696292, 4.70998] | Test Loss: [2.6122777, 0.42740363, 4.797152]\n",
      "450: Train Loss: [2.364542, 0.32725126, 4.4018326] | Test Loss: [2.6841261, 0.5066708, 4.8615813]\n",
      "451: Train Loss: [2.4152215, 0.36651245, 4.4639306] | Test Loss: [2.625111, 0.40549746, 4.8447247]\n",
      "452: Train Loss: [2.379924, 0.32339483, 4.4364533] | Test Loss: [2.7042577, 0.37517104, 5.0333443]\n",
      "453: Train Loss: [2.3616211, 0.37022296, 4.353019] | Test Loss: [2.9193606, 0.47297686, 5.3657446]\n",
      "454: Train Loss: [2.553793, 0.38228002, 4.725306] | Test Loss: [2.691563, 0.33628467, 5.046841]\n",
      "455: Train Loss: [2.4310856, 0.31934673, 4.5428243] | Test Loss: [2.6916425, 0.45664918, 4.9266357]\n",
      "456: Train Loss: [2.4133854, 0.32937807, 4.4973927] | Test Loss: [2.506175, 0.32049212, 4.691858]\n",
      "457: Train Loss: [2.6124165, 0.3732229, 4.85161] | Test Loss: [2.5876603, 0.33372912, 4.8415914]\n",
      "458: Train Loss: [2.590423, 0.38644674, 4.7943993] | Test Loss: [2.571133, 0.33261648, 4.8096495]\n",
      "459: Train Loss: [2.5652099, 0.34520906, 4.7852106] | Test Loss: [2.6256754, 0.3484315, 4.9029193]\n",
      "460: Train Loss: [2.482494, 0.43455422, 4.530434] | Test Loss: [2.6581156, 0.33438206, 4.981849]\n",
      "461: Train Loss: [2.3894706, 0.40576676, 4.373174] | Test Loss: [2.6311123, 0.35846853, 4.903756]\n",
      "462: Train Loss: [2.3960304, 0.36053774, 4.4315233] | Test Loss: [2.7556822, 0.36358526, 5.147779]\n",
      "463: Train Loss: [2.4539165, 0.39828753, 4.509546] | Test Loss: [2.4711082, 0.2934027, 4.6488137]\n",
      "464: Train Loss: [2.4389138, 0.41765127, 4.4601765] | Test Loss: [2.6218524, 0.41548994, 4.8282146]\n",
      "465: Train Loss: [2.3883433, 0.36312276, 4.4135637] | Test Loss: [2.6335192, 0.33319563, 4.9338427]\n",
      "466: Train Loss: [2.5520868, 0.3969603, 4.7072134] | Test Loss: [2.6615908, 0.37999293, 4.9431887]\n",
      "467: Train Loss: [2.494005, 0.3001456, 4.6878643] | Test Loss: [2.6713972, 0.38931218, 4.953482]\n",
      "468: Train Loss: [2.5977356, 0.31652775, 4.8789434] | Test Loss: [2.5014882, 0.3820359, 4.6209407]\n",
      "469: Train Loss: [2.4700804, 0.3558599, 4.584301] | Test Loss: [2.234436, 0.23878586, 4.2300863]\n",
      "470: Train Loss: [2.3137894, 0.39431474, 4.233264] | Test Loss: [2.614589, 0.3353637, 4.893814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471: Train Loss: [2.407109, 0.31968412, 4.494534] | Test Loss: [2.5721, 0.47667792, 4.667522]\n",
      "472: Train Loss: [2.2145352, 0.4552308, 3.9738395] | Test Loss: [2.7221022, 0.44683272, 4.9973717]\n",
      "473: Train Loss: [2.4283173, 0.30075595, 4.5558786] | Test Loss: [2.6281192, 0.35653, 4.8997083]\n",
      "474: Train Loss: [2.4616466, 0.33949155, 4.5838017] | Test Loss: [2.5774374, 0.37564948, 4.7792253]\n",
      "475: Train Loss: [2.549581, 0.37181485, 4.7273474] | Test Loss: [2.488191, 0.33246478, 4.643917]\n",
      "476: Train Loss: [2.4746237, 0.34036994, 4.6088777] | Test Loss: [2.6335561, 0.36615905, 4.9009533]\n",
      "477: Train Loss: [2.4537091, 0.3448531, 4.5625653] | Test Loss: [2.5683672, 0.36777586, 4.7689586]\n",
      "478: Train Loss: [2.4303226, 0.3426167, 4.5180287] | Test Loss: [2.626987, 0.40512848, 4.8488455]\n",
      "479: Train Loss: [2.4292383, 0.35192865, 4.506548] | Test Loss: [2.6894462, 0.36164814, 5.0172443]\n",
      "480: Train Loss: [2.2897792, 0.33855698, 4.2410016] | Test Loss: [2.7268102, 0.42079058, 5.0328298]\n",
      "481: Train Loss: [2.4561164, 0.35788196, 4.554351] | Test Loss: [2.3835793, 0.3435392, 4.4236193]\n",
      "482: Train Loss: [2.4643805, 0.30882606, 4.619935] | Test Loss: [2.656847, 0.32825202, 4.985442]\n",
      "483: Train Loss: [2.279783, 0.3333272, 4.2262387] | Test Loss: [2.5341787, 0.32366616, 4.7446914]\n",
      "484: Train Loss: [2.373767, 0.3132256, 4.434308] | Test Loss: [2.6660645, 0.3706073, 4.9615216]\n",
      "485: Train Loss: [2.4448156, 0.34994683, 4.5396843] | Test Loss: [2.6054716, 0.37687236, 4.8340707]\n",
      "486: Train Loss: [2.3833706, 0.3867609, 4.3799806] | Test Loss: [2.688802, 0.3133089, 5.0642953]\n",
      "487: Train Loss: [2.4550154, 0.32595834, 4.5840726] | Test Loss: [2.7556891, 0.35876793, 5.1526103]\n",
      "488: Train Loss: [2.486442, 0.38539526, 4.587489] | Test Loss: [2.5892813, 0.38392103, 4.7946415]\n",
      "489: Train Loss: [2.4153168, 0.40460747, 4.4260263] | Test Loss: [2.6953852, 0.3640955, 5.0266747]\n",
      "490: Train Loss: [2.4014382, 0.33533597, 4.4675403] | Test Loss: [2.5886579, 0.35704243, 4.8202734]\n",
      "491: Train Loss: [2.6401777, 0.35802695, 4.9223285] | Test Loss: [2.420197, 0.30716294, 4.5332313]\n",
      "492: Train Loss: [2.5091672, 0.39873052, 4.619604] | Test Loss: [2.6419375, 0.41159913, 4.872276]\n",
      "493: Train Loss: [2.389159, 0.44509417, 4.333224] | Test Loss: [2.6776345, 0.35124904, 5.0040197]\n",
      "494: Train Loss: [2.5672991, 0.39632544, 4.7382727] | Test Loss: [2.7089689, 0.37966108, 5.0382767]\n",
      "495: Train Loss: [2.4695346, 0.37510797, 4.5639615] | Test Loss: [2.4571714, 0.38319218, 4.531151]\n",
      "496: Train Loss: [2.5018625, 0.37594536, 4.6277795] | Test Loss: [2.5982008, 0.3888043, 4.807597]\n",
      "497: Train Loss: [2.4263654, 0.3153255, 4.537405] | Test Loss: [2.5695548, 0.34360155, 4.795508]\n",
      "498: Train Loss: [2.4396167, 0.40055498, 4.478678] | Test Loss: [2.7880738, 0.50650835, 5.069639]\n",
      "499: Train Loss: [2.458602, 0.37542897, 4.5417747] | Test Loss: [2.5787067, 0.3351035, 4.82231]\n",
      "500: Train Loss: [2.5065691, 0.37340367, 4.6397347] | Test Loss: [2.6263704, 0.32136717, 4.9313736]\n",
      "501: Train Loss: [2.4239924, 0.32140982, 4.526575] | Test Loss: [2.5856614, 0.4100543, 4.7612686]\n",
      "502: Train Loss: [2.4908433, 0.34546697, 4.6362195] | Test Loss: [2.7734087, 0.36788502, 5.178932]\n",
      "503: Train Loss: [2.3448803, 0.3983531, 4.2914076] | Test Loss: [2.8065953, 0.33946335, 5.2737274]\n",
      "504: Train Loss: [2.3763626, 0.31995174, 4.4327736] | Test Loss: [2.5093544, 0.3676243, 4.6510844]\n",
      "505: Train Loss: [2.4898076, 0.39864272, 4.5809727] | Test Loss: [2.5205789, 0.3206654, 4.7204924]\n",
      "506: Train Loss: [2.4677427, 0.3648707, 4.570615] | Test Loss: [2.5430112, 0.38952795, 4.6964946]\n",
      "507: Train Loss: [2.437522, 0.33956468, 4.535479] | Test Loss: [2.4992807, 0.3850059, 4.6135554]\n",
      "508: Train Loss: [2.3750753, 0.30128625, 4.4488645] | Test Loss: [2.711261, 0.33759448, 5.0849276]\n",
      "509: Train Loss: [2.4532924, 0.3511977, 4.555387] | Test Loss: [2.383483, 0.37651145, 4.3904543]\n",
      "510: Train Loss: [2.5271413, 0.38109034, 4.6731925] | Test Loss: [2.6455283, 0.4379515, 4.853105]\n",
      "511: Train Loss: [2.3748906, 0.31898186, 4.4307995] | Test Loss: [2.4856682, 0.37899166, 4.5923448]\n",
      "512: Train Loss: [2.3992567, 0.3851947, 4.4133186] | Test Loss: [2.7755203, 0.36137998, 5.1896605]\n",
      "513: Train Loss: [2.2744403, 0.3436345, 4.205246] | Test Loss: [2.5846088, 0.36123866, 4.807979]\n",
      "514: Train Loss: [2.4263697, 0.32855934, 4.52418] | Test Loss: [2.6067595, 0.314803, 4.898716]\n",
      "515: Train Loss: [2.5267057, 0.33948836, 4.713923] | Test Loss: [2.7618644, 0.34324533, 5.1804833]\n",
      "516: Train Loss: [2.3739371, 0.3579926, 4.3898816] | Test Loss: [2.596459, 0.42552918, 4.767389]\n",
      "517: Train Loss: [2.386113, 0.45533738, 4.3168883] | Test Loss: [2.887174, 0.38665622, 5.3876915]\n",
      "518: Train Loss: [2.4285715, 0.32557124, 4.531572] | Test Loss: [2.4789186, 0.39289403, 4.564943]\n",
      "519: Train Loss: [2.359874, 0.30210716, 4.4176407] | Test Loss: [2.5185192, 0.3256148, 4.7114234]\n",
      "520: Train Loss: [2.4795034, 0.4505997, 4.508407] | Test Loss: [2.6895616, 0.40169555, 4.9774275]\n",
      "521: Train Loss: [2.4863846, 0.5661359, 4.4066334] | Test Loss: [2.5699573, 0.35523677, 4.7846775]\n",
      "522: Train Loss: [2.1959789, 0.33399624, 4.0579615] | Test Loss: [2.7937825, 0.3583012, 5.229264]\n",
      "523: Train Loss: [2.4607654, 0.41040003, 4.511131] | Test Loss: [2.6044986, 0.32092297, 4.8880744]\n",
      "Epoch 13\n",
      "0: Train Loss: [2.316103, 0.37785718, 4.2543488] | Test Loss: [2.6359425, 0.37821394, 4.893671]\n",
      "1: Train Loss: [2.2843246, 0.37216833, 4.1964808] | Test Loss: [2.5047693, 0.44836512, 4.5611734]\n",
      "2: Train Loss: [2.2740984, 0.4430577, 4.1051393] | Test Loss: [2.5917332, 0.38356227, 4.7999043]\n",
      "3: Train Loss: [2.2878091, 0.41848812, 4.1571302] | Test Loss: [2.4059787, 0.35996726, 4.45199]\n",
      "4: Train Loss: [2.3159685, 0.36461383, 4.267323] | Test Loss: [2.6515262, 0.48376817, 4.8192844]\n",
      "5: Train Loss: [2.262128, 0.3917475, 4.1325088] | Test Loss: [2.7423809, 0.35484573, 5.129916]\n",
      "6: Train Loss: [2.2402122, 0.3230341, 4.15739] | Test Loss: [2.6496696, 0.353453, 4.945886]\n",
      "7: Train Loss: [2.3615427, 0.31144702, 4.4116383] | Test Loss: [2.6143603, 0.34041953, 4.8883014]\n",
      "8: Train Loss: [2.0563517, 0.34650815, 3.766195] | Test Loss: [2.7707663, 0.4919036, 5.0496287]\n",
      "9: Train Loss: [2.3840044, 0.39731055, 4.370698] | Test Loss: [2.6234171, 0.33923346, 4.907601]\n",
      "10: Train Loss: [2.16702, 0.3180279, 4.016012] | Test Loss: [2.437245, 0.4417035, 4.4327865]\n",
      "11: Train Loss: [2.309681, 0.37261134, 4.2467504] | Test Loss: [2.7014253, 0.31102568, 5.091825]\n",
      "12: Train Loss: [2.4084866, 0.48428452, 4.332689] | Test Loss: [2.661038, 0.37497756, 4.9470983]\n",
      "13: Train Loss: [2.427346, 0.33893946, 4.5157523] | Test Loss: [2.4720235, 0.40767565, 4.536371]\n",
      "14: Train Loss: [2.2808273, 0.34587255, 4.215782] | Test Loss: [2.596188, 0.3724497, 4.8199263]\n",
      "15: Train Loss: [2.258144, 0.35834312, 4.1579447] | Test Loss: [2.7227204, 0.34567487, 5.099766]\n",
      "16: Train Loss: [2.3116267, 0.36802366, 4.2552295] | Test Loss: [2.3581402, 0.36750132, 4.348779]\n",
      "17: Train Loss: [2.2984545, 0.3511491, 4.24576] | Test Loss: [2.7499325, 0.35824212, 5.141623]\n",
      "18: Train Loss: [2.234454, 0.36374193, 4.105166] | Test Loss: [2.6244218, 0.4080122, 4.8408313]\n",
      "19: Train Loss: [2.384082, 0.3942762, 4.373888] | Test Loss: [2.5446486, 0.38396105, 4.705336]\n",
      "20: Train Loss: [2.296686, 0.31067768, 4.2826943] | Test Loss: [2.6307418, 0.38582358, 4.87566]\n",
      "21: Train Loss: [2.2949467, 0.33516806, 4.2547255] | Test Loss: [2.6310515, 0.28438336, 4.97772]\n",
      "22: Train Loss: [2.4134424, 0.3923825, 4.434502] | Test Loss: [2.5971029, 0.4415644, 4.752641]\n",
      "23: Train Loss: [2.3888886, 0.43731302, 4.340464] | Test Loss: [2.5397887, 0.38936415, 4.690213]\n",
      "24: Train Loss: [2.2728581, 0.37429833, 4.1714177] | Test Loss: [2.528202, 0.34517175, 4.711232]\n",
      "25: Train Loss: [2.1576385, 0.3134374, 4.0018396] | Test Loss: [2.583596, 0.36179417, 4.805398]\n",
      "26: Train Loss: [2.2509158, 0.3771505, 4.124681] | Test Loss: [2.6522005, 0.34368217, 4.9607186]\n",
      "27: Train Loss: [2.2713256, 0.30811688, 4.2345343] | Test Loss: [2.4182138, 0.39134696, 4.4450808]\n",
      "28: Train Loss: [2.2563558, 0.34933028, 4.163381] | Test Loss: [2.6185973, 0.37018073, 4.867014]\n",
      "29: Train Loss: [2.461025, 0.4852, 4.43685] | Test Loss: [2.967807, 0.45545685, 5.4801574]\n",
      "30: Train Loss: [2.1790533, 0.34311807, 4.0149884] | Test Loss: [2.582009, 0.31010726, 4.853911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31: Train Loss: [2.1788614, 0.3331381, 4.024585] | Test Loss: [2.4638896, 0.36668, 4.561099]\n",
      "32: Train Loss: [2.3366356, 0.38445562, 4.2888155] | Test Loss: [2.5247095, 0.33127826, 4.7181406]\n",
      "33: Train Loss: [2.3404186, 0.37601677, 4.3048205] | Test Loss: [2.8204668, 0.31783915, 5.3230944]\n",
      "34: Train Loss: [2.3052542, 0.32921746, 4.281291] | Test Loss: [2.6417403, 0.40221733, 4.8812633]\n",
      "35: Train Loss: [2.2497883, 0.30565253, 4.193924] | Test Loss: [2.6432984, 0.3189519, 4.9676447]\n",
      "36: Train Loss: [2.2353745, 0.3581523, 4.1125965] | Test Loss: [2.6245193, 0.34672695, 4.902312]\n",
      "37: Train Loss: [2.2947392, 0.37815818, 4.2113204] | Test Loss: [2.4278598, 0.323109, 4.5326104]\n",
      "38: Train Loss: [2.240798, 0.37164605, 4.10995] | Test Loss: [2.6735702, 0.30461827, 5.042522]\n",
      "39: Train Loss: [2.1893644, 0.39189047, 3.9868383] | Test Loss: [2.8304038, 0.36318293, 5.2976246]\n",
      "40: Train Loss: [2.2346988, 0.29871365, 4.170684] | Test Loss: [2.689582, 0.35734743, 5.0218167]\n",
      "41: Train Loss: [2.3439305, 0.43280262, 4.2550583] | Test Loss: [2.523199, 0.3568723, 4.689526]\n",
      "42: Train Loss: [2.3552253, 0.44096434, 4.2694864] | Test Loss: [2.560841, 0.3630219, 4.7586603]\n",
      "43: Train Loss: [2.4012022, 0.36650854, 4.435896] | Test Loss: [2.4465666, 0.35317272, 4.5399604]\n",
      "44: Train Loss: [2.29449, 0.3572686, 4.2317114] | Test Loss: [2.5833778, 0.3789955, 4.7877603]\n",
      "45: Train Loss: [2.4067822, 0.32288337, 4.4906807] | Test Loss: [2.7258704, 0.37752664, 5.074214]\n",
      "46: Train Loss: [2.3336773, 0.33745885, 4.3298955] | Test Loss: [2.738633, 0.47514254, 5.0021234]\n",
      "47: Train Loss: [2.3953657, 0.37450555, 4.416226] | Test Loss: [2.6185951, 0.30845237, 4.9287376]\n",
      "48: Train Loss: [2.471624, 0.3747679, 4.56848] | Test Loss: [2.5518928, 0.41642904, 4.6873565]\n",
      "49: Train Loss: [2.3862038, 0.3561668, 4.4162407] | Test Loss: [2.6061788, 0.40801695, 4.8043404]\n",
      "50: Train Loss: [2.3356366, 0.3551807, 4.3160925] | Test Loss: [2.7143457, 0.35423508, 5.074456]\n",
      "51: Train Loss: [2.3595955, 0.39014733, 4.329044] | Test Loss: [2.723491, 0.36151093, 5.085471]\n",
      "52: Train Loss: [2.4035141, 0.3881106, 4.4189177] | Test Loss: [2.6088843, 0.4387799, 4.778989]\n",
      "53: Train Loss: [2.3911037, 0.37518713, 4.4070206] | Test Loss: [2.81569, 0.4036368, 5.227743]\n",
      "54: Train Loss: [2.3650959, 0.39170417, 4.3384876] | Test Loss: [2.616866, 0.33443654, 4.899296]\n",
      "55: Train Loss: [2.4393923, 0.33533725, 4.5434475] | Test Loss: [2.7173555, 0.38202077, 5.05269]\n",
      "56: Train Loss: [2.289739, 0.346094, 4.2333837] | Test Loss: [2.6437554, 0.4236219, 4.863889]\n",
      "57: Train Loss: [2.332088, 0.28842935, 4.3757467] | Test Loss: [2.6423368, 0.3580658, 4.926608]\n",
      "58: Train Loss: [2.4446797, 0.3855342, 4.503825] | Test Loss: [2.7972827, 0.48089176, 5.1136737]\n",
      "59: Train Loss: [2.4942627, 0.54959345, 4.438932] | Test Loss: [2.7083642, 0.3432395, 5.073489]\n",
      "60: Train Loss: [2.510972, 0.40640688, 4.615537] | Test Loss: [2.7024662, 0.40565073, 4.999282]\n",
      "61: Train Loss: [2.4960272, 0.3707527, 4.6213017] | Test Loss: [2.7517908, 0.3460611, 5.1575203]\n",
      "62: Train Loss: [2.3431084, 0.33873722, 4.34748] | Test Loss: [2.6586692, 0.3728735, 4.944465]\n",
      "63: Train Loss: [2.3437176, 0.36463127, 4.322804] | Test Loss: [2.6664393, 0.3103102, 5.022568]\n",
      "64: Train Loss: [2.3679466, 0.4330908, 4.3028026] | Test Loss: [2.7083168, 0.40995935, 5.0066743]\n",
      "65: Train Loss: [2.528894, 0.31833628, 4.7394514] | Test Loss: [2.6713827, 0.31734404, 5.025421]\n",
      "66: Train Loss: [2.5411804, 0.38101187, 4.701349] | Test Loss: [2.889059, 0.33912748, 5.4389906]\n",
      "67: Train Loss: [2.3613286, 0.3518825, 4.3707747] | Test Loss: [2.6278656, 0.3834627, 4.872268]\n",
      "68: Train Loss: [2.4166055, 0.31289655, 4.520314] | Test Loss: [2.6850922, 0.4503412, 4.919843]\n",
      "69: Train Loss: [2.480021, 0.29403973, 4.6660023] | Test Loss: [2.6585739, 0.3100606, 5.007087]\n",
      "70: Train Loss: [2.4673657, 0.37788925, 4.5568423] | Test Loss: [2.7648125, 0.29824147, 5.2313833]\n",
      "71: Train Loss: [2.4553208, 0.36204094, 4.5486007] | Test Loss: [2.5434775, 0.36950004, 4.717455]\n",
      "72: Train Loss: [2.3014848, 0.3357515, 4.267218] | Test Loss: [2.8013847, 0.32732257, 5.275447]\n",
      "73: Train Loss: [2.3923912, 0.35818702, 4.426595] | Test Loss: [2.5087843, 0.35848564, 4.659083]\n",
      "74: Train Loss: [2.4618244, 0.3885088, 4.53514] | Test Loss: [2.6784232, 0.31873456, 5.0381117]\n",
      "75: Train Loss: [2.254814, 0.33248055, 4.1771474] | Test Loss: [2.7389617, 0.3288496, 5.1490736]\n",
      "76: Train Loss: [2.4666262, 0.3820598, 4.5511928] | Test Loss: [2.7279603, 0.4052768, 5.050644]\n",
      "77: Train Loss: [2.5369596, 0.34724542, 4.726674] | Test Loss: [2.6223626, 0.34365794, 4.9010673]\n",
      "78: Train Loss: [2.2562053, 0.44429067, 4.06812] | Test Loss: [2.6246915, 0.36948133, 4.8799014]\n",
      "79: Train Loss: [2.3862233, 0.35173094, 4.420716] | Test Loss: [2.7173946, 0.35191363, 5.0828757]\n",
      "80: Train Loss: [2.4161325, 0.34474328, 4.4875216] | Test Loss: [2.517806, 0.37429982, 4.661312]\n",
      "81: Train Loss: [2.6012642, 0.3787627, 4.8237658] | Test Loss: [2.5980587, 0.33800387, 4.858114]\n",
      "82: Train Loss: [2.5674214, 0.39242828, 4.7424145] | Test Loss: [2.534226, 0.3603499, 4.7081017]\n",
      "83: Train Loss: [2.3862374, 0.39697388, 4.3755007] | Test Loss: [2.7296867, 0.42826408, 5.0311093]\n",
      "84: Train Loss: [2.3736553, 0.35405585, 4.3932548] | Test Loss: [2.72431, 0.37945384, 5.069166]\n",
      "85: Train Loss: [2.2878215, 0.41282424, 4.162819] | Test Loss: [2.6708827, 0.44984698, 4.891918]\n",
      "86: Train Loss: [2.5599701, 0.3932819, 4.7266583] | Test Loss: [2.659613, 0.4059201, 4.9133058]\n",
      "87: Train Loss: [2.327422, 0.36185646, 4.2929873] | Test Loss: [2.7771287, 0.35905853, 5.195199]\n",
      "88: Train Loss: [2.3631678, 0.3685871, 4.3577485] | Test Loss: [2.804812, 0.3624494, 5.2471747]\n",
      "89: Train Loss: [2.6042287, 0.33894578, 4.8695116] | Test Loss: [2.9931643, 0.5041701, 5.4821587]\n",
      "90: Train Loss: [2.4721394, 0.45169497, 4.4925838] | Test Loss: [2.5443287, 0.3647884, 4.723869]\n",
      "91: Train Loss: [2.444865, 0.36688122, 4.5228486] | Test Loss: [2.8009522, 0.38226488, 5.2196393]\n",
      "92: Train Loss: [2.5584908, 0.37505975, 4.741922] | Test Loss: [2.5717778, 0.36836028, 4.775195]\n",
      "93: Train Loss: [2.331724, 0.3103988, 4.3530493] | Test Loss: [2.6189134, 0.41280854, 4.8250184]\n",
      "94: Train Loss: [2.414564, 0.37975234, 4.4493756] | Test Loss: [2.6589193, 0.4127159, 4.9051228]\n",
      "95: Train Loss: [2.5923536, 0.4620815, 4.7226257] | Test Loss: [2.7855039, 0.34438148, 5.2266264]\n",
      "96: Train Loss: [2.5590327, 0.37757927, 4.740486] | Test Loss: [2.6607022, 0.3581048, 4.9632998]\n",
      "97: Train Loss: [2.5066097, 0.31824577, 4.6949735] | Test Loss: [2.816419, 0.37847987, 5.254358]\n",
      "98: Train Loss: [2.4023192, 0.32180655, 4.482832] | Test Loss: [2.5814269, 0.28880385, 4.8740497]\n",
      "99: Train Loss: [2.4284863, 0.39239907, 4.4645734] | Test Loss: [2.6739566, 0.44737998, 4.900533]\n",
      "100: Train Loss: [2.4517338, 0.3530774, 4.5503902] | Test Loss: [2.7791004, 0.3652856, 5.1929154]\n",
      "101: Train Loss: [2.2116234, 0.3462557, 4.076991] | Test Loss: [2.688753, 0.35374996, 5.023756]\n",
      "102: Train Loss: [2.332459, 0.34397224, 4.3209457] | Test Loss: [2.656731, 0.37807137, 4.9353905]\n",
      "103: Train Loss: [2.3091931, 0.35298356, 4.265403] | Test Loss: [2.5701303, 0.36134425, 4.7789164]\n",
      "104: Train Loss: [2.4679894, 0.33590376, 4.6000752] | Test Loss: [2.801076, 0.34302145, 5.2591305]\n",
      "105: Train Loss: [2.384047, 0.30206823, 4.466026] | Test Loss: [2.8881392, 0.4403283, 5.3359504]\n",
      "106: Train Loss: [2.4325783, 0.31772378, 4.547433] | Test Loss: [2.6362193, 0.34659746, 4.925841]\n",
      "107: Train Loss: [2.5145211, 0.30797705, 4.721065] | Test Loss: [2.733066, 0.43661, 5.029522]\n",
      "108: Train Loss: [2.5071218, 0.32643306, 4.6878104] | Test Loss: [2.4381902, 0.42631653, 4.4500637]\n",
      "109: Train Loss: [2.5280929, 0.4256668, 4.630519] | Test Loss: [2.6985545, 0.42168483, 4.9754243]\n",
      "110: Train Loss: [2.4606287, 0.3285393, 4.592718] | Test Loss: [2.7186077, 0.35545185, 5.0817633]\n",
      "111: Train Loss: [2.587279, 0.3667445, 4.8078136] | Test Loss: [2.6353235, 0.35212618, 4.918521]\n",
      "112: Train Loss: [2.276147, 0.35414204, 4.1981516] | Test Loss: [2.7209356, 0.35057887, 5.0912924]\n",
      "113: Train Loss: [2.3531296, 0.3246308, 4.3816285] | Test Loss: [2.776335, 0.35813025, 5.1945395]\n",
      "114: Train Loss: [2.45605, 0.3671488, 4.544951] | Test Loss: [2.5151625, 0.33784226, 4.6924825]\n",
      "115: Train Loss: [2.4603615, 0.361139, 4.559584] | Test Loss: [2.6469657, 0.3414722, 4.9524593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116: Train Loss: [2.6191754, 0.4833345, 4.7550163] | Test Loss: [2.713361, 0.35943398, 5.067288]\n",
      "117: Train Loss: [2.5479872, 0.37919885, 4.7167754] | Test Loss: [2.6354508, 0.35183942, 4.919062]\n",
      "118: Train Loss: [2.4305904, 0.34295118, 4.5182295] | Test Loss: [2.5391433, 0.36127755, 4.717009]\n",
      "119: Train Loss: [2.5102854, 0.35647815, 4.6640925] | Test Loss: [2.8063378, 0.3968752, 5.2158003]\n",
      "120: Train Loss: [2.432102, 0.34843573, 4.515768] | Test Loss: [2.68304, 0.3688746, 4.9972053]\n",
      "121: Train Loss: [2.4117367, 0.32354966, 4.4999237] | Test Loss: [2.48625, 0.4105208, 4.5619793]\n",
      "122: Train Loss: [2.5795667, 0.37033212, 4.788801] | Test Loss: [2.6484156, 0.32729158, 4.9695396]\n",
      "123: Train Loss: [2.448813, 0.36444837, 4.5331774] | Test Loss: [2.616948, 0.4509165, 4.7829795]\n",
      "124: Train Loss: [2.3470323, 0.29925066, 4.394814] | Test Loss: [2.6207144, 0.3995271, 4.841902]\n",
      "125: Train Loss: [2.3439512, 0.3534063, 4.334496] | Test Loss: [2.8734667, 0.35017636, 5.396757]\n",
      "126: Train Loss: [2.5556607, 0.29486945, 4.816452] | Test Loss: [2.6759303, 0.3507105, 5.00115]\n",
      "127: Train Loss: [2.4839938, 0.38211572, 4.5858717] | Test Loss: [2.662507, 0.38187388, 4.94314]\n",
      "128: Train Loss: [2.4826918, 0.37674433, 4.5886393] | Test Loss: [2.6645927, 0.3033175, 5.025868]\n",
      "129: Train Loss: [2.2610393, 0.33380818, 4.18827] | Test Loss: [2.583446, 0.38822094, 4.7786713]\n",
      "130: Train Loss: [2.3999078, 0.3126481, 4.4871674] | Test Loss: [2.748654, 0.41025868, 5.087049]\n",
      "131: Train Loss: [2.5784812, 0.32590568, 4.8310566] | Test Loss: [2.625662, 0.3421551, 4.909169]\n",
      "132: Train Loss: [2.565175, 0.33250138, 4.7978487] | Test Loss: [2.7402365, 0.37507427, 5.1053987]\n",
      "133: Train Loss: [2.5624247, 0.4424753, 4.682374] | Test Loss: [2.5499399, 0.33650285, 4.7633767]\n",
      "134: Train Loss: [2.4946952, 0.34419137, 4.645199] | Test Loss: [2.6874597, 0.40312633, 4.971793]\n",
      "135: Train Loss: [2.5504625, 0.3480393, 4.752886] | Test Loss: [2.549195, 0.44335163, 4.6550384]\n",
      "136: Train Loss: [2.5842557, 0.34320456, 4.825307] | Test Loss: [2.6220493, 0.45259458, 4.791504]\n",
      "137: Train Loss: [2.4674222, 0.49907678, 4.4357677] | Test Loss: [2.5544996, 0.3711185, 4.7378807]\n",
      "138: Train Loss: [2.3629558, 0.3294718, 4.39644] | Test Loss: [2.6208563, 0.41237444, 4.829338]\n",
      "139: Train Loss: [2.4573202, 0.47115666, 4.443484] | Test Loss: [2.8913167, 0.46828413, 5.314349]\n",
      "140: Train Loss: [2.4661021, 0.34509203, 4.5871124] | Test Loss: [2.4507296, 0.3811401, 4.520319]\n",
      "141: Train Loss: [2.423817, 0.42415944, 4.4234743] | Test Loss: [2.3787184, 0.34275413, 4.414683]\n",
      "142: Train Loss: [2.485632, 0.35632575, 4.6149383] | Test Loss: [2.7811933, 0.33840165, 5.2239847]\n",
      "143: Train Loss: [2.539414, 0.3725763, 4.7062516] | Test Loss: [2.698744, 0.35792893, 5.0395594]\n",
      "144: Train Loss: [2.2786093, 0.36182332, 4.1953955] | Test Loss: [2.553364, 0.3460899, 4.760638]\n",
      "145: Train Loss: [2.505729, 0.32070243, 4.6907554] | Test Loss: [2.8419802, 0.36124155, 5.322719]\n",
      "146: Train Loss: [2.5194674, 0.29894662, 4.7399883] | Test Loss: [2.685328, 0.39012402, 4.980532]\n",
      "147: Train Loss: [2.6661162, 0.3378909, 4.9943414] | Test Loss: [2.802117, 0.32036817, 5.283866]\n",
      "148: Train Loss: [2.5485213, 0.43807992, 4.6589627] | Test Loss: [2.597411, 0.33238778, 4.862434]\n",
      "149: Train Loss: [2.3393044, 0.40281436, 4.2757945] | Test Loss: [2.71408, 0.46504384, 4.963116]\n",
      "150: Train Loss: [2.3179355, 0.3408962, 4.294975] | Test Loss: [2.6936157, 0.36169508, 5.025536]\n",
      "151: Train Loss: [2.4812307, 0.3162168, 4.6462445] | Test Loss: [2.755152, 0.3626465, 5.1476574]\n",
      "152: Train Loss: [2.562891, 0.36367446, 4.7621074] | Test Loss: [2.661807, 0.34492514, 4.978689]\n",
      "153: Train Loss: [2.47252, 0.40202504, 4.543015] | Test Loss: [2.884033, 0.31915763, 5.4489083]\n",
      "154: Train Loss: [2.3442833, 0.37334487, 4.315222] | Test Loss: [2.7095046, 0.38637924, 5.03263]\n",
      "155: Train Loss: [2.460156, 0.35907796, 4.561234] | Test Loss: [2.6486619, 0.31246036, 4.9848633]\n",
      "156: Train Loss: [2.5586553, 0.38489416, 4.732416] | Test Loss: [2.546138, 0.3647274, 4.7275486]\n",
      "157: Train Loss: [2.4383352, 0.37509048, 4.5015798] | Test Loss: [2.6101818, 0.3900832, 4.8302803]\n",
      "158: Train Loss: [2.3754256, 0.3275975, 4.4232535] | Test Loss: [2.8819287, 0.3562148, 5.4076424]\n",
      "159: Train Loss: [2.5497887, 0.3526627, 4.746915] | Test Loss: [2.6912992, 0.35026357, 5.032335]\n",
      "160: Train Loss: [2.4641228, 0.32216656, 4.606079] | Test Loss: [2.6900258, 0.36891448, 5.011137]\n",
      "161: Train Loss: [2.373803, 0.32476327, 4.4228425] | Test Loss: [2.5439806, 0.3246314, 4.76333]\n",
      "162: Train Loss: [2.4047008, 0.34048393, 4.4689174] | Test Loss: [2.6028605, 0.3350811, 4.87064]\n",
      "163: Train Loss: [2.6195688, 0.3519057, 4.887232] | Test Loss: [2.923689, 0.45818228, 5.3891954]\n",
      "164: Train Loss: [2.442055, 0.34800115, 4.536109] | Test Loss: [2.6592152, 0.3441996, 4.974231]\n",
      "165: Train Loss: [2.2893908, 0.41993394, 4.158848] | Test Loss: [2.6552384, 0.35947388, 4.951003]\n",
      "166: Train Loss: [2.6277814, 0.3527, 4.9028625] | Test Loss: [2.626826, 0.3634585, 4.8901935]\n",
      "167: Train Loss: [2.535362, 0.3668186, 4.7039056] | Test Loss: [2.6649265, 0.3541586, 4.9756947]\n",
      "168: Train Loss: [2.303302, 0.36210558, 4.2444987] | Test Loss: [2.8125985, 0.40229243, 5.2229047]\n",
      "169: Train Loss: [2.4733827, 0.3214276, 4.6253376] | Test Loss: [2.698589, 0.35771132, 5.039467]\n",
      "170: Train Loss: [2.4503787, 0.39295503, 4.5078025] | Test Loss: [2.7989957, 0.33289358, 5.265098]\n",
      "171: Train Loss: [2.439911, 0.36079508, 4.5190268] | Test Loss: [2.6730914, 0.3572267, 4.988956]\n",
      "172: Train Loss: [2.3785298, 0.31605476, 4.4410048] | Test Loss: [2.6806178, 0.3444154, 5.0168204]\n",
      "173: Train Loss: [2.417913, 0.33006904, 4.505757] | Test Loss: [2.486327, 0.38735238, 4.5853014]\n",
      "174: Train Loss: [2.5379932, 0.41718084, 4.6588054] | Test Loss: [2.6184394, 0.3784752, 4.8584037]\n",
      "175: Train Loss: [2.5593498, 0.42359933, 4.6951003] | Test Loss: [2.719912, 0.37919018, 5.060634]\n",
      "176: Train Loss: [2.521236, 0.34667665, 4.695795] | Test Loss: [2.610247, 0.35356385, 4.86693]\n",
      "177: Train Loss: [2.5107253, 0.3608447, 4.660606] | Test Loss: [2.679577, 0.3683089, 4.990845]\n",
      "178: Train Loss: [2.4202788, 0.3729184, 4.467639] | Test Loss: [2.572977, 0.3491284, 4.796826]\n",
      "179: Train Loss: [2.6013138, 0.3681149, 4.8345127] | Test Loss: [2.6073534, 0.38600865, 4.828698]\n",
      "180: Train Loss: [2.508157, 0.5138872, 4.5024266] | Test Loss: [2.739714, 0.3787046, 5.1007233]\n",
      "181: Train Loss: [2.4627573, 0.34237596, 4.583139] | Test Loss: [2.4950395, 0.40741733, 4.5826616]\n",
      "182: Train Loss: [2.4482598, 0.31741658, 4.579103] | Test Loss: [2.6385913, 0.42128706, 4.8558955]\n",
      "183: Train Loss: [2.550111, 0.3333326, 4.7668896] | Test Loss: [2.3957884, 0.41549557, 4.3760815]\n",
      "184: Train Loss: [2.5310774, 0.3267242, 4.7354307] | Test Loss: [2.804824, 0.48358628, 5.126062]\n",
      "185: Train Loss: [2.44579, 0.40893885, 4.482641] | Test Loss: [3.015096, 0.37750858, 5.6526833]\n",
      "186: Train Loss: [2.6553054, 0.385665, 4.924946] | Test Loss: [2.6924057, 0.3584219, 5.0263896]\n",
      "187: Train Loss: [2.3808563, 0.3362647, 4.425448] | Test Loss: [2.740708, 0.36851215, 5.112904]\n",
      "188: Train Loss: [2.53555, 0.38359362, 4.6875067] | Test Loss: [2.5168176, 0.32133317, 4.712302]\n",
      "189: Train Loss: [2.5212712, 0.4171159, 4.625427] | Test Loss: [2.7520108, 0.47741413, 5.0266075]\n",
      "190: Train Loss: [2.4326324, 0.32444763, 4.5408173] | Test Loss: [2.5324612, 0.35215235, 4.71277]\n",
      "191: Train Loss: [2.5692143, 0.3427745, 4.7956543] | Test Loss: [2.5312862, 0.33248645, 4.730086]\n",
      "192: Train Loss: [2.675157, 0.37127703, 4.9790373] | Test Loss: [2.6504729, 0.37695137, 4.9239945]\n",
      "193: Train Loss: [2.3983445, 0.34668747, 4.4500017] | Test Loss: [2.6688805, 0.3712969, 4.966464]\n",
      "194: Train Loss: [2.552478, 0.36178416, 4.743172] | Test Loss: [2.725506, 0.4546574, 4.9963546]\n",
      "195: Train Loss: [2.6016004, 0.37522262, 4.827978] | Test Loss: [2.7934775, 0.37674725, 5.210208]\n",
      "196: Train Loss: [2.2527251, 0.3364327, 4.1690173] | Test Loss: [2.6592665, 0.3408202, 4.9777126]\n",
      "197: Train Loss: [2.4251108, 0.33548337, 4.514738] | Test Loss: [2.8321116, 0.32541224, 5.338811]\n",
      "198: Train Loss: [2.509047, 0.380638, 4.637456] | Test Loss: [2.766069, 0.34780172, 5.184336]\n",
      "199: Train Loss: [2.2911003, 0.46114722, 4.121053] | Test Loss: [2.5525393, 0.36604717, 4.7390313]\n",
      "200: Train Loss: [2.5689647, 0.3391897, 4.79874] | Test Loss: [2.5899482, 0.33769357, 4.8422027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201: Train Loss: [2.3363934, 0.33783263, 4.3349543] | Test Loss: [2.5244548, 0.4178112, 4.6310983]\n",
      "202: Train Loss: [2.557056, 0.35440758, 4.759704] | Test Loss: [2.6387699, 0.36927631, 4.908263]\n",
      "203: Train Loss: [2.5464988, 0.33258653, 4.760411] | Test Loss: [2.624786, 0.33227897, 4.9172926]\n",
      "204: Train Loss: [2.5018404, 0.3990775, 4.6046033] | Test Loss: [2.7729573, 0.45446733, 5.0914474]\n",
      "205: Train Loss: [2.4193618, 0.32866317, 4.5100603] | Test Loss: [2.6800666, 0.3419807, 5.018152]\n",
      "206: Train Loss: [2.544064, 0.33198935, 4.756139] | Test Loss: [2.535142, 0.37361488, 4.696669]\n",
      "207: Train Loss: [2.3720453, 0.33973822, 4.404352] | Test Loss: [2.5534143, 0.35057688, 4.756252]\n",
      "208: Train Loss: [2.6078644, 0.5476329, 4.668096] | Test Loss: [2.6983047, 0.42432693, 4.9722824]\n",
      "209: Train Loss: [2.546709, 0.31154662, 4.7818713] | Test Loss: [2.6841207, 0.33918047, 5.029061]\n",
      "210: Train Loss: [2.526188, 0.4155602, 4.6368155] | Test Loss: [2.6434596, 0.34074834, 4.946171]\n",
      "211: Train Loss: [2.4488518, 0.3422336, 4.55547] | Test Loss: [2.5971465, 0.33838806, 4.855905]\n",
      "212: Train Loss: [2.451881, 0.30307344, 4.6006885] | Test Loss: [2.6740239, 0.3915976, 4.95645]\n",
      "213: Train Loss: [2.5821936, 0.34234744, 4.8220396] | Test Loss: [2.7490206, 0.47892138, 5.0191197]\n",
      "214: Train Loss: [2.3613853, 0.45038465, 4.272386] | Test Loss: [2.5505126, 0.3556552, 4.74537]\n",
      "215: Train Loss: [2.4313004, 0.3338332, 4.5287676] | Test Loss: [2.3889341, 0.3701396, 4.4077287]\n",
      "216: Train Loss: [2.464645, 0.35398424, 4.5753055] | Test Loss: [2.6293385, 0.38671598, 4.871961]\n",
      "217: Train Loss: [2.3983033, 0.35629886, 4.4403076] | Test Loss: [2.902833, 0.3981281, 5.407538]\n",
      "218: Train Loss: [2.4816415, 0.37296975, 4.5903134] | Test Loss: [2.6263974, 0.3409103, 4.9118843]\n",
      "219: Train Loss: [2.469065, 0.32190263, 4.616227] | Test Loss: [2.6530168, 0.3549203, 4.951113]\n",
      "220: Train Loss: [2.5616913, 0.3025086, 4.820874] | Test Loss: [2.6832561, 0.4759134, 4.890599]\n",
      "221: Train Loss: [2.568185, 0.39597386, 4.7403965] | Test Loss: [2.7165987, 0.36792347, 5.0652742]\n",
      "222: Train Loss: [2.441641, 0.40088022, 4.482402] | Test Loss: [2.7636514, 0.4510346, 5.076268]\n",
      "223: Train Loss: [2.3350677, 0.32127506, 4.3488603] | Test Loss: [2.672292, 0.34422082, 5.0003633]\n",
      "224: Train Loss: [2.3780825, 0.3141852, 4.44198] | Test Loss: [2.6726427, 0.35325435, 4.992031]\n",
      "225: Train Loss: [2.4207191, 0.33658022, 4.504858] | Test Loss: [2.5281892, 0.41631764, 4.640061]\n",
      "226: Train Loss: [2.521622, 0.32123154, 4.7220125] | Test Loss: [2.7008457, 0.34727684, 5.0544147]\n",
      "227: Train Loss: [2.420577, 0.32262143, 4.5185328] | Test Loss: [2.3937936, 0.30671233, 4.480875]\n",
      "228: Train Loss: [2.2501757, 0.27139696, 4.2289543] | Test Loss: [2.7812479, 0.39464033, 5.1678553]\n",
      "229: Train Loss: [2.608371, 0.32302678, 4.8937154] | Test Loss: [2.635275, 0.44771528, 4.8228345]\n",
      "230: Train Loss: [2.4452684, 0.4716795, 4.418857] | Test Loss: [2.8969731, 0.42461503, 5.3693314]\n",
      "231: Train Loss: [2.3497088, 0.32998103, 4.3694367] | Test Loss: [2.6206846, 0.34353167, 4.8978376]\n",
      "232: Train Loss: [2.4576762, 0.32553098, 4.5898213] | Test Loss: [2.6188393, 0.3153304, 4.922348]\n",
      "233: Train Loss: [2.2292762, 0.3025148, 4.156038] | Test Loss: [2.490924, 0.3852653, 4.5965824]\n",
      "234: Train Loss: [2.413409, 0.32391384, 4.502904] | Test Loss: [2.5962381, 0.35261402, 4.8398623]\n",
      "235: Train Loss: [2.4282653, 0.33332857, 4.523202] | Test Loss: [2.8074887, 0.3649622, 5.2500153]\n",
      "236: Train Loss: [2.363954, 0.36984903, 4.358059] | Test Loss: [2.6167903, 0.3631784, 4.8704023]\n",
      "237: Train Loss: [2.4698334, 0.31939158, 4.620275] | Test Loss: [2.599876, 0.3377931, 4.8619585]\n",
      "238: Train Loss: [2.4007614, 0.35121492, 4.450308] | Test Loss: [2.628704, 0.37794057, 4.8794675]\n",
      "239: Train Loss: [2.3643873, 0.35560125, 4.373173] | Test Loss: [2.4902039, 0.29652873, 4.683879]\n",
      "240: Train Loss: [2.2885513, 0.3108016, 4.266301] | Test Loss: [2.5901778, 0.40085763, 4.779498]\n",
      "241: Train Loss: [2.4318757, 0.34264633, 4.5211053] | Test Loss: [2.6181548, 0.3619818, 4.8743277]\n",
      "242: Train Loss: [2.5085144, 0.46556827, 4.5514607] | Test Loss: [2.6104147, 0.385866, 4.8349633]\n",
      "243: Train Loss: [2.3986723, 0.34730446, 4.4500403] | Test Loss: [2.6357942, 0.35370234, 4.917886]\n",
      "244: Train Loss: [2.415261, 0.3559199, 4.474602] | Test Loss: [2.617655, 0.40035868, 4.8349514]\n",
      "245: Train Loss: [2.3864303, 0.3583201, 4.4145403] | Test Loss: [2.4934657, 0.32448545, 4.662446]\n",
      "246: Train Loss: [2.420877, 0.30691192, 4.534842] | Test Loss: [2.58678, 0.42789942, 4.745661]\n",
      "247: Train Loss: [2.2191489, 0.35811213, 4.0801854] | Test Loss: [2.5279815, 0.38680202, 4.669161]\n",
      "248: Train Loss: [2.288203, 0.34976548, 4.2266407] | Test Loss: [2.7327714, 0.3699803, 5.0955625]\n",
      "249: Train Loss: [2.4490263, 0.36107007, 4.5369825] | Test Loss: [2.5326185, 0.42726052, 4.6379766]\n",
      "250: Train Loss: [2.433098, 0.37544343, 4.4907527] | Test Loss: [2.7125897, 0.31294897, 5.1122303]\n",
      "251: Train Loss: [2.4716825, 0.41363832, 4.529727] | Test Loss: [2.7439165, 0.35994214, 5.127891]\n",
      "252: Train Loss: [2.367684, 0.36759105, 4.367777] | Test Loss: [2.709597, 0.34707916, 5.072115]\n",
      "253: Train Loss: [2.45645, 0.36991355, 4.5429864] | Test Loss: [2.8404562, 0.3474314, 5.3334813]\n",
      "254: Train Loss: [2.5022306, 0.36760354, 4.6368575] | Test Loss: [2.6767936, 0.40160576, 4.9519815]\n",
      "255: Train Loss: [2.3773525, 0.37887278, 4.375832] | Test Loss: [2.7744942, 0.41291964, 5.136069]\n",
      "256: Train Loss: [2.3773003, 0.31909108, 4.4355097] | Test Loss: [2.8699825, 0.42270538, 5.31726]\n",
      "257: Train Loss: [2.5034468, 0.3094875, 4.6974063] | Test Loss: [2.5878642, 0.33061618, 4.8451123]\n",
      "258: Train Loss: [2.5154977, 0.32561803, 4.705377] | Test Loss: [2.6741164, 0.37548536, 4.9727473]\n",
      "259: Train Loss: [2.4528136, 0.34407163, 4.561556] | Test Loss: [2.668327, 0.39654392, 4.94011]\n",
      "260: Train Loss: [2.6804388, 0.40715945, 4.953718] | Test Loss: [2.556026, 0.3545222, 4.7575297]\n",
      "261: Train Loss: [2.26267, 0.35462916, 4.170711] | Test Loss: [2.6554363, 0.42376482, 4.887108]\n",
      "262: Train Loss: [2.4912057, 0.32013738, 4.662274] | Test Loss: [2.7488391, 0.444399, 5.0532794]\n",
      "263: Train Loss: [2.6720896, 0.37634438, 4.967835] | Test Loss: [2.7527125, 0.36230797, 5.143117]\n",
      "264: Train Loss: [2.4895902, 0.29276696, 4.6864133] | Test Loss: [2.550398, 0.35690367, 4.7438927]\n",
      "265: Train Loss: [2.5269516, 0.33155242, 4.7223506] | Test Loss: [2.4792185, 0.3614838, 4.5969534]\n",
      "266: Train Loss: [2.4631164, 0.3418984, 4.5843344] | Test Loss: [2.591003, 0.3523829, 4.829623]\n",
      "267: Train Loss: [2.4826114, 0.2972646, 4.6679583] | Test Loss: [2.764721, 0.34184787, 5.187594]\n",
      "268: Train Loss: [2.4013155, 0.3817361, 4.4208946] | Test Loss: [2.5008874, 0.42293644, 4.5788383]\n",
      "269: Train Loss: [2.5096288, 0.34007412, 4.6791835] | Test Loss: [2.4635742, 0.30886012, 4.618288]\n",
      "270: Train Loss: [2.5214932, 0.4292467, 4.6137395] | Test Loss: [2.5719538, 0.37187266, 4.7720346]\n",
      "271: Train Loss: [2.4712806, 0.35395548, 4.588606] | Test Loss: [2.7573507, 0.31515118, 5.19955]\n",
      "272: Train Loss: [2.5666091, 0.3789716, 4.7542467] | Test Loss: [2.5193996, 0.40050244, 4.638297]\n",
      "273: Train Loss: [2.501662, 0.39368477, 4.609639] | Test Loss: [2.599109, 0.3933921, 4.804826]\n",
      "274: Train Loss: [2.3564005, 0.41931105, 4.29349] | Test Loss: [2.6492894, 0.32814848, 4.9704304]\n",
      "275: Train Loss: [2.551174, 0.46895027, 4.6333976] | Test Loss: [2.4553115, 0.3737292, 4.536894]\n",
      "276: Train Loss: [2.505005, 0.34608588, 4.6639237] | Test Loss: [2.6828315, 0.37027052, 4.9953923]\n",
      "277: Train Loss: [2.4644704, 0.3783776, 4.5505633] | Test Loss: [2.7866738, 0.37531716, 5.1980305]\n",
      "278: Train Loss: [2.42071, 0.3496481, 4.491772] | Test Loss: [2.8214347, 0.3546743, 5.288195]\n",
      "279: Train Loss: [2.4626846, 0.3874789, 4.5378904] | Test Loss: [2.7954454, 0.40509585, 5.185795]\n",
      "280: Train Loss: [2.4229264, 0.34855202, 4.4973006] | Test Loss: [2.6579378, 0.3730303, 4.9428453]\n",
      "281: Train Loss: [2.5771291, 0.35341305, 4.800845] | Test Loss: [2.5844717, 0.3808205, 4.7881227]\n",
      "282: Train Loss: [2.4481933, 0.36238325, 4.5340033] | Test Loss: [2.896031, 0.3593222, 5.4327397]\n",
      "283: Train Loss: [2.4506009, 0.40510842, 4.4960933] | Test Loss: [2.6293228, 0.40193975, 4.8567057]\n",
      "284: Train Loss: [2.7296836, 0.3993104, 5.0600567] | Test Loss: [2.6994505, 0.34855348, 5.0503473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285: Train Loss: [2.5993552, 0.3921184, 4.806592] | Test Loss: [2.6536527, 0.4155753, 4.89173]\n",
      "286: Train Loss: [2.4571836, 0.33258215, 4.581785] | Test Loss: [2.6407855, 0.3382484, 4.9433227]\n",
      "287: Train Loss: [2.5034535, 0.33596402, 4.670943] | Test Loss: [2.6572034, 0.32714665, 4.9872603]\n",
      "288: Train Loss: [2.6123207, 0.39160016, 4.833041] | Test Loss: [2.7227893, 0.43602106, 5.0095577]\n",
      "289: Train Loss: [2.3974593, 0.33602047, 4.458898] | Test Loss: [2.6384203, 0.35785702, 4.9189835]\n",
      "290: Train Loss: [2.3049107, 0.34728342, 4.262538] | Test Loss: [2.527018, 0.37086236, 4.6831737]\n",
      "291: Train Loss: [2.441228, 0.32403594, 4.5584197] | Test Loss: [2.4262824, 0.38685682, 4.465708]\n",
      "292: Train Loss: [2.3870993, 0.41132915, 4.3628693] | Test Loss: [2.5515509, 0.39793897, 4.705163]\n",
      "293: Train Loss: [2.5671854, 0.4284267, 4.705944] | Test Loss: [2.660907, 0.40614706, 4.915667]\n",
      "294: Train Loss: [2.2378626, 0.34345996, 4.132265] | Test Loss: [2.577704, 0.36678702, 4.788621]\n",
      "295: Train Loss: [2.3423603, 0.31468043, 4.37004] | Test Loss: [2.6820066, 0.49892682, 4.8650866]\n",
      "296: Train Loss: [2.3399262, 0.35064536, 4.329207] | Test Loss: [2.6311896, 0.39087057, 4.8715086]\n",
      "297: Train Loss: [2.43243, 0.36709416, 4.497766] | Test Loss: [2.5770617, 0.30697864, 4.8471446]\n",
      "298: Train Loss: [2.5147839, 0.39313677, 4.6364307] | Test Loss: [2.651387, 0.35319263, 4.949581]\n",
      "299: Train Loss: [2.3886113, 0.34527263, 4.43195] | Test Loss: [2.7433338, 0.3930813, 5.0935864]\n",
      "300: Train Loss: [2.412462, 0.36727327, 4.4576507] | Test Loss: [2.571584, 0.32892144, 4.8142467]\n",
      "301: Train Loss: [2.3528078, 0.3387398, 4.3668756] | Test Loss: [2.6439724, 0.3510572, 4.9368877]\n",
      "302: Train Loss: [2.3360493, 0.328603, 4.343496] | Test Loss: [2.4791124, 0.39640233, 4.5618224]\n",
      "303: Train Loss: [2.3273375, 0.40609118, 4.248584] | Test Loss: [2.6544662, 0.29648784, 5.0124445]\n",
      "304: Train Loss: [2.3812177, 0.30272382, 4.4597116] | Test Loss: [2.5045173, 0.37378424, 4.6352506]\n",
      "305: Train Loss: [2.3215585, 0.3141188, 4.328998] | Test Loss: [2.3661194, 0.2990338, 4.433205]\n",
      "306: Train Loss: [2.5956805, 0.3196655, 4.8716955] | Test Loss: [2.7481632, 0.39840293, 5.0979233]\n",
      "307: Train Loss: [2.4552178, 0.33436862, 4.576067] | Test Loss: [2.8149848, 0.38988534, 5.240084]\n",
      "308: Train Loss: [2.506545, 0.3839521, 4.629138] | Test Loss: [2.7845006, 0.43719986, 5.131801]\n",
      "309: Train Loss: [2.43636, 0.31915316, 4.5535665] | Test Loss: [2.5519767, 0.35849375, 4.7454596]\n",
      "310: Train Loss: [2.6432571, 0.36738548, 4.919129] | Test Loss: [2.8159478, 0.4057957, 5.2261]\n",
      "311: Train Loss: [2.296025, 0.33396575, 4.2580843] | Test Loss: [2.576753, 0.35791102, 4.7955947]\n",
      "312: Train Loss: [2.424751, 0.39719552, 4.4523067] | Test Loss: [2.5045674, 0.33038217, 4.6787524]\n",
      "313: Train Loss: [2.4784753, 0.3097607, 4.64719] | Test Loss: [2.6279285, 0.40834495, 4.8475122]\n",
      "314: Train Loss: [2.428434, 0.37682787, 4.48004] | Test Loss: [2.4239206, 0.4055664, 4.442275]\n",
      "315: Train Loss: [2.4668596, 0.35918823, 4.574531] | Test Loss: [2.6822383, 0.3367863, 5.0276904]\n",
      "316: Train Loss: [2.5851924, 0.4268087, 4.743576] | Test Loss: [2.7732139, 0.42644647, 5.1199813]\n",
      "317: Train Loss: [2.3781288, 0.36639073, 4.389867] | Test Loss: [2.471888, 0.3213486, 4.6224275]\n",
      "318: Train Loss: [2.345639, 0.3105962, 4.380682] | Test Loss: [2.6393747, 0.39786166, 4.880888]\n",
      "319: Train Loss: [2.4640977, 0.35055768, 4.5776377] | Test Loss: [2.651106, 0.38254052, 4.9196715]\n",
      "320: Train Loss: [2.590273, 0.41154456, 4.769001] | Test Loss: [2.593198, 0.36747026, 4.818926]\n",
      "321: Train Loss: [2.386637, 0.42309284, 4.350181] | Test Loss: [2.5046146, 0.33162504, 4.677604]\n",
      "322: Train Loss: [2.6095858, 0.5322311, 4.6869407] | Test Loss: [2.817502, 0.3595576, 5.2754464]\n",
      "323: Train Loss: [2.4788404, 0.3418305, 4.6158504] | Test Loss: [2.6047397, 0.35363677, 4.8558426]\n",
      "324: Train Loss: [2.4708924, 0.35834593, 4.583439] | Test Loss: [2.6483045, 0.34817696, 4.948432]\n",
      "325: Train Loss: [2.3580685, 0.36536813, 4.350769] | Test Loss: [2.6235871, 0.3956409, 4.8515334]\n",
      "326: Train Loss: [2.447801, 0.3247571, 4.570845] | Test Loss: [2.6492388, 0.36986777, 4.92861]\n",
      "327: Train Loss: [2.3917265, 0.44302082, 4.340432] | Test Loss: [2.7311378, 0.40593505, 5.0563407]\n",
      "328: Train Loss: [2.2924771, 0.38168725, 4.203267] | Test Loss: [2.3708715, 0.32817718, 4.413566]\n",
      "329: Train Loss: [2.5665054, 0.36976066, 4.7632504] | Test Loss: [2.4445865, 0.37346232, 4.515711]\n",
      "330: Train Loss: [2.61391, 0.40131384, 4.826506] | Test Loss: [2.7475107, 0.39122707, 5.103794]\n",
      "331: Train Loss: [2.4535844, 0.37653735, 4.5306315] | Test Loss: [2.7964096, 0.36654717, 5.226272]\n",
      "332: Train Loss: [2.4543877, 0.4240956, 4.4846797] | Test Loss: [2.452468, 0.3348802, 4.5700555]\n",
      "333: Train Loss: [2.4472325, 0.33322838, 4.5612364] | Test Loss: [2.5842164, 0.4168891, 4.7515435]\n",
      "334: Train Loss: [2.5192988, 0.32887524, 4.7097225] | Test Loss: [2.6389034, 0.33760306, 4.9402037]\n",
      "335: Train Loss: [2.5889902, 0.37143862, 4.806542] | Test Loss: [2.8221714, 0.44049108, 5.2038517]\n",
      "336: Train Loss: [2.3745418, 0.35160512, 4.3974786] | Test Loss: [2.5479527, 0.33183923, 4.764066]\n",
      "337: Train Loss: [2.5395918, 0.33950877, 4.739675] | Test Loss: [2.5548177, 0.36202455, 4.747611]\n",
      "338: Train Loss: [2.5537908, 0.33808443, 4.7694974] | Test Loss: [2.5651515, 0.36540636, 4.7648964]\n",
      "339: Train Loss: [2.4890044, 0.37242362, 4.605585] | Test Loss: [2.4908395, 0.41230243, 4.5693765]\n",
      "340: Train Loss: [2.4122717, 0.39613923, 4.4284043] | Test Loss: [2.6298723, 0.436478, 4.8232665]\n",
      "341: Train Loss: [2.3631318, 0.47373274, 4.2525306] | Test Loss: [2.7294784, 0.42689395, 5.032063]\n",
      "342: Train Loss: [2.6198714, 0.35168487, 4.8880577] | Test Loss: [2.6767378, 0.35717952, 4.996296]\n",
      "343: Train Loss: [2.368496, 0.3406919, 4.3963] | Test Loss: [2.6233547, 0.3798871, 4.8668222]\n",
      "344: Train Loss: [2.5362248, 0.37371427, 4.698735] | Test Loss: [2.62714, 0.3674133, 4.8868666]\n",
      "345: Train Loss: [2.4122374, 0.34048116, 4.4839935] | Test Loss: [2.5289092, 0.37530497, 4.682513]\n",
      "346: Train Loss: [2.4092662, 0.32432938, 4.494203] | Test Loss: [2.6531427, 0.3902499, 4.9160357]\n",
      "347: Train Loss: [2.4212513, 0.2952794, 4.547223] | Test Loss: [2.589868, 0.35625857, 4.8234777]\n",
      "348: Train Loss: [2.5203426, 0.40524563, 4.6354394] | Test Loss: [2.6536372, 0.33587238, 4.971402]\n",
      "349: Train Loss: [2.311918, 0.403829, 4.220007] | Test Loss: [2.5806365, 0.33228934, 4.828984]\n",
      "350: Train Loss: [2.5423737, 0.33634865, 4.748399] | Test Loss: [2.5271318, 0.37470102, 4.6795626]\n",
      "351: Train Loss: [2.4741836, 0.35862786, 4.5897393] | Test Loss: [2.686339, 0.3408512, 5.0318265]\n",
      "352: Train Loss: [2.6130002, 0.381468, 4.8445325] | Test Loss: [2.552385, 0.37301534, 4.731755]\n",
      "353: Train Loss: [2.4903796, 0.4480184, 4.5327406] | Test Loss: [2.8405168, 0.42035973, 5.260674]\n",
      "354: Train Loss: [2.3735888, 0.33178648, 4.415391] | Test Loss: [2.7833848, 0.33006328, 5.2367063]\n",
      "355: Train Loss: [2.3599916, 0.36438286, 4.3556004] | Test Loss: [2.7006881, 0.35819674, 5.0431795]\n",
      "356: Train Loss: [2.4764414, 0.35269257, 4.60019] | Test Loss: [2.583276, 0.37761724, 4.7889347]\n",
      "357: Train Loss: [2.4551647, 0.34596694, 4.5643625] | Test Loss: [2.4798372, 0.32990918, 4.629765]\n",
      "358: Train Loss: [2.3828635, 0.34927198, 4.4164553] | Test Loss: [2.720761, 0.3914833, 5.050039]\n",
      "359: Train Loss: [2.504621, 0.29868382, 4.7105584] | Test Loss: [2.7521515, 0.45049727, 5.053806]\n",
      "360: Train Loss: [2.5908732, 0.47156072, 4.7101855] | Test Loss: [2.4836285, 0.40780228, 4.559455]\n",
      "361: Train Loss: [2.545026, 0.37880713, 4.711245] | Test Loss: [2.8614626, 0.3575628, 5.365362]\n",
      "362: Train Loss: [2.5127826, 0.35269552, 4.6728697] | Test Loss: [2.6278102, 0.40660447, 4.849016]\n",
      "363: Train Loss: [2.290708, 0.34780216, 4.233614] | Test Loss: [2.550323, 0.34841016, 4.752236]\n",
      "364: Train Loss: [2.418075, 0.37529942, 4.4608507] | Test Loss: [2.605884, 0.3817447, 4.8300233]\n",
      "365: Train Loss: [2.3575199, 0.36514327, 4.3498964] | Test Loss: [2.7065725, 0.41899472, 4.99415]\n",
      "366: Train Loss: [2.5743814, 0.3535192, 4.7952437] | Test Loss: [2.5128176, 0.3850549, 4.64058]\n",
      "367: Train Loss: [2.5994325, 0.37073043, 4.8281345] | Test Loss: [2.5543182, 0.3484936, 4.760143]\n",
      "368: Train Loss: [2.2570872, 0.3664183, 4.147756] | Test Loss: [2.5613391, 0.35396278, 4.7687154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369: Train Loss: [2.464766, 0.3329265, 4.5966053] | Test Loss: [2.5389452, 0.41821927, 4.6596713]\n",
      "370: Train Loss: [2.4325945, 0.33771044, 4.5274787] | Test Loss: [2.641883, 0.34214342, 4.9416223]\n",
      "371: Train Loss: [2.4338267, 0.34469837, 4.522955] | Test Loss: [2.531352, 0.3809744, 4.68173]\n",
      "372: Train Loss: [2.4922318, 0.3834498, 4.601014] | Test Loss: [2.6918218, 0.32901698, 5.0546265]\n",
      "373: Train Loss: [2.3408027, 0.3338104, 4.347795] | Test Loss: [2.5905752, 0.34863344, 4.832517]\n",
      "374: Train Loss: [2.2421796, 0.3286127, 4.1557465] | Test Loss: [2.4675803, 0.33138266, 4.603778]\n",
      "375: Train Loss: [2.5997283, 0.4146728, 4.784784] | Test Loss: [2.5214088, 0.38669837, 4.6561193]\n",
      "376: Train Loss: [2.4625444, 0.3391876, 4.5859013] | Test Loss: [2.6229572, 0.4015519, 4.8443627]\n",
      "377: Train Loss: [2.452329, 0.38655034, 4.5181074] | Test Loss: [2.7388718, 0.36654267, 5.111201]\n",
      "378: Train Loss: [2.581746, 0.35406834, 4.809424] | Test Loss: [2.496106, 0.3954528, 4.596759]\n",
      "379: Train Loss: [2.3110936, 0.37045228, 4.2517347] | Test Loss: [2.449626, 0.37812564, 4.5211263]\n",
      "380: Train Loss: [2.4625382, 0.3049699, 4.6201067] | Test Loss: [2.6444564, 0.33997715, 4.9489355]\n",
      "381: Train Loss: [2.3859954, 0.3057778, 4.466213] | Test Loss: [2.556393, 0.33361694, 4.779169]\n",
      "382: Train Loss: [2.250815, 0.31774223, 4.1838875] | Test Loss: [2.712276, 0.40372258, 5.020829]\n",
      "383: Train Loss: [2.2690518, 0.3247084, 4.213395] | Test Loss: [2.7776365, 0.34525317, 5.21002]\n",
      "384: Train Loss: [2.623095, 0.3992051, 4.846985] | Test Loss: [2.5292401, 0.35871387, 4.699766]\n",
      "385: Train Loss: [2.3961987, 0.32673413, 4.4656634] | Test Loss: [2.9146361, 0.4759751, 5.353297]\n",
      "386: Train Loss: [2.3086262, 0.35686815, 4.260384] | Test Loss: [2.7425659, 0.39140126, 5.0937304]\n",
      "387: Train Loss: [2.4782364, 0.40147728, 4.5549955] | Test Loss: [2.5865629, 0.4365311, 4.7365947]\n",
      "388: Train Loss: [2.406299, 0.34694648, 4.4656515] | Test Loss: [2.7317574, 0.31849572, 5.145019]\n",
      "389: Train Loss: [2.3941493, 0.32675126, 4.4615474] | Test Loss: [2.614802, 0.361795, 4.867809]\n",
      "390: Train Loss: [2.44229, 0.3330581, 4.5515223] | Test Loss: [2.5961733, 0.39598447, 4.796362]\n",
      "391: Train Loss: [2.6678329, 0.32727695, 5.0083885] | Test Loss: [2.8023822, 0.4099392, 5.194825]\n",
      "392: Train Loss: [2.5417666, 0.4090834, 4.67445] | Test Loss: [2.5384722, 0.3384561, 4.738488]\n",
      "393: Train Loss: [2.348697, 0.33850324, 4.3588905] | Test Loss: [2.6749268, 0.35714862, 4.992705]\n",
      "394: Train Loss: [2.5038714, 0.30753067, 4.700212] | Test Loss: [2.5958838, 0.36147553, 4.830292]\n",
      "395: Train Loss: [2.5206776, 0.35090792, 4.6904473] | Test Loss: [3.4003935, 0.4253498, 6.3754373]\n",
      "396: Train Loss: [2.3405077, 0.36933506, 4.3116803] | Test Loss: [2.4119775, 0.418292, 4.405663]\n",
      "397: Train Loss: [2.2955947, 0.28007644, 4.311113] | Test Loss: [2.529637, 0.3956683, 4.6636057]\n",
      "398: Train Loss: [2.4367461, 0.35225794, 4.5212345] | Test Loss: [2.7084498, 0.36206168, 5.054838]\n",
      "399: Train Loss: [2.4254236, 0.3724571, 4.47839] | Test Loss: [2.7437978, 0.522941, 4.9646544]\n",
      "400: Train Loss: [2.2900164, 0.34121624, 4.2388167] | Test Loss: [2.4720223, 0.38074586, 4.5632987]\n",
      "401: Train Loss: [2.2970529, 0.3424933, 4.251612] | Test Loss: [2.4627404, 0.3925331, 4.5329475]\n",
      "402: Train Loss: [2.3909235, 0.48629785, 4.2955494] | Test Loss: [2.7790556, 0.3805346, 5.1775765]\n",
      "403: Train Loss: [2.3359292, 0.3330378, 4.3388205] | Test Loss: [2.78348, 0.3415762, 5.2253838]\n",
      "404: Train Loss: [2.4321432, 0.33835357, 4.525933] | Test Loss: [2.735877, 0.35953796, 5.112216]\n",
      "405: Train Loss: [2.3970604, 0.3820598, 4.4120607] | Test Loss: [2.7582436, 0.44264686, 5.07384]\n",
      "406: Train Loss: [2.4753332, 0.3568903, 4.593776] | Test Loss: [2.5871415, 0.32305074, 4.8512325]\n",
      "407: Train Loss: [2.433184, 0.40352386, 4.462844] | Test Loss: [2.446517, 0.31081602, 4.582218]\n",
      "408: Train Loss: [2.517225, 0.3647775, 4.6696725] | Test Loss: [2.5128615, 0.4011394, 4.6245837]\n",
      "409: Train Loss: [2.3887434, 0.35411763, 4.4233694] | Test Loss: [2.6729922, 0.3515168, 4.9944677]\n",
      "410: Train Loss: [2.4675972, 0.3667919, 4.568403] | Test Loss: [2.5396624, 0.38057923, 4.6987453]\n",
      "411: Train Loss: [2.4279172, 0.39143676, 4.464398] | Test Loss: [2.5397282, 0.3846634, 4.6947927]\n",
      "412: Train Loss: [2.479085, 0.39478394, 4.563386] | Test Loss: [2.7140477, 0.31033498, 5.11776]\n",
      "413: Train Loss: [2.5898993, 0.33788025, 4.8419185] | Test Loss: [2.6415224, 0.3635385, 4.9195065]\n",
      "414: Train Loss: [2.4924555, 0.33064964, 4.654261] | Test Loss: [2.6433725, 0.38604566, 4.9006996]\n",
      "415: Train Loss: [2.5553255, 0.4367061, 4.673945] | Test Loss: [2.6347835, 0.3281413, 4.941426]\n",
      "416: Train Loss: [2.4617205, 0.39085495, 4.532586] | Test Loss: [2.5419786, 0.4074046, 4.676553]\n",
      "417: Train Loss: [2.459798, 0.44616932, 4.473427] | Test Loss: [2.5889297, 0.3625048, 4.8153543]\n",
      "418: Train Loss: [2.4729686, 0.34214848, 4.603789] | Test Loss: [2.7193346, 0.31372216, 5.124947]\n",
      "419: Train Loss: [2.3372927, 0.38575065, 4.2888346] | Test Loss: [2.5845137, 0.42154714, 4.7474804]\n",
      "420: Train Loss: [2.3947866, 0.3929806, 4.3965926] | Test Loss: [2.4934115, 0.3201601, 4.666663]\n",
      "421: Train Loss: [2.4609568, 0.32323977, 4.598674] | Test Loss: [2.6802125, 0.35304713, 5.0073776]\n",
      "422: Train Loss: [2.5616636, 0.39534974, 4.7279778] | Test Loss: [2.619924, 0.347633, 4.8922153]\n",
      "423: Train Loss: [2.4930735, 0.42616475, 4.5599823] | Test Loss: [2.597952, 0.35275015, 4.8431535]\n",
      "424: Train Loss: [2.2896786, 0.3868968, 4.1924605] | Test Loss: [2.7399192, 0.37989336, 5.099945]\n",
      "425: Train Loss: [2.5062041, 0.35188684, 4.6605215] | Test Loss: [2.4828913, 0.47631672, 4.4894657]\n",
      "426: Train Loss: [2.4501715, 0.3454325, 4.5549107] | Test Loss: [2.550752, 0.31289473, 4.788609]\n",
      "427: Train Loss: [2.4134054, 0.42093492, 4.405876] | Test Loss: [2.5550964, 0.3504243, 4.7597685]\n",
      "428: Train Loss: [2.348943, 0.31002417, 4.3878617] | Test Loss: [2.491832, 0.34565055, 4.6380134]\n",
      "429: Train Loss: [2.3702526, 0.33813548, 4.4023695] | Test Loss: [2.5086172, 0.35870865, 4.6585255]\n",
      "430: Train Loss: [2.4832482, 0.36800793, 4.5984883] | Test Loss: [2.5575626, 0.34116653, 4.7739587]\n",
      "431: Train Loss: [2.4316015, 0.31801155, 4.5451913] | Test Loss: [2.6680741, 0.40258747, 4.933561]\n",
      "432: Train Loss: [2.539589, 0.4958008, 4.583377] | Test Loss: [2.716193, 0.42675787, 5.005628]\n",
      "433: Train Loss: [2.4918332, 0.30232573, 4.6813407] | Test Loss: [2.6168892, 0.4401728, 4.793606]\n",
      "434: Train Loss: [2.5040114, 0.36025658, 4.647766] | Test Loss: [2.7390916, 0.38796517, 5.090218]\n",
      "435: Train Loss: [2.4999008, 0.3792182, 4.6205835] | Test Loss: [2.5859108, 0.34030235, 4.831519]\n",
      "436: Train Loss: [2.38927, 0.34465942, 4.433881] | Test Loss: [2.4979959, 0.31553477, 4.680457]\n",
      "437: Train Loss: [2.6413946, 0.53574914, 4.7470403] | Test Loss: [2.5409918, 0.3422134, 4.7397704]\n",
      "438: Train Loss: [2.5511775, 0.37528095, 4.727074] | Test Loss: [2.66024, 0.3411561, 4.979324]\n",
      "439: Train Loss: [2.2931213, 0.3133813, 4.2728615] | Test Loss: [2.7388334, 0.41519728, 5.0624695]\n",
      "440: Train Loss: [2.359302, 0.34239888, 4.3762054] | Test Loss: [2.6502304, 0.38497567, 4.9154854]\n",
      "441: Train Loss: [2.611029, 0.35442427, 4.8676333] | Test Loss: [2.6375072, 0.36651358, 4.9085007]\n",
      "442: Train Loss: [2.6025689, 0.3512499, 4.853888] | Test Loss: [2.6335323, 0.34377322, 4.923291]\n",
      "443: Train Loss: [2.5355253, 0.332347, 4.7387037] | Test Loss: [2.6307251, 0.39739874, 4.8640513]\n",
      "444: Train Loss: [2.4932945, 0.35425624, 4.632333] | Test Loss: [2.572958, 0.3859077, 4.7600083]\n",
      "445: Train Loss: [2.350295, 0.4813166, 4.2192736] | Test Loss: [2.7376544, 0.29075226, 5.1845565]\n",
      "446: Train Loss: [2.4078922, 0.35085657, 4.4649277] | Test Loss: [2.438796, 0.36958855, 4.5080037]\n",
      "447: Train Loss: [2.5424187, 0.28231126, 4.802526] | Test Loss: [2.6434102, 0.40423477, 4.8825855]\n",
      "448: Train Loss: [2.524488, 0.33995503, 4.709021] | Test Loss: [2.7223363, 0.3522839, 5.0923886]\n",
      "449: Train Loss: [2.4904819, 0.34130186, 4.639662] | Test Loss: [2.704528, 0.4375969, 4.9714594]\n",
      "450: Train Loss: [2.5928938, 0.29045817, 4.8953295] | Test Loss: [2.6645496, 0.34244165, 4.9866576]\n",
      "451: Train Loss: [2.4456258, 0.3297781, 4.5614734] | Test Loss: [2.6409693, 0.42194915, 4.8599896]\n",
      "452: Train Loss: [2.494528, 0.34782287, 4.6412334] | Test Loss: [2.6572094, 0.4632061, 4.8512125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453: Train Loss: [2.6999702, 0.5170858, 4.882855] | Test Loss: [2.7030146, 0.40203017, 5.003999]\n",
      "454: Train Loss: [2.5616624, 0.32604304, 4.7972817] | Test Loss: [2.6301692, 0.36055675, 4.8997817]\n",
      "455: Train Loss: [2.443814, 0.3171472, 4.570481] | Test Loss: [2.634831, 0.3265027, 4.943159]\n",
      "456: Train Loss: [2.41256, 0.34791017, 4.47721] | Test Loss: [2.635078, 0.33134767, 4.9388084]\n",
      "457: Train Loss: [2.4522822, 0.34753016, 4.557034] | Test Loss: [2.61006, 0.38667196, 4.833448]\n",
      "458: Train Loss: [2.5395365, 0.31851092, 4.760562] | Test Loss: [2.5032918, 0.32226294, 4.684321]\n",
      "459: Train Loss: [2.4604928, 0.39296687, 4.528019] | Test Loss: [2.5244024, 0.36691913, 4.6818857]\n",
      "460: Train Loss: [2.360559, 0.26155862, 4.4595594] | Test Loss: [2.6972275, 0.39516845, 4.9992867]\n",
      "461: Train Loss: [2.5437553, 0.3522539, 4.7352567] | Test Loss: [2.662061, 0.33514386, 4.988978]\n",
      "462: Train Loss: [2.4729178, 0.2792012, 4.6666346] | Test Loss: [2.6146014, 0.47758374, 4.751619]\n",
      "463: Train Loss: [2.5989554, 0.3337732, 4.8641376] | Test Loss: [2.6262314, 0.34043464, 4.9120283]\n",
      "464: Train Loss: [2.5037656, 0.3477886, 4.6597424] | Test Loss: [2.6567404, 0.38180754, 4.9316735]\n",
      "465: Train Loss: [2.4109547, 0.3368565, 4.485053] | Test Loss: [2.8878925, 0.46353784, 5.3122473]\n",
      "466: Train Loss: [2.3618078, 0.30762577, 4.41599] | Test Loss: [2.6413271, 0.35341388, 4.92924]\n",
      "467: Train Loss: [2.550511, 0.38891333, 4.7121086] | Test Loss: [2.7081892, 0.37738803, 5.0389905]\n",
      "468: Train Loss: [2.4653423, 0.34539783, 4.5852866] | Test Loss: [2.7607574, 0.36480027, 5.1567144]\n",
      "469: Train Loss: [2.2232275, 0.34139764, 4.1050572] | Test Loss: [2.5844548, 0.4091846, 4.759725]\n",
      "470: Train Loss: [2.3244643, 0.31807017, 4.330858] | Test Loss: [2.5335548, 0.2874212, 4.7796884]\n",
      "471: Train Loss: [2.3807538, 0.45007446, 4.311433] | Test Loss: [2.7318273, 0.31594172, 5.1477127]\n",
      "472: Train Loss: [2.5983233, 0.35628077, 4.840366] | Test Loss: [2.5483797, 0.3274375, 4.769322]\n",
      "473: Train Loss: [2.3015764, 0.2991613, 4.3039913] | Test Loss: [2.6250877, 0.35207996, 4.8980956]\n",
      "474: Train Loss: [2.4417443, 0.36351654, 4.5199723] | Test Loss: [2.542694, 0.31162748, 4.773761]\n",
      "475: Train Loss: [2.6155157, 0.42337024, 4.807661] | Test Loss: [2.6399891, 0.42247018, 4.857508]\n",
      "476: Train Loss: [2.4939823, 0.4144399, 4.573525] | Test Loss: [2.6396015, 0.4207005, 4.8585024]\n",
      "477: Train Loss: [2.4105036, 0.3137495, 4.507258] | Test Loss: [2.59395, 0.33440977, 4.8534904]\n",
      "478: Train Loss: [2.3107116, 0.35399172, 4.2674317] | Test Loss: [2.493358, 0.331481, 4.655235]\n",
      "479: Train Loss: [2.461651, 0.3634873, 4.559815] | Test Loss: [2.5521834, 0.3627883, 4.7415786]\n",
      "480: Train Loss: [2.4088938, 0.36468378, 4.453104] | Test Loss: [2.8030243, 0.4039142, 5.202134]\n",
      "481: Train Loss: [2.3657064, 0.3398435, 4.3915696] | Test Loss: [2.6121774, 0.33837816, 4.885977]\n",
      "482: Train Loss: [2.5336673, 0.35095948, 4.7163754] | Test Loss: [2.5339198, 0.35083148, 4.717008]\n",
      "483: Train Loss: [2.6127346, 0.37073183, 4.8547373] | Test Loss: [2.4247413, 0.5119386, 4.337544]\n",
      "484: Train Loss: [2.5199845, 0.37412784, 4.665841] | Test Loss: [2.707843, 0.41899422, 4.9966917]\n",
      "485: Train Loss: [2.410946, 0.3488097, 4.473082] | Test Loss: [2.4300833, 0.35457113, 4.505595]\n",
      "486: Train Loss: [2.4331532, 0.34205315, 4.5242534] | Test Loss: [2.6318607, 0.3748049, 4.8889165]\n",
      "487: Train Loss: [2.4835556, 0.29394248, 4.6731687] | Test Loss: [2.5993767, 0.39458212, 4.804171]\n",
      "488: Train Loss: [2.459865, 0.27774933, 4.6419806] | Test Loss: [2.555949, 0.40060487, 4.711293]\n",
      "489: Train Loss: [2.3250117, 0.30351546, 4.346508] | Test Loss: [2.5349634, 0.4160881, 4.6538386]\n",
      "490: Train Loss: [2.4659705, 0.36197025, 4.5699706] | Test Loss: [2.7338138, 0.4110067, 5.0566206]\n",
      "491: Train Loss: [2.4207869, 0.37039828, 4.471175] | Test Loss: [2.6525054, 0.37450972, 4.930501]\n",
      "492: Train Loss: [2.5531912, 0.3492096, 4.7571726] | Test Loss: [2.6289349, 0.3205279, 4.9373417]\n",
      "493: Train Loss: [2.4967632, 0.37373996, 4.6197863] | Test Loss: [2.7978685, 0.37881264, 5.216924]\n",
      "494: Train Loss: [2.2824001, 0.3177003, 4.2471] | Test Loss: [2.6303988, 0.37090248, 4.889895]\n",
      "495: Train Loss: [2.4692478, 0.39561412, 4.5428815] | Test Loss: [2.6675901, 0.3084743, 5.026706]\n",
      "496: Train Loss: [2.5491176, 0.37591255, 4.7223225] | Test Loss: [2.5524719, 0.34704262, 4.757901]\n",
      "497: Train Loss: [2.3251808, 0.33965835, 4.3107033] | Test Loss: [2.5565796, 0.35383326, 4.759326]\n",
      "498: Train Loss: [2.4615965, 0.32522938, 4.597964] | Test Loss: [2.489114, 0.41217768, 4.5660505]\n",
      "499: Train Loss: [2.4418106, 0.37525, 4.5083714] | Test Loss: [2.5547082, 0.3494418, 4.7599745]\n",
      "500: Train Loss: [2.3946972, 0.36205143, 4.427343] | Test Loss: [2.582665, 0.34618655, 4.8191433]\n",
      "501: Train Loss: [2.5464303, 0.3279737, 4.764887] | Test Loss: [2.4714754, 0.3268685, 4.616082]\n",
      "502: Train Loss: [2.4985337, 0.3940265, 4.603041] | Test Loss: [2.756659, 0.37517262, 5.1381454]\n",
      "503: Train Loss: [2.3560765, 0.36596808, 4.3461847] | Test Loss: [2.6977036, 0.35420647, 5.0412006]\n",
      "504: Train Loss: [2.3974955, 0.41297343, 4.3820176] | Test Loss: [2.7373695, 0.40832973, 5.066409]\n",
      "505: Train Loss: [2.411019, 0.37256247, 4.449476] | Test Loss: [2.7274048, 0.34360135, 5.1112084]\n",
      "506: Train Loss: [2.289927, 0.3991632, 4.180691] | Test Loss: [2.637281, 0.38487822, 4.8896837]\n",
      "507: Train Loss: [2.4999769, 0.4045444, 4.5954094] | Test Loss: [2.9273734, 0.5072642, 5.3474827]\n",
      "508: Train Loss: [2.6237316, 0.3538159, 4.893647] | Test Loss: [2.5592184, 0.38780683, 4.73063]\n",
      "509: Train Loss: [2.4411433, 0.33702818, 4.5452585] | Test Loss: [2.526735, 0.35711426, 4.696356]\n",
      "510: Train Loss: [2.3626413, 0.3464496, 4.3788333] | Test Loss: [2.5370264, 0.3451126, 4.72894]\n",
      "511: Train Loss: [2.4408464, 0.34896252, 4.5327306] | Test Loss: [2.5704584, 0.46562767, 4.675289]\n",
      "512: Train Loss: [2.3890674, 0.34216166, 4.435973] | Test Loss: [2.709042, 0.3405295, 5.0775547]\n",
      "513: Train Loss: [2.3546443, 0.3398824, 4.369406] | Test Loss: [2.6444223, 0.43706912, 4.8517756]\n",
      "514: Train Loss: [2.4100626, 0.3546917, 4.4654336] | Test Loss: [2.5743644, 0.3035268, 4.845202]\n",
      "515: Train Loss: [2.3681352, 0.3821059, 4.3541646] | Test Loss: [2.2848918, 0.37422693, 4.1955566]\n",
      "516: Train Loss: [2.501529, 0.34760395, 4.655454] | Test Loss: [2.6955774, 0.328369, 5.0627856]\n",
      "517: Train Loss: [2.4182453, 0.3994882, 4.4370027] | Test Loss: [2.6658797, 0.35656, 4.975199]\n",
      "518: Train Loss: [2.3398743, 0.32571778, 4.3540306] | Test Loss: [2.4921322, 0.3447155, 4.639549]\n",
      "519: Train Loss: [2.5610044, 0.35257298, 4.769436] | Test Loss: [2.4745595, 0.46548992, 4.483629]\n",
      "520: Train Loss: [2.4911492, 0.35892376, 4.6233745] | Test Loss: [2.6825588, 0.32996976, 5.0351477]\n",
      "521: Train Loss: [2.1560466, 0.37428868, 3.9378047] | Test Loss: [2.5747957, 0.39804575, 4.751546]\n",
      "522: Train Loss: [2.4862418, 0.34099245, 4.631491] | Test Loss: [2.5873523, 0.340696, 4.8340087]\n",
      "523: Train Loss: [2.4280188, 0.28485474, 4.5711827] | Test Loss: [2.5952747, 0.37398353, 4.816566]\n",
      "Epoch 14\n",
      "0: Train Loss: [2.328767, 0.3357039, 4.3218303] | Test Loss: [2.6466799, 0.3600746, 4.933285]\n",
      "1: Train Loss: [2.3465989, 0.362023, 4.331175] | Test Loss: [2.6728976, 0.5026676, 4.8431277]\n",
      "2: Train Loss: [2.3867364, 0.35252044, 4.4209523] | Test Loss: [2.8513863, 0.5022559, 5.2005167]\n",
      "3: Train Loss: [2.362525, 0.3647045, 4.3603454] | Test Loss: [2.520068, 0.35471323, 4.6854224]\n",
      "4: Train Loss: [2.4729142, 0.39600778, 4.549821] | Test Loss: [2.6933465, 0.32832414, 5.0583687]\n",
      "5: Train Loss: [2.1266654, 0.33671066, 3.9166203] | Test Loss: [2.6124935, 0.35532483, 4.8696623]\n",
      "6: Train Loss: [2.2520776, 0.29566064, 4.2084947] | Test Loss: [2.623742, 0.3334152, 4.914069]\n",
      "7: Train Loss: [2.27718, 0.30815417, 4.246206] | Test Loss: [2.6265817, 0.36360765, 4.889556]\n",
      "8: Train Loss: [2.1986537, 0.35734862, 4.039959] | Test Loss: [2.4873314, 0.3858925, 4.5887704]\n",
      "9: Train Loss: [2.3957648, 0.34898213, 4.4425473] | Test Loss: [2.4558043, 0.39581418, 4.5157948]\n",
      "10: Train Loss: [2.3428342, 0.34306344, 4.342605] | Test Loss: [2.6587203, 0.30450055, 5.01294]\n",
      "11: Train Loss: [2.2102346, 0.3487351, 4.071734] | Test Loss: [2.7876287, 0.493262, 5.0819955]\n",
      "12: Train Loss: [2.2981124, 0.4264032, 4.1698217] | Test Loss: [2.595864, 0.3731934, 4.818535]\n",
      "13: Train Loss: [2.252279, 0.33922136, 4.1653366] | Test Loss: [2.6437044, 0.37310126, 4.9143076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14: Train Loss: [2.1862695, 0.33176246, 4.0407767] | Test Loss: [2.6176617, 0.36444956, 4.870874]\n",
      "15: Train Loss: [2.2152627, 0.33558565, 4.0949397] | Test Loss: [2.5126932, 0.354046, 4.6713405]\n",
      "16: Train Loss: [2.2790134, 0.35342726, 4.2045994] | Test Loss: [2.6585963, 0.34290484, 4.9742875]\n",
      "17: Train Loss: [2.4272444, 0.50133073, 4.353158] | Test Loss: [2.7429826, 0.39947262, 5.0864925]\n",
      "18: Train Loss: [2.3163376, 0.36387658, 4.268799] | Test Loss: [2.4687345, 0.37592477, 4.5615444]\n",
      "19: Train Loss: [2.4411058, 0.3303118, 4.5519] | Test Loss: [2.6240783, 0.34920537, 4.898951]\n",
      "20: Train Loss: [2.1429925, 0.3777501, 3.9082348] | Test Loss: [2.5640454, 0.33971828, 4.7883725]\n",
      "21: Train Loss: [2.263133, 0.33779827, 4.188468] | Test Loss: [2.6791906, 0.31893915, 5.039442]\n",
      "22: Train Loss: [2.284628, 0.30001953, 4.269236] | Test Loss: [2.610433, 0.3542315, 4.866635]\n",
      "23: Train Loss: [2.2336535, 0.2951899, 4.172117] | Test Loss: [2.7219517, 0.37726185, 5.066642]\n",
      "24: Train Loss: [2.2976878, 0.34384403, 4.2515316] | Test Loss: [2.7563035, 0.40320274, 5.1094046]\n",
      "25: Train Loss: [2.3480163, 0.3716115, 4.324421] | Test Loss: [2.694831, 0.4289599, 4.960702]\n",
      "26: Train Loss: [2.312094, 0.34461585, 4.279572] | Test Loss: [2.724279, 0.43255228, 5.0160055]\n",
      "27: Train Loss: [2.3877237, 0.407618, 4.3678293] | Test Loss: [2.5558379, 0.35025814, 4.7614174]\n",
      "28: Train Loss: [2.121183, 0.36132526, 3.8810403] | Test Loss: [2.684391, 0.32816842, 5.0406137]\n",
      "29: Train Loss: [2.2135868, 0.4380763, 3.989097] | Test Loss: [2.5462096, 0.374097, 4.7183223]\n",
      "30: Train Loss: [2.3059254, 0.47705308, 4.1347976] | Test Loss: [2.5879264, 0.38055673, 4.795296]\n",
      "31: Train Loss: [2.4191053, 0.3876148, 4.450596] | Test Loss: [2.621696, 0.40301707, 4.840375]\n",
      "32: Train Loss: [2.1916113, 0.4143851, 3.9688373] | Test Loss: [2.67159, 0.41117138, 4.9320087]\n",
      "33: Train Loss: [2.4040523, 0.37577298, 4.4323316] | Test Loss: [2.493591, 0.41988808, 4.567294]\n",
      "34: Train Loss: [2.202179, 0.3451391, 4.059219] | Test Loss: [2.644473, 0.3926939, 4.896252]\n",
      "35: Train Loss: [2.207433, 0.3959104, 4.0189557] | Test Loss: [2.7531095, 0.40676892, 5.09945]\n",
      "36: Train Loss: [2.3532195, 0.36608708, 4.340352] | Test Loss: [2.584993, 0.38431796, 4.785668]\n",
      "37: Train Loss: [2.251195, 0.3501811, 4.152209] | Test Loss: [2.6098228, 0.3406297, 4.879016]\n",
      "38: Train Loss: [2.2917697, 0.30047473, 4.283065] | Test Loss: [2.6164935, 0.3193912, 4.9135957]\n",
      "39: Train Loss: [2.555178, 0.60190016, 4.5084558] | Test Loss: [2.642731, 0.33506957, 4.9503922]\n",
      "40: Train Loss: [2.4082072, 0.34272918, 4.4736853] | Test Loss: [2.727134, 0.3741774, 5.0800905]\n",
      "41: Train Loss: [2.3286412, 0.3394219, 4.3178606] | Test Loss: [2.70544, 0.4393416, 4.9715385]\n",
      "42: Train Loss: [2.2148275, 0.3598157, 4.0698395] | Test Loss: [2.3848286, 0.3415095, 4.428148]\n",
      "43: Train Loss: [2.3627915, 0.36977172, 4.355811] | Test Loss: [2.8477838, 0.40561193, 5.2899556]\n",
      "44: Train Loss: [2.2990263, 0.36609563, 4.231957] | Test Loss: [2.6166904, 0.380743, 4.852638]\n",
      "45: Train Loss: [2.2909465, 0.37757662, 4.204316] | Test Loss: [2.5486991, 0.38497856, 4.7124195]\n",
      "46: Train Loss: [2.338254, 0.37563917, 4.300869] | Test Loss: [2.4574835, 0.4062805, 4.5086865]\n",
      "47: Train Loss: [2.169493, 0.339369, 3.9996169] | Test Loss: [2.5827253, 0.35318205, 4.8122687]\n",
      "48: Train Loss: [2.4529605, 0.38959482, 4.516326] | Test Loss: [2.629195, 0.34377226, 4.9146175]\n",
      "49: Train Loss: [2.2666981, 0.29093045, 4.242466] | Test Loss: [2.4875078, 0.38021272, 4.594803]\n",
      "50: Train Loss: [2.2999551, 0.39434767, 4.2055626] | Test Loss: [2.501546, 0.33081663, 4.672275]\n",
      "51: Train Loss: [2.2601717, 0.37077138, 4.149572] | Test Loss: [2.8722782, 0.50673366, 5.237823]\n",
      "52: Train Loss: [2.2715263, 0.31311035, 4.2299423] | Test Loss: [2.5775404, 0.3529708, 4.80211]\n",
      "53: Train Loss: [2.4129014, 0.32867292, 4.49713] | Test Loss: [2.7993875, 0.4600246, 5.13875]\n",
      "54: Train Loss: [2.2304106, 0.37236267, 4.0884585] | Test Loss: [2.5390978, 0.35587475, 4.722321]\n",
      "55: Train Loss: [2.1593451, 0.33089328, 3.987797] | Test Loss: [2.7053635, 0.32514155, 5.0855856]\n",
      "56: Train Loss: [2.4895263, 0.34973136, 4.629321] | Test Loss: [2.60789, 0.34723082, 4.868549]\n",
      "57: Train Loss: [2.286468, 0.36414826, 4.208788] | Test Loss: [2.5182269, 0.3500719, 4.686382]\n",
      "58: Train Loss: [2.3482819, 0.3632797, 4.333284] | Test Loss: [2.698886, 0.43726233, 4.9605093]\n",
      "59: Train Loss: [2.255732, 0.40517962, 4.1062846] | Test Loss: [2.607789, 0.32389623, 4.8916817]\n",
      "60: Train Loss: [2.2495773, 0.36692715, 4.1322274] | Test Loss: [2.6711068, 0.3522896, 4.989924]\n",
      "61: Train Loss: [2.2893004, 0.38677973, 4.191821] | Test Loss: [2.6006527, 0.34428266, 4.857023]\n",
      "62: Train Loss: [2.2715833, 0.34447268, 4.1986938] | Test Loss: [2.6471426, 0.36620584, 4.9280796]\n",
      "63: Train Loss: [2.4238715, 0.32100403, 4.526739] | Test Loss: [2.6517243, 0.3920322, 4.9114165]\n",
      "64: Train Loss: [2.3430195, 0.40761057, 4.2784286] | Test Loss: [2.7107635, 0.32546845, 5.0960584]\n",
      "65: Train Loss: [2.2330613, 0.4371146, 4.029008] | Test Loss: [2.6473422, 0.37173656, 4.922948]\n",
      "66: Train Loss: [2.4034307, 0.35841358, 4.4484477] | Test Loss: [2.5006323, 0.3629656, 4.638299]\n",
      "67: Train Loss: [2.2149317, 0.28323683, 4.1466265] | Test Loss: [2.5483887, 0.4155837, 4.681194]\n",
      "68: Train Loss: [2.4699142, 0.6173219, 4.3225064] | Test Loss: [2.479409, 0.42361498, 4.535203]\n",
      "69: Train Loss: [2.2313507, 0.41105163, 4.0516496] | Test Loss: [2.5617797, 0.34577528, 4.7777843]\n",
      "70: Train Loss: [2.4043314, 0.38472226, 4.4239407] | Test Loss: [2.7945414, 0.41545674, 5.173626]\n",
      "71: Train Loss: [2.2708695, 0.33739567, 4.2043433] | Test Loss: [2.664089, 0.3750604, 4.9531174]\n",
      "72: Train Loss: [2.2075984, 0.38302884, 4.032168] | Test Loss: [2.677975, 0.3699048, 4.986045]\n",
      "73: Train Loss: [2.3980978, 0.33191213, 4.4642835] | Test Loss: [2.6615913, 0.32956517, 4.9936175]\n",
      "74: Train Loss: [2.3142335, 0.34815472, 4.2803125] | Test Loss: [2.5438411, 0.4133171, 4.674365]\n",
      "75: Train Loss: [2.4319794, 0.36883268, 4.4951262] | Test Loss: [2.5203004, 0.4406024, 4.5999985]\n",
      "76: Train Loss: [2.2566242, 0.3666478, 4.1466007] | Test Loss: [2.51107, 0.33732966, 4.68481]\n",
      "77: Train Loss: [2.4087636, 0.39703968, 4.4204874] | Test Loss: [2.7572384, 0.37087938, 5.1435976]\n",
      "78: Train Loss: [2.2198708, 0.38561222, 4.0541296] | Test Loss: [2.5733035, 0.32502374, 4.8215833]\n",
      "79: Train Loss: [2.4186678, 0.34095222, 4.496383] | Test Loss: [2.6886744, 0.37436715, 5.0029817]\n",
      "80: Train Loss: [2.2915628, 0.33510393, 4.2480216] | Test Loss: [2.56783, 0.39648443, 4.739176]\n",
      "81: Train Loss: [2.3210819, 0.3602311, 4.281933] | Test Loss: [2.8568707, 0.31680351, 5.396938]\n",
      "82: Train Loss: [2.2792714, 0.34378254, 4.2147603] | Test Loss: [2.5400736, 0.35285684, 4.7272906]\n",
      "83: Train Loss: [2.1994298, 0.34113738, 4.057722] | Test Loss: [2.6320703, 0.34292305, 4.9212174]\n",
      "84: Train Loss: [2.4135408, 0.33082938, 4.4962525] | Test Loss: [2.3591363, 0.3049952, 4.4132776]\n",
      "85: Train Loss: [2.4230638, 0.39916104, 4.4469666] | Test Loss: [2.7511082, 0.38328445, 5.118932]\n",
      "86: Train Loss: [2.2070525, 0.29670224, 4.1174026] | Test Loss: [2.6724114, 0.41721356, 4.9276094]\n",
      "87: Train Loss: [2.3677967, 0.34337834, 4.392215] | Test Loss: [2.710755, 0.4557174, 4.9657927]\n",
      "88: Train Loss: [2.4581475, 0.37082472, 4.54547] | Test Loss: [2.5850544, 0.37472773, 4.795381]\n",
      "89: Train Loss: [2.2311082, 0.298957, 4.1632595] | Test Loss: [2.7199326, 0.41638303, 5.0234823]\n",
      "90: Train Loss: [2.211975, 0.31818703, 4.105763] | Test Loss: [2.544437, 0.36963478, 4.719239]\n",
      "91: Train Loss: [2.300406, 0.36787817, 4.232934] | Test Loss: [2.7157226, 0.33889174, 5.0925536]\n",
      "92: Train Loss: [2.4413521, 0.3409452, 4.541759] | Test Loss: [2.592365, 0.3767311, 4.807999]\n",
      "93: Train Loss: [2.3843982, 0.42969486, 4.339102] | Test Loss: [2.6650903, 0.34703308, 4.9831476]\n",
      "94: Train Loss: [2.3786612, 0.46827796, 4.2890444] | Test Loss: [2.5079563, 0.42944682, 4.586466]\n",
      "95: Train Loss: [2.274896, 0.35758266, 4.1922092] | Test Loss: [2.7824688, 0.48836982, 5.0765676]\n",
      "96: Train Loss: [2.4033542, 0.33970332, 4.467005] | Test Loss: [2.67494, 0.44655207, 4.903328]\n",
      "97: Train Loss: [2.4146612, 0.33819023, 4.4911323] | Test Loss: [2.6728601, 0.37607428, 4.969646]\n",
      "98: Train Loss: [2.1630254, 0.34544435, 3.9806063] | Test Loss: [2.574572, 0.33242142, 4.816723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: Train Loss: [2.3418357, 0.33428493, 4.3493867] | Test Loss: [2.5576038, 0.33725345, 4.777954]\n",
      "100: Train Loss: [2.3583539, 0.35598776, 4.36072] | Test Loss: [2.635937, 0.38282013, 4.889054]\n",
      "101: Train Loss: [2.3788805, 0.4355142, 4.322247] | Test Loss: [2.6231515, 0.34885782, 4.897445]\n",
      "102: Train Loss: [2.5713353, 0.51189387, 4.630777] | Test Loss: [2.6648269, 0.35918742, 4.970466]\n",
      "103: Train Loss: [2.4289436, 0.2934275, 4.56446] | Test Loss: [2.789036, 0.38078374, 5.1972885]\n",
      "104: Train Loss: [2.2949586, 0.3449572, 4.24496] | Test Loss: [2.784192, 0.40445173, 5.1639323]\n",
      "105: Train Loss: [2.435079, 0.38478303, 4.4853754] | Test Loss: [2.4693832, 0.36218503, 4.5765815]\n",
      "106: Train Loss: [2.3971245, 0.36892116, 4.425328] | Test Loss: [2.6688561, 0.3996307, 4.9380817]\n",
      "107: Train Loss: [2.4156022, 0.35818562, 4.4730186] | Test Loss: [2.6308284, 0.34465992, 4.916997]\n",
      "108: Train Loss: [2.3662703, 0.3685626, 4.363978] | Test Loss: [2.7277908, 0.33409697, 5.1214848]\n",
      "109: Train Loss: [2.4202387, 0.354514, 4.4859633] | Test Loss: [2.4522831, 0.29052904, 4.614037]\n",
      "110: Train Loss: [2.4017727, 0.40397292, 4.3995724] | Test Loss: [2.7390413, 0.41673928, 5.061343]\n",
      "111: Train Loss: [2.37567, 0.35234094, 4.3989987] | Test Loss: [2.7383103, 0.30527687, 5.171344]\n",
      "112: Train Loss: [2.314739, 0.34811798, 4.28136] | Test Loss: [2.6203525, 0.37830958, 4.8623953]\n",
      "113: Train Loss: [2.2369227, 0.32217878, 4.1516666] | Test Loss: [2.5489552, 0.40722063, 4.6906896]\n",
      "114: Train Loss: [2.41118, 0.38947898, 4.432881] | Test Loss: [2.653296, 0.30326718, 5.003325]\n",
      "115: Train Loss: [2.36119, 0.32653493, 4.3958454] | Test Loss: [2.534223, 0.47651863, 4.5919275]\n",
      "116: Train Loss: [2.222911, 0.4124365, 4.0333853] | Test Loss: [2.4345286, 0.3374046, 4.5316525]\n",
      "117: Train Loss: [2.2684255, 0.33821902, 4.198632] | Test Loss: [2.6892896, 0.3418895, 5.0366898]\n",
      "118: Train Loss: [2.2444785, 0.41344985, 4.075507] | Test Loss: [2.6997912, 0.31945792, 5.0801244]\n",
      "119: Train Loss: [2.2633617, 0.37643164, 4.150292] | Test Loss: [2.5656035, 0.39685175, 4.7343554]\n",
      "120: Train Loss: [2.2610989, 0.32056144, 4.2016363] | Test Loss: [2.4326024, 0.39764288, 4.4675617]\n",
      "121: Train Loss: [2.4363773, 0.3478883, 4.524866] | Test Loss: [2.629199, 0.3442719, 4.914126]\n",
      "122: Train Loss: [2.199142, 0.3660326, 4.0322514] | Test Loss: [2.5566695, 0.4023872, 4.710952]\n",
      "123: Train Loss: [2.4249177, 0.37622553, 4.47361] | Test Loss: [2.7639706, 0.37379017, 5.154151]\n",
      "124: Train Loss: [2.3353467, 0.32415238, 4.346541] | Test Loss: [2.843805, 0.37271288, 5.314897]\n",
      "125: Train Loss: [2.545035, 0.34099987, 4.7490697] | Test Loss: [2.52205, 0.3713968, 4.672703]\n",
      "126: Train Loss: [2.4701447, 0.4414049, 4.4988847] | Test Loss: [2.6009047, 0.45935825, 4.742451]\n",
      "127: Train Loss: [2.284845, 0.39090335, 4.1787868] | Test Loss: [2.5231752, 0.3201978, 4.726153]\n",
      "128: Train Loss: [2.3130171, 0.31558743, 4.3104467] | Test Loss: [2.592139, 0.39535093, 4.788927]\n",
      "129: Train Loss: [2.4072788, 0.3559586, 4.458599] | Test Loss: [2.7379878, 0.33265218, 5.1433234]\n",
      "130: Train Loss: [2.382632, 0.36953923, 4.395725] | Test Loss: [2.5680938, 0.33150887, 4.804679]\n",
      "131: Train Loss: [2.3272827, 0.30976936, 4.344796] | Test Loss: [2.6601777, 0.32668692, 4.9936686]\n",
      "132: Train Loss: [2.3463693, 0.36468577, 4.3280525] | Test Loss: [2.670564, 0.4077977, 4.93333]\n",
      "133: Train Loss: [2.3683534, 0.36637872, 4.370328] | Test Loss: [2.8209028, 0.44093615, 5.2008696]\n",
      "134: Train Loss: [2.3808663, 0.3265204, 4.435212] | Test Loss: [2.6454778, 0.37207904, 4.9188766]\n",
      "135: Train Loss: [2.3963256, 0.35465774, 4.4379935] | Test Loss: [2.5947595, 0.37000847, 4.8195105]\n",
      "136: Train Loss: [2.4657288, 0.39838356, 4.533074] | Test Loss: [2.6950808, 0.3644586, 5.025703]\n",
      "137: Train Loss: [2.5043063, 0.30968094, 4.6989317] | Test Loss: [2.7539895, 0.43765756, 5.0703216]\n",
      "138: Train Loss: [2.3895206, 0.39212775, 4.3869133] | Test Loss: [2.5590875, 0.38301238, 4.7351627]\n",
      "139: Train Loss: [2.4389498, 0.3849879, 4.492912] | Test Loss: [2.5746708, 0.3768636, 4.772478]\n",
      "140: Train Loss: [2.306333, 0.33810169, 4.2745643] | Test Loss: [2.5680356, 0.34848848, 4.787583]\n",
      "141: Train Loss: [2.3697426, 0.33784768, 4.4016376] | Test Loss: [2.9132957, 0.34200466, 5.4845867]\n",
      "142: Train Loss: [2.170274, 0.3443748, 3.9961731] | Test Loss: [2.7938385, 0.3451143, 5.242563]\n",
      "143: Train Loss: [2.4730277, 0.34750533, 4.5985503] | Test Loss: [2.5137565, 0.3794652, 4.648048]\n",
      "144: Train Loss: [2.3875062, 0.3658986, 4.409114] | Test Loss: [2.6316621, 0.34259832, 4.920726]\n",
      "145: Train Loss: [2.3419254, 0.3238644, 4.3599863] | Test Loss: [2.7035542, 0.33317995, 5.0739284]\n",
      "146: Train Loss: [2.3670115, 0.3569455, 4.3770776] | Test Loss: [2.800057, 0.3748448, 5.225269]\n",
      "147: Train Loss: [2.2833073, 0.31595394, 4.250661] | Test Loss: [2.5220423, 0.3339302, 4.7101545]\n",
      "148: Train Loss: [2.2697935, 0.26911396, 4.270473] | Test Loss: [2.6327844, 0.3624618, 4.903107]\n",
      "149: Train Loss: [2.3878045, 0.3333559, 4.442253] | Test Loss: [2.5947862, 0.35293558, 4.8366365]\n",
      "150: Train Loss: [2.525513, 0.3728587, 4.6781673] | Test Loss: [2.655763, 0.32372248, 4.9878035]\n",
      "151: Train Loss: [2.3929074, 0.48685497, 4.2989597] | Test Loss: [2.658959, 0.47928423, 4.8386335]\n",
      "152: Train Loss: [2.4370475, 0.38035533, 4.4937396] | Test Loss: [2.5313625, 0.36543033, 4.6972947]\n",
      "153: Train Loss: [2.5069785, 0.34329888, 4.670658] | Test Loss: [2.5516496, 0.41798785, 4.6853113]\n",
      "154: Train Loss: [2.3030746, 0.39170107, 4.214448] | Test Loss: [2.5995364, 0.3916312, 4.8074417]\n",
      "155: Train Loss: [2.3444605, 0.3920935, 4.2968273] | Test Loss: [2.672899, 0.3408147, 5.0049834]\n",
      "156: Train Loss: [2.3411717, 0.350826, 4.3315177] | Test Loss: [2.5875154, 0.39323503, 4.7817955]\n",
      "157: Train Loss: [2.2831202, 0.3035011, 4.262739] | Test Loss: [2.608378, 0.39943898, 4.817317]\n",
      "158: Train Loss: [2.5564923, 0.3144156, 4.798569] | Test Loss: [2.6183064, 0.386682, 4.849931]\n",
      "159: Train Loss: [2.3614895, 0.36299986, 4.359979] | Test Loss: [2.7360303, 0.4806677, 4.991393]\n",
      "160: Train Loss: [2.3477318, 0.2780787, 4.417385] | Test Loss: [2.6829584, 0.38010034, 4.9858165]\n",
      "161: Train Loss: [2.4148731, 0.40511155, 4.424635] | Test Loss: [2.6218443, 0.38979703, 4.8538914]\n",
      "162: Train Loss: [2.3894196, 0.2910861, 4.487753] | Test Loss: [2.777904, 0.43394426, 5.121864]\n",
      "163: Train Loss: [2.2776184, 0.3101621, 4.2450747] | Test Loss: [2.6557913, 0.4463513, 4.8652315]\n",
      "164: Train Loss: [2.3104098, 0.48950478, 4.1313148] | Test Loss: [2.6500618, 0.34743947, 4.9526844]\n",
      "165: Train Loss: [2.3816283, 0.34454113, 4.4187155] | Test Loss: [2.5939817, 0.39339823, 4.794565]\n",
      "166: Train Loss: [2.4260402, 0.31683263, 4.535248] | Test Loss: [2.4919066, 0.34737864, 4.6364346]\n",
      "167: Train Loss: [2.4225922, 0.3744012, 4.470783] | Test Loss: [2.4937541, 0.35386848, 4.63364]\n",
      "168: Train Loss: [2.2855284, 0.3742316, 4.196825] | Test Loss: [2.6881292, 0.35144055, 5.024818]\n",
      "169: Train Loss: [2.3930488, 0.35156065, 4.434537] | Test Loss: [2.6203902, 0.36076203, 4.880018]\n",
      "170: Train Loss: [2.420169, 0.35044158, 4.489897] | Test Loss: [2.7454102, 0.34395376, 5.146867]\n",
      "171: Train Loss: [2.3231957, 0.3082566, 4.338135] | Test Loss: [2.9802322, 0.33672166, 5.6237426]\n",
      "172: Train Loss: [2.2858825, 0.32753766, 4.2442274] | Test Loss: [2.6112034, 0.39305237, 4.8293543]\n",
      "173: Train Loss: [2.3811715, 0.28562564, 4.4767175] | Test Loss: [2.6407924, 0.3762551, 4.9053297]\n",
      "174: Train Loss: [2.3638747, 0.32103142, 4.406718] | Test Loss: [2.6214833, 0.34808704, 4.89488]\n",
      "175: Train Loss: [2.3643374, 0.389344, 4.3393307] | Test Loss: [2.668779, 0.4030129, 4.934545]\n",
      "176: Train Loss: [2.411101, 0.37656137, 4.445641] | Test Loss: [2.7461255, 0.3775869, 5.114664]\n",
      "177: Train Loss: [2.26118, 0.30243477, 4.219925] | Test Loss: [2.6465628, 0.3753887, 4.917737]\n",
      "178: Train Loss: [2.4735742, 0.41366377, 4.5334845] | Test Loss: [2.6137898, 0.3518399, 4.8757396]\n",
      "179: Train Loss: [2.4097676, 0.35745367, 4.4620814] | Test Loss: [2.66649, 0.33581692, 4.9971633]\n",
      "180: Train Loss: [2.3370757, 0.4054602, 4.268691] | Test Loss: [2.7477355, 0.38382512, 5.1116457]\n",
      "181: Train Loss: [2.4623952, 0.36013985, 4.5646505] | Test Loss: [2.7959492, 0.3483845, 5.243514]\n",
      "182: Train Loss: [2.4316292, 0.39071763, 4.472541] | Test Loss: [2.6743026, 0.36868715, 4.979918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183: Train Loss: [2.5030005, 0.38223228, 4.623769] | Test Loss: [2.5736008, 0.3407757, 4.806426]\n",
      "184: Train Loss: [2.3515801, 0.3294384, 4.373722] | Test Loss: [2.6505618, 0.48515493, 4.8159685]\n",
      "185: Train Loss: [2.263567, 0.451883, 4.075251] | Test Loss: [2.638068, 0.34766877, 4.9284673]\n",
      "186: Train Loss: [2.4085126, 0.38491443, 4.432111] | Test Loss: [2.695526, 0.35996792, 5.031084]\n",
      "187: Train Loss: [2.4247842, 0.3414652, 4.5081034] | Test Loss: [2.561512, 0.41863433, 4.7043896]\n",
      "188: Train Loss: [2.337257, 0.33045864, 4.344055] | Test Loss: [2.736663, 0.34522265, 5.1281037]\n",
      "189: Train Loss: [2.3057334, 0.36144415, 4.250023] | Test Loss: [2.5143738, 0.33506233, 4.693685]\n",
      "190: Train Loss: [2.238849, 0.36216086, 4.115537] | Test Loss: [2.5022957, 0.36377785, 4.640814]\n",
      "191: Train Loss: [2.3456979, 0.36109486, 4.330301] | Test Loss: [2.770408, 0.3602544, 5.1805615]\n",
      "192: Train Loss: [2.4040437, 0.39742053, 4.410667] | Test Loss: [2.6454039, 0.32967928, 4.961128]\n",
      "193: Train Loss: [2.4135098, 0.35491908, 4.4721007] | Test Loss: [2.5323603, 0.42735073, 4.63737]\n",
      "194: Train Loss: [2.3165374, 0.3326159, 4.300459] | Test Loss: [2.4553652, 0.36111745, 4.549613]\n",
      "195: Train Loss: [2.4486678, 0.37759626, 4.519739] | Test Loss: [2.7382138, 0.45332962, 5.023098]\n",
      "196: Train Loss: [2.3547347, 0.34174585, 4.3677235] | Test Loss: [2.731121, 0.45126605, 5.010976]\n",
      "197: Train Loss: [2.4613178, 0.37531316, 4.5473223] | Test Loss: [2.5512507, 0.28917763, 4.813324]\n",
      "198: Train Loss: [2.3414152, 0.34935507, 4.333475] | Test Loss: [2.5446515, 0.41727152, 4.6720314]\n",
      "199: Train Loss: [2.3070295, 0.3138593, 4.3001995] | Test Loss: [2.634906, 0.38182494, 4.887987]\n",
      "200: Train Loss: [2.3741562, 0.40207443, 4.346238] | Test Loss: [2.600152, 0.37502295, 4.825281]\n",
      "201: Train Loss: [2.5217376, 0.30699748, 4.736478] | Test Loss: [2.5717654, 0.35861957, 4.784911]\n",
      "202: Train Loss: [2.4803092, 0.40976706, 4.5508513] | Test Loss: [2.7226896, 0.40515208, 5.040227]\n",
      "203: Train Loss: [2.3492389, 0.3573393, 4.3411384] | Test Loss: [2.5282052, 0.29406005, 4.76235]\n",
      "204: Train Loss: [2.3869379, 0.36419266, 4.409683] | Test Loss: [2.7964828, 0.47989842, 5.113067]\n",
      "205: Train Loss: [2.2474897, 0.34638757, 4.148592] | Test Loss: [2.6582727, 0.3644279, 4.9521174]\n",
      "206: Train Loss: [2.3308733, 0.36637336, 4.295373] | Test Loss: [2.6172576, 0.3428634, 4.8916516]\n",
      "207: Train Loss: [2.4429665, 0.32930475, 4.556628] | Test Loss: [2.3508475, 0.3666844, 4.3350105]\n",
      "208: Train Loss: [2.294673, 0.33219093, 4.257155] | Test Loss: [2.7113228, 0.31019223, 5.1124535]\n",
      "209: Train Loss: [2.6214004, 0.5180982, 4.7247024] | Test Loss: [2.5992482, 0.33817214, 4.8603244]\n",
      "210: Train Loss: [2.298148, 0.3537746, 4.2425213] | Test Loss: [2.6415467, 0.3282128, 4.9548807]\n",
      "211: Train Loss: [2.462639, 0.37130064, 4.5539775] | Test Loss: [2.672273, 0.35155824, 4.9929876]\n",
      "212: Train Loss: [2.5165493, 0.31205666, 4.721042] | Test Loss: [2.693486, 0.3642715, 5.0227003]\n",
      "213: Train Loss: [2.1583056, 0.3797053, 3.936906] | Test Loss: [2.5144267, 0.3581151, 4.670738]\n",
      "214: Train Loss: [2.2771683, 0.32733664, 4.2269998] | Test Loss: [2.8275483, 0.37764004, 5.2774563]\n",
      "215: Train Loss: [2.4636245, 0.36548144, 4.5617676] | Test Loss: [2.6465833, 0.38571605, 4.9074507]\n",
      "216: Train Loss: [2.3889508, 0.33203977, 4.445862] | Test Loss: [2.6352823, 0.3737564, 4.896808]\n",
      "217: Train Loss: [2.419975, 0.407225, 4.432725] | Test Loss: [2.5220766, 0.3812363, 4.6629167]\n",
      "218: Train Loss: [2.4972355, 0.33090213, 4.663569] | Test Loss: [2.5677743, 0.35851866, 4.77703]\n",
      "219: Train Loss: [2.4763825, 0.34432888, 4.608436] | Test Loss: [2.9033644, 0.39111307, 5.4156156]\n",
      "220: Train Loss: [2.2705796, 0.426086, 4.115073] | Test Loss: [2.5296814, 0.4404567, 4.618906]\n",
      "221: Train Loss: [2.365019, 0.31865993, 4.4113784] | Test Loss: [2.5811229, 0.34913075, 4.813115]\n",
      "222: Train Loss: [2.5082822, 0.3251049, 4.6914597] | Test Loss: [2.5424984, 0.39810875, 4.6868877]\n",
      "223: Train Loss: [2.2885046, 0.3333055, 4.243704] | Test Loss: [2.5638196, 0.4105281, 4.717111]\n",
      "224: Train Loss: [2.3911457, 0.35391814, 4.4283733] | Test Loss: [2.6197577, 0.37257698, 4.866938]\n",
      "225: Train Loss: [2.3955328, 0.3831743, 4.4078913] | Test Loss: [2.496633, 0.32008547, 4.6731806]\n",
      "226: Train Loss: [2.410325, 0.33930755, 4.4813423] | Test Loss: [2.6407244, 0.3490481, 4.9324007]\n",
      "227: Train Loss: [2.5346217, 0.38266787, 4.6865754] | Test Loss: [2.6545978, 0.39874482, 4.910451]\n",
      "228: Train Loss: [2.2935457, 0.33507296, 4.2520185] | Test Loss: [2.681696, 0.36317122, 5.000221]\n",
      "229: Train Loss: [2.4467652, 0.5358666, 4.3576636] | Test Loss: [2.7301707, 0.3947727, 5.065569]\n",
      "230: Train Loss: [2.2623851, 0.37702388, 4.1477466] | Test Loss: [2.5639954, 0.34420952, 4.783781]\n",
      "231: Train Loss: [2.309758, 0.39619237, 4.2233233] | Test Loss: [2.553316, 0.35600588, 4.7506266]\n",
      "232: Train Loss: [2.2912703, 0.33463135, 4.247909] | Test Loss: [2.7055228, 0.3326712, 5.0783744]\n",
      "233: Train Loss: [2.39869, 0.35550368, 4.4418764] | Test Loss: [2.702647, 0.46552774, 4.9397664]\n",
      "234: Train Loss: [2.5189447, 0.34469563, 4.693194] | Test Loss: [2.6822972, 0.4150834, 4.949511]\n",
      "235: Train Loss: [2.4220672, 0.30854118, 4.535593] | Test Loss: [2.7456272, 0.4400644, 5.05119]\n",
      "236: Train Loss: [2.420858, 0.44089508, 4.4008207] | Test Loss: [2.752653, 0.4493996, 5.0559063]\n",
      "237: Train Loss: [2.4793718, 0.3883058, 4.570438] | Test Loss: [2.608233, 0.3527994, 4.8636665]\n",
      "238: Train Loss: [2.3417473, 0.33675882, 4.346736] | Test Loss: [2.456974, 0.36886072, 4.5450873]\n",
      "239: Train Loss: [2.3814857, 0.32174873, 4.4412227] | Test Loss: [2.8066828, 0.4278465, 5.185519]\n",
      "240: Train Loss: [2.5764189, 0.46811807, 4.6847196] | Test Loss: [2.5465164, 0.29854512, 4.794488]\n",
      "241: Train Loss: [2.3200462, 0.39167267, 4.24842] | Test Loss: [2.7671387, 0.39713734, 5.1371403]\n",
      "242: Train Loss: [2.3287373, 0.30491295, 4.3525615] | Test Loss: [2.583768, 0.35231394, 4.815222]\n",
      "243: Train Loss: [2.4354548, 0.33607554, 4.5348344] | Test Loss: [2.5481758, 0.32662335, 4.769728]\n",
      "244: Train Loss: [2.616923, 0.39930165, 4.8345447] | Test Loss: [2.6796985, 0.33146352, 5.0279336]\n",
      "245: Train Loss: [2.4203925, 0.33051023, 4.510275] | Test Loss: [2.5405865, 0.35106972, 4.730103]\n",
      "246: Train Loss: [2.4516845, 0.33078793, 4.572581] | Test Loss: [2.6506407, 0.32751244, 4.973769]\n",
      "247: Train Loss: [2.4071245, 0.3341725, 4.4800763] | Test Loss: [2.6013994, 0.35077068, 4.8520284]\n",
      "248: Train Loss: [2.3462555, 0.39483726, 4.2976737] | Test Loss: [2.781673, 0.4935382, 5.0698075]\n",
      "249: Train Loss: [2.3614414, 0.3952809, 4.327602] | Test Loss: [2.717108, 0.402044, 5.032172]\n",
      "250: Train Loss: [2.4463348, 0.43303195, 4.4596376] | Test Loss: [2.7061472, 0.31450102, 5.0977936]\n",
      "251: Train Loss: [2.4809651, 0.4023683, 4.559562] | Test Loss: [2.5326068, 0.3470331, 4.7181807]\n",
      "252: Train Loss: [2.4419203, 0.36091524, 4.5229254] | Test Loss: [2.5553207, 0.38252896, 4.7281127]\n",
      "253: Train Loss: [2.4578323, 0.37682945, 4.538835] | Test Loss: [2.5573854, 0.3394313, 4.7753396]\n",
      "254: Train Loss: [2.4143493, 0.39173847, 4.43696] | Test Loss: [2.4948103, 0.3846625, 4.604958]\n",
      "255: Train Loss: [2.426512, 0.35579598, 4.497228] | Test Loss: [2.7325053, 0.37443614, 5.0905743]\n",
      "256: Train Loss: [2.4712248, 0.36082095, 4.581629] | Test Loss: [2.6087947, 0.3822231, 4.8353662]\n",
      "257: Train Loss: [2.353472, 0.3740228, 4.332921] | Test Loss: [2.55735, 0.39945376, 4.715246]\n",
      "258: Train Loss: [2.39964, 0.34741232, 4.451868] | Test Loss: [2.6555707, 0.39394683, 4.917195]\n",
      "259: Train Loss: [2.3835976, 0.3414227, 4.4257727] | Test Loss: [2.6225548, 0.34363654, 4.901473]\n",
      "260: Train Loss: [2.4468715, 0.36606854, 4.5276747] | Test Loss: [2.4509754, 0.43441996, 4.4675307]\n",
      "261: Train Loss: [2.6501374, 0.39138147, 4.9088936] | Test Loss: [2.4649992, 0.30847177, 4.6215267]\n",
      "262: Train Loss: [2.3247554, 0.4265309, 4.22298] | Test Loss: [2.5206003, 0.31552577, 4.7256746]\n",
      "263: Train Loss: [2.4606502, 0.37793872, 4.5433617] | Test Loss: [2.5838099, 0.34739456, 4.8202252]\n",
      "264: Train Loss: [2.1740294, 0.3461463, 4.0019126] | Test Loss: [2.6598892, 0.3884242, 4.931354]\n",
      "265: Train Loss: [2.5004902, 0.5760748, 4.424906] | Test Loss: [2.7179132, 0.43512282, 5.0007033]\n",
      "266: Train Loss: [2.2704787, 0.3664502, 4.174507] | Test Loss: [2.5452251, 0.39478368, 4.695667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267: Train Loss: [2.3282359, 0.3702134, 4.286258] | Test Loss: [2.5677755, 0.32784984, 4.807701]\n",
      "268: Train Loss: [2.4085064, 0.3574391, 4.4595737] | Test Loss: [2.7568562, 0.4713772, 5.042335]\n",
      "269: Train Loss: [2.4871445, 0.38300857, 4.5912805] | Test Loss: [2.52056, 0.40223148, 4.6388884]\n",
      "270: Train Loss: [2.3736634, 0.3059017, 4.4414253] | Test Loss: [2.9094837, 0.39318988, 5.4257774]\n",
      "271: Train Loss: [2.287681, 0.36382014, 4.211542] | Test Loss: [2.7218702, 0.3894838, 5.0542564]\n",
      "272: Train Loss: [2.39315, 0.39787102, 4.388429] | Test Loss: [2.5174284, 0.30334973, 4.731507]\n",
      "273: Train Loss: [2.301941, 0.40801442, 4.1958675] | Test Loss: [2.526132, 0.31273863, 4.739526]\n",
      "274: Train Loss: [2.244351, 0.3119885, 4.1767135] | Test Loss: [2.775115, 0.44003797, 5.1101923]\n",
      "275: Train Loss: [2.4665406, 0.37315184, 4.5599294] | Test Loss: [2.687896, 0.43284303, 4.942949]\n",
      "276: Train Loss: [2.4332612, 0.42640975, 4.4401126] | Test Loss: [2.569203, 0.32244536, 4.8159604]\n",
      "277: Train Loss: [2.348535, 0.4218752, 4.275195] | Test Loss: [2.8218896, 0.46985725, 5.173922]\n",
      "278: Train Loss: [2.309738, 0.3980287, 4.221447] | Test Loss: [2.549692, 0.38261566, 4.7167683]\n",
      "279: Train Loss: [2.4120965, 0.36829764, 4.4558954] | Test Loss: [2.674215, 0.40216303, 4.946267]\n",
      "280: Train Loss: [2.5317736, 0.4151192, 4.648428] | Test Loss: [2.5964084, 0.38671207, 4.8061047]\n",
      "281: Train Loss: [2.6287868, 0.36520725, 4.8923664] | Test Loss: [2.6141312, 0.33258334, 4.895679]\n",
      "282: Train Loss: [2.2568998, 0.3454325, 4.1683674] | Test Loss: [2.6802447, 0.3041287, 5.0563607]\n",
      "283: Train Loss: [2.4635403, 0.38830534, 4.5387754] | Test Loss: [2.72637, 0.34673172, 5.1060085]\n",
      "284: Train Loss: [2.4651601, 0.36267614, 4.567644] | Test Loss: [2.5776947, 0.42832187, 4.7270675]\n",
      "285: Train Loss: [2.6227405, 0.4340933, 4.8113875] | Test Loss: [2.511882, 0.35365728, 4.670107]\n",
      "286: Train Loss: [2.3141563, 0.46417862, 4.164134] | Test Loss: [2.7151325, 0.3724831, 5.0577817]\n",
      "287: Train Loss: [2.397514, 0.36090833, 4.4341197] | Test Loss: [2.5907426, 0.37549222, 4.805993]\n",
      "288: Train Loss: [2.478684, 0.32747862, 4.6298895] | Test Loss: [2.352472, 0.3479443, 4.357]\n",
      "289: Train Loss: [2.4645262, 0.3618299, 4.5672226] | Test Loss: [2.7184129, 0.42339382, 5.013432]\n",
      "290: Train Loss: [2.345588, 0.3466309, 4.344545] | Test Loss: [2.488171, 0.36680818, 4.609534]\n",
      "291: Train Loss: [2.3064916, 0.38176137, 4.2312217] | Test Loss: [2.868088, 0.40026847, 5.3359075]\n",
      "292: Train Loss: [2.3305452, 0.3669286, 4.294162] | Test Loss: [2.440187, 0.36207086, 4.518303]\n",
      "293: Train Loss: [2.4006155, 0.32762492, 4.473606] | Test Loss: [2.543989, 0.3887326, 4.6992455]\n",
      "294: Train Loss: [2.503801, 0.3594615, 4.648141] | Test Loss: [2.7157266, 0.2983664, 5.1330867]\n",
      "295: Train Loss: [2.5398932, 0.5290416, 4.5507445] | Test Loss: [2.6599426, 0.33891407, 4.9809713]\n",
      "296: Train Loss: [2.315211, 0.3455215, 4.2849007] | Test Loss: [2.7155323, 0.39249736, 5.038567]\n",
      "297: Train Loss: [2.5031223, 0.35587972, 4.650365] | Test Loss: [2.6436808, 0.34719372, 4.940168]\n",
      "298: Train Loss: [2.3680096, 0.3535655, 4.3824534] | Test Loss: [2.6635184, 0.35576773, 4.971269]\n",
      "299: Train Loss: [2.3457885, 0.34965867, 4.3419185] | Test Loss: [2.4907072, 0.38644648, 4.594968]\n",
      "300: Train Loss: [2.2892134, 0.31956485, 4.258862] | Test Loss: [2.51493, 0.39439055, 4.6354694]\n",
      "301: Train Loss: [2.4011865, 0.3517287, 4.450644] | Test Loss: [2.542042, 0.3443617, 4.7397223]\n",
      "302: Train Loss: [2.2995596, 0.3176637, 4.2814555] | Test Loss: [2.7790864, 0.43613708, 5.1220355]\n",
      "303: Train Loss: [2.522139, 0.38708562, 4.6571927] | Test Loss: [2.5644143, 0.3942149, 4.7346134]\n",
      "304: Train Loss: [2.333083, 0.32516095, 4.341005] | Test Loss: [2.6731877, 0.40248704, 4.9438887]\n",
      "305: Train Loss: [2.238083, 0.2979697, 4.178196] | Test Loss: [2.5438824, 0.40428865, 4.683476]\n",
      "306: Train Loss: [2.480083, 0.3705728, 4.5895934] | Test Loss: [2.847378, 0.3257571, 5.368999]\n",
      "307: Train Loss: [2.2943873, 0.3373273, 4.251447] | Test Loss: [2.529046, 0.38323027, 4.674862]\n",
      "308: Train Loss: [2.4942179, 0.35680577, 4.63163] | Test Loss: [2.6709533, 0.38446, 4.9574466]\n",
      "309: Train Loss: [2.4718046, 0.35461476, 4.5889945] | Test Loss: [2.7197423, 0.3587578, 5.0807266]\n",
      "310: Train Loss: [2.4447641, 0.29111275, 4.5984154] | Test Loss: [2.6396146, 0.32646877, 4.95276]\n",
      "311: Train Loss: [2.3279345, 0.31722727, 4.3386416] | Test Loss: [2.6727407, 0.3676875, 4.9777937]\n",
      "312: Train Loss: [2.4469018, 0.40200645, 4.491797] | Test Loss: [2.6432333, 0.37065676, 4.9158096]\n",
      "313: Train Loss: [2.2964914, 0.34323895, 4.249744] | Test Loss: [2.6760018, 0.42235774, 4.929646]\n",
      "314: Train Loss: [2.4223957, 0.4047327, 4.4400587] | Test Loss: [2.5517974, 0.32739636, 4.7761984]\n",
      "315: Train Loss: [2.3608146, 0.36563808, 4.355991] | Test Loss: [2.52668, 0.33930954, 4.7140503]\n",
      "316: Train Loss: [2.5969524, 0.35466927, 4.839236] | Test Loss: [2.7493494, 0.36738622, 5.1313124]\n",
      "317: Train Loss: [2.555043, 0.32519683, 4.784889] | Test Loss: [2.5302258, 0.370605, 4.6898465]\n",
      "318: Train Loss: [2.3958163, 0.31324193, 4.4783907] | Test Loss: [2.597361, 0.41846317, 4.776259]\n",
      "319: Train Loss: [2.5129414, 0.40626138, 4.6196213] | Test Loss: [2.6160252, 0.33676016, 4.8952904]\n",
      "320: Train Loss: [2.4530842, 0.4142707, 4.4918976] | Test Loss: [2.5709689, 0.4220128, 4.719925]\n",
      "321: Train Loss: [2.32381, 0.35042062, 4.2971997] | Test Loss: [2.8379006, 0.3213202, 5.354481]\n",
      "322: Train Loss: [2.4448729, 0.3361245, 4.5536213] | Test Loss: [2.6262045, 0.33406553, 4.9183435]\n",
      "323: Train Loss: [2.3093195, 0.34137565, 4.277263] | Test Loss: [2.540195, 0.40080377, 4.6795864]\n",
      "324: Train Loss: [2.3838952, 0.37097222, 4.396818] | Test Loss: [2.3490527, 0.47498116, 4.223124]\n",
      "325: Train Loss: [2.4183867, 0.32787138, 4.508902] | Test Loss: [2.461769, 0.33152533, 4.592013]\n",
      "326: Train Loss: [2.50464, 0.38239732, 4.626883] | Test Loss: [2.659481, 0.43782353, 4.8811383]\n",
      "327: Train Loss: [2.3483973, 0.31479734, 4.381997] | Test Loss: [2.7695014, 0.42132694, 5.117676]\n",
      "328: Train Loss: [2.371338, 0.30088052, 4.4417953] | Test Loss: [2.592515, 0.37210473, 4.8129253]\n",
      "329: Train Loss: [2.316074, 0.32699925, 4.3051486] | Test Loss: [2.587019, 0.3500359, 4.8240023]\n",
      "330: Train Loss: [2.2577703, 0.32617322, 4.1893673] | Test Loss: [2.5622988, 0.36697295, 4.7576246]\n",
      "331: Train Loss: [2.4495015, 0.32444537, 4.574558] | Test Loss: [2.773443, 0.4791048, 5.067781]\n",
      "332: Train Loss: [2.3302798, 0.32478544, 4.3357744] | Test Loss: [2.677322, 0.36980167, 4.9848423]\n",
      "333: Train Loss: [2.3695395, 0.3867959, 4.352283] | Test Loss: [2.4700243, 0.35404363, 4.586005]\n",
      "334: Train Loss: [2.471456, 0.3629746, 4.5799375] | Test Loss: [2.6915379, 0.382174, 5.0009017]\n",
      "335: Train Loss: [2.4948885, 0.41059, 4.579187] | Test Loss: [2.795718, 0.4045628, 5.186873]\n",
      "336: Train Loss: [2.4892766, 0.30935013, 4.6692033] | Test Loss: [2.7731113, 0.2980314, 5.2481914]\n",
      "337: Train Loss: [2.3153813, 0.37206066, 4.258702] | Test Loss: [2.7261038, 0.35917214, 5.093035]\n",
      "338: Train Loss: [2.3884792, 0.3680416, 4.408917] | Test Loss: [2.7776747, 0.3621366, 5.193213]\n",
      "339: Train Loss: [2.322429, 0.3952263, 4.2496314] | Test Loss: [2.7360394, 0.35496095, 5.117118]\n",
      "340: Train Loss: [2.3500304, 0.3902049, 4.309856] | Test Loss: [2.6577284, 0.35880336, 4.9566536]\n",
      "341: Train Loss: [2.5217507, 0.3634945, 4.680007] | Test Loss: [2.5125656, 0.34322652, 4.681905]\n",
      "342: Train Loss: [2.4782546, 0.3509523, 4.605557] | Test Loss: [2.6948085, 0.34719574, 5.0424213]\n",
      "343: Train Loss: [2.7673423, 0.32812658, 5.206558] | Test Loss: [2.4177866, 0.3589496, 4.4766235]\n",
      "344: Train Loss: [2.4867103, 0.3364596, 4.636961] | Test Loss: [2.57515, 0.34873486, 4.801565]\n",
      "345: Train Loss: [2.4300902, 0.36792266, 4.4922576] | Test Loss: [2.7124352, 0.4304572, 4.9944134]\n",
      "346: Train Loss: [2.357419, 0.37364438, 4.3411937] | Test Loss: [2.6909404, 0.3174571, 5.0644236]\n",
      "347: Train Loss: [2.3568187, 0.3159511, 4.3976865] | Test Loss: [2.5255442, 0.3919819, 4.6591063]\n",
      "348: Train Loss: [2.3665705, 0.40952495, 4.323616] | Test Loss: [2.6314995, 0.3156719, 4.947327]\n",
      "349: Train Loss: [2.4958286, 0.45784968, 4.5338078] | Test Loss: [2.5142202, 0.38670385, 4.6417365]\n",
      "350: Train Loss: [2.327543, 0.3724871, 4.282599] | Test Loss: [2.5825746, 0.38519284, 4.7799563]\n",
      "351: Train Loss: [2.4507303, 0.38871476, 4.512746] | Test Loss: [2.3640785, 0.33463514, 4.393522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352: Train Loss: [2.4252586, 0.33325505, 4.5172625] | Test Loss: [2.6662889, 0.42950627, 4.9030714]\n",
      "353: Train Loss: [2.4831147, 0.4031943, 4.563035] | Test Loss: [2.783789, 0.49953604, 5.068042]\n",
      "354: Train Loss: [2.428498, 0.4662048, 4.3907914] | Test Loss: [2.5255222, 0.35370898, 4.6973357]\n",
      "355: Train Loss: [2.4541252, 0.367947, 4.540303] | Test Loss: [2.681334, 0.38879657, 4.973871]\n",
      "356: Train Loss: [2.3708148, 0.3244288, 4.417201] | Test Loss: [2.5395966, 0.35672593, 4.722467]\n",
      "357: Train Loss: [2.3487325, 0.33922905, 4.358236] | Test Loss: [2.5538077, 0.35656744, 4.751048]\n",
      "358: Train Loss: [2.3023427, 0.33623627, 4.268449] | Test Loss: [2.4573758, 0.3611402, 4.5536113]\n",
      "359: Train Loss: [2.3515956, 0.35071403, 4.352477] | Test Loss: [2.6658013, 0.38927695, 4.9423256]\n",
      "360: Train Loss: [2.4624512, 0.4088026, 4.5161] | Test Loss: [2.5250936, 0.40866727, 4.64152]\n",
      "361: Train Loss: [2.3752227, 0.35800955, 4.392436] | Test Loss: [2.5604281, 0.32876945, 4.7920866]\n",
      "362: Train Loss: [2.363931, 0.27470937, 4.4531527] | Test Loss: [2.8957086, 0.41403034, 5.3773866]\n",
      "363: Train Loss: [2.4426212, 0.29115987, 4.5940824] | Test Loss: [2.6287444, 0.3794034, 4.878085]\n",
      "364: Train Loss: [2.443889, 0.41063035, 4.4771476] | Test Loss: [2.6834812, 0.3752029, 4.99176]\n",
      "365: Train Loss: [2.396064, 0.31685722, 4.4752707] | Test Loss: [2.5207064, 0.34145987, 4.699953]\n",
      "366: Train Loss: [2.2700489, 0.39434287, 4.145755] | Test Loss: [2.5676937, 0.40690687, 4.7284803]\n",
      "367: Train Loss: [2.264713, 0.34121224, 4.188214] | Test Loss: [2.5955815, 0.37937284, 4.8117905]\n",
      "368: Train Loss: [2.2959564, 0.35432023, 4.2375927] | Test Loss: [2.543097, 0.3565292, 4.729665]\n",
      "369: Train Loss: [2.4516656, 0.32786354, 4.5754676] | Test Loss: [2.644894, 0.43641645, 4.853371]\n",
      "370: Train Loss: [2.433606, 0.31649113, 4.5507207] | Test Loss: [2.5553818, 0.34445828, 4.7663054]\n",
      "371: Train Loss: [2.4581897, 0.38550794, 4.5308714] | Test Loss: [2.683186, 0.37247872, 4.9938936]\n",
      "372: Train Loss: [2.5378263, 0.36688784, 4.7087646] | Test Loss: [2.7084014, 0.36211422, 5.0546885]\n",
      "373: Train Loss: [2.4658737, 0.34099174, 4.590756] | Test Loss: [2.6203992, 0.32920346, 4.911595]\n",
      "374: Train Loss: [2.4684746, 0.34780365, 4.5891457] | Test Loss: [2.79881, 0.32032332, 5.2772965]\n",
      "375: Train Loss: [2.3645606, 0.38911843, 4.3400025] | Test Loss: [2.6054735, 0.40025705, 4.81069]\n",
      "376: Train Loss: [2.5015414, 0.40545866, 4.5976243] | Test Loss: [2.4934886, 0.3159212, 4.671056]\n",
      "377: Train Loss: [2.4591138, 0.36438656, 4.553841] | Test Loss: [2.6901543, 0.34486905, 5.0354395]\n",
      "378: Train Loss: [2.4109097, 0.37221125, 4.449608] | Test Loss: [2.6430497, 0.3062936, 4.979806]\n",
      "379: Train Loss: [2.4035506, 0.3881831, 4.418918] | Test Loss: [2.5591047, 0.35857064, 4.759639]\n",
      "380: Train Loss: [2.4950473, 0.35386607, 4.6362286] | Test Loss: [2.7353292, 0.3386388, 5.1320195]\n",
      "381: Train Loss: [2.3962102, 0.3872708, 4.4051495] | Test Loss: [2.6343493, 0.5225307, 4.746168]\n",
      "382: Train Loss: [2.4360013, 0.33623573, 4.535767] | Test Loss: [2.698962, 0.33578345, 5.0621405]\n",
      "383: Train Loss: [2.397595, 0.3714491, 4.423741] | Test Loss: [2.791672, 0.33970314, 5.243641]\n",
      "384: Train Loss: [2.329766, 0.3027409, 4.356791] | Test Loss: [2.6267302, 0.44870463, 4.8047557]\n",
      "385: Train Loss: [2.3866744, 0.3706304, 4.4027185] | Test Loss: [2.5985947, 0.31106448, 4.8861246]\n",
      "386: Train Loss: [2.3768454, 0.36855334, 4.3851376] | Test Loss: [2.7090087, 0.3315476, 5.0864697]\n",
      "387: Train Loss: [2.4561315, 0.3195829, 4.59268] | Test Loss: [2.76204, 0.37144268, 5.152637]\n",
      "388: Train Loss: [2.320579, 0.36020625, 4.280952] | Test Loss: [2.6915722, 0.38141075, 5.001734]\n",
      "389: Train Loss: [2.4219444, 0.39362183, 4.450267] | Test Loss: [2.8124478, 0.3550226, 5.269873]\n",
      "390: Train Loss: [2.4951096, 0.38394, 4.606279] | Test Loss: [2.500239, 0.32139227, 4.6790857]\n",
      "391: Train Loss: [2.4199443, 0.4004248, 4.4394636] | Test Loss: [2.436382, 0.34979787, 4.5229664]\n",
      "392: Train Loss: [2.4692562, 0.42099145, 4.517521] | Test Loss: [2.488433, 0.37729266, 4.599573]\n",
      "393: Train Loss: [2.5510588, 0.41759038, 4.684527] | Test Loss: [2.6892512, 0.402464, 4.9760385]\n",
      "394: Train Loss: [2.426879, 0.38107723, 4.4726806] | Test Loss: [2.6551597, 0.34208673, 4.9682326]\n",
      "395: Train Loss: [2.3705785, 0.33074725, 4.41041] | Test Loss: [2.6350007, 0.38322428, 4.886777]\n",
      "396: Train Loss: [2.3095896, 0.33279613, 4.286383] | Test Loss: [2.586972, 0.35206574, 4.8218784]\n",
      "397: Train Loss: [2.3386233, 0.3062487, 4.370998] | Test Loss: [2.624032, 0.4319799, 4.8160844]\n",
      "398: Train Loss: [2.3419833, 0.37014672, 4.31382] | Test Loss: [2.6266932, 0.34832114, 4.9050655]\n",
      "399: Train Loss: [2.4247208, 0.31891704, 4.5305243] | Test Loss: [2.7252645, 0.39792365, 5.0526056]\n",
      "400: Train Loss: [2.296529, 0.32779944, 4.265259] | Test Loss: [2.8604944, 0.46623644, 5.254752]\n",
      "401: Train Loss: [2.4333773, 0.33196092, 4.5347934] | Test Loss: [2.6709642, 0.36741593, 4.9745126]\n",
      "402: Train Loss: [2.5715764, 0.33317655, 4.809976] | Test Loss: [2.5522118, 0.3794676, 4.724956]\n",
      "403: Train Loss: [2.3237371, 0.34029037, 4.3071837] | Test Loss: [2.758701, 0.44216117, 5.075241]\n",
      "404: Train Loss: [2.383521, 0.37752694, 4.3895154] | Test Loss: [2.6363437, 0.38505903, 4.8876286]\n",
      "405: Train Loss: [2.3713832, 0.37607735, 4.366689] | Test Loss: [2.7241552, 0.38132322, 5.066987]\n",
      "406: Train Loss: [2.5733159, 0.3643146, 4.782317] | Test Loss: [2.3896394, 0.44418058, 4.3350983]\n",
      "407: Train Loss: [2.4641502, 0.32390165, 4.6043987] | Test Loss: [2.557177, 0.39977232, 4.714582]\n",
      "408: Train Loss: [2.2988074, 0.33859047, 4.259024] | Test Loss: [2.4823425, 0.33983657, 4.6248484]\n",
      "409: Train Loss: [2.506978, 0.40604663, 4.607909] | Test Loss: [2.494295, 0.35966623, 4.6289234]\n",
      "410: Train Loss: [2.2795596, 0.42413756, 4.1349816] | Test Loss: [2.5670562, 0.38354066, 4.7505717]\n",
      "411: Train Loss: [2.4724946, 0.34346187, 4.601527] | Test Loss: [2.342163, 0.35363862, 4.3306875]\n",
      "412: Train Loss: [2.4844499, 0.37284863, 4.596051] | Test Loss: [2.725412, 0.47595432, 4.9748693]\n",
      "413: Train Loss: [2.2788565, 0.34385887, 4.2138543] | Test Loss: [2.6369498, 0.33307812, 4.9408216]\n",
      "414: Train Loss: [2.346802, 0.3634447, 4.330159] | Test Loss: [2.7219937, 0.33546212, 5.1085253]\n",
      "415: Train Loss: [2.3790333, 0.33647186, 4.4215946] | Test Loss: [2.6674201, 0.40378305, 4.9310575]\n",
      "416: Train Loss: [2.4483314, 0.31510547, 4.5815573] | Test Loss: [2.67986, 0.39606562, 4.9636545]\n",
      "417: Train Loss: [2.5370028, 0.31370804, 4.760298] | Test Loss: [2.6885622, 0.34710658, 5.030018]\n",
      "418: Train Loss: [2.3094552, 0.338241, 4.280669] | Test Loss: [2.6376858, 0.37047717, 4.9048944]\n",
      "419: Train Loss: [2.4148576, 0.43075532, 4.39896] | Test Loss: [2.6498506, 0.38977128, 4.9099298]\n",
      "420: Train Loss: [2.2879498, 0.4523355, 4.1235642] | Test Loss: [2.6281426, 0.39726707, 4.8590183]\n",
      "421: Train Loss: [2.3475447, 0.30572686, 4.3893623] | Test Loss: [2.5500684, 0.357269, 4.742868]\n",
      "422: Train Loss: [2.451012, 0.3461477, 4.5558763] | Test Loss: [2.4803457, 0.321669, 4.6390224]\n",
      "423: Train Loss: [2.3275375, 0.33790675, 4.317168] | Test Loss: [2.7206998, 0.31401303, 5.1273866]\n",
      "424: Train Loss: [2.4487414, 0.3043626, 4.59312] | Test Loss: [2.5455463, 0.3642279, 4.726865]\n",
      "425: Train Loss: [2.513653, 0.3363032, 4.691003] | Test Loss: [2.597657, 0.3114666, 4.883847]\n",
      "426: Train Loss: [2.4759002, 0.34262198, 4.6091785] | Test Loss: [2.5769396, 0.33891684, 4.8149624]\n",
      "427: Train Loss: [2.3910954, 0.34814677, 4.434044] | Test Loss: [2.7175627, 0.41070855, 5.024417]\n",
      "428: Train Loss: [2.3619323, 0.36623555, 4.357629] | Test Loss: [2.6745284, 0.3549953, 4.9940615]\n",
      "429: Train Loss: [2.3922875, 0.3530034, 4.4315715] | Test Loss: [2.4796393, 0.4448193, 4.514459]\n",
      "430: Train Loss: [2.4087243, 0.32839584, 4.489053] | Test Loss: [2.795136, 0.3804853, 5.209787]\n",
      "431: Train Loss: [2.4041917, 0.33889416, 4.469489] | Test Loss: [2.6370213, 0.39913172, 4.874911]\n",
      "432: Train Loss: [2.403882, 0.31800258, 4.4897614] | Test Loss: [2.5371022, 0.31855989, 4.755645]\n",
      "433: Train Loss: [2.328816, 0.3263029, 4.331329] | Test Loss: [2.698035, 0.35610545, 5.0399647]\n",
      "434: Train Loss: [2.4335535, 0.34400794, 4.523099] | Test Loss: [2.5755394, 0.4578516, 4.6932273]\n",
      "435: Train Loss: [2.482182, 0.40973255, 4.5546317] | Test Loss: [2.3104703, 0.37120092, 4.2497396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436: Train Loss: [2.278227, 0.3382192, 4.218235] | Test Loss: [2.737607, 0.4493637, 5.0258503]\n",
      "437: Train Loss: [2.52847, 0.3594182, 4.6975217] | Test Loss: [2.5806646, 0.34819072, 4.8131385]\n",
      "438: Train Loss: [2.4512153, 0.32359862, 4.578832] | Test Loss: [2.5475059, 0.34076416, 4.7542477]\n",
      "439: Train Loss: [2.389401, 0.5197323, 4.2590694] | Test Loss: [2.6432426, 0.38938755, 4.8970976]\n",
      "440: Train Loss: [2.525825, 0.33419028, 4.7174597] | Test Loss: [2.465038, 0.32989988, 4.6001763]\n",
      "441: Train Loss: [2.420707, 0.3920607, 4.449353] | Test Loss: [2.686637, 0.29424748, 5.079026]\n",
      "442: Train Loss: [2.4498615, 0.3711825, 4.5285406] | Test Loss: [2.7771163, 0.4682505, 5.085982]\n",
      "443: Train Loss: [2.5181475, 0.34500802, 4.691287] | Test Loss: [2.5509474, 0.3230425, 4.7788525]\n",
      "444: Train Loss: [2.5449913, 0.36013976, 4.7298427] | Test Loss: [2.583417, 0.41584793, 4.750986]\n",
      "445: Train Loss: [2.5148747, 0.39224574, 4.6375036] | Test Loss: [2.5675125, 0.34590754, 4.7891173]\n",
      "446: Train Loss: [2.6335988, 0.33901837, 4.9281793] | Test Loss: [2.641923, 0.32026348, 4.9635825]\n",
      "447: Train Loss: [2.2601056, 0.33483568, 4.1853757] | Test Loss: [2.812579, 0.4341009, 5.1910567]\n",
      "448: Train Loss: [2.335161, 0.34418833, 4.3261337] | Test Loss: [2.6975765, 0.38003373, 5.0151196]\n",
      "449: Train Loss: [2.4421575, 0.30601224, 4.578303] | Test Loss: [2.4712617, 0.4236913, 4.518832]\n",
      "450: Train Loss: [2.2339585, 0.35456792, 4.113349] | Test Loss: [2.572218, 0.41441715, 4.7300186]\n",
      "451: Train Loss: [2.458088, 0.34134945, 4.5748262] | Test Loss: [2.6484084, 0.36586136, 4.9309554]\n",
      "452: Train Loss: [2.4565375, 0.40925658, 4.5038185] | Test Loss: [2.5402813, 0.39733908, 4.6832237]\n",
      "453: Train Loss: [2.2717187, 0.34224048, 4.201197] | Test Loss: [2.5489242, 0.39601192, 4.7018366]\n",
      "454: Train Loss: [2.4717386, 0.34148797, 4.6019893] | Test Loss: [2.6365902, 0.38600272, 4.887178]\n",
      "455: Train Loss: [2.5077415, 0.4101349, 4.605348] | Test Loss: [2.4635684, 0.40052378, 4.526613]\n",
      "456: Train Loss: [2.2612996, 0.37839246, 4.1442065] | Test Loss: [2.7006624, 0.3794019, 5.021923]\n",
      "457: Train Loss: [2.36614, 0.32529142, 4.406988] | Test Loss: [2.5983644, 0.33205783, 4.8646708]\n",
      "458: Train Loss: [2.5318651, 0.31848714, 4.745243] | Test Loss: [2.610471, 0.37733233, 4.84361]\n",
      "459: Train Loss: [2.3681893, 0.3625206, 4.373858] | Test Loss: [2.554067, 0.3735912, 4.7345424]\n",
      "460: Train Loss: [2.4435258, 0.43093765, 4.456114] | Test Loss: [2.850131, 0.42598724, 5.274275]\n",
      "461: Train Loss: [2.235407, 0.35164183, 4.1191726] | Test Loss: [2.6112, 0.3150359, 4.9073644]\n",
      "462: Train Loss: [2.2997384, 0.31198525, 4.287492] | Test Loss: [2.6781185, 0.39591262, 4.9603243]\n",
      "463: Train Loss: [2.497093, 0.41647545, 4.5777106] | Test Loss: [2.5622358, 0.36235324, 4.7621183]\n",
      "464: Train Loss: [2.411315, 0.38171262, 4.4409175] | Test Loss: [2.6690514, 0.32385737, 5.0142455]\n",
      "465: Train Loss: [2.4543214, 0.31408668, 4.5945563] | Test Loss: [2.5444176, 0.3338047, 4.7550306]\n",
      "466: Train Loss: [2.3813636, 0.413304, 4.3494234] | Test Loss: [2.6379807, 0.3587019, 4.9172597]\n",
      "467: Train Loss: [2.4387255, 0.37140867, 4.5060425] | Test Loss: [2.3820844, 0.32912534, 4.4350433]\n",
      "468: Train Loss: [2.5358672, 0.32307824, 4.7486563] | Test Loss: [2.5193958, 0.38954017, 4.6492515]\n",
      "469: Train Loss: [2.3745105, 0.3277486, 4.4212723] | Test Loss: [2.812327, 0.35755473, 5.267099]\n",
      "470: Train Loss: [2.4823656, 0.2874502, 4.677281] | Test Loss: [2.6087186, 0.30035695, 4.9170804]\n",
      "471: Train Loss: [2.3471575, 0.35829628, 4.3360186] | Test Loss: [2.9395537, 0.41354662, 5.465561]\n",
      "472: Train Loss: [2.3932955, 0.38242877, 4.4041624] | Test Loss: [2.7446733, 0.36992997, 5.1194167]\n",
      "473: Train Loss: [2.5389555, 0.37658757, 4.7013235] | Test Loss: [2.3654876, 0.35868853, 4.372287]\n",
      "474: Train Loss: [2.4410114, 0.456924, 4.425099] | Test Loss: [2.7162547, 0.46126676, 4.971243]\n",
      "475: Train Loss: [2.3996756, 0.32749793, 4.4718533] | Test Loss: [2.6195707, 0.39944634, 4.839695]\n",
      "476: Train Loss: [2.5070038, 0.40393126, 4.6100764] | Test Loss: [2.7231152, 0.377773, 5.0684576]\n",
      "477: Train Loss: [2.358309, 0.3484822, 4.368136] | Test Loss: [2.7629614, 0.3689862, 5.1569366]\n",
      "478: Train Loss: [2.3707776, 0.3579928, 4.3835626] | Test Loss: [2.769108, 0.36574692, 5.172469]\n",
      "479: Train Loss: [2.3819323, 0.3709697, 4.3928947] | Test Loss: [2.5945408, 0.33579433, 4.853287]\n",
      "480: Train Loss: [2.5165923, 0.3350491, 4.6981354] | Test Loss: [2.5628905, 0.30208588, 4.823695]\n",
      "481: Train Loss: [2.2940035, 0.4045564, 4.1834507] | Test Loss: [2.4946275, 0.35170916, 4.6375456]\n",
      "482: Train Loss: [2.4537108, 0.3586659, 4.5487556] | Test Loss: [2.7062955, 0.28297785, 5.129613]\n",
      "483: Train Loss: [2.3952498, 0.33666518, 4.4538345] | Test Loss: [2.779974, 0.48783225, 5.072116]\n",
      "484: Train Loss: [2.3509474, 0.3250234, 4.3768716] | Test Loss: [2.8130426, 0.39041078, 5.2356744]\n",
      "485: Train Loss: [2.371853, 0.3228349, 4.4208713] | Test Loss: [2.7280126, 0.3724975, 5.0835276]\n",
      "486: Train Loss: [2.3916547, 0.39261064, 4.390699] | Test Loss: [2.3523147, 0.3171316, 4.387498]\n",
      "487: Train Loss: [2.4362295, 0.3085516, 4.563907] | Test Loss: [2.5247645, 0.34384912, 4.70568]\n",
      "488: Train Loss: [2.4211352, 0.3508094, 4.491461] | Test Loss: [2.5473664, 0.43798593, 4.656747]\n",
      "489: Train Loss: [2.4187555, 0.42516083, 4.41235] | Test Loss: [2.4241452, 0.33995226, 4.508338]\n",
      "490: Train Loss: [2.406378, 0.38926417, 4.423492] | Test Loss: [2.618228, 0.37977687, 4.856679]\n",
      "491: Train Loss: [2.4245753, 0.31968236, 4.5294685] | Test Loss: [2.476487, 0.3446485, 4.6083255]\n",
      "492: Train Loss: [2.4349816, 0.3651535, 4.50481] | Test Loss: [2.606086, 0.3914724, 4.8206997]\n",
      "493: Train Loss: [2.361673, 0.40037045, 4.3229756] | Test Loss: [2.6756177, 0.34583008, 5.0054054]\n",
      "494: Train Loss: [2.3814697, 0.377576, 4.3853636] | Test Loss: [2.633747, 0.38313782, 4.8843565]\n",
      "495: Train Loss: [2.507746, 0.41709322, 4.5983987] | Test Loss: [2.6748855, 0.37860495, 4.971166]\n",
      "496: Train Loss: [2.5346274, 0.42009276, 4.6491623] | Test Loss: [2.516786, 0.36320218, 4.67037]\n",
      "497: Train Loss: [2.5157857, 0.38049856, 4.651073] | Test Loss: [2.5700977, 0.32504275, 4.8151526]\n",
      "498: Train Loss: [2.4312987, 0.31158772, 4.5510097] | Test Loss: [2.7255185, 0.41261938, 5.0384173]\n",
      "499: Train Loss: [2.3743706, 0.39738664, 4.3513546] | Test Loss: [2.5068734, 0.3356623, 4.6780844]\n",
      "500: Train Loss: [2.3713043, 0.3565056, 4.386103] | Test Loss: [2.5629623, 0.46183428, 4.66409]\n",
      "501: Train Loss: [2.5019174, 0.3875158, 4.6163187] | Test Loss: [2.7321942, 0.36281684, 5.1015716]\n",
      "502: Train Loss: [2.418789, 0.3661428, 4.471435] | Test Loss: [2.7887254, 0.47686014, 5.1005907]\n",
      "503: Train Loss: [2.5615494, 0.40888512, 4.714214] | Test Loss: [2.5800195, 0.42561147, 4.7344275]\n",
      "504: Train Loss: [2.336253, 0.33750403, 4.335002] | Test Loss: [2.7790782, 0.37149057, 5.186666]\n",
      "505: Train Loss: [2.4608932, 0.37251344, 4.549273] | Test Loss: [2.5909047, 0.3135052, 4.8683043]\n",
      "506: Train Loss: [2.2730927, 0.32661858, 4.219567] | Test Loss: [2.782266, 0.48516685, 5.079365]\n",
      "507: Train Loss: [2.3540726, 0.3980634, 4.310082] | Test Loss: [2.5961952, 0.32052216, 4.871868]\n",
      "508: Train Loss: [2.272279, 0.3554693, 4.189089] | Test Loss: [2.6488023, 0.2870284, 5.0105762]\n",
      "509: Train Loss: [2.2659132, 0.38005334, 4.151773] | Test Loss: [2.511755, 0.32291624, 4.700594]\n",
      "510: Train Loss: [2.4789941, 0.3433467, 4.6146417] | Test Loss: [2.6423707, 0.31624752, 4.968494]\n",
      "511: Train Loss: [2.375405, 0.3267039, 4.424106] | Test Loss: [2.5222683, 0.43201274, 4.612524]\n",
      "512: Train Loss: [2.4908848, 0.46349928, 4.5182705] | Test Loss: [2.690007, 0.3966905, 4.9833236]\n",
      "513: Train Loss: [2.2916505, 0.3491096, 4.2341914] | Test Loss: [2.615987, 0.43543592, 4.7965384]\n",
      "514: Train Loss: [2.356017, 0.30903953, 4.4029946] | Test Loss: [2.5427485, 0.39069086, 4.694806]\n",
      "515: Train Loss: [2.2442374, 0.3431251, 4.14535] | Test Loss: [2.5262587, 0.3478623, 4.704655]\n",
      "516: Train Loss: [2.433158, 0.29302505, 4.573291] | Test Loss: [2.6064484, 0.38114795, 4.831749]\n",
      "517: Train Loss: [2.5603676, 0.33401003, 4.786725] | Test Loss: [2.5360825, 0.3694792, 4.702686]\n",
      "518: Train Loss: [2.4642138, 0.46132454, 4.467103] | Test Loss: [2.5532196, 0.33705392, 4.7693853]\n",
      "519: Train Loss: [2.418685, 0.33061308, 4.506757] | Test Loss: [2.6779451, 0.33978966, 5.0161004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520: Train Loss: [2.4525106, 0.33753017, 4.567491] | Test Loss: [2.451749, 0.38795573, 4.5155425]\n",
      "521: Train Loss: [2.3859138, 0.3519194, 4.419908] | Test Loss: [2.6793706, 0.41464046, 4.944101]\n",
      "522: Train Loss: [2.338127, 0.31198537, 4.3642683] | Test Loss: [2.686721, 0.37998745, 4.993455]\n",
      "523: Train Loss: [2.5208204, 0.33260643, 4.7090344] | Test Loss: [2.4836147, 0.32471877, 4.6425104]\n",
      "524: Train Loss: [2.3361201, 0.37976193, 4.292478] | Test Loss: [2.617894, 0.34752527, 4.8882627]\n",
      "Epoch 15\n",
      "0: Train Loss: [2.091582, 0.342225, 3.8409393] | Test Loss: [2.5724847, 0.36779955, 4.7771697]\n",
      "1: Train Loss: [2.2217429, 0.3282851, 4.1152005] | Test Loss: [2.6654527, 0.35480335, 4.976102]\n",
      "2: Train Loss: [2.2796412, 0.29050118, 4.268781] | Test Loss: [2.6122482, 0.33884725, 4.885649]\n",
      "3: Train Loss: [2.3432705, 0.36631033, 4.320231] | Test Loss: [2.6851394, 0.34964228, 5.0206366]\n",
      "4: Train Loss: [2.3462906, 0.3877912, 4.30479] | Test Loss: [2.5833585, 0.3539416, 4.8127756]\n",
      "5: Train Loss: [2.1468468, 0.29490024, 3.9987934] | Test Loss: [2.6777623, 0.35708424, 4.9984403]\n",
      "6: Train Loss: [2.2706256, 0.49268618, 4.048565] | Test Loss: [2.0884328, 0.42098144, 3.7558842]\n",
      "7: Train Loss: [2.240136, 0.37091, 4.1093616] | Test Loss: [2.645635, 0.40439412, 4.8868756]\n",
      "8: Train Loss: [2.1716561, 0.37209383, 3.9712186] | Test Loss: [2.5591073, 0.36304265, 4.755172]\n",
      "9: Train Loss: [2.1718893, 0.43337384, 3.9104047] | Test Loss: [2.7268562, 0.3813754, 5.072337]\n",
      "10: Train Loss: [2.3269293, 0.3532548, 4.300604] | Test Loss: [2.6148894, 0.3578423, 4.8719363]\n",
      "11: Train Loss: [2.2564163, 0.3691856, 4.143647] | Test Loss: [2.7293038, 0.43442905, 5.0241785]\n",
      "12: Train Loss: [2.3938386, 0.35981587, 4.427861] | Test Loss: [2.6272464, 0.34557578, 4.908917]\n",
      "13: Train Loss: [2.294588, 0.31084928, 4.278327] | Test Loss: [2.6003199, 0.4347652, 4.7658744]\n",
      "14: Train Loss: [2.2315376, 0.37647069, 4.0866046] | Test Loss: [2.4690485, 0.3535228, 4.584574]\n",
      "15: Train Loss: [2.2003624, 0.36967316, 4.0310516] | Test Loss: [2.3808951, 0.34563515, 4.4161553]\n",
      "16: Train Loss: [2.1443365, 0.31768975, 3.9709833] | Test Loss: [2.8668156, 0.37485412, 5.358777]\n",
      "17: Train Loss: [2.3167598, 0.35886288, 4.274657] | Test Loss: [2.809651, 0.32801786, 5.291284]\n",
      "18: Train Loss: [2.21042, 0.36422142, 4.056618] | Test Loss: [2.6646879, 0.37501055, 4.9543653]\n",
      "19: Train Loss: [2.2983916, 0.31653136, 4.280252] | Test Loss: [2.6022274, 0.3844295, 4.8200254]\n",
      "20: Train Loss: [2.2560396, 0.34447294, 4.1676064] | Test Loss: [2.5217083, 0.36176533, 4.681651]\n",
      "21: Train Loss: [2.3565915, 0.34858587, 4.364597] | Test Loss: [2.5651147, 0.35828027, 4.7719493]\n",
      "22: Train Loss: [2.216205, 0.41357934, 4.0188303] | Test Loss: [2.643968, 0.37813678, 4.9097996]\n",
      "23: Train Loss: [2.20097, 0.3248554, 4.0770845] | Test Loss: [2.5890763, 0.35271937, 4.8254333]\n",
      "24: Train Loss: [2.3451056, 0.4697721, 4.220439] | Test Loss: [2.6270716, 0.44523314, 4.80891]\n",
      "25: Train Loss: [2.2796223, 0.43110636, 4.128138] | Test Loss: [2.608965, 0.37331372, 4.844616]\n",
      "26: Train Loss: [2.3476515, 0.35039732, 4.344906] | Test Loss: [2.7281768, 0.39793453, 5.058419]\n",
      "27: Train Loss: [2.2969315, 0.32552186, 4.268341] | Test Loss: [2.6959698, 0.41190043, 4.980039]\n",
      "28: Train Loss: [2.2029667, 0.32617766, 4.079756] | Test Loss: [2.542715, 0.33201158, 4.7534184]\n",
      "29: Train Loss: [2.0516646, 0.3781348, 3.7251945] | Test Loss: [2.6577091, 0.39205542, 4.9233627]\n",
      "30: Train Loss: [2.1980786, 0.35415456, 4.0420027] | Test Loss: [2.6158993, 0.34363216, 4.8881664]\n",
      "31: Train Loss: [2.318951, 0.40909228, 4.2288094] | Test Loss: [2.5878563, 0.32826257, 4.84745]\n",
      "32: Train Loss: [2.2111497, 0.36602807, 4.056271] | Test Loss: [2.5455463, 0.30482438, 4.786268]\n",
      "33: Train Loss: [2.4304783, 0.41058934, 4.4503675] | Test Loss: [2.5620496, 0.38949004, 4.734609]\n",
      "34: Train Loss: [2.093223, 0.33176225, 3.854684] | Test Loss: [2.5998743, 0.38412192, 4.8156266]\n",
      "35: Train Loss: [2.2249606, 0.3439085, 4.106013] | Test Loss: [2.7321308, 0.35527137, 5.10899]\n",
      "36: Train Loss: [2.2736742, 0.34619462, 4.2011538] | Test Loss: [2.9013364, 0.58710325, 5.2155695]\n",
      "37: Train Loss: [2.3012528, 0.29984027, 4.302665] | Test Loss: [2.6690824, 0.37952158, 4.9586434]\n",
      "38: Train Loss: [2.390917, 0.3592585, 4.4225755] | Test Loss: [2.5241535, 0.3997443, 4.6485624]\n",
      "39: Train Loss: [2.215379, 0.34199, 4.088768] | Test Loss: [2.6635587, 0.43254027, 4.894577]\n",
      "40: Train Loss: [2.3232913, 0.33777615, 4.3088064] | Test Loss: [2.418064, 0.42449865, 4.4116297]\n",
      "41: Train Loss: [2.207284, 0.30695567, 4.107612] | Test Loss: [2.7250364, 0.3489469, 5.1011257]\n",
      "42: Train Loss: [2.2725284, 0.42678812, 4.1182685] | Test Loss: [2.4657907, 0.34577352, 4.585808]\n",
      "43: Train Loss: [2.1501408, 0.3210948, 3.9791865] | Test Loss: [2.5627756, 0.31340975, 4.8121414]\n",
      "44: Train Loss: [2.0625594, 0.37784678, 3.747272] | Test Loss: [2.543628, 0.36431822, 4.7229376]\n",
      "45: Train Loss: [2.2068427, 0.3335825, 4.080103] | Test Loss: [2.7438142, 0.37285396, 5.1147747]\n",
      "46: Train Loss: [2.3166974, 0.37523317, 4.2581615] | Test Loss: [2.7435596, 0.36790544, 5.1192136]\n",
      "47: Train Loss: [2.2712848, 0.32515514, 4.2174144] | Test Loss: [2.4857478, 0.34681898, 4.6246767]\n",
      "48: Train Loss: [2.2679076, 0.36933947, 4.166476] | Test Loss: [2.5846303, 0.42761153, 4.741649]\n",
      "49: Train Loss: [2.4225988, 0.43821234, 4.4069853] | Test Loss: [2.3606372, 0.4339079, 4.2873664]\n",
      "50: Train Loss: [2.0752761, 0.32138288, 3.8291695] | Test Loss: [2.5964127, 0.44029358, 4.7525315]\n",
      "51: Train Loss: [2.223692, 0.33928788, 4.108096] | Test Loss: [2.7065086, 0.33040226, 5.082615]\n",
      "52: Train Loss: [2.330701, 0.42102444, 4.240378] | Test Loss: [2.9106903, 0.50134116, 5.3200393]\n",
      "53: Train Loss: [2.2606184, 0.39315522, 4.128082] | Test Loss: [2.8847992, 0.39669663, 5.372902]\n",
      "54: Train Loss: [2.3022192, 0.36482984, 4.2396083] | Test Loss: [2.4464989, 0.3392144, 4.5537834]\n",
      "55: Train Loss: [2.3408327, 0.3126749, 4.3689904] | Test Loss: [2.4374774, 0.3800292, 4.4949255]\n",
      "56: Train Loss: [2.3002315, 0.43225408, 4.1682086] | Test Loss: [2.5429156, 0.35533932, 4.7304916]\n",
      "57: Train Loss: [2.4379146, 0.45645183, 4.4193773] | Test Loss: [2.6476047, 0.3596168, 4.9355927]\n",
      "58: Train Loss: [2.3347, 0.33021694, 4.3391833] | Test Loss: [2.5000184, 0.38960215, 4.6104345]\n",
      "59: Train Loss: [2.2950425, 0.38326055, 4.2068243] | Test Loss: [2.6694257, 0.34049904, 4.9983525]\n",
      "60: Train Loss: [2.2868845, 0.32231134, 4.2514577] | Test Loss: [2.4863234, 0.34523997, 4.6274066]\n",
      "61: Train Loss: [2.2561822, 0.38907316, 4.123291] | Test Loss: [2.659022, 0.34860474, 4.9694395]\n",
      "62: Train Loss: [2.2269242, 0.3781064, 4.075742] | Test Loss: [2.7337866, 0.3577311, 5.1098423]\n",
      "63: Train Loss: [2.2029474, 0.3340267, 4.071868] | Test Loss: [2.7955966, 0.28145546, 5.3097377]\n",
      "64: Train Loss: [2.341204, 0.3266069, 4.355801] | Test Loss: [2.6829844, 0.35008883, 5.01588]\n",
      "65: Train Loss: [2.207589, 0.3636461, 4.051532] | Test Loss: [2.6846354, 0.3075894, 5.0616813]\n",
      "66: Train Loss: [2.2490032, 0.3708755, 4.127131] | Test Loss: [2.3994288, 0.39119816, 4.4076595]\n",
      "67: Train Loss: [2.2639318, 0.3624083, 4.1654553] | Test Loss: [2.7418852, 0.4728812, 5.010889]\n",
      "68: Train Loss: [2.3434553, 0.38635382, 4.3005567] | Test Loss: [2.670248, 0.4356248, 4.9048715]\n",
      "69: Train Loss: [2.339501, 0.32096413, 4.3580375] | Test Loss: [2.5626884, 0.36204842, 4.763328]\n",
      "70: Train Loss: [2.3480792, 0.39102787, 4.3051305] | Test Loss: [2.7858174, 0.44406933, 5.1275654]\n",
      "71: Train Loss: [2.2833302, 0.5493757, 4.017285] | Test Loss: [2.7169557, 0.3177285, 5.116183]\n",
      "72: Train Loss: [2.1410606, 0.4122329, 3.8698883] | Test Loss: [2.5906339, 0.37976104, 4.8015065]\n",
      "73: Train Loss: [2.2300904, 0.38108686, 4.079094] | Test Loss: [2.4983435, 0.37137774, 4.625309]\n",
      "74: Train Loss: [2.288846, 0.38228306, 4.195409] | Test Loss: [2.4150708, 0.36440915, 4.4657326]\n",
      "75: Train Loss: [2.213366, 0.33092415, 4.095808] | Test Loss: [2.7145958, 0.38282415, 5.0463676]\n",
      "76: Train Loss: [2.291981, 0.3214523, 4.26251] | Test Loss: [2.4361236, 0.4106592, 4.461588]\n",
      "77: Train Loss: [2.2294686, 0.5068754, 3.9520617] | Test Loss: [2.7980297, 0.42566612, 5.170393]\n",
      "78: Train Loss: [2.2365232, 0.32061705, 4.152429] | Test Loss: [2.72685, 0.35561264, 5.0980873]\n",
      "79: Train Loss: [2.3561, 0.40974307, 4.3024573] | Test Loss: [2.6845665, 0.31001422, 5.0591187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80: Train Loss: [2.234456, 0.42308295, 4.0458293] | Test Loss: [2.512283, 0.33515072, 4.6894155]\n",
      "81: Train Loss: [2.2010005, 0.36496982, 4.037031] | Test Loss: [2.5408025, 0.3414079, 4.740197]\n",
      "82: Train Loss: [2.3546631, 0.34858242, 4.360744] | Test Loss: [2.7127383, 0.36895135, 5.056525]\n",
      "83: Train Loss: [2.28404, 0.41660428, 4.151476] | Test Loss: [2.6355743, 0.4070977, 4.864051]\n",
      "84: Train Loss: [2.3807528, 0.38164133, 4.379864] | Test Loss: [2.546786, 0.36605436, 4.7275176]\n",
      "85: Train Loss: [2.2589045, 0.35371643, 4.1640925] | Test Loss: [2.5262883, 0.3654889, 4.6870875]\n",
      "86: Train Loss: [2.2829409, 0.3776977, 4.1881843] | Test Loss: [2.6496007, 0.35436562, 4.9448357]\n",
      "87: Train Loss: [2.2885082, 0.36580685, 4.2112093] | Test Loss: [2.5203342, 0.35407156, 4.686597]\n",
      "88: Train Loss: [2.1785893, 0.3912867, 3.965892] | Test Loss: [2.6569684, 0.34356788, 4.970369]\n",
      "89: Train Loss: [2.33232, 0.43668893, 4.227951] | Test Loss: [2.6187139, 0.46174285, 4.775685]\n",
      "90: Train Loss: [2.4265516, 0.35623008, 4.496873] | Test Loss: [2.6988194, 0.3278634, 5.0697756]\n",
      "91: Train Loss: [2.3846543, 0.4050383, 4.36427] | Test Loss: [2.8988624, 0.44328123, 5.3544436]\n",
      "92: Train Loss: [2.1729462, 0.3720639, 3.9738286] | Test Loss: [2.5615, 0.38091698, 4.742083]\n",
      "93: Train Loss: [2.3066025, 0.36817637, 4.2450285] | Test Loss: [2.7252016, 0.3612618, 5.0891414]\n",
      "94: Train Loss: [2.2606804, 0.33563316, 4.1857276] | Test Loss: [2.470889, 0.37561402, 4.566164]\n",
      "95: Train Loss: [2.2184007, 0.39195094, 4.0448503] | Test Loss: [2.5284038, 0.33248588, 4.724322]\n",
      "96: Train Loss: [2.3050885, 0.3567634, 4.2534137] | Test Loss: [2.828703, 0.3760652, 5.2813406]\n",
      "97: Train Loss: [2.3380969, 0.3809803, 4.295213] | Test Loss: [2.4794023, 0.32418817, 4.6346164]\n",
      "98: Train Loss: [2.2781932, 0.4155253, 4.140861] | Test Loss: [2.6866155, 0.37637705, 4.996854]\n",
      "99: Train Loss: [2.25317, 0.31851307, 4.187827] | Test Loss: [2.6488745, 0.39685658, 4.9008923]\n",
      "100: Train Loss: [2.2184384, 0.35014388, 4.086733] | Test Loss: [2.5970645, 0.4250514, 4.769078]\n",
      "101: Train Loss: [2.2264562, 0.42154127, 4.031371] | Test Loss: [2.5602517, 0.3356302, 4.784873]\n",
      "102: Train Loss: [2.3022091, 0.34957066, 4.2548475] | Test Loss: [2.8309243, 0.39633158, 5.2655168]\n",
      "103: Train Loss: [2.1932023, 0.3590456, 4.027359] | Test Loss: [2.6455505, 0.38637787, 4.904723]\n",
      "104: Train Loss: [2.283126, 0.41080037, 4.155452] | Test Loss: [2.674197, 0.38600284, 4.962391]\n",
      "105: Train Loss: [2.3838844, 0.362831, 4.4049377] | Test Loss: [2.4220967, 0.4285934, 4.4156]\n",
      "106: Train Loss: [2.303528, 0.3031004, 4.3039556] | Test Loss: [2.5289798, 0.33386776, 4.724092]\n",
      "107: Train Loss: [2.3761952, 0.34434092, 4.4080496] | Test Loss: [2.62575, 0.33785003, 4.91365]\n",
      "108: Train Loss: [2.3539586, 0.3522248, 4.3556924] | Test Loss: [2.4966328, 0.30975524, 4.6835103]\n",
      "109: Train Loss: [2.3692772, 0.30981067, 4.428744] | Test Loss: [2.4784513, 0.36746967, 4.5894327]\n",
      "110: Train Loss: [2.3600926, 0.3481147, 4.372071] | Test Loss: [2.6487496, 0.34051588, 4.956983]\n",
      "111: Train Loss: [2.2680547, 0.4046825, 4.131427] | Test Loss: [2.65412, 0.37660095, 4.931639]\n",
      "112: Train Loss: [2.1878245, 0.28034452, 4.0953045] | Test Loss: [2.4465542, 0.36998728, 4.523121]\n",
      "113: Train Loss: [2.2036629, 0.34854627, 4.0587792] | Test Loss: [2.5736582, 0.36589196, 4.7814245]\n",
      "114: Train Loss: [2.1943066, 0.38559148, 4.0030217] | Test Loss: [2.5585377, 0.28844392, 4.8286314]\n",
      "115: Train Loss: [2.1163225, 0.29651412, 3.9361308] | Test Loss: [2.7028036, 0.34942004, 5.056187]\n",
      "116: Train Loss: [2.382161, 0.32298782, 4.441334] | Test Loss: [2.7002132, 0.36757925, 5.032847]\n",
      "117: Train Loss: [2.105753, 0.2640366, 3.9474695] | Test Loss: [2.5908217, 0.43981677, 4.7418265]\n",
      "118: Train Loss: [2.298804, 0.2828191, 4.314789] | Test Loss: [2.6855073, 0.34296402, 5.0280504]\n",
      "119: Train Loss: [2.2128377, 0.35854858, 4.0671268] | Test Loss: [2.8505933, 0.55377644, 5.1474104]\n",
      "120: Train Loss: [2.3388817, 0.38334933, 4.294414] | Test Loss: [2.6773694, 0.378016, 4.9767227]\n",
      "121: Train Loss: [2.2335994, 0.32740927, 4.1397896] | Test Loss: [2.6458569, 0.34958398, 4.9421296]\n",
      "122: Train Loss: [2.2701044, 0.35501283, 4.185196] | Test Loss: [2.5961142, 0.40700507, 4.785223]\n",
      "123: Train Loss: [2.299988, 0.34516653, 4.2548094] | Test Loss: [2.6898859, 0.30684873, 5.072923]\n",
      "124: Train Loss: [2.351512, 0.35778913, 4.345235] | Test Loss: [2.5951655, 0.39970362, 4.7906275]\n",
      "125: Train Loss: [2.2319765, 0.3426566, 4.1212964] | Test Loss: [2.7787662, 0.41654092, 5.140991]\n",
      "126: Train Loss: [2.2822905, 0.33858484, 4.225996] | Test Loss: [2.1040485, 0.3219313, 3.8861659]\n",
      "127: Train Loss: [2.4215238, 0.38966233, 4.4533854] | Test Loss: [2.6936393, 0.36085305, 5.0264254]\n",
      "128: Train Loss: [2.3746932, 0.30805078, 4.4413357] | Test Loss: [2.6409993, 0.47521707, 4.806782]\n",
      "129: Train Loss: [2.2239778, 0.33735678, 4.110599] | Test Loss: [2.8292081, 0.36108944, 5.297327]\n",
      "130: Train Loss: [2.3437386, 0.35464695, 4.33283] | Test Loss: [2.750759, 0.41543385, 5.086084]\n",
      "131: Train Loss: [2.2670538, 0.311338, 4.2227697] | Test Loss: [2.6800327, 0.37938046, 4.9806848]\n",
      "132: Train Loss: [2.2636547, 0.3115691, 4.21574] | Test Loss: [2.468044, 0.34670383, 4.589384]\n",
      "133: Train Loss: [2.5040517, 0.31714204, 4.6909614] | Test Loss: [2.5978389, 0.37670565, 4.818972]\n",
      "134: Train Loss: [2.2954776, 0.34241405, 4.2485414] | Test Loss: [2.6326544, 0.33869556, 4.9266133]\n",
      "135: Train Loss: [2.1804137, 0.35626683, 4.0045605] | Test Loss: [2.6453795, 0.36017358, 4.9305854]\n",
      "136: Train Loss: [2.406463, 0.3647665, 4.448159] | Test Loss: [2.6698923, 0.44216025, 4.8976245]\n",
      "137: Train Loss: [2.27484, 0.46723744, 4.0824428] | Test Loss: [2.584199, 0.37684616, 4.7915516]\n",
      "138: Train Loss: [2.3018475, 0.39176378, 4.211931] | Test Loss: [2.649617, 0.41420498, 4.885029]\n",
      "139: Train Loss: [2.3026788, 0.30739775, 4.29796] | Test Loss: [2.3739293, 0.43760237, 4.310256]\n",
      "140: Train Loss: [2.3985276, 0.32316574, 4.4738894] | Test Loss: [2.565403, 0.3273388, 4.8034673]\n",
      "141: Train Loss: [2.2832952, 0.317899, 4.248691] | Test Loss: [2.5362446, 0.3547738, 4.7177153]\n",
      "142: Train Loss: [2.402858, 0.30802447, 4.4976916] | Test Loss: [2.6961608, 0.3565564, 5.035765]\n",
      "143: Train Loss: [2.2414026, 0.41151324, 4.071292] | Test Loss: [2.683917, 0.40315288, 4.964681]\n",
      "144: Train Loss: [2.2257702, 0.31589207, 4.1356483] | Test Loss: [2.6022863, 0.36601362, 4.838559]\n",
      "145: Train Loss: [2.342532, 0.35035032, 4.3347135] | Test Loss: [2.7619553, 0.37376514, 5.1501455]\n",
      "146: Train Loss: [2.275001, 0.3882499, 4.161752] | Test Loss: [2.4435449, 0.45033133, 4.4367585]\n",
      "147: Train Loss: [2.2700543, 0.3653246, 4.174784] | Test Loss: [2.7706766, 0.33077574, 5.2105775]\n",
      "148: Train Loss: [2.472326, 0.36335337, 4.581299] | Test Loss: [2.5892587, 0.32380432, 4.854713]\n",
      "149: Train Loss: [2.2888713, 0.35424963, 4.223493] | Test Loss: [2.548274, 0.33873293, 4.7578154]\n",
      "150: Train Loss: [2.5044913, 0.46423876, 4.544744] | Test Loss: [2.6289034, 0.3964003, 4.8614063]\n",
      "151: Train Loss: [2.3477895, 0.3495666, 4.3460126] | Test Loss: [2.6940954, 0.34179717, 5.0463934]\n",
      "152: Train Loss: [2.2296283, 0.3757901, 4.0834665] | Test Loss: [2.6211345, 0.40583712, 4.836432]\n",
      "153: Train Loss: [2.2687602, 0.3630247, 4.1744957] | Test Loss: [2.6722622, 0.34296107, 5.001563]\n",
      "154: Train Loss: [2.270735, 0.35303554, 4.1884346] | Test Loss: [2.5680366, 0.39543623, 4.740637]\n",
      "155: Train Loss: [2.2663326, 0.32517564, 4.2074895] | Test Loss: [2.5106096, 0.39076, 4.6304593]\n",
      "156: Train Loss: [2.0781531, 0.3082416, 3.848065] | Test Loss: [2.1565163, 0.35615596, 3.9568765]\n",
      "157: Train Loss: [2.1614795, 0.37436926, 3.9485898] | Test Loss: [2.7343507, 0.40295735, 5.065744]\n",
      "158: Train Loss: [2.4290926, 0.37679943, 4.4813857] | Test Loss: [2.579984, 0.44476494, 4.715203]\n",
      "159: Train Loss: [2.2377045, 0.38973626, 4.085673] | Test Loss: [2.5989034, 0.34091428, 4.8568926]\n",
      "160: Train Loss: [2.2821293, 0.45166484, 4.1125937] | Test Loss: [2.5415952, 0.41277626, 4.670414]\n",
      "161: Train Loss: [2.4774587, 0.47617212, 4.4787455] | Test Loss: [2.5798516, 0.38393465, 4.7757688]\n",
      "162: Train Loss: [2.4350767, 0.4176092, 4.452544] | Test Loss: [2.4829175, 0.40645838, 4.5593767]\n",
      "163: Train Loss: [2.3657339, 0.41166633, 4.3198013] | Test Loss: [2.5101724, 0.39556614, 4.6247787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164: Train Loss: [2.2650344, 0.38880578, 4.141263] | Test Loss: [2.533737, 0.34758022, 4.7198935]\n",
      "165: Train Loss: [2.2831044, 0.35767213, 4.2085366] | Test Loss: [2.8671973, 0.45473635, 5.2796583]\n",
      "166: Train Loss: [2.3659716, 0.36516833, 4.366775] | Test Loss: [2.6879523, 0.39339542, 4.982509]\n",
      "167: Train Loss: [2.4358113, 0.3260633, 4.5455594] | Test Loss: [2.5457704, 0.3517996, 4.7397413]\n",
      "168: Train Loss: [2.3540735, 0.40481558, 4.3033314] | Test Loss: [2.654831, 0.36428005, 4.9453816]\n",
      "169: Train Loss: [2.2444327, 0.40361613, 4.0852494] | Test Loss: [2.5062008, 0.49543992, 4.5169616]\n",
      "170: Train Loss: [2.4753408, 0.43920058, 4.5114813] | Test Loss: [2.6169865, 0.4425158, 4.791457]\n",
      "171: Train Loss: [2.3011587, 0.34651363, 4.2558036] | Test Loss: [2.7839148, 0.3563453, 5.2114844]\n",
      "172: Train Loss: [2.4813268, 0.41254365, 4.55011] | Test Loss: [2.497339, 0.3196968, 4.674981]\n",
      "173: Train Loss: [2.182897, 0.33703876, 4.0287557] | Test Loss: [2.5330591, 0.35016844, 4.71595]\n",
      "174: Train Loss: [2.4105487, 0.3506389, 4.4704585] | Test Loss: [2.6647148, 0.4117165, 4.917713]\n",
      "175: Train Loss: [2.195565, 0.35778466, 4.033345] | Test Loss: [2.745116, 0.38247457, 5.1077576]\n",
      "176: Train Loss: [2.3817239, 0.37234908, 4.3910985] | Test Loss: [2.7036214, 0.34802666, 5.059216]\n",
      "177: Train Loss: [2.4368136, 0.39959282, 4.4740343] | Test Loss: [2.6036458, 0.36130193, 4.8459897]\n",
      "178: Train Loss: [2.3178453, 0.4051085, 4.230582] | Test Loss: [2.6411655, 0.36219972, 4.920131]\n",
      "179: Train Loss: [2.4289908, 0.38859528, 4.4693866] | Test Loss: [2.6156359, 0.3578528, 4.873419]\n",
      "180: Train Loss: [2.4941735, 0.3986892, 4.589658] | Test Loss: [2.7517133, 0.43088558, 5.0725408]\n",
      "181: Train Loss: [2.3284225, 0.34032416, 4.3165207] | Test Loss: [2.7594538, 0.4074574, 5.11145]\n",
      "182: Train Loss: [2.3608308, 0.46961394, 4.2520475] | Test Loss: [2.562663, 0.3702481, 4.755078]\n",
      "183: Train Loss: [2.2553434, 0.3228291, 4.1878576] | Test Loss: [2.746124, 0.34328997, 5.148958]\n",
      "184: Train Loss: [2.4090388, 0.41793555, 4.400142] | Test Loss: [2.71477, 0.4750213, 4.954519]\n",
      "185: Train Loss: [2.5095084, 0.38027164, 4.6387453] | Test Loss: [2.5142524, 0.3443567, 4.6841483]\n",
      "186: Train Loss: [2.32098, 0.35834673, 4.283613] | Test Loss: [2.6686416, 0.32329023, 5.013993]\n",
      "187: Train Loss: [2.2598941, 0.34642097, 4.1733675] | Test Loss: [2.6573691, 0.28693056, 5.0278077]\n",
      "188: Train Loss: [2.33214, 0.37630296, 4.2879767] | Test Loss: [2.630553, 0.38246146, 4.8786445]\n",
      "189: Train Loss: [2.234844, 0.3899826, 4.079705] | Test Loss: [2.6173081, 0.34985998, 4.884756]\n",
      "190: Train Loss: [2.3492508, 0.36306992, 4.3354316] | Test Loss: [2.6413403, 0.34638104, 4.9362993]\n",
      "191: Train Loss: [2.2833328, 0.3639529, 4.2027125] | Test Loss: [2.4760365, 0.35175505, 4.600318]\n",
      "192: Train Loss: [2.3434868, 0.5218857, 4.1650877] | Test Loss: [2.7258492, 0.4300621, 5.021636]\n",
      "193: Train Loss: [2.4157736, 0.35909003, 4.4724574] | Test Loss: [2.4547439, 0.37779358, 4.531694]\n",
      "194: Train Loss: [2.1838317, 0.35719803, 4.010465] | Test Loss: [2.558292, 0.3520667, 4.7645173]\n",
      "195: Train Loss: [2.320742, 0.37916124, 4.2623224] | Test Loss: [2.7016072, 0.41814476, 4.9850698]\n",
      "196: Train Loss: [2.3625069, 0.3650862, 4.3599277] | Test Loss: [2.628994, 0.39291275, 4.865075]\n",
      "197: Train Loss: [2.4368992, 0.43591595, 4.4378824] | Test Loss: [2.6416297, 0.39980847, 4.883451]\n",
      "198: Train Loss: [2.4883654, 0.4011205, 4.57561] | Test Loss: [2.5621119, 0.3567329, 4.767491]\n",
      "199: Train Loss: [2.4045994, 0.36156103, 4.447638] | Test Loss: [2.685149, 0.3503008, 5.019997]\n",
      "200: Train Loss: [2.2189019, 0.33620167, 4.101602] | Test Loss: [2.6121278, 0.41008818, 4.8141675]\n",
      "201: Train Loss: [2.3697984, 0.42700937, 4.3125873] | Test Loss: [2.510466, 0.3138356, 4.7070966]\n",
      "202: Train Loss: [2.3728955, 0.26979125, 4.476] | Test Loss: [2.758824, 0.35944033, 5.158208]\n",
      "203: Train Loss: [2.3902187, 0.32250193, 4.4579353] | Test Loss: [2.6828897, 0.34279305, 5.0229864]\n",
      "204: Train Loss: [2.2615995, 0.2802625, 4.2429366] | Test Loss: [2.5866067, 0.3399503, 4.8332634]\n",
      "205: Train Loss: [2.1686661, 0.34192285, 3.9954095] | Test Loss: [2.5800307, 0.33618158, 4.8238797]\n",
      "206: Train Loss: [2.290645, 0.3133443, 4.2679453] | Test Loss: [2.6436315, 0.43278265, 4.8544803]\n",
      "207: Train Loss: [2.3998358, 0.5836941, 4.2159777] | Test Loss: [2.6057956, 0.41848114, 4.79311]\n",
      "208: Train Loss: [2.202262, 0.3920023, 4.0125217] | Test Loss: [2.6464148, 0.3428088, 4.950021]\n",
      "209: Train Loss: [2.276416, 0.3720242, 4.180808] | Test Loss: [2.7274685, 0.40696898, 5.047968]\n",
      "210: Train Loss: [2.3874192, 0.39578676, 4.3790517] | Test Loss: [2.7477632, 0.3740392, 5.121487]\n",
      "211: Train Loss: [2.324442, 0.31673926, 4.3321447] | Test Loss: [2.6695259, 0.42855448, 4.910497]\n",
      "212: Train Loss: [2.4144077, 0.35970297, 4.4691124] | Test Loss: [2.5920427, 0.41866586, 4.7654195]\n",
      "213: Train Loss: [2.228755, 0.35818598, 4.099324] | Test Loss: [2.572055, 0.34474623, 4.799364]\n",
      "214: Train Loss: [2.4191449, 0.3858104, 4.4524794] | Test Loss: [2.5592504, 0.40106472, 4.717436]\n",
      "215: Train Loss: [2.3540504, 0.33396414, 4.3741364] | Test Loss: [2.643901, 0.31319648, 4.9746056]\n",
      "216: Train Loss: [2.4048321, 0.42980707, 4.379857] | Test Loss: [2.3836231, 0.46105528, 4.306191]\n",
      "217: Train Loss: [2.358865, 0.37681004, 4.34092] | Test Loss: [2.6588016, 0.38776755, 4.9298353]\n",
      "218: Train Loss: [2.306133, 0.41502464, 4.1972413] | Test Loss: [2.7333593, 0.42261067, 5.044108]\n",
      "219: Train Loss: [2.3982027, 0.3422604, 4.454145] | Test Loss: [2.7138014, 0.35272703, 5.074876]\n",
      "220: Train Loss: [2.3040614, 0.36490965, 4.243213] | Test Loss: [2.4742186, 0.40738073, 4.5410566]\n",
      "221: Train Loss: [2.3561714, 0.30070835, 4.4116344] | Test Loss: [2.6152506, 0.3263579, 4.9041433]\n",
      "222: Train Loss: [2.2990403, 0.3313857, 4.266695] | Test Loss: [2.5452979, 0.39345405, 4.6971416]\n",
      "223: Train Loss: [2.300861, 0.30040285, 4.301319] | Test Loss: [2.8407388, 0.48776704, 5.1937103]\n",
      "224: Train Loss: [2.2361448, 0.33768666, 4.134603] | Test Loss: [2.5570617, 0.32328516, 4.7908382]\n",
      "225: Train Loss: [2.3334568, 0.36955717, 4.297356] | Test Loss: [2.5810828, 0.32042587, 4.8417397]\n",
      "226: Train Loss: [2.3231065, 0.37731564, 4.2688975] | Test Loss: [2.705981, 0.4091563, 5.0028057]\n",
      "227: Train Loss: [2.2551322, 0.38569817, 4.124566] | Test Loss: [2.8208683, 0.46987855, 5.171858]\n",
      "228: Train Loss: [2.4455645, 0.37462366, 4.5165052] | Test Loss: [2.4822664, 0.37456936, 4.5899634]\n",
      "229: Train Loss: [2.2656693, 0.34633577, 4.185003] | Test Loss: [2.5037277, 0.34486717, 4.662588]\n",
      "230: Train Loss: [2.1640642, 0.32673636, 4.001392] | Test Loss: [2.6796043, 0.39308017, 4.9661283]\n",
      "231: Train Loss: [2.2256117, 0.3926723, 4.058551] | Test Loss: [2.6778073, 0.3830956, 4.972519]\n",
      "232: Train Loss: [2.3625433, 0.32681546, 4.398271] | Test Loss: [2.5528412, 0.41348878, 4.6921935]\n",
      "233: Train Loss: [2.2613125, 0.3898513, 4.132774] | Test Loss: [2.7775986, 0.3107482, 5.244449]\n",
      "234: Train Loss: [2.4256124, 0.36413833, 4.487087] | Test Loss: [2.6152234, 0.3763421, 4.8541045]\n",
      "235: Train Loss: [2.3560297, 0.3304285, 4.381631] | Test Loss: [2.7374222, 0.3781834, 5.096661]\n",
      "236: Train Loss: [2.5871425, 0.3670501, 4.807235] | Test Loss: [2.5371315, 0.32849905, 4.7457643]\n",
      "237: Train Loss: [2.1498148, 0.29010448, 4.0095253] | Test Loss: [2.5919845, 0.358851, 4.825118]\n",
      "238: Train Loss: [2.3604953, 0.31558427, 4.4054065] | Test Loss: [2.565581, 0.3879305, 4.743232]\n",
      "239: Train Loss: [2.2835183, 0.29952174, 4.2675147] | Test Loss: [2.5034459, 0.3283564, 4.6785355]\n",
      "240: Train Loss: [2.3489642, 0.34185648, 4.356072] | Test Loss: [2.6000798, 0.3280632, 4.8720965]\n",
      "241: Train Loss: [2.3655484, 0.34230933, 4.3887873] | Test Loss: [2.713318, 0.42521444, 5.001422]\n",
      "242: Train Loss: [2.2654486, 0.33165193, 4.1992455] | Test Loss: [2.5663323, 0.38617998, 4.7464848]\n",
      "243: Train Loss: [2.1951125, 0.3225655, 4.0676594] | Test Loss: [2.5343585, 0.32813936, 4.7405777]\n",
      "244: Train Loss: [2.367134, 0.36388373, 4.3703847] | Test Loss: [2.7114387, 0.35590118, 5.066976]\n",
      "245: Train Loss: [2.3521292, 0.347697, 4.3565617] | Test Loss: [2.6296196, 0.3721009, 4.8871384]\n",
      "246: Train Loss: [2.340939, 0.31127524, 4.370603] | Test Loss: [2.4976006, 0.36384988, 4.631351]\n",
      "247: Train Loss: [2.2997472, 0.3662233, 4.233271] | Test Loss: [2.6953685, 0.4112362, 4.979501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248: Train Loss: [2.406602, 0.3083256, 4.504878] | Test Loss: [2.5926397, 0.33069515, 4.854584]\n",
      "249: Train Loss: [2.3604412, 0.323018, 4.3978643] | Test Loss: [2.7536232, 0.4513699, 5.0558767]\n",
      "250: Train Loss: [2.2633202, 0.37681356, 4.149827] | Test Loss: [2.4728236, 0.38805014, 4.557597]\n",
      "251: Train Loss: [2.3358207, 0.32961792, 4.3420234] | Test Loss: [2.5926123, 0.34236747, 4.842857]\n",
      "252: Train Loss: [2.3456106, 0.36691573, 4.3243055] | Test Loss: [2.6200495, 0.32097876, 4.9191203]\n",
      "253: Train Loss: [2.337133, 0.4257543, 4.248512] | Test Loss: [2.548702, 0.37823904, 4.719165]\n",
      "254: Train Loss: [2.336081, 0.3658336, 4.3063283] | Test Loss: [2.609455, 0.39115855, 4.8277516]\n",
      "255: Train Loss: [2.4919908, 0.32420316, 4.6597786] | Test Loss: [2.7298121, 0.38463476, 5.0749893]\n",
      "256: Train Loss: [2.2970667, 0.38869748, 4.2054358] | Test Loss: [2.6577437, 0.4110222, 4.904465]\n",
      "257: Train Loss: [2.147731, 0.35924044, 3.9362218] | Test Loss: [2.592383, 0.39364645, 4.7911196]\n",
      "258: Train Loss: [2.423165, 0.34962985, 4.4967003] | Test Loss: [2.5509202, 0.3383309, 4.7635098]\n",
      "259: Train Loss: [2.437556, 0.38724005, 4.487872] | Test Loss: [2.6066008, 0.3453124, 4.867889]\n",
      "260: Train Loss: [2.3269038, 0.37082592, 4.282982] | Test Loss: [2.6137576, 0.38020304, 4.847312]\n",
      "261: Train Loss: [2.3472571, 0.35521534, 4.3392987] | Test Loss: [2.5330045, 0.37885007, 4.687159]\n",
      "262: Train Loss: [2.3729544, 0.35187364, 4.3940353] | Test Loss: [2.9057536, 0.470494, 5.3410134]\n",
      "263: Train Loss: [2.220054, 0.36640605, 4.073702] | Test Loss: [2.7780561, 0.48782653, 5.068286]\n",
      "264: Train Loss: [2.266443, 0.3094954, 4.2233906] | Test Loss: [2.8874977, 0.31619018, 5.458805]\n",
      "265: Train Loss: [2.251294, 0.32470548, 4.177882] | Test Loss: [2.5701258, 0.36033162, 4.77992]\n",
      "266: Train Loss: [2.493319, 0.35444406, 4.632194] | Test Loss: [2.511267, 0.35607386, 4.66646]\n",
      "267: Train Loss: [2.3377006, 0.35736942, 4.318032] | Test Loss: [2.6482308, 0.33929035, 4.9571714]\n",
      "268: Train Loss: [2.3904967, 0.3509273, 4.430066] | Test Loss: [2.7474668, 0.29065436, 5.2042794]\n",
      "269: Train Loss: [2.3172476, 0.35371152, 4.2807837] | Test Loss: [2.4413917, 0.33403677, 4.5487466]\n",
      "270: Train Loss: [2.4517088, 0.3475358, 4.555882] | Test Loss: [2.579594, 0.43328813, 4.7258997]\n",
      "271: Train Loss: [2.300942, 0.3629283, 4.2389555] | Test Loss: [2.580318, 0.33222345, 4.8284125]\n",
      "272: Train Loss: [2.3197367, 0.33450937, 4.304964] | Test Loss: [2.5858247, 0.4048028, 4.7668467]\n",
      "273: Train Loss: [2.2437584, 0.37634122, 4.1111755] | Test Loss: [2.626538, 0.3540932, 4.898983]\n",
      "274: Train Loss: [2.2571602, 0.39100963, 4.1233106] | Test Loss: [2.4958375, 0.28449285, 4.707182]\n",
      "275: Train Loss: [2.2241087, 0.31892058, 4.129297] | Test Loss: [2.714442, 0.3574765, 5.0714073]\n",
      "276: Train Loss: [2.4558218, 0.38323796, 4.5284057] | Test Loss: [2.654855, 0.33391732, 4.975793]\n",
      "277: Train Loss: [2.2689977, 0.33465934, 4.203336] | Test Loss: [2.563566, 0.38354158, 4.7435904]\n",
      "278: Train Loss: [2.3562791, 0.34164575, 4.3709126] | Test Loss: [2.663705, 0.408112, 4.919298]\n",
      "279: Train Loss: [2.2617521, 0.3146625, 4.208842] | Test Loss: [2.4886498, 0.31529593, 4.6620035]\n",
      "280: Train Loss: [2.2927163, 0.3317364, 4.253696] | Test Loss: [2.9202483, 0.34972388, 5.4907727]\n",
      "281: Train Loss: [2.430988, 0.2990476, 4.5629287] | Test Loss: [2.573689, 0.40466744, 4.7427106]\n",
      "282: Train Loss: [2.3694823, 0.32100135, 4.417963] | Test Loss: [2.5726998, 0.37504584, 4.770354]\n",
      "283: Train Loss: [2.455182, 0.34552056, 4.5648437] | Test Loss: [2.6697366, 0.3149091, 5.0245643]\n",
      "284: Train Loss: [2.4474828, 0.3451707, 4.549795] | Test Loss: [2.6082265, 0.4102348, 4.806218]\n",
      "285: Train Loss: [2.4031456, 0.3607937, 4.4454975] | Test Loss: [2.6958308, 0.32666084, 5.065001]\n",
      "286: Train Loss: [2.3232894, 0.27713263, 4.3694463] | Test Loss: [2.6579423, 0.34650552, 4.969379]\n",
      "287: Train Loss: [2.4548419, 0.3165972, 4.5930867] | Test Loss: [2.4941678, 0.3517487, 4.6365867]\n",
      "288: Train Loss: [2.5431068, 0.31109154, 4.775122] | Test Loss: [2.6716952, 0.35823038, 4.98516]\n",
      "289: Train Loss: [2.416997, 0.35571435, 4.4782796] | Test Loss: [2.510522, 0.37883607, 4.6422076]\n",
      "290: Train Loss: [2.3458774, 0.32440713, 4.3673477] | Test Loss: [2.842168, 0.3395442, 5.344792]\n",
      "291: Train Loss: [2.4339094, 0.43190983, 4.435909] | Test Loss: [2.4325402, 0.3947174, 4.470363]\n",
      "292: Train Loss: [2.3046498, 0.32240298, 4.2868967] | Test Loss: [2.566513, 0.2901021, 4.842924]\n",
      "293: Train Loss: [2.403644, 0.3175633, 4.4897246] | Test Loss: [2.889155, 0.42971689, 5.3485928]\n",
      "294: Train Loss: [2.403686, 0.36671805, 4.4406543] | Test Loss: [2.4775257, 0.35291225, 4.602139]\n",
      "295: Train Loss: [2.3255467, 0.3311543, 4.319939] | Test Loss: [2.3809934, 0.40436128, 4.3576255]\n",
      "296: Train Loss: [2.375867, 0.31013292, 4.441601] | Test Loss: [2.6319547, 0.3594931, 4.904416]\n",
      "297: Train Loss: [2.37185, 0.368465, 4.375235] | Test Loss: [2.5908947, 0.38028425, 4.801505]\n",
      "298: Train Loss: [2.3522086, 0.40325856, 4.301159] | Test Loss: [2.601548, 0.44908497, 4.754011]\n",
      "299: Train Loss: [2.490048, 0.50792956, 4.4721665] | Test Loss: [2.697126, 0.37036404, 5.0238876]\n",
      "300: Train Loss: [2.3595273, 0.34781295, 4.3712416] | Test Loss: [2.5706897, 0.39149404, 4.749885]\n",
      "301: Train Loss: [2.405282, 0.3614337, 4.4491305] | Test Loss: [2.619267, 0.3708517, 4.8676825]\n",
      "302: Train Loss: [2.297344, 0.33649334, 4.2581944] | Test Loss: [2.6932173, 0.33048055, 5.055954]\n",
      "303: Train Loss: [2.426443, 0.37795582, 4.4749303] | Test Loss: [2.6484075, 0.37416446, 4.9226503]\n",
      "304: Train Loss: [2.355184, 0.37578252, 4.3345857] | Test Loss: [2.5907001, 0.4011001, 4.7803]\n",
      "305: Train Loss: [2.3132887, 0.29928795, 4.3272896] | Test Loss: [2.659482, 0.4144134, 4.9045506]\n",
      "306: Train Loss: [2.2621787, 0.34098053, 4.183377] | Test Loss: [3.0881538, 0.26413274, 5.912175]\n",
      "307: Train Loss: [2.405714, 0.3829852, 4.428443] | Test Loss: [2.6536155, 0.36678433, 4.940447]\n",
      "308: Train Loss: [2.3780255, 0.38119665, 4.3748546] | Test Loss: [2.638257, 0.3922203, 4.8842936]\n",
      "309: Train Loss: [2.3359044, 0.32267264, 4.349136] | Test Loss: [2.5925884, 0.44067973, 4.7444973]\n",
      "310: Train Loss: [2.2756507, 0.3663777, 4.1849236] | Test Loss: [2.7350647, 0.38212675, 5.0880027]\n",
      "311: Train Loss: [2.3563323, 0.38284865, 4.329816] | Test Loss: [2.3916006, 0.36136064, 4.4218407]\n",
      "312: Train Loss: [2.3961368, 0.42420727, 4.3680663] | Test Loss: [2.9194558, 0.49016508, 5.3487463]\n",
      "313: Train Loss: [2.395756, 0.32865956, 4.4628525] | Test Loss: [2.6800761, 0.36464447, 4.9955077]\n",
      "314: Train Loss: [2.2639806, 0.3505412, 4.17742] | Test Loss: [2.5396755, 0.33443326, 4.744918]\n",
      "315: Train Loss: [2.3683329, 0.3761171, 4.3605485] | Test Loss: [2.6326199, 0.44873807, 4.8165016]\n",
      "316: Train Loss: [2.1215968, 0.38986683, 3.8533268] | Test Loss: [2.595189, 0.32339907, 4.866979]\n",
      "317: Train Loss: [2.3464384, 0.275894, 4.4169827] | Test Loss: [2.7040954, 0.37204927, 5.0361414]\n",
      "318: Train Loss: [2.283328, 0.3321444, 4.234512] | Test Loss: [2.6073785, 0.35479006, 4.8599668]\n",
      "319: Train Loss: [2.3210914, 0.34351632, 4.2986665] | Test Loss: [2.5901937, 0.44695592, 4.733432]\n",
      "320: Train Loss: [2.3876872, 0.37697673, 4.3983974] | Test Loss: [2.646558, 0.4326242, 4.8604918]\n",
      "321: Train Loss: [2.3677187, 0.32272795, 4.412709] | Test Loss: [2.7772343, 0.37232992, 5.182139]\n",
      "322: Train Loss: [2.2863839, 0.3560324, 4.2167354] | Test Loss: [2.534208, 0.3413564, 4.72706]\n",
      "323: Train Loss: [2.336083, 0.31092396, 4.361242] | Test Loss: [2.651781, 0.35430267, 4.9492593]\n",
      "324: Train Loss: [2.3525949, 0.32227248, 4.3829174] | Test Loss: [2.468218, 0.3626007, 4.5738354]\n",
      "325: Train Loss: [2.2585573, 0.35425362, 4.162861] | Test Loss: [2.6912823, 0.32925287, 5.053312]\n",
      "326: Train Loss: [2.250148, 0.29488716, 4.205409] | Test Loss: [2.6017125, 0.37024918, 4.8331757]\n",
      "327: Train Loss: [2.4647777, 0.3908347, 4.5387206] | Test Loss: [2.7534754, 0.34297284, 5.163978]\n",
      "328: Train Loss: [2.352645, 0.4303999, 4.27489] | Test Loss: [2.6306226, 0.34159234, 4.919653]\n",
      "329: Train Loss: [2.402221, 0.34386805, 4.4605737] | Test Loss: [2.61615, 0.30248052, 4.929819]\n",
      "330: Train Loss: [2.4722612, 0.3978666, 4.5466557] | Test Loss: [2.621155, 0.35957125, 4.8827386]\n",
      "331: Train Loss: [2.502849, 0.35648745, 4.649211] | Test Loss: [2.569253, 0.40283304, 4.735673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332: Train Loss: [2.3873663, 0.35955337, 4.4151793] | Test Loss: [2.5939853, 0.3134514, 4.8745193]\n",
      "333: Train Loss: [2.4055867, 0.36654425, 4.444629] | Test Loss: [2.6514497, 0.4041423, 4.898757]\n",
      "334: Train Loss: [2.3903542, 0.30723843, 4.4734697] | Test Loss: [2.6796238, 0.3468351, 5.0124125]\n",
      "335: Train Loss: [2.2666337, 0.33795244, 4.195315] | Test Loss: [2.5646865, 0.39550862, 4.7338643]\n",
      "336: Train Loss: [2.5320716, 0.35742486, 4.7067184] | Test Loss: [2.7116027, 0.3509541, 5.0722513]\n",
      "337: Train Loss: [2.494418, 0.46606132, 4.5227747] | Test Loss: [2.7044168, 0.38670906, 5.0221243]\n",
      "338: Train Loss: [2.2781007, 0.31353208, 4.2426696] | Test Loss: [2.530761, 0.45677662, 4.6047454]\n",
      "339: Train Loss: [2.509754, 0.3629246, 4.6565833] | Test Loss: [2.9313798, 0.46846846, 5.394291]\n",
      "340: Train Loss: [2.4968479, 0.47529712, 4.518399] | Test Loss: [2.6469905, 0.36265293, 4.9313283]\n",
      "341: Train Loss: [2.4205918, 0.341897, 4.4992867] | Test Loss: [2.697016, 0.35056248, 5.0434694]\n",
      "342: Train Loss: [2.3353076, 0.36331648, 4.3072987] | Test Loss: [2.5576048, 0.4408521, 4.6743574]\n",
      "343: Train Loss: [2.2530167, 0.34053564, 4.165498] | Test Loss: [2.4865975, 0.36641455, 4.6067805]\n",
      "344: Train Loss: [2.4734845, 0.33288682, 4.6140823] | Test Loss: [2.5489776, 0.3331838, 4.7647715]\n",
      "345: Train Loss: [2.3955762, 0.3840373, 4.407115] | Test Loss: [2.5444546, 0.42887676, 4.6600323]\n",
      "346: Train Loss: [2.2813466, 0.4157684, 4.1469245] | Test Loss: [2.7124603, 0.4250878, 4.9998326]\n",
      "347: Train Loss: [2.368015, 0.3131275, 4.4229026] | Test Loss: [2.5241332, 0.3445159, 4.7037506]\n",
      "348: Train Loss: [2.32998, 0.28906736, 4.3708925] | Test Loss: [2.5603042, 0.3788003, 4.741808]\n",
      "349: Train Loss: [2.3829856, 0.39038318, 4.375588] | Test Loss: [2.6903632, 0.31365284, 5.0670733]\n",
      "350: Train Loss: [2.313162, 0.4481527, 4.1781716] | Test Loss: [2.5260422, 0.33682686, 4.7152576]\n",
      "351: Train Loss: [2.510245, 0.3453802, 4.67511] | Test Loss: [2.6047716, 0.3778511, 4.831692]\n",
      "352: Train Loss: [2.3282964, 0.41517252, 4.2414203] | Test Loss: [2.5384793, 0.31743616, 4.7595224]\n",
      "353: Train Loss: [2.1907473, 0.3495466, 4.031948] | Test Loss: [2.6094587, 0.3457259, 4.8731914]\n",
      "354: Train Loss: [2.273088, 0.33052754, 4.215648] | Test Loss: [2.4738874, 0.33190092, 4.615874]\n",
      "355: Train Loss: [2.3815563, 0.34499875, 4.4181137] | Test Loss: [2.7527328, 0.36495227, 5.1405134]\n",
      "356: Train Loss: [2.4727485, 0.38698256, 4.5585146] | Test Loss: [2.5753145, 0.40921095, 4.741418]\n",
      "357: Train Loss: [2.406435, 0.39032292, 4.4225473] | Test Loss: [2.557176, 0.3679919, 4.7463603]\n",
      "358: Train Loss: [2.2954717, 0.41021997, 4.180723] | Test Loss: [2.655666, 0.40247443, 4.908858]\n",
      "359: Train Loss: [2.3117092, 0.30243692, 4.3209815] | Test Loss: [2.8046553, 0.36413097, 5.2451797]\n",
      "360: Train Loss: [2.3923976, 0.35287362, 4.4319215] | Test Loss: [2.788919, 0.43228078, 5.145557]\n",
      "361: Train Loss: [2.4260252, 0.2938534, 4.558197] | Test Loss: [2.7483692, 0.33140218, 5.165336]\n",
      "362: Train Loss: [2.450332, 0.35839948, 4.5422645] | Test Loss: [2.6152818, 0.36294985, 4.867614]\n",
      "363: Train Loss: [2.2988172, 0.30687413, 4.29076] | Test Loss: [2.5146787, 0.33504274, 4.6943145]\n",
      "364: Train Loss: [2.2986808, 0.3883903, 4.2089715] | Test Loss: [2.6691468, 0.33834213, 4.9999514]\n",
      "365: Train Loss: [2.326405, 0.3490912, 4.303719] | Test Loss: [2.5794325, 0.33844098, 4.820424]\n",
      "366: Train Loss: [2.3316607, 0.28052258, 4.382799] | Test Loss: [2.4531283, 0.44154114, 4.4647155]\n",
      "367: Train Loss: [2.289877, 0.34299132, 4.2367625] | Test Loss: [2.599361, 0.3508051, 4.8479166]\n",
      "368: Train Loss: [2.3934622, 0.34009987, 4.4468246] | Test Loss: [2.4393892, 0.35683933, 4.5219393]\n",
      "369: Train Loss: [2.3752897, 0.36612839, 4.384451] | Test Loss: [2.8388102, 0.46209586, 5.2155247]\n",
      "370: Train Loss: [2.458786, 0.32134837, 4.596224] | Test Loss: [2.664798, 0.43890432, 4.8906918]\n",
      "371: Train Loss: [2.194928, 0.33429575, 4.05556] | Test Loss: [2.7515354, 0.3971727, 5.1058984]\n",
      "372: Train Loss: [2.3986635, 0.36098215, 4.436345] | Test Loss: [2.5006933, 0.3223546, 4.679032]\n",
      "373: Train Loss: [2.388424, 0.3743894, 4.4024587] | Test Loss: [2.6129704, 0.4096394, 4.8163013]\n",
      "374: Train Loss: [2.3425431, 0.34325495, 4.341831] | Test Loss: [2.6509228, 0.33398318, 4.9678626]\n",
      "375: Train Loss: [2.226719, 0.288449, 4.164989] | Test Loss: [2.7207415, 0.47055775, 4.9709253]\n",
      "376: Train Loss: [2.4187303, 0.38692543, 4.4505353] | Test Loss: [2.5082538, 0.39938873, 4.617119]\n",
      "377: Train Loss: [2.4868183, 0.3509175, 4.6227193] | Test Loss: [2.5827425, 0.33511886, 4.830366]\n",
      "378: Train Loss: [2.4132364, 0.39432114, 4.432152] | Test Loss: [2.6226842, 0.36761886, 4.8777494]\n",
      "379: Train Loss: [2.4831603, 0.5000005, 4.46632] | Test Loss: [2.8664625, 0.3911721, 5.341753]\n",
      "380: Train Loss: [2.415991, 0.52158403, 4.310398] | Test Loss: [2.7785861, 0.45416453, 5.103008]\n",
      "381: Train Loss: [2.3730042, 0.37079477, 4.3752136] | Test Loss: [2.673847, 0.41924062, 4.9284534]\n",
      "382: Train Loss: [2.3185916, 0.38449883, 4.2526846] | Test Loss: [2.5340843, 0.39363122, 4.6745377]\n",
      "383: Train Loss: [2.350543, 0.3386887, 4.362397] | Test Loss: [2.6056113, 0.35689384, 4.8543286]\n",
      "384: Train Loss: [2.409581, 0.31201994, 4.507142] | Test Loss: [2.467521, 0.3403117, 4.5947304]\n",
      "385: Train Loss: [2.4510348, 0.37467036, 4.527399] | Test Loss: [2.6126091, 0.35436013, 4.870858]\n",
      "386: Train Loss: [2.447104, 0.36924815, 4.52496] | Test Loss: [2.7439723, 0.4593135, 5.028631]\n",
      "387: Train Loss: [2.2733507, 0.3662301, 4.1804714] | Test Loss: [2.5877085, 0.36441568, 4.8110013]\n",
      "388: Train Loss: [2.3766115, 0.33645898, 4.416764] | Test Loss: [2.517869, 0.3378408, 4.697897]\n",
      "389: Train Loss: [2.4634554, 0.42660937, 4.5003014] | Test Loss: [2.5830648, 0.4430019, 4.723128]\n",
      "390: Train Loss: [2.3527505, 0.4624478, 4.2430534] | Test Loss: [2.5110261, 0.33285898, 4.6891932]\n",
      "391: Train Loss: [2.3673763, 0.3781791, 4.3565736] | Test Loss: [2.698529, 0.3790212, 5.018037]\n",
      "392: Train Loss: [2.33958, 0.37503484, 4.3041253] | Test Loss: [2.6960912, 0.35403448, 5.038148]\n",
      "393: Train Loss: [2.3052785, 0.35215938, 4.2583976] | Test Loss: [2.7551525, 0.36638528, 5.1439195]\n",
      "394: Train Loss: [2.525583, 0.3759368, 4.675229] | Test Loss: [2.5203397, 0.3452238, 4.6954556]\n",
      "395: Train Loss: [2.483373, 0.36901248, 4.5977335] | Test Loss: [2.5656686, 0.3322482, 4.799089]\n",
      "396: Train Loss: [2.3965476, 0.38003764, 4.4130573] | Test Loss: [2.9824107, 0.36208573, 5.6027355]\n",
      "397: Train Loss: [2.3539524, 0.28879467, 4.4191103] | Test Loss: [2.5605047, 0.34553036, 4.775479]\n",
      "398: Train Loss: [2.3048606, 0.31169933, 4.298022] | Test Loss: [2.7147865, 0.37911072, 5.0504622]\n",
      "399: Train Loss: [2.3611982, 0.35695907, 4.3654375] | Test Loss: [2.5928612, 0.37845784, 4.8072643]\n",
      "400: Train Loss: [2.359711, 0.37357363, 4.345848] | Test Loss: [2.4044151, 0.31864798, 4.4901824]\n",
      "401: Train Loss: [2.4518902, 0.32043216, 4.5833483] | Test Loss: [2.6056256, 0.33721322, 4.874038]\n",
      "402: Train Loss: [2.3155675, 0.4091408, 4.2219944] | Test Loss: [2.7236328, 0.38337588, 5.06389]\n",
      "403: Train Loss: [2.4598916, 0.3345649, 4.5852184] | Test Loss: [2.6612358, 0.39256433, 4.9299073]\n",
      "404: Train Loss: [2.5303388, 0.38482088, 4.6758566] | Test Loss: [2.7517161, 0.36470667, 5.1387258]\n",
      "405: Train Loss: [2.5094392, 0.37286195, 4.6460166] | Test Loss: [2.6942396, 0.42460188, 4.963877]\n",
      "406: Train Loss: [2.4649045, 0.3666871, 4.563122] | Test Loss: [2.7268689, 0.3491392, 5.1045985]\n",
      "407: Train Loss: [2.30477, 0.3606319, 4.248908] | Test Loss: [2.3852155, 0.36909246, 4.4013386]\n",
      "408: Train Loss: [2.4029932, 0.39891535, 4.407071] | Test Loss: [2.5960755, 0.38586453, 4.8062863]\n",
      "409: Train Loss: [2.4076533, 0.32190037, 4.4934063] | Test Loss: [2.4935455, 0.36265233, 4.624439]\n",
      "410: Train Loss: [2.3774803, 0.34437647, 4.410584] | Test Loss: [2.6004248, 0.37832314, 4.8225265]\n",
      "411: Train Loss: [2.508039, 0.44817743, 4.5679007] | Test Loss: [2.749014, 0.4797572, 5.0182705]\n",
      "412: Train Loss: [2.294771, 0.28456718, 4.3049746] | Test Loss: [2.6620963, 0.39141077, 4.9327817]\n",
      "413: Train Loss: [2.5097075, 0.43301383, 4.586401] | Test Loss: [2.5857813, 0.32646438, 4.8450985]\n",
      "414: Train Loss: [2.5024836, 0.437386, 4.567581] | Test Loss: [2.8393269, 0.4543779, 5.2242756]\n",
      "415: Train Loss: [2.5788944, 0.3900462, 4.7677426] | Test Loss: [2.6181667, 0.34160396, 4.8947296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416: Train Loss: [2.4567614, 0.36208066, 4.551442] | Test Loss: [2.745151, 0.41036195, 5.0799403]\n",
      "417: Train Loss: [2.3647256, 0.40280578, 4.3266454] | Test Loss: [2.6630938, 0.39815864, 4.928029]\n",
      "418: Train Loss: [2.3504062, 0.33096746, 4.369845] | Test Loss: [2.737528, 0.3593121, 5.115744]\n",
      "419: Train Loss: [2.4355965, 0.38086906, 4.490324] | Test Loss: [2.6650958, 0.35742328, 4.9727683]\n",
      "420: Train Loss: [2.317233, 0.32285753, 4.311609] | Test Loss: [2.411231, 0.3185861, 4.503876]\n",
      "421: Train Loss: [2.4103782, 0.3227962, 4.49796] | Test Loss: [2.7084255, 0.35790488, 5.058946]\n",
      "422: Train Loss: [2.4819863, 0.4445339, 4.5194387] | Test Loss: [2.5387285, 0.34486574, 4.732591]\n",
      "423: Train Loss: [2.554146, 0.4674385, 4.6408534] | Test Loss: [2.72354, 0.45712376, 4.9899564]\n",
      "424: Train Loss: [2.4473386, 0.4027221, 4.4919553] | Test Loss: [2.6380856, 0.46562585, 4.8105454]\n",
      "425: Train Loss: [2.4148755, 0.41229317, 4.417458] | Test Loss: [2.5615506, 0.39452887, 4.7285724]\n",
      "426: Train Loss: [2.4159205, 0.39443323, 4.437408] | Test Loss: [2.3713953, 0.36556143, 4.377229]\n",
      "427: Train Loss: [2.367909, 0.3548927, 4.380925] | Test Loss: [2.597902, 0.3242097, 4.8715944]\n",
      "428: Train Loss: [2.317375, 0.3523211, 4.2824287] | Test Loss: [2.6030645, 0.38254005, 4.823589]\n",
      "429: Train Loss: [2.2577496, 0.38283804, 4.132661] | Test Loss: [2.5763385, 0.38810056, 4.7645764]\n",
      "430: Train Loss: [2.4055562, 0.41504616, 4.396066] | Test Loss: [2.7822366, 0.3553226, 5.2091503]\n",
      "431: Train Loss: [2.5034063, 0.35506725, 4.6517453] | Test Loss: [2.7027287, 0.35581708, 5.0496407]\n",
      "432: Train Loss: [2.3321004, 0.34659645, 4.3176045] | Test Loss: [2.7083907, 0.33874387, 5.0780377]\n",
      "433: Train Loss: [2.2780492, 0.32504848, 4.23105] | Test Loss: [2.4628367, 0.41151878, 4.514155]\n",
      "434: Train Loss: [2.5229568, 0.3216769, 4.724237] | Test Loss: [2.7947872, 0.37154987, 5.2180243]\n",
      "435: Train Loss: [2.4787157, 0.3372484, 4.620183] | Test Loss: [2.6570053, 0.37138757, 4.942623]\n",
      "436: Train Loss: [2.469053, 0.35956708, 4.578539] | Test Loss: [2.6043315, 0.38294584, 4.825717]\n",
      "437: Train Loss: [2.326282, 0.43487036, 4.217694] | Test Loss: [2.5671508, 0.3501915, 4.78411]\n",
      "438: Train Loss: [2.2947927, 0.3835294, 4.206056] | Test Loss: [2.6188436, 0.37282377, 4.8648634]\n",
      "439: Train Loss: [2.269699, 0.39813435, 4.141264] | Test Loss: [2.6904635, 0.43665242, 4.944275]\n",
      "440: Train Loss: [2.3864398, 0.35351706, 4.4193625] | Test Loss: [2.4869535, 0.38786986, 4.586037]\n",
      "441: Train Loss: [2.328561, 0.35528126, 4.301841] | Test Loss: [2.6533954, 0.39854762, 4.908243]\n",
      "442: Train Loss: [2.3097742, 0.3329331, 4.2866154] | Test Loss: [2.6349008, 0.35426655, 4.915535]\n",
      "443: Train Loss: [2.399892, 0.40997916, 4.389805] | Test Loss: [2.580279, 0.3561849, 4.8043733]\n",
      "444: Train Loss: [2.33487, 0.34761512, 4.322125] | Test Loss: [2.560756, 0.4105113, 4.7110004]\n",
      "445: Train Loss: [2.4353058, 0.3815561, 4.4890556] | Test Loss: [2.6898167, 0.347915, 5.0317183]\n",
      "446: Train Loss: [2.2762945, 0.36677647, 4.1858125] | Test Loss: [2.61491, 0.39371964, 4.8361]\n",
      "447: Train Loss: [2.4887462, 0.32970682, 4.6477857] | Test Loss: [2.7250748, 0.43475646, 5.0153933]\n",
      "448: Train Loss: [2.40157, 0.36819914, 4.434941] | Test Loss: [2.5465693, 0.2805122, 4.8126264]\n",
      "449: Train Loss: [2.283896, 0.38501465, 4.1827774] | Test Loss: [2.6102555, 0.32408154, 4.8964295]\n",
      "450: Train Loss: [2.5701292, 0.40886745, 4.731391] | Test Loss: [2.5286927, 0.3698217, 4.687564]\n",
      "451: Train Loss: [2.3495035, 0.34750715, 4.3515] | Test Loss: [2.6316826, 0.41548902, 4.847876]\n",
      "452: Train Loss: [2.3121648, 0.38461503, 4.2397146] | Test Loss: [2.5374384, 0.354617, 4.7202597]\n",
      "453: Train Loss: [2.5289593, 0.3693927, 4.6885257] | Test Loss: [2.8310993, 0.44033962, 5.221859]\n",
      "454: Train Loss: [2.3208468, 0.31650105, 4.3251925] | Test Loss: [2.653067, 0.44038486, 4.8657494]\n",
      "455: Train Loss: [2.3350015, 0.29584038, 4.3741627] | Test Loss: [2.4966745, 0.32381713, 4.669532]\n",
      "456: Train Loss: [2.56106, 0.37866136, 4.7434587] | Test Loss: [1.53939, 0.22500981, 2.85377]\n",
      "457: Train Loss: [2.3731165, 0.3486496, 4.3975835] | Test Loss: [2.6244898, 0.43108153, 4.817898]\n",
      "458: Train Loss: [2.2518663, 0.39533082, 4.108402] | Test Loss: [2.723331, 0.3809621, 5.0657]\n",
      "459: Train Loss: [2.286161, 0.3729472, 4.1993747] | Test Loss: [2.5180373, 0.3508711, 4.6852036]\n",
      "460: Train Loss: [2.480396, 0.38735658, 4.5734353] | Test Loss: [2.5512137, 0.3048401, 4.7975874]\n",
      "461: Train Loss: [2.4321954, 0.39876223, 4.4656286] | Test Loss: [2.5738785, 0.39229062, 4.7554665]\n",
      "462: Train Loss: [2.4777198, 0.31717274, 4.638267] | Test Loss: [2.8202214, 0.4010941, 5.239349]\n",
      "463: Train Loss: [2.422581, 0.34809986, 4.497062] | Test Loss: [2.6743724, 0.3564376, 4.992307]\n",
      "464: Train Loss: [2.4627817, 0.3630889, 4.5624743] | Test Loss: [2.7102203, 0.3483848, 5.072056]\n",
      "465: Train Loss: [2.4648771, 0.31365877, 4.6160955] | Test Loss: [2.5373628, 0.28411403, 4.7906117]\n",
      "466: Train Loss: [2.4193733, 0.3342488, 4.5044975] | Test Loss: [2.582244, 0.3606341, 4.803854]\n",
      "467: Train Loss: [2.346748, 0.34318784, 4.3503084] | Test Loss: [2.5024323, 0.43233675, 4.572528]\n",
      "468: Train Loss: [2.4198601, 0.3371455, 4.502575] | Test Loss: [2.6105173, 0.37723684, 4.8437977]\n",
      "469: Train Loss: [2.4130619, 0.36047444, 4.465649] | Test Loss: [2.6900496, 0.37786952, 5.0022297]\n",
      "470: Train Loss: [2.4982874, 0.4389577, 4.557617] | Test Loss: [2.7465372, 0.29272714, 5.2003474]\n",
      "471: Train Loss: [2.2787306, 0.32383788, 4.2336235] | Test Loss: [2.732797, 0.40774256, 5.0578513]\n",
      "472: Train Loss: [2.353742, 0.37750953, 4.329974] | Test Loss: [2.470587, 0.39137274, 4.5498013]\n",
      "473: Train Loss: [2.2647264, 0.31179112, 4.217662] | Test Loss: [2.6725643, 0.34124738, 5.003881]\n",
      "474: Train Loss: [2.1438246, 0.38076743, 3.9068818] | Test Loss: [2.935659, 0.4003037, 5.471014]\n",
      "475: Train Loss: [2.3032036, 0.3557055, 4.250702] | Test Loss: [2.554565, 0.33329692, 4.775833]\n",
      "476: Train Loss: [2.3806703, 0.3746809, 4.3866596] | Test Loss: [2.6726542, 0.48196575, 4.863343]\n",
      "477: Train Loss: [2.3876238, 0.33550918, 4.4397383] | Test Loss: [2.565231, 0.36717007, 4.7632923]\n",
      "478: Train Loss: [2.3939195, 0.3485326, 4.4393063] | Test Loss: [2.528297, 0.35006642, 4.706527]\n",
      "479: Train Loss: [2.6746523, 0.46612045, 4.8831844] | Test Loss: [2.5056748, 0.3607493, 4.6506004]\n",
      "480: Train Loss: [2.4668062, 0.4469825, 4.48663] | Test Loss: [2.753291, 0.40087324, 5.1057086]\n",
      "481: Train Loss: [2.3654506, 0.40272728, 4.328174] | Test Loss: [2.3398416, 0.40205538, 4.277628]\n",
      "482: Train Loss: [2.3358383, 0.38769126, 4.283985] | Test Loss: [2.4666207, 0.40874186, 4.5244994]\n",
      "483: Train Loss: [2.4126384, 0.365112, 4.460165] | Test Loss: [2.472296, 0.37341592, 4.571176]\n",
      "484: Train Loss: [2.3694293, 0.40664423, 4.3322144] | Test Loss: [2.707779, 0.36543843, 5.0501194]\n",
      "485: Train Loss: [2.3413765, 0.37470448, 4.3080487] | Test Loss: [2.8033946, 0.32472143, 5.282068]\n",
      "486: Train Loss: [2.2982445, 0.31842932, 4.2780595] | Test Loss: [2.4547043, 0.5073987, 4.40201]\n",
      "487: Train Loss: [2.3883817, 0.34709874, 4.4296646] | Test Loss: [2.5543754, 0.42754406, 4.6812067]\n",
      "488: Train Loss: [2.4991157, 0.38059235, 4.617639] | Test Loss: [2.7965844, 0.3739036, 5.219265]\n",
      "489: Train Loss: [2.5654736, 0.35309276, 4.7778544] | Test Loss: [2.7173982, 0.33268622, 5.10211]\n",
      "490: Train Loss: [2.3908532, 0.404829, 4.3768773] | Test Loss: [2.5088751, 0.34005368, 4.6776967]\n",
      "491: Train Loss: [2.4114137, 0.35590568, 4.466922] | Test Loss: [2.525213, 0.34722784, 4.703198]\n",
      "492: Train Loss: [2.3685164, 0.3397057, 4.397327] | Test Loss: [2.6044421, 0.36000192, 4.848882]\n",
      "493: Train Loss: [2.413663, 0.34568062, 4.481645] | Test Loss: [2.4673476, 0.36696473, 4.5677304]\n",
      "494: Train Loss: [2.5126863, 0.40895084, 4.6164217] | Test Loss: [2.5240867, 0.32880923, 4.719364]\n",
      "495: Train Loss: [2.3472266, 0.58904046, 4.105413] | Test Loss: [2.5487707, 0.4119865, 4.685555]\n",
      "496: Train Loss: [2.184237, 0.32302588, 4.0454483] | Test Loss: [2.5296774, 0.43846175, 4.620893]\n",
      "497: Train Loss: [2.2659907, 0.37439284, 4.1575885] | Test Loss: [2.6642127, 0.3909549, 4.9374704]\n",
      "498: Train Loss: [2.4990191, 0.3339043, 4.664134] | Test Loss: [2.621945, 0.36304623, 4.8808436]\n",
      "499: Train Loss: [2.3192742, 0.3170999, 4.3214483] | Test Loss: [2.7020156, 0.34339228, 5.060639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: Train Loss: [2.2458768, 0.3514808, 4.1402726] | Test Loss: [2.7904663, 0.45608786, 5.1248446]\n",
      "501: Train Loss: [2.5171826, 0.42862093, 4.6057444] | Test Loss: [2.7053587, 0.35185513, 5.058862]\n",
      "502: Train Loss: [2.4294093, 0.42360315, 4.4352155] | Test Loss: [2.5404387, 0.35113642, 4.729741]\n",
      "503: Train Loss: [2.435044, 0.36658797, 4.5035] | Test Loss: [2.6049616, 0.3748289, 4.8350945]\n",
      "504: Train Loss: [2.3876894, 0.33979896, 4.43558] | Test Loss: [2.6759133, 0.36735085, 4.9844756]\n",
      "505: Train Loss: [2.3065438, 0.33361304, 4.2794747] | Test Loss: [2.521422, 0.38408366, 4.65876]\n",
      "506: Train Loss: [2.3172052, 0.34796277, 4.2864475] | Test Loss: [2.6864095, 0.34709492, 5.025724]\n",
      "507: Train Loss: [2.4094408, 0.37185067, 4.447031] | Test Loss: [2.7283382, 0.39861757, 5.0580587]\n",
      "508: Train Loss: [2.4812071, 0.367654, 4.5947604] | Test Loss: [2.5194569, 0.33092055, 4.707993]\n",
      "509: Train Loss: [2.370613, 0.36338723, 4.377839] | Test Loss: [2.6760142, 0.43730152, 4.9147267]\n",
      "510: Train Loss: [2.3020084, 0.30403072, 4.299986] | Test Loss: [2.6164196, 0.3369014, 4.895938]\n",
      "511: Train Loss: [2.437592, 0.40560985, 4.469574] | Test Loss: [2.7197914, 0.3349931, 5.10459]\n",
      "512: Train Loss: [2.3358684, 0.35664952, 4.3150873] | Test Loss: [2.5719807, 0.4087094, 4.735252]\n",
      "513: Train Loss: [2.3102593, 0.35691866, 4.2636] | Test Loss: [2.5179539, 0.336155, 4.699753]\n",
      "514: Train Loss: [2.4065995, 0.33574224, 4.4774566] | Test Loss: [2.8048177, 0.36430278, 5.2453327]\n",
      "515: Train Loss: [2.4395921, 0.34145108, 4.537733] | Test Loss: [2.638234, 0.40166962, 4.8747983]\n",
      "516: Train Loss: [2.3941746, 0.41668877, 4.37166] | Test Loss: [2.7677798, 0.38978216, 5.1457777]\n",
      "517: Train Loss: [2.4507236, 0.3781218, 4.5233254] | Test Loss: [2.6002092, 0.38113153, 4.819287]\n",
      "518: Train Loss: [2.3398156, 0.3176803, 4.361951] | Test Loss: [2.7518473, 0.4138409, 5.089854]\n",
      "519: Train Loss: [2.5078425, 0.32771218, 4.687973] | Test Loss: [2.4043539, 0.3237601, 4.4849477]\n",
      "520: Train Loss: [2.4408147, 0.37125385, 4.5103755] | Test Loss: [2.6447012, 0.32972857, 4.959674]\n",
      "521: Train Loss: [2.4108953, 0.36087647, 4.460914] | Test Loss: [2.5843124, 0.33222842, 4.8363967]\n",
      "522: Train Loss: [2.4486544, 0.31330216, 4.584007] | Test Loss: [2.7966585, 0.39121723, 5.2021]\n",
      "523: Train Loss: [2.422157, 0.363272, 4.481042] | Test Loss: [2.5357964, 0.36841553, 4.7031775]\n",
      "Epoch 16\n",
      "0: Train Loss: [2.294599, 0.34401852, 4.2451797] | Test Loss: [2.8526762, 0.41199416, 5.2933583]\n",
      "1: Train Loss: [2.1657946, 0.36733678, 3.9642525] | Test Loss: [2.589296, 0.33051202, 4.84808]\n",
      "2: Train Loss: [2.328006, 0.32333958, 4.3326726] | Test Loss: [2.3298683, 0.4053638, 4.2543726]\n",
      "3: Train Loss: [2.1416557, 0.34020722, 3.943104] | Test Loss: [2.7202506, 0.31938863, 5.121113]\n",
      "4: Train Loss: [2.2865121, 0.32118744, 4.251837] | Test Loss: [2.5693235, 0.38389885, 4.7547483]\n",
      "5: Train Loss: [2.299614, 0.33564955, 4.2635784] | Test Loss: [2.7612853, 0.3570012, 5.1655693]\n",
      "6: Train Loss: [2.1814206, 0.38773647, 3.9751046] | Test Loss: [2.608563, 0.35744777, 4.8596783]\n",
      "7: Train Loss: [2.2354987, 0.31835717, 4.1526403] | Test Loss: [2.5372367, 0.3960081, 4.6784654]\n",
      "8: Train Loss: [2.2148252, 0.31471536, 4.114935] | Test Loss: [2.6513789, 0.4673172, 4.8354406]\n",
      "9: Train Loss: [2.1962805, 0.40164992, 3.990911] | Test Loss: [2.4990366, 0.35658032, 4.641493]\n",
      "10: Train Loss: [2.080564, 0.33461094, 3.826517] | Test Loss: [2.6006522, 0.41534266, 4.7859616]\n",
      "11: Train Loss: [2.2323325, 0.33399487, 4.13067] | Test Loss: [2.5903883, 0.34507623, 4.8357005]\n",
      "12: Train Loss: [2.2388024, 0.35532677, 4.122278] | Test Loss: [2.4748201, 0.28380045, 4.6658397]\n",
      "13: Train Loss: [2.2465112, 0.34545216, 4.14757] | Test Loss: [2.6835217, 0.447398, 4.9196453]\n",
      "14: Train Loss: [2.1541731, 0.33540618, 3.97294] | Test Loss: [2.6851823, 0.4046921, 4.9656725]\n",
      "15: Train Loss: [2.2568686, 0.33532226, 4.178415] | Test Loss: [2.7804914, 0.43501693, 5.1259656]\n",
      "16: Train Loss: [2.1841297, 0.3666145, 4.001645] | Test Loss: [2.3666651, 0.33398432, 4.399346]\n",
      "17: Train Loss: [2.1968062, 0.32441694, 4.0691953] | Test Loss: [2.738117, 0.4315853, 5.0446486]\n",
      "18: Train Loss: [2.3613822, 0.4122699, 4.3104944] | Test Loss: [2.6762407, 0.3823714, 4.97011]\n",
      "19: Train Loss: [2.0714374, 0.30665803, 3.8362167] | Test Loss: [2.6800635, 0.30884117, 5.0512857]\n",
      "20: Train Loss: [2.1747842, 0.38965076, 3.9599178] | Test Loss: [2.7108374, 0.33352906, 5.0881457]\n",
      "21: Train Loss: [2.1262946, 0.4319822, 3.820607] | Test Loss: [2.5255997, 0.3212682, 4.7299314]\n",
      "22: Train Loss: [2.1683564, 0.337354, 3.9993587] | Test Loss: [2.5901504, 0.40441254, 4.775888]\n",
      "23: Train Loss: [2.0549445, 0.32823968, 3.781649] | Test Loss: [2.6805377, 0.34724757, 5.013828]\n",
      "24: Train Loss: [2.0674062, 0.37052065, 3.7642918] | Test Loss: [2.4954176, 0.3117939, 4.6790414]\n",
      "25: Train Loss: [2.1665504, 0.36531156, 3.9677892] | Test Loss: [2.656524, 0.36354834, 4.9494996]\n",
      "26: Train Loss: [2.2543766, 0.36384147, 4.144912] | Test Loss: [2.7500331, 0.33111823, 5.168948]\n",
      "27: Train Loss: [2.1079335, 0.3015409, 3.914326] | Test Loss: [2.6077533, 0.32974306, 4.8857636]\n",
      "28: Train Loss: [2.0708306, 0.30679446, 3.8348668] | Test Loss: [2.589188, 0.33847207, 4.8399043]\n",
      "29: Train Loss: [2.1976948, 0.29010582, 4.1052837] | Test Loss: [2.5677261, 0.3565051, 4.7789474]\n",
      "30: Train Loss: [2.1549294, 0.37935895, 3.9305] | Test Loss: [2.701684, 0.38522512, 5.0181427]\n",
      "31: Train Loss: [2.1518905, 0.34264392, 3.961137] | Test Loss: [2.5506623, 0.34550425, 4.7558203]\n",
      "32: Train Loss: [2.1374662, 0.34867093, 3.9262614] | Test Loss: [2.726843, 0.39519817, 5.058488]\n",
      "33: Train Loss: [2.1512117, 0.3107948, 3.9916286] | Test Loss: [2.4848366, 0.38027745, 4.5893955]\n",
      "34: Train Loss: [2.2742245, 0.34297326, 4.205476] | Test Loss: [2.6690316, 0.4498261, 4.888237]\n",
      "35: Train Loss: [2.3505487, 0.38746387, 4.3136334] | Test Loss: [2.8362608, 0.4302778, 5.242244]\n",
      "36: Train Loss: [2.1444154, 0.28322867, 4.005602] | Test Loss: [2.6759353, 0.37576175, 4.976109]\n",
      "37: Train Loss: [2.167838, 0.3475445, 3.9881318] | Test Loss: [2.5833595, 0.363094, 4.803625]\n",
      "38: Train Loss: [2.17776, 0.3304291, 4.0250907] | Test Loss: [2.5891361, 0.34340632, 4.834866]\n",
      "39: Train Loss: [2.2416744, 0.35370243, 4.1296463] | Test Loss: [2.5847597, 0.4104165, 4.759103]\n",
      "40: Train Loss: [2.2744856, 0.34598288, 4.202988] | Test Loss: [2.6265514, 0.38018802, 4.872915]\n",
      "41: Train Loss: [2.2359638, 0.39400175, 4.0779257] | Test Loss: [2.3448465, 0.3747226, 4.3149705]\n",
      "42: Train Loss: [2.2082276, 0.346978, 4.069477] | Test Loss: [2.632737, 0.31835806, 4.947116]\n",
      "43: Train Loss: [2.1295931, 0.37454793, 3.8846383] | Test Loss: [2.4934678, 0.38105303, 4.6058826]\n",
      "44: Train Loss: [2.2021217, 0.34635088, 4.057893] | Test Loss: [2.5535445, 0.3826495, 4.7244396]\n",
      "45: Train Loss: [2.241386, 0.33718202, 4.14559] | Test Loss: [2.6787353, 0.3430851, 5.014385]\n",
      "46: Train Loss: [2.332891, 0.32556665, 4.340215] | Test Loss: [2.6214712, 0.30216056, 4.9407816]\n",
      "47: Train Loss: [2.211646, 0.35977644, 4.0635157] | Test Loss: [2.748609, 0.38788706, 5.109331]\n",
      "48: Train Loss: [2.2858078, 0.33847624, 4.2331395] | Test Loss: [2.5486119, 0.43749455, 4.659729]\n",
      "49: Train Loss: [2.2684417, 0.33596632, 4.2009172] | Test Loss: [2.6409323, 0.46325466, 4.81861]\n",
      "50: Train Loss: [2.2077925, 0.3501907, 4.0653944] | Test Loss: [2.514711, 0.3660144, 4.6634073]\n",
      "51: Train Loss: [2.1421123, 0.41127434, 3.8729503] | Test Loss: [2.8195853, 0.4232968, 5.2158737]\n",
      "52: Train Loss: [2.4455078, 0.38465643, 4.506359] | Test Loss: [2.6420147, 0.3007018, 4.983328]\n",
      "53: Train Loss: [2.3012247, 0.31705725, 4.2853923] | Test Loss: [2.6907763, 0.46976173, 4.911791]\n",
      "54: Train Loss: [2.2007706, 0.34656075, 4.0549803] | Test Loss: [2.6804316, 0.3616627, 4.9992003]\n",
      "55: Train Loss: [2.1882105, 0.40383887, 3.9725823] | Test Loss: [2.7223692, 0.3566838, 5.0880547]\n",
      "56: Train Loss: [2.2797809, 0.39424846, 4.1653132] | Test Loss: [2.6794634, 0.39040256, 4.968524]\n",
      "57: Train Loss: [2.1393287, 0.366274, 3.9123836] | Test Loss: [2.5760558, 0.32379478, 4.8283167]\n",
      "58: Train Loss: [2.1045265, 0.3736539, 3.835399] | Test Loss: [2.7279308, 0.32337272, 5.1324887]\n",
      "59: Train Loss: [2.31009, 0.3830237, 4.2371564] | Test Loss: [2.6533608, 0.32994667, 4.976775]\n",
      "60: Train Loss: [2.4027236, 0.34803084, 4.457416] | Test Loss: [2.524732, 0.36416492, 4.6852994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61: Train Loss: [2.2206728, 0.39887133, 4.0424743] | Test Loss: [2.43587, 0.29409125, 4.5776486]\n",
      "62: Train Loss: [2.2013233, 0.29791057, 4.104736] | Test Loss: [2.6154377, 0.34283355, 4.888042]\n",
      "63: Train Loss: [2.229409, 0.40507907, 4.053739] | Test Loss: [2.4987268, 0.46939763, 4.528056]\n",
      "64: Train Loss: [2.2852058, 0.32602105, 4.2443905] | Test Loss: [2.4729943, 0.38824013, 4.5577483]\n",
      "65: Train Loss: [2.2964618, 0.35468632, 4.2382374] | Test Loss: [2.4857016, 0.47450912, 4.496894]\n",
      "66: Train Loss: [2.2364426, 0.33088496, 4.142] | Test Loss: [2.5773437, 0.32659292, 4.8280945]\n",
      "67: Train Loss: [2.3337774, 0.39121133, 4.2763433] | Test Loss: [2.6338296, 0.4113654, 4.8562937]\n",
      "68: Train Loss: [2.1704297, 0.33592823, 4.004931] | Test Loss: [2.7207701, 0.3899898, 5.0515504]\n",
      "69: Train Loss: [2.1410975, 0.35266665, 3.9295282] | Test Loss: [2.6703718, 0.37931746, 4.9614263]\n",
      "70: Train Loss: [2.2685409, 0.4672714, 4.0698104] | Test Loss: [2.6413023, 0.33890465, 4.9437]\n",
      "71: Train Loss: [2.3572643, 0.32641467, 4.388114] | Test Loss: [2.6291237, 0.41304237, 4.845205]\n",
      "72: Train Loss: [2.2773032, 0.3809259, 4.173681] | Test Loss: [2.6104064, 0.3039837, 4.916829]\n",
      "73: Train Loss: [2.251284, 0.31300354, 4.189564] | Test Loss: [2.5388165, 0.43168682, 4.645946]\n",
      "74: Train Loss: [2.1827269, 0.37387812, 3.9915757] | Test Loss: [2.5674286, 0.4048344, 4.730023]\n",
      "75: Train Loss: [2.2179108, 0.34845823, 4.0873632] | Test Loss: [2.8096664, 0.31453195, 5.304801]\n",
      "76: Train Loss: [2.304272, 0.374304, 4.23424] | Test Loss: [2.5439405, 0.3142455, 4.7736354]\n",
      "77: Train Loss: [2.4123836, 0.5316454, 4.293122] | Test Loss: [2.5822654, 0.39053258, 4.7739983]\n",
      "78: Train Loss: [2.3253872, 0.39881146, 4.251963] | Test Loss: [2.7162278, 0.41580826, 5.0166473]\n",
      "79: Train Loss: [2.2720983, 0.3574334, 4.1867633] | Test Loss: [2.603516, 0.35914496, 4.847887]\n",
      "80: Train Loss: [2.2553818, 0.36448562, 4.146278] | Test Loss: [2.7475138, 0.35354453, 5.141483]\n",
      "81: Train Loss: [2.2231667, 0.38824993, 4.0580835] | Test Loss: [2.8180504, 0.35237867, 5.283722]\n",
      "82: Train Loss: [2.2212303, 0.3285585, 4.113902] | Test Loss: [2.5728705, 0.46355173, 4.6821895]\n",
      "83: Train Loss: [2.0911734, 0.36057633, 3.8217707] | Test Loss: [2.6985235, 0.4427743, 4.9542727]\n",
      "84: Train Loss: [2.1969116, 0.3439211, 4.049902] | Test Loss: [2.6436124, 0.3852053, 4.9020195]\n",
      "85: Train Loss: [2.3326814, 0.42285267, 4.2425103] | Test Loss: [2.646989, 0.368391, 4.925587]\n",
      "86: Train Loss: [2.2935832, 0.5264178, 4.0607486] | Test Loss: [2.7465923, 0.30624768, 5.186937]\n",
      "87: Train Loss: [2.28607, 0.38938314, 4.182757] | Test Loss: [2.6157002, 0.4566596, 4.7747407]\n",
      "88: Train Loss: [2.3636005, 0.3580602, 4.3691406] | Test Loss: [2.6118653, 0.3533067, 4.870424]\n",
      "89: Train Loss: [2.2070174, 0.32108578, 4.092949] | Test Loss: [2.7041252, 0.39530474, 5.0129457]\n",
      "90: Train Loss: [2.319222, 0.37956494, 4.258879] | Test Loss: [2.5496082, 0.3551773, 4.744039]\n",
      "91: Train Loss: [2.3162904, 0.3260864, 4.306494] | Test Loss: [2.618394, 0.33675936, 4.900028]\n",
      "92: Train Loss: [2.3484025, 0.42994273, 4.2668624] | Test Loss: [2.6525521, 0.33073366, 4.9743705]\n",
      "93: Train Loss: [2.2291927, 0.3503022, 4.1080832] | Test Loss: [2.72409, 0.4271028, 5.0210776]\n",
      "94: Train Loss: [2.3574052, 0.36666074, 4.34815] | Test Loss: [2.3518236, 0.34127894, 4.362368]\n",
      "95: Train Loss: [2.2378454, 0.3712792, 4.1044116] | Test Loss: [2.5604293, 0.3752627, 4.745596]\n",
      "96: Train Loss: [2.2090883, 0.29089066, 4.127286] | Test Loss: [2.6450624, 0.34627426, 4.9438505]\n",
      "97: Train Loss: [2.3390753, 0.2864652, 4.3916855] | Test Loss: [2.60276, 0.3684003, 4.83712]\n",
      "98: Train Loss: [2.2970858, 0.30678284, 4.287389] | Test Loss: [2.521469, 0.42571497, 4.6172233]\n",
      "99: Train Loss: [2.182172, 0.3097214, 4.0546227] | Test Loss: [2.5728652, 0.4530017, 4.692729]\n",
      "100: Train Loss: [2.2071226, 0.33692867, 4.0773163] | Test Loss: [2.6220121, 0.38049355, 4.8635306]\n",
      "101: Train Loss: [2.1700125, 0.3898418, 3.950183] | Test Loss: [2.6181324, 0.32640153, 4.909863]\n",
      "102: Train Loss: [2.2796156, 0.46916348, 4.090068] | Test Loss: [2.6658, 0.31499568, 5.0166044]\n",
      "103: Train Loss: [2.315, 0.39210278, 4.2378974] | Test Loss: [2.6445255, 0.3672521, 4.9217987]\n",
      "104: Train Loss: [2.1572125, 0.31889653, 3.9955285] | Test Loss: [2.4790902, 0.37259966, 4.585581]\n",
      "105: Train Loss: [2.3287098, 0.3325328, 4.324887] | Test Loss: [2.6666288, 0.37282714, 4.9604306]\n",
      "106: Train Loss: [2.2885683, 0.34940305, 4.2277336] | Test Loss: [2.7632377, 0.3814176, 5.1450577]\n",
      "107: Train Loss: [2.3139539, 0.50066686, 4.127241] | Test Loss: [2.7601583, 0.3984852, 5.1218314]\n",
      "108: Train Loss: [2.4108512, 0.33202288, 4.48968] | Test Loss: [2.7256274, 0.40603215, 5.0452228]\n",
      "109: Train Loss: [2.227401, 0.34295878, 4.111843] | Test Loss: [2.4706802, 0.37905034, 4.56231]\n",
      "110: Train Loss: [2.1724815, 0.4018672, 3.943096] | Test Loss: [2.717348, 0.34052524, 5.094171]\n",
      "111: Train Loss: [2.169323, 0.35242125, 3.9862247] | Test Loss: [2.7402635, 0.34428194, 5.136245]\n",
      "112: Train Loss: [2.3222935, 0.34830728, 4.29628] | Test Loss: [2.6046298, 0.45130906, 4.7579503]\n",
      "113: Train Loss: [2.2510262, 0.386606, 4.115446] | Test Loss: [2.6120794, 0.36151648, 4.8626423]\n",
      "114: Train Loss: [2.2972507, 0.35561016, 4.238891] | Test Loss: [2.6007683, 0.36362448, 4.837912]\n",
      "115: Train Loss: [2.3969564, 0.34341818, 4.450495] | Test Loss: [2.822906, 0.40979284, 5.236019]\n",
      "116: Train Loss: [2.1820168, 0.28178728, 4.0822463] | Test Loss: [2.5910037, 0.31810188, 4.8639054]\n",
      "117: Train Loss: [2.4395034, 0.41538727, 4.4636197] | Test Loss: [2.679806, 0.3730998, 4.986512]\n",
      "118: Train Loss: [2.1879735, 0.34126508, 4.034682] | Test Loss: [2.6598866, 0.39468995, 4.925083]\n",
      "119: Train Loss: [2.3210773, 0.3438471, 4.2983074] | Test Loss: [2.4612494, 0.3522891, 4.5702095]\n",
      "120: Train Loss: [2.202238, 0.5323062, 3.87217] | Test Loss: [2.6830668, 0.37126127, 4.9948726]\n",
      "121: Train Loss: [2.237419, 0.3012655, 4.173572] | Test Loss: [2.6102335, 0.38615397, 4.834313]\n",
      "122: Train Loss: [2.4378512, 0.35077944, 4.524923] | Test Loss: [2.539047, 0.37475076, 4.7033434]\n",
      "123: Train Loss: [2.336748, 0.34904334, 4.3244524] | Test Loss: [2.6225133, 0.33367315, 4.9113536]\n",
      "124: Train Loss: [2.2685473, 0.35737765, 4.179717] | Test Loss: [2.527314, 0.42544928, 4.6291785]\n",
      "125: Train Loss: [2.309124, 0.3299956, 4.2882524] | Test Loss: [2.5240126, 0.30659032, 4.7414346]\n",
      "126: Train Loss: [2.2713223, 0.36601236, 4.176632] | Test Loss: [2.5689209, 0.32155812, 4.8162837]\n",
      "127: Train Loss: [2.345636, 0.45770943, 4.2335625] | Test Loss: [2.5592682, 0.3800474, 4.738489]\n",
      "128: Train Loss: [2.2854357, 0.31200272, 4.2588687] | Test Loss: [2.739702, 0.34369496, 5.135709]\n",
      "129: Train Loss: [2.2351174, 0.3796733, 4.0905614] | Test Loss: [2.7686434, 0.37658033, 5.1607065]\n",
      "130: Train Loss: [2.2461846, 0.39762765, 4.0947413] | Test Loss: [2.645892, 0.3149332, 4.9768505]\n",
      "131: Train Loss: [2.40651, 0.36004487, 4.4529753] | Test Loss: [2.7852209, 0.4023996, 5.168042]\n",
      "132: Train Loss: [2.4470177, 0.3903845, 4.5036507] | Test Loss: [2.6792455, 0.37538612, 4.9831047]\n",
      "133: Train Loss: [2.3120947, 0.32283184, 4.3013577] | Test Loss: [2.424455, 0.3086609, 4.540249]\n",
      "134: Train Loss: [2.332816, 0.3473646, 4.3182673] | Test Loss: [2.5965776, 0.3780242, 4.815131]\n",
      "135: Train Loss: [2.3929095, 0.3879357, 4.3978834] | Test Loss: [2.5739734, 0.33875072, 4.809196]\n",
      "136: Train Loss: [2.3580544, 0.3336324, 4.3824763] | Test Loss: [2.515476, 0.34543338, 4.6855187]\n",
      "137: Train Loss: [2.4802048, 0.3430654, 4.6173444] | Test Loss: [2.731656, 0.41392073, 5.0493913]\n",
      "138: Train Loss: [2.2921872, 0.34603387, 4.2383404] | Test Loss: [2.648502, 0.32725555, 4.9697485]\n",
      "139: Train Loss: [2.3513134, 0.4569096, 4.245717] | Test Loss: [2.8061917, 0.5244234, 5.08796]\n",
      "140: Train Loss: [2.3159425, 0.3010938, 4.3307915] | Test Loss: [2.655094, 0.5309636, 4.7792244]\n",
      "141: Train Loss: [2.2086916, 0.35975957, 4.057624] | Test Loss: [2.620491, 0.35832483, 4.882657]\n",
      "142: Train Loss: [2.327035, 0.36112761, 4.292942] | Test Loss: [3.1398122, 0.4208212, 5.8588033]\n",
      "143: Train Loss: [2.191967, 0.36594504, 4.017989] | Test Loss: [2.638919, 0.36566398, 4.912174]\n",
      "144: Train Loss: [2.3335078, 0.33616617, 4.330849] | Test Loss: [2.8441484, 0.42018119, 5.2681155]\n",
      "145: Train Loss: [2.3494563, 0.3955651, 4.3033476] | Test Loss: [2.5381827, 0.38053644, 4.695829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146: Train Loss: [2.2933264, 0.42277712, 4.1638756] | Test Loss: [2.545544, 0.46980318, 4.6212845]\n",
      "147: Train Loss: [2.2655883, 0.35394508, 4.1772313] | Test Loss: [2.6762576, 0.34736744, 5.005148]\n",
      "148: Train Loss: [2.3503847, 0.4481854, 4.252584] | Test Loss: [2.629332, 0.37431026, 4.884354]\n",
      "149: Train Loss: [2.366635, 0.38511875, 4.348151] | Test Loss: [2.6338406, 0.3705307, 4.8971505]\n",
      "150: Train Loss: [2.2729306, 0.33062539, 4.2152357] | Test Loss: [2.6454916, 0.32453778, 4.9664454]\n",
      "151: Train Loss: [2.2035804, 0.33820567, 4.068955] | Test Loss: [2.4629407, 0.3223392, 4.6035423]\n",
      "152: Train Loss: [2.3314173, 0.33302912, 4.3298054] | Test Loss: [2.6036873, 0.3594555, 4.847919]\n",
      "153: Train Loss: [2.3289123, 0.3641171, 4.2937074] | Test Loss: [2.6403966, 0.36955467, 4.9112387]\n",
      "154: Train Loss: [2.3750505, 0.41449746, 4.3356037] | Test Loss: [2.732894, 0.34161934, 5.1241684]\n",
      "155: Train Loss: [2.28984, 0.3811695, 4.1985106] | Test Loss: [2.656538, 0.3612781, 4.951798]\n",
      "156: Train Loss: [2.3364024, 0.3213332, 4.3514714] | Test Loss: [2.5984945, 0.39725715, 4.7997317]\n",
      "157: Train Loss: [2.311361, 0.3285603, 4.294162] | Test Loss: [2.7176578, 0.37225252, 5.063063]\n",
      "158: Train Loss: [2.2358136, 0.34855527, 4.123072] | Test Loss: [2.6041462, 0.3640527, 4.8442397]\n",
      "159: Train Loss: [2.2468464, 0.3213826, 4.1723104] | Test Loss: [2.7822225, 0.37497693, 5.189468]\n",
      "160: Train Loss: [2.3714073, 0.42645127, 4.3163633] | Test Loss: [2.6276066, 0.34602442, 4.9091887]\n",
      "161: Train Loss: [2.3355122, 0.32998413, 4.34104] | Test Loss: [2.694097, 0.35083222, 5.037362]\n",
      "162: Train Loss: [2.437498, 0.44572356, 4.4292727] | Test Loss: [2.6863341, 0.35836744, 5.014301]\n",
      "163: Train Loss: [2.3718662, 0.39378285, 4.34995] | Test Loss: [2.7291224, 0.35945264, 5.098792]\n",
      "164: Train Loss: [2.2201555, 0.30430654, 4.1360044] | Test Loss: [2.4799438, 0.3152992, 4.6445885]\n",
      "165: Train Loss: [2.332893, 0.3777384, 4.2880473] | Test Loss: [2.7308476, 0.35829797, 5.1033974]\n",
      "166: Train Loss: [2.3492928, 0.30829358, 4.3902917] | Test Loss: [2.5710034, 0.37839234, 4.7636147]\n",
      "167: Train Loss: [2.2309175, 0.33927092, 4.122564] | Test Loss: [2.597111, 0.36420664, 4.830015]\n",
      "168: Train Loss: [2.1977458, 0.27783364, 4.117658] | Test Loss: [2.5146465, 0.4210124, 4.6082807]\n",
      "169: Train Loss: [2.323045, 0.31195763, 4.334132] | Test Loss: [2.6945968, 0.42226863, 4.9669247]\n",
      "170: Train Loss: [2.30881, 0.42356238, 4.1940575] | Test Loss: [2.6188662, 0.38986427, 4.847868]\n",
      "171: Train Loss: [2.3090727, 0.4503005, 4.167845] | Test Loss: [2.5929801, 0.4362054, 4.749755]\n",
      "172: Train Loss: [2.189992, 0.3565483, 4.0234356] | Test Loss: [2.6998448, 0.34239995, 5.0572896]\n",
      "173: Train Loss: [2.3479693, 0.3436176, 4.352321] | Test Loss: [2.6759603, 0.3842776, 4.967643]\n",
      "174: Train Loss: [2.3028603, 0.35371447, 4.252006] | Test Loss: [2.6209512, 0.3642053, 4.877697]\n",
      "175: Train Loss: [2.2206666, 0.37905183, 4.0622816] | Test Loss: [2.6041303, 0.32272798, 4.8855324]\n",
      "176: Train Loss: [2.2710612, 0.37099317, 4.171129] | Test Loss: [2.4473863, 0.31805968, 4.576713]\n",
      "177: Train Loss: [2.283871, 0.37697217, 4.1907697] | Test Loss: [2.5115275, 0.36334422, 4.659711]\n",
      "178: Train Loss: [2.1578083, 0.27328366, 4.042333] | Test Loss: [2.7029452, 0.3706902, 5.0352]\n",
      "179: Train Loss: [2.2339292, 0.31339675, 4.1544614] | Test Loss: [2.809337, 0.3577202, 5.2609534]\n",
      "180: Train Loss: [2.309217, 0.3410439, 4.27739] | Test Loss: [2.5725029, 0.334883, 4.8101225]\n",
      "181: Train Loss: [2.231991, 0.3473046, 4.1166773] | Test Loss: [2.6647766, 0.29100502, 5.038548]\n",
      "182: Train Loss: [2.3919303, 0.37217447, 4.4116864] | Test Loss: [2.5951915, 0.4306838, 4.7596993]\n",
      "183: Train Loss: [2.3454413, 0.43278405, 4.2580986] | Test Loss: [2.8387983, 0.47939685, 5.1981997]\n",
      "184: Train Loss: [2.28515, 0.3296884, 4.2406116] | Test Loss: [2.6246068, 0.38183606, 4.8673778]\n",
      "185: Train Loss: [2.372276, 0.43930787, 4.3052444] | Test Loss: [2.5220444, 0.4167269, 4.627362]\n",
      "186: Train Loss: [2.3275046, 0.40867972, 4.24633] | Test Loss: [2.7235441, 0.4111035, 5.035985]\n",
      "187: Train Loss: [2.3341033, 0.36158842, 4.306618] | Test Loss: [2.666211, 0.39400312, 4.938419]\n",
      "188: Train Loss: [2.1922705, 0.46005735, 3.9244838] | Test Loss: [2.6799989, 0.36614025, 4.9938574]\n",
      "189: Train Loss: [2.3311355, 0.34165, 4.320621] | Test Loss: [2.6004539, 0.3960266, 4.804881]\n",
      "190: Train Loss: [2.1762056, 0.36669, 3.9857213] | Test Loss: [2.7416124, 0.31824633, 5.1649785]\n",
      "191: Train Loss: [2.1623683, 0.34894657, 3.97579] | Test Loss: [2.5136342, 0.34753317, 4.679735]\n",
      "192: Train Loss: [2.277005, 0.36650145, 4.1875086] | Test Loss: [2.7042296, 0.45608327, 4.952376]\n",
      "193: Train Loss: [2.1573236, 0.30756703, 4.00708] | Test Loss: [2.5008988, 0.45870608, 4.543092]\n",
      "194: Train Loss: [2.231471, 0.3061269, 4.156815] | Test Loss: [2.746628, 0.4174157, 5.0758405]\n",
      "195: Train Loss: [2.1959813, 0.4106098, 3.9813528] | Test Loss: [2.6254573, 0.41328502, 4.83763]\n",
      "196: Train Loss: [2.2513323, 0.29828602, 4.2043786] | Test Loss: [2.69148, 0.39454982, 4.98841]\n",
      "197: Train Loss: [2.3402598, 0.3918557, 4.288664] | Test Loss: [2.482627, 0.28279498, 4.682459]\n",
      "198: Train Loss: [2.2448049, 0.36006856, 4.129541] | Test Loss: [2.4885454, 0.3658218, 4.611269]\n",
      "199: Train Loss: [2.4372315, 0.33711088, 4.537352] | Test Loss: [2.590443, 0.33432668, 4.846559]\n",
      "200: Train Loss: [2.1909213, 0.31749564, 4.064347] | Test Loss: [2.8547173, 0.46583727, 5.243597]\n",
      "201: Train Loss: [2.5231085, 0.40534607, 4.640871] | Test Loss: [2.22582, 0.40762478, 4.0440154]\n",
      "202: Train Loss: [2.1891606, 0.30781734, 4.0705037] | Test Loss: [2.569218, 0.34479362, 4.793642]\n",
      "203: Train Loss: [2.2502882, 0.3587305, 4.141846] | Test Loss: [2.6248665, 0.4086801, 4.841053]\n",
      "204: Train Loss: [2.2939513, 0.3528543, 4.2350483] | Test Loss: [2.580599, 0.3545333, 4.806665]\n",
      "205: Train Loss: [2.424862, 0.34119534, 4.5085287] | Test Loss: [2.5041795, 0.3588573, 4.649502]\n",
      "206: Train Loss: [2.3274646, 0.3816965, 4.2732325] | Test Loss: [2.4119039, 0.29682818, 4.5269794]\n",
      "207: Train Loss: [2.2089677, 0.34847847, 4.069457] | Test Loss: [2.6132882, 0.39245543, 4.8341208]\n",
      "208: Train Loss: [2.259494, 0.32264173, 4.1963463] | Test Loss: [2.7100816, 0.34393233, 5.076231]\n",
      "209: Train Loss: [2.4080417, 0.3236678, 4.4924154] | Test Loss: [2.6853795, 0.3710637, 4.9996953]\n",
      "210: Train Loss: [2.1923404, 0.36065364, 4.0240273] | Test Loss: [2.5246537, 0.40326267, 4.6460447]\n",
      "211: Train Loss: [2.1728632, 0.337613, 4.0081134] | Test Loss: [2.6945863, 0.3749844, 5.0141883]\n",
      "212: Train Loss: [2.2094789, 0.29743963, 4.121518] | Test Loss: [2.6812532, 0.3571512, 5.0053554]\n",
      "213: Train Loss: [2.1580555, 0.357856, 3.958255] | Test Loss: [2.783063, 0.3726682, 5.1934576]\n",
      "214: Train Loss: [2.2847788, 0.35422918, 4.2153287] | Test Loss: [2.6481488, 0.4205362, 4.8757615]\n",
      "215: Train Loss: [2.180435, 0.321, 4.03987] | Test Loss: [2.6731272, 0.35332596, 4.9929285]\n",
      "216: Train Loss: [2.155514, 0.29845047, 4.0125775] | Test Loss: [2.6228342, 0.40096408, 4.844704]\n",
      "217: Train Loss: [2.3674564, 0.37488, 4.360033] | Test Loss: [2.8660765, 0.36404505, 5.368108]\n",
      "218: Train Loss: [2.3524568, 0.3365558, 4.3683577] | Test Loss: [2.6674192, 0.37619886, 4.9586396]\n",
      "219: Train Loss: [2.3244133, 0.32571965, 4.323107] | Test Loss: [2.576889, 0.3529674, 4.800811]\n",
      "220: Train Loss: [2.2994342, 0.38426924, 4.214599] | Test Loss: [2.5461793, 0.35864824, 4.7337103]\n",
      "221: Train Loss: [2.3526103, 0.2772159, 4.4280047] | Test Loss: [2.4456782, 0.3309953, 4.5603614]\n",
      "222: Train Loss: [2.4032907, 0.3385185, 4.468063] | Test Loss: [2.7930946, 0.360953, 5.2252364]\n",
      "223: Train Loss: [2.4877825, 0.3913742, 4.584191] | Test Loss: [2.5781066, 0.28658152, 4.869632]\n",
      "224: Train Loss: [2.3183517, 0.35493457, 4.281769] | Test Loss: [2.6840615, 0.38952073, 4.9786024]\n",
      "225: Train Loss: [2.3493712, 0.3880708, 4.310672] | Test Loss: [2.4966047, 0.34729773, 4.6459117]\n",
      "226: Train Loss: [2.2130249, 0.33826256, 4.087787] | Test Loss: [2.6568034, 0.36883116, 4.9447756]\n",
      "227: Train Loss: [2.3996735, 0.37007904, 4.429268] | Test Loss: [2.5120726, 0.45138082, 4.5727644]\n",
      "228: Train Loss: [2.3735476, 0.37185836, 4.3752365] | Test Loss: [3.0565484, 0.4481843, 5.664912]\n",
      "229: Train Loss: [2.3074312, 0.39856294, 4.2162995] | Test Loss: [2.4438117, 0.39950532, 4.488118]\n",
      "230: Train Loss: [2.4373958, 0.36272034, 4.512071] | Test Loss: [2.5673773, 0.35121095, 4.7835436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231: Train Loss: [2.373536, 0.3568873, 4.390185] | Test Loss: [2.2467196, 0.24150059, 4.251939]\n",
      "232: Train Loss: [2.318423, 0.35259816, 4.284248] | Test Loss: [2.720067, 0.38558164, 5.0545526]\n",
      "233: Train Loss: [2.3594296, 0.39992046, 4.3189387] | Test Loss: [2.7207532, 0.36817825, 5.073328]\n",
      "234: Train Loss: [2.2952118, 0.3351777, 4.2552457] | Test Loss: [2.697837, 0.42527327, 4.970401]\n",
      "235: Train Loss: [2.3486078, 0.35994467, 4.3372707] | Test Loss: [2.563036, 0.3704947, 4.755577]\n",
      "236: Train Loss: [2.3914082, 0.33714768, 4.4456687] | Test Loss: [2.6737401, 0.46559995, 4.8818803]\n",
      "237: Train Loss: [2.3001962, 0.30564943, 4.294743] | Test Loss: [2.8167322, 0.39421335, 5.239251]\n",
      "238: Train Loss: [2.3976884, 0.33177543, 4.463601] | Test Loss: [2.5564022, 0.35680434, 4.756]\n",
      "239: Train Loss: [2.401548, 0.51133436, 4.2917614] | Test Loss: [2.6635013, 0.37402254, 4.95298]\n",
      "240: Train Loss: [2.2157602, 0.33000016, 4.1015205] | Test Loss: [2.7393112, 0.3890201, 5.0896025]\n",
      "241: Train Loss: [2.3041, 0.35636228, 4.2518377] | Test Loss: [2.6692612, 0.30977547, 5.028747]\n",
      "242: Train Loss: [2.2942145, 0.3502496, 4.238179] | Test Loss: [2.6175642, 0.361132, 4.8739963]\n",
      "243: Train Loss: [2.1569436, 0.33536246, 3.9785244] | Test Loss: [2.5097327, 0.29657546, 4.72289]\n",
      "244: Train Loss: [2.3587492, 0.470578, 4.24692] | Test Loss: [2.4528036, 0.42123368, 4.4843736]\n",
      "245: Train Loss: [2.3237307, 0.4073617, 4.2401] | Test Loss: [2.645226, 0.381568, 4.908884]\n",
      "246: Train Loss: [2.3373876, 0.29242557, 4.3823495] | Test Loss: [2.5112836, 0.36803094, 4.6545362]\n",
      "247: Train Loss: [2.3151097, 0.29809946, 4.33212] | Test Loss: [2.8202112, 0.42570132, 5.214721]\n",
      "248: Train Loss: [2.293338, 0.39909235, 4.187584] | Test Loss: [2.7583234, 0.4732271, 5.04342]\n",
      "249: Train Loss: [2.26137, 0.2981541, 4.224586] | Test Loss: [2.6134841, 0.3101967, 4.9167714]\n",
      "250: Train Loss: [2.4188967, 0.34015176, 4.4976416] | Test Loss: [2.3985896, 0.33197036, 4.465209]\n",
      "251: Train Loss: [2.3051422, 0.36756033, 4.242724] | Test Loss: [2.781307, 0.43016508, 5.1324487]\n",
      "252: Train Loss: [2.2148101, 0.33898777, 4.0906324] | Test Loss: [2.4706402, 0.36770523, 4.573575]\n",
      "253: Train Loss: [2.3518658, 0.3420495, 4.361682] | Test Loss: [2.6776037, 0.29758474, 5.057623]\n",
      "254: Train Loss: [2.2254887, 0.35663486, 4.0943427] | Test Loss: [2.70154, 0.3616273, 5.041453]\n",
      "255: Train Loss: [2.4164739, 0.41050765, 4.42244] | Test Loss: [2.5650165, 0.3825594, 4.7474737]\n",
      "256: Train Loss: [2.3670447, 0.39941728, 4.334672] | Test Loss: [2.6493948, 0.37201416, 4.9267755]\n",
      "257: Train Loss: [2.2113464, 0.2784208, 4.144272] | Test Loss: [2.64566, 0.30390102, 4.9874187]\n",
      "258: Train Loss: [2.2753785, 0.31473508, 4.236022] | Test Loss: [2.6138508, 0.39198205, 4.8357196]\n",
      "259: Train Loss: [2.4393198, 0.37454966, 4.50409] | Test Loss: [2.6549253, 0.30772758, 5.0021234]\n",
      "260: Train Loss: [2.314517, 0.3193215, 4.3097124] | Test Loss: [2.6658897, 0.41129813, 4.920481]\n",
      "261: Train Loss: [2.261516, 0.30933493, 4.2136974] | Test Loss: [2.5317898, 0.4559156, 4.607664]\n",
      "262: Train Loss: [2.2691653, 0.4349275, 4.103403] | Test Loss: [2.4541366, 0.3745539, 4.5337195]\n",
      "263: Train Loss: [2.257482, 0.32498613, 4.189978] | Test Loss: [2.5230162, 0.36344814, 4.6825843]\n",
      "264: Train Loss: [2.1089914, 0.3095152, 3.9084675] | Test Loss: [2.6818852, 0.31931797, 5.0444527]\n",
      "265: Train Loss: [2.3211663, 0.46374458, 4.178588] | Test Loss: [2.3975348, 0.3097706, 4.485299]\n",
      "266: Train Loss: [2.3703742, 0.40568078, 4.3350677] | Test Loss: [2.7600875, 0.34859776, 5.171577]\n",
      "267: Train Loss: [2.194369, 0.3080203, 4.080718] | Test Loss: [2.6400318, 0.37847245, 4.9015913]\n",
      "268: Train Loss: [2.3414624, 0.3415201, 4.3414044] | Test Loss: [2.6009035, 0.36318278, 4.8386245]\n",
      "269: Train Loss: [2.3697555, 0.35123423, 4.3882766] | Test Loss: [2.7619147, 0.39783955, 5.12599]\n",
      "270: Train Loss: [2.5788088, 0.40114665, 4.7564707] | Test Loss: [2.6474447, 0.4976618, 4.797228]\n",
      "271: Train Loss: [2.1824656, 0.3199412, 4.04499] | Test Loss: [2.6452816, 0.37352818, 4.917035]\n",
      "272: Train Loss: [2.1993234, 0.32060608, 4.0780406] | Test Loss: [2.577928, 0.35585177, 4.8000045]\n",
      "273: Train Loss: [2.4496756, 0.32927033, 4.5700808] | Test Loss: [2.9543712, 0.51471865, 5.394024]\n",
      "274: Train Loss: [2.2624474, 0.33044237, 4.1944523] | Test Loss: [2.7019634, 0.42084143, 4.9830856]\n",
      "275: Train Loss: [2.228212, 0.3532391, 4.103185] | Test Loss: [2.7046554, 0.39013636, 5.0191746]\n",
      "276: Train Loss: [2.4713862, 0.32498324, 4.6177893] | Test Loss: [2.6657386, 0.43865904, 4.892818]\n",
      "277: Train Loss: [2.3498635, 0.41623574, 4.283491] | Test Loss: [2.6760335, 0.3409768, 5.0110903]\n",
      "278: Train Loss: [2.3316267, 0.40678644, 4.256467] | Test Loss: [2.6027303, 0.311206, 4.8942547]\n",
      "279: Train Loss: [2.3066838, 0.3770934, 4.2362742] | Test Loss: [2.5668366, 0.35484573, 4.7788277]\n",
      "280: Train Loss: [2.367101, 0.34201014, 4.392192] | Test Loss: [2.7654545, 0.41515768, 5.1157513]\n",
      "281: Train Loss: [2.2792346, 0.4006505, 4.157819] | Test Loss: [2.6003413, 0.34987545, 4.850807]\n",
      "282: Train Loss: [2.405498, 0.36943445, 4.4415617] | Test Loss: [2.659463, 0.40174773, 4.917178]\n",
      "283: Train Loss: [2.3552172, 0.33642104, 4.3740134] | Test Loss: [2.374623, 0.32470858, 4.4245377]\n",
      "284: Train Loss: [2.207874, 0.33769992, 4.078048] | Test Loss: [2.7155454, 0.3246966, 5.1063943]\n",
      "285: Train Loss: [2.2109485, 0.33698657, 4.0849104] | Test Loss: [2.7063541, 0.43291545, 4.979793]\n",
      "286: Train Loss: [2.306615, 0.3923807, 4.2208495] | Test Loss: [2.5662134, 0.34934303, 4.783084]\n",
      "287: Train Loss: [2.2824874, 0.36154515, 4.2034297] | Test Loss: [2.600644, 0.33314505, 4.868143]\n",
      "288: Train Loss: [2.4062066, 0.39725232, 4.415161] | Test Loss: [2.5369563, 0.33000854, 4.743904]\n",
      "289: Train Loss: [2.348641, 0.4069299, 4.290352] | Test Loss: [2.4482558, 0.48912457, 4.407387]\n",
      "290: Train Loss: [2.322166, 0.33608627, 4.3082457] | Test Loss: [2.61359, 0.3479333, 4.8792467]\n",
      "291: Train Loss: [2.172494, 0.351616, 3.9933717] | Test Loss: [2.4498758, 0.26625454, 4.633497]\n",
      "292: Train Loss: [2.5690594, 0.3401462, 4.7979727] | Test Loss: [2.5639331, 0.3952164, 4.73265]\n",
      "293: Train Loss: [2.3031373, 0.35600865, 4.250266] | Test Loss: [2.6401396, 0.36364245, 4.9166365]\n",
      "294: Train Loss: [2.208895, 0.41330507, 4.0044847] | Test Loss: [2.6311336, 0.3691588, 4.8931084]\n",
      "295: Train Loss: [2.2940977, 0.3715379, 4.2166576] | Test Loss: [2.5532124, 0.3995146, 4.70691]\n",
      "296: Train Loss: [2.4054785, 0.3764126, 4.4345446] | Test Loss: [2.6607766, 0.34114495, 4.980408]\n",
      "297: Train Loss: [2.301639, 0.36759198, 4.2356863] | Test Loss: [2.605954, 0.30776033, 4.9041476]\n",
      "298: Train Loss: [2.3236787, 0.40355, 4.2438073] | Test Loss: [2.5004234, 0.35103685, 4.64981]\n",
      "299: Train Loss: [2.3836098, 0.43381777, 4.3334017] | Test Loss: [2.8355699, 0.33180308, 5.339337]\n",
      "300: Train Loss: [2.287318, 0.38201317, 4.1926227] | Test Loss: [2.5482495, 0.4392804, 4.6572185]\n",
      "301: Train Loss: [2.519222, 0.51578087, 4.522663] | Test Loss: [2.709922, 0.3721204, 5.047724]\n",
      "302: Train Loss: [2.4190476, 0.34328318, 4.494812] | Test Loss: [2.5754635, 0.37296188, 4.777965]\n",
      "303: Train Loss: [2.375114, 0.34293, 4.407298] | Test Loss: [2.534382, 0.35933623, 4.709428]\n",
      "304: Train Loss: [2.1453998, 0.37807697, 3.9127228] | Test Loss: [2.487494, 0.35676995, 4.618218]\n",
      "305: Train Loss: [2.26581, 0.3549743, 4.1766458] | Test Loss: [2.6172855, 0.35790455, 4.8766665]\n",
      "306: Train Loss: [2.4084249, 0.4338666, 4.382983] | Test Loss: [2.7496812, 0.37318003, 5.1261826]\n",
      "307: Train Loss: [2.4113524, 0.31879386, 4.503911] | Test Loss: [2.6382859, 0.39351004, 4.883062]\n",
      "308: Train Loss: [2.3453956, 0.32667154, 4.3641195] | Test Loss: [2.7419472, 0.4801253, 5.003769]\n",
      "309: Train Loss: [2.374534, 0.32666034, 4.4224076] | Test Loss: [2.647926, 0.42395192, 4.8719]\n",
      "310: Train Loss: [2.2405305, 0.33987755, 4.1411834] | Test Loss: [2.7918267, 0.28414968, 5.299504]\n",
      "311: Train Loss: [2.3267336, 0.35849202, 4.2949753] | Test Loss: [2.5259147, 0.44576478, 4.606065]\n",
      "312: Train Loss: [2.1820395, 0.29116398, 4.072915] | Test Loss: [2.647409, 0.37145364, 4.923364]\n",
      "313: Train Loss: [2.3613162, 0.35631356, 4.3663187] | Test Loss: [2.4853134, 0.34566188, 4.624965]\n",
      "314: Train Loss: [2.3098454, 0.372329, 4.2473617] | Test Loss: [2.5164933, 0.40786234, 4.6251245]\n",
      "315: Train Loss: [2.2044668, 0.36129403, 4.04764] | Test Loss: [2.423715, 0.32782617, 4.519604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316: Train Loss: [2.2188818, 0.3659874, 4.0717764] | Test Loss: [2.741592, 0.31863394, 5.16455]\n",
      "317: Train Loss: [2.4128263, 0.37245542, 4.453197] | Test Loss: [2.6090176, 0.38905525, 4.82898]\n",
      "318: Train Loss: [2.3356712, 0.43116438, 4.240178] | Test Loss: [2.609712, 0.42558014, 4.7938437]\n",
      "319: Train Loss: [2.3052032, 0.4025021, 4.2079043] | Test Loss: [2.7463188, 0.33879918, 5.1538386]\n",
      "320: Train Loss: [2.4401338, 0.39083108, 4.4894366] | Test Loss: [2.8641353, 0.45291638, 5.2753544]\n",
      "321: Train Loss: [2.3247323, 0.37692505, 4.2725396] | Test Loss: [2.5352535, 0.32073, 4.749777]\n",
      "322: Train Loss: [2.2607572, 0.3504801, 4.1710343] | Test Loss: [2.788543, 0.324794, 5.252292]\n",
      "323: Train Loss: [2.30352, 0.34511533, 4.2619247] | Test Loss: [2.7993894, 0.37279105, 5.225988]\n",
      "324: Train Loss: [2.3779194, 0.35125297, 4.404586] | Test Loss: [2.6344297, 0.29691583, 4.9719434]\n",
      "325: Train Loss: [2.3455245, 0.26416743, 4.426882] | Test Loss: [2.6226413, 0.37480634, 4.8704762]\n",
      "326: Train Loss: [2.351131, 0.33325258, 4.3690095] | Test Loss: [2.6415665, 0.3864784, 4.8966546]\n",
      "327: Train Loss: [2.3619926, 0.38135183, 4.3426332] | Test Loss: [2.6390455, 0.32231298, 4.955778]\n",
      "328: Train Loss: [2.3259177, 0.42894292, 4.2228928] | Test Loss: [2.6113563, 0.39569005, 4.8270226]\n",
      "329: Train Loss: [2.4158714, 0.31337973, 4.518363] | Test Loss: [2.7734296, 0.36222163, 5.1846375]\n",
      "330: Train Loss: [2.401241, 0.30978632, 4.492696] | Test Loss: [2.561365, 0.39235762, 4.730372]\n",
      "331: Train Loss: [2.4391992, 0.37324128, 4.505157] | Test Loss: [2.7107043, 0.33676553, 5.0846434]\n",
      "332: Train Loss: [2.3559384, 0.36026615, 4.3516107] | Test Loss: [2.7309322, 0.54425645, 4.9176083]\n",
      "333: Train Loss: [2.3128757, 0.33218533, 4.293566] | Test Loss: [2.5545564, 0.36880916, 4.7403035]\n",
      "334: Train Loss: [2.616779, 0.40922767, 4.8243303] | Test Loss: [2.7354932, 0.37052715, 5.100459]\n",
      "335: Train Loss: [2.3419142, 0.34375173, 4.3400764] | Test Loss: [2.6020825, 0.3266015, 4.8775635]\n",
      "336: Train Loss: [2.4208424, 0.30959874, 4.532086] | Test Loss: [2.5952432, 0.44274506, 4.747741]\n",
      "337: Train Loss: [2.488793, 0.32103154, 4.656554] | Test Loss: [2.6470897, 0.40040785, 4.8937716]\n",
      "338: Train Loss: [2.5193624, 0.39704224, 4.6416826] | Test Loss: [2.767205, 0.3334567, 5.2009535]\n",
      "339: Train Loss: [2.4623382, 0.37155816, 4.553118] | Test Loss: [2.6154182, 0.35965997, 4.8711762]\n",
      "340: Train Loss: [2.310946, 0.33489242, 4.2869997] | Test Loss: [2.6557474, 0.3628903, 4.9486046]\n",
      "341: Train Loss: [2.4312062, 0.32601196, 4.5364003] | Test Loss: [2.58552, 0.33744588, 4.8335943]\n",
      "342: Train Loss: [2.3331397, 0.3473717, 4.3189077] | Test Loss: [2.5146167, 0.41508168, 4.614152]\n",
      "343: Train Loss: [2.3578463, 0.32108855, 4.394604] | Test Loss: [2.3526385, 0.33790344, 4.3673735]\n",
      "344: Train Loss: [2.3113055, 0.4083326, 4.214278] | Test Loss: [2.5949323, 0.4140621, 4.7758026]\n",
      "345: Train Loss: [2.3302777, 0.34507063, 4.3154845] | Test Loss: [2.6299214, 0.3833586, 4.8764844]\n",
      "346: Train Loss: [2.3700056, 0.37017682, 4.3698344] | Test Loss: [2.80355, 0.43608478, 5.1710153]\n",
      "347: Train Loss: [2.3308115, 0.3679027, 4.2937202] | Test Loss: [2.6811244, 0.32817394, 5.034075]\n",
      "348: Train Loss: [2.2637994, 0.436094, 4.091505] | Test Loss: [2.503752, 0.3575217, 4.6499825]\n",
      "349: Train Loss: [2.31029, 0.36902592, 4.2515545] | Test Loss: [2.5266147, 0.37767434, 4.6755548]\n",
      "350: Train Loss: [2.396801, 0.33757785, 4.456024] | Test Loss: [2.742374, 0.32383955, 5.160908]\n",
      "351: Train Loss: [2.365759, 0.31975463, 4.411763] | Test Loss: [2.7146811, 0.4111918, 5.0181704]\n",
      "352: Train Loss: [2.236962, 0.3526789, 4.1212454] | Test Loss: [2.7015681, 0.34630895, 5.056827]\n",
      "353: Train Loss: [2.4508924, 0.32271305, 4.579072] | Test Loss: [2.6630652, 0.4146286, 4.911502]\n",
      "354: Train Loss: [2.453344, 0.47426197, 4.4324265] | Test Loss: [2.571426, 0.3550258, 4.787826]\n",
      "355: Train Loss: [2.3500319, 0.35939014, 4.3406734] | Test Loss: [2.782154, 0.5032811, 5.061027]\n",
      "356: Train Loss: [2.364253, 0.40199494, 4.3265114] | Test Loss: [2.5713027, 0.3517042, 4.790901]\n",
      "357: Train Loss: [2.363917, 0.4547515, 4.2730827] | Test Loss: [2.7429492, 0.44457972, 5.041319]\n",
      "358: Train Loss: [2.2584343, 0.35792392, 4.1589446] | Test Loss: [2.7604103, 0.3649156, 5.1559052]\n",
      "359: Train Loss: [2.4200618, 0.34046322, 4.4996605] | Test Loss: [2.5323803, 0.3964122, 4.6683483]\n",
      "360: Train Loss: [2.2131875, 0.31747028, 4.108905] | Test Loss: [2.6467865, 0.38348955, 4.9100833]\n",
      "361: Train Loss: [2.2771542, 0.35113946, 4.203169] | Test Loss: [2.5135398, 0.355838, 4.6712418]\n",
      "362: Train Loss: [2.3584177, 0.36465904, 4.3521767] | Test Loss: [2.5993254, 0.35470548, 4.8439455]\n",
      "363: Train Loss: [2.4335852, 0.31528234, 4.551888] | Test Loss: [2.6353035, 0.40680265, 4.8638043]\n",
      "364: Train Loss: [2.4931571, 0.38338387, 4.6029305] | Test Loss: [2.801231, 0.36859578, 5.233866]\n",
      "365: Train Loss: [2.3424037, 0.30541, 4.3793974] | Test Loss: [2.6417375, 0.34110427, 4.942371]\n",
      "366: Train Loss: [2.3174236, 0.36251393, 4.272333] | Test Loss: [2.7145882, 0.3835179, 5.0456586]\n",
      "367: Train Loss: [2.346787, 0.2984992, 4.395075] | Test Loss: [2.5752568, 0.3429294, 4.8075843]\n",
      "368: Train Loss: [2.398057, 0.38133326, 4.4147806] | Test Loss: [2.4910104, 0.34059083, 4.64143]\n",
      "369: Train Loss: [2.3025115, 0.30163237, 4.3033905] | Test Loss: [2.4906454, 0.42854083, 4.55275]\n",
      "370: Train Loss: [2.3696868, 0.34138998, 4.3979836] | Test Loss: [2.5529048, 0.32390657, 4.7819033]\n",
      "371: Train Loss: [2.33029, 0.36577278, 4.2948074] | Test Loss: [2.655142, 0.39738375, 4.9129004]\n",
      "372: Train Loss: [2.4537616, 0.30860537, 4.598918] | Test Loss: [2.6815567, 0.37441087, 4.9887023]\n",
      "373: Train Loss: [2.211184, 0.31488723, 4.107481] | Test Loss: [2.8839977, 0.46117705, 5.3068185]\n",
      "374: Train Loss: [2.3374503, 0.32003456, 4.354866] | Test Loss: [2.5458572, 0.3707633, 4.720951]\n",
      "375: Train Loss: [2.2920766, 0.28013894, 4.304014] | Test Loss: [2.5349994, 0.31074443, 4.7592545]\n",
      "376: Train Loss: [2.3865082, 0.40307844, 4.369938] | Test Loss: [2.8240464, 0.33179924, 5.3162937]\n",
      "377: Train Loss: [2.3308408, 0.3425894, 4.3190923] | Test Loss: [2.7529976, 0.37635225, 5.129643]\n",
      "378: Train Loss: [2.3947077, 0.36572656, 4.423689] | Test Loss: [2.6190805, 0.35888, 4.879281]\n",
      "379: Train Loss: [2.4343684, 0.36050007, 4.508237] | Test Loss: [2.4535341, 0.32332626, 4.583742]\n",
      "380: Train Loss: [2.32831, 0.38295823, 4.2736616] | Test Loss: [2.535498, 0.33071277, 4.740283]\n",
      "381: Train Loss: [2.3479319, 0.38650212, 4.3093615] | Test Loss: [2.702033, 0.40536007, 4.998706]\n",
      "382: Train Loss: [2.3394465, 0.33047014, 4.348423] | Test Loss: [2.5492568, 0.42339927, 4.675114]\n",
      "383: Train Loss: [2.3614411, 0.33160418, 4.3912783] | Test Loss: [2.6426246, 0.38890058, 4.8963485]\n",
      "384: Train Loss: [2.19639, 0.32931608, 4.0634637] | Test Loss: [2.7668865, 0.4497105, 5.0840626]\n",
      "385: Train Loss: [2.4266472, 0.32785392, 4.52544] | Test Loss: [2.7200356, 0.4190778, 5.020993]\n",
      "386: Train Loss: [2.355956, 0.34348872, 4.3684235] | Test Loss: [2.5413837, 0.3454875, 4.73728]\n",
      "387: Train Loss: [2.2630093, 0.3472236, 4.178795] | Test Loss: [2.6187582, 0.45301947, 4.784497]\n",
      "388: Train Loss: [2.4559467, 0.3830773, 4.528816] | Test Loss: [2.5749514, 0.4055701, 4.744333]\n",
      "389: Train Loss: [2.459103, 0.37600127, 4.542205] | Test Loss: [2.7144217, 0.33734632, 5.0914974]\n",
      "390: Train Loss: [2.2265093, 0.3252533, 4.127765] | Test Loss: [2.7793708, 0.38801217, 5.170729]\n",
      "391: Train Loss: [2.3123465, 0.3558777, 4.268815] | Test Loss: [2.4530046, 0.4295593, 4.47645]\n",
      "392: Train Loss: [2.4241788, 0.39577755, 4.45258] | Test Loss: [2.502135, 0.3135789, 4.690691]\n",
      "393: Train Loss: [2.2392852, 0.29937917, 4.179191] | Test Loss: [2.485673, 0.33472434, 4.6366215]\n",
      "394: Train Loss: [2.344528, 0.3040635, 4.3849926] | Test Loss: [2.4249566, 0.37914264, 4.4707704]\n",
      "395: Train Loss: [2.3367782, 0.35448807, 4.3190684] | Test Loss: [2.895463, 0.45184729, 5.339079]\n",
      "396: Train Loss: [2.3470075, 0.29269674, 4.401318] | Test Loss: [2.5699847, 0.34731835, 4.792651]\n",
      "397: Train Loss: [2.431156, 0.32273996, 4.539572] | Test Loss: [2.7810297, 0.40600833, 5.156051]\n",
      "398: Train Loss: [2.3957212, 0.37783214, 4.4136105] | Test Loss: [2.6254756, 0.32926834, 4.921683]\n",
      "399: Train Loss: [2.4593966, 0.40938082, 4.5094123] | Test Loss: [2.597363, 0.32370389, 4.871022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400: Train Loss: [2.325826, 0.2747009, 4.3769507] | Test Loss: [2.7576716, 0.41023576, 5.1051073]\n",
      "401: Train Loss: [2.3792424, 0.4116453, 4.3468394] | Test Loss: [2.676568, 0.3293888, 5.0237474]\n",
      "402: Train Loss: [2.353406, 0.38083133, 4.3259807] | Test Loss: [2.6197338, 0.33453342, 4.9049344]\n",
      "403: Train Loss: [2.436401, 0.38851184, 4.48429] | Test Loss: [2.7394166, 0.37299484, 5.1058383]\n",
      "404: Train Loss: [2.3193052, 0.39560544, 4.243005] | Test Loss: [2.7962217, 0.36190048, 5.230543]\n",
      "405: Train Loss: [2.4527242, 0.3595799, 4.5458684] | Test Loss: [2.4909549, 0.3572393, 4.6246705]\n",
      "406: Train Loss: [2.3275194, 0.386262, 4.268777] | Test Loss: [2.734037, 0.37099433, 5.0970793]\n",
      "407: Train Loss: [2.535505, 0.370288, 4.700722] | Test Loss: [2.5017622, 0.37633717, 4.6271873]\n",
      "408: Train Loss: [2.3065267, 0.42643103, 4.186622] | Test Loss: [2.5780525, 0.35375386, 4.802351]\n",
      "409: Train Loss: [2.286471, 0.30611625, 4.2668257] | Test Loss: [2.739758, 0.30370367, 5.1758122]\n",
      "410: Train Loss: [2.4202657, 0.30492413, 4.5356073] | Test Loss: [2.6840549, 0.3161772, 5.0519323]\n",
      "411: Train Loss: [2.323907, 0.34377155, 4.3040423] | Test Loss: [2.3796566, 0.30835724, 4.450956]\n",
      "412: Train Loss: [2.4772336, 0.46780363, 4.486664] | Test Loss: [2.572726, 0.33655137, 4.808901]\n",
      "413: Train Loss: [2.3876326, 0.4599595, 4.3153057] | Test Loss: [2.4995089, 0.3583905, 4.6406274]\n",
      "414: Train Loss: [2.4139876, 0.44353652, 4.384439] | Test Loss: [2.788111, 0.4201146, 5.1561074]\n",
      "415: Train Loss: [2.4673429, 0.37962598, 4.55506] | Test Loss: [2.7569985, 0.33756137, 5.1764355]\n",
      "416: Train Loss: [2.3615189, 0.38037372, 4.342664] | Test Loss: [2.6818867, 0.39445952, 4.9693136]\n",
      "417: Train Loss: [2.2531638, 0.4004275, 4.1059003] | Test Loss: [2.6182673, 0.40537396, 4.8311605]\n",
      "418: Train Loss: [2.5562146, 0.39558068, 4.7168484] | Test Loss: [2.7596762, 0.38533986, 5.1340127]\n",
      "419: Train Loss: [2.2738056, 0.32608098, 4.2215304] | Test Loss: [2.3350484, 0.36674166, 4.303355]\n",
      "420: Train Loss: [2.3347616, 0.31360328, 4.35592] | Test Loss: [2.6601307, 0.35326353, 4.966998]\n",
      "421: Train Loss: [2.463407, 0.3843068, 4.542507] | Test Loss: [2.711019, 0.33225283, 5.089785]\n",
      "422: Train Loss: [2.3145642, 0.37475052, 4.254378] | Test Loss: [2.8318348, 0.44808614, 5.2155833]\n",
      "423: Train Loss: [2.3583019, 0.27430245, 4.4423013] | Test Loss: [2.5251398, 0.35036469, 4.699915]\n",
      "424: Train Loss: [2.3423307, 0.38163254, 4.303029] | Test Loss: [2.5154274, 0.3197982, 4.7110567]\n",
      "425: Train Loss: [2.4085114, 0.37970883, 4.437314] | Test Loss: [2.5479476, 0.46895888, 4.6269364]\n",
      "426: Train Loss: [2.2469525, 0.3361609, 4.157744] | Test Loss: [2.6834664, 0.3350955, 5.0318375]\n",
      "427: Train Loss: [2.3952668, 0.30258504, 4.4879484] | Test Loss: [2.9330418, 0.37275264, 5.493331]\n",
      "428: Train Loss: [2.4359896, 0.48164985, 4.3903294] | Test Loss: [2.5040226, 0.4227359, 4.5853095]\n",
      "429: Train Loss: [2.398277, 0.32451564, 4.4720383] | Test Loss: [2.732259, 0.41390356, 5.0506144]\n",
      "430: Train Loss: [2.3009884, 0.4017752, 4.2002015] | Test Loss: [2.6999114, 0.3696187, 5.030204]\n",
      "431: Train Loss: [2.1628366, 0.34645295, 3.9792202] | Test Loss: [2.6774142, 0.34203824, 5.01279]\n",
      "432: Train Loss: [2.3660805, 0.35645968, 4.3757014] | Test Loss: [2.765341, 0.3777334, 5.152949]\n",
      "433: Train Loss: [2.497713, 0.36639318, 4.629033] | Test Loss: [2.6152055, 0.36470944, 4.8657017]\n",
      "434: Train Loss: [2.284805, 0.35521588, 4.214394] | Test Loss: [2.569667, 0.4349589, 4.7043753]\n",
      "435: Train Loss: [2.427029, 0.3517587, 4.5022993] | Test Loss: [2.6916895, 0.4527282, 4.9306507]\n",
      "436: Train Loss: [2.2194417, 0.34723154, 4.091652] | Test Loss: [2.5995314, 0.38538128, 4.8136816]\n",
      "437: Train Loss: [2.4371765, 0.36226496, 4.512088] | Test Loss: [2.6385646, 0.4307139, 4.8464155]\n",
      "438: Train Loss: [2.3274977, 0.35866877, 4.2963266] | Test Loss: [2.6521535, 0.37439743, 4.9299097]\n",
      "439: Train Loss: [2.3823314, 0.35059485, 4.4140677] | Test Loss: [2.4941058, 0.32073438, 4.667477]\n",
      "440: Train Loss: [2.3201869, 0.4498125, 4.1905613] | Test Loss: [2.6313033, 0.32479733, 4.9378095]\n",
      "441: Train Loss: [2.5733423, 0.46053374, 4.686151] | Test Loss: [2.4712343, 0.34011984, 4.602349]\n",
      "442: Train Loss: [2.2499342, 0.33980548, 4.160063] | Test Loss: [2.6914177, 0.3699065, 5.012929]\n",
      "443: Train Loss: [2.304439, 0.38925877, 4.2196193] | Test Loss: [2.6687627, 0.3826889, 4.9548364]\n",
      "444: Train Loss: [2.3480606, 0.395765, 4.3003564] | Test Loss: [2.6617513, 0.37276676, 4.9507356]\n",
      "445: Train Loss: [2.360214, 0.41311476, 4.3073134] | Test Loss: [2.6092236, 0.35396597, 4.8644814]\n",
      "446: Train Loss: [2.406077, 0.3358459, 4.476308] | Test Loss: [2.6076503, 0.3455438, 4.8697567]\n",
      "447: Train Loss: [2.476054, 0.37882033, 4.5732875] | Test Loss: [2.7241185, 0.37121943, 5.0770173]\n",
      "448: Train Loss: [2.4256587, 0.36129984, 4.4900174] | Test Loss: [2.5640888, 0.3527527, 4.775425]\n",
      "449: Train Loss: [2.343872, 0.39847735, 4.2892666] | Test Loss: [2.7003412, 0.35645568, 5.0442266]\n",
      "450: Train Loss: [2.278555, 0.378026, 4.179084] | Test Loss: [2.699225, 0.46071517, 4.9377346]\n",
      "451: Train Loss: [2.3935082, 0.3669516, 4.420065] | Test Loss: [2.7682276, 0.45831066, 5.0781446]\n",
      "452: Train Loss: [2.363094, 0.31747758, 4.4087105] | Test Loss: [2.6305864, 0.339483, 4.92169]\n",
      "453: Train Loss: [2.4356272, 0.33358812, 4.5376663] | Test Loss: [2.3614357, 0.37194446, 4.350927]\n",
      "454: Train Loss: [2.4874244, 0.42739755, 4.547451] | Test Loss: [2.8689387, 0.34293383, 5.3949437]\n",
      "455: Train Loss: [2.3601322, 0.4468308, 4.2734337] | Test Loss: [2.6296299, 0.39015138, 4.869108]\n",
      "456: Train Loss: [2.500297, 0.35127178, 4.6493225] | Test Loss: [2.7915928, 0.36640972, 5.216776]\n",
      "457: Train Loss: [2.4547184, 0.3861298, 4.523307] | Test Loss: [2.5806582, 0.4020657, 4.7592506]\n",
      "458: Train Loss: [2.4750094, 0.4113877, 4.538631] | Test Loss: [2.7795548, 0.4093191, 5.149791]\n",
      "459: Train Loss: [2.4755824, 0.36045548, 4.590709] | Test Loss: [2.600194, 0.34449106, 4.855897]\n",
      "460: Train Loss: [2.4042609, 0.52289444, 4.2856274] | Test Loss: [2.5479062, 0.35085672, 4.7449555]\n",
      "461: Train Loss: [2.2395358, 0.35348153, 4.1255903] | Test Loss: [2.742357, 0.4314253, 5.053289]\n",
      "462: Train Loss: [2.479429, 0.37675804, 4.5821] | Test Loss: [2.4877071, 0.3777462, 4.597668]\n",
      "463: Train Loss: [2.464087, 0.46750548, 4.4606686] | Test Loss: [2.662919, 0.3859411, 4.939897]\n",
      "464: Train Loss: [2.4956472, 0.35850865, 4.632786] | Test Loss: [2.7415576, 0.3967981, 5.086317]\n",
      "465: Train Loss: [2.3682353, 0.33201852, 4.4044523] | Test Loss: [2.6685681, 0.33043656, 5.0066996]\n",
      "466: Train Loss: [2.3701217, 0.3661422, 4.374101] | Test Loss: [2.5590162, 0.36413497, 4.7538977]\n",
      "467: Train Loss: [2.453675, 0.37221017, 4.53514] | Test Loss: [2.5528984, 0.36570352, 4.740093]\n",
      "468: Train Loss: [2.3781245, 0.33778086, 4.418468] | Test Loss: [2.6996067, 0.29930314, 5.0999103]\n",
      "469: Train Loss: [2.3429391, 0.34150606, 4.3443723] | Test Loss: [2.5461993, 0.37919104, 4.7132077]\n",
      "470: Train Loss: [2.3962798, 0.44780076, 4.344759] | Test Loss: [2.6269152, 0.42031127, 4.833519]\n",
      "471: Train Loss: [2.5008934, 0.38546962, 4.6163173] | Test Loss: [2.8446279, 0.31634834, 5.372907]\n",
      "472: Train Loss: [2.3085089, 0.31420878, 4.302809] | Test Loss: [2.635039, 0.35772407, 4.912354]\n",
      "473: Train Loss: [2.3106585, 0.37213767, 4.2491794] | Test Loss: [2.7170625, 0.34168658, 5.092438]\n",
      "474: Train Loss: [2.3117883, 0.4396159, 4.183961] | Test Loss: [2.7590458, 0.40775797, 5.110334]\n",
      "475: Train Loss: [2.4184747, 0.3276245, 4.509325] | Test Loss: [2.4125216, 0.4326745, 4.392369]\n",
      "476: Train Loss: [2.3961623, 0.2953749, 4.4969497] | Test Loss: [2.5205207, 0.36186352, 4.6791778]\n",
      "477: Train Loss: [2.5337305, 0.42583573, 4.6416254] | Test Loss: [2.7623186, 0.39031404, 5.134323]\n",
      "478: Train Loss: [2.3210826, 0.36550885, 4.276656] | Test Loss: [2.5365896, 0.36208054, 4.7110987]\n",
      "479: Train Loss: [2.402302, 0.3628997, 4.4417043] | Test Loss: [2.6871126, 0.38572076, 4.9885044]\n",
      "480: Train Loss: [2.5354989, 0.41804573, 4.652952] | Test Loss: [2.7025704, 0.29863515, 5.106506]\n",
      "481: Train Loss: [2.4432385, 0.32880026, 4.557677] | Test Loss: [2.7125797, 0.37623358, 5.048926]\n",
      "482: Train Loss: [2.5039117, 0.36339095, 4.6444325] | Test Loss: [2.6089032, 0.34063572, 4.8771706]\n",
      "483: Train Loss: [2.3182046, 0.36999816, 4.2664113] | Test Loss: [2.509457, 0.3438415, 4.6750727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484: Train Loss: [2.265191, 0.36882395, 4.161558] | Test Loss: [2.657635, 0.36109963, 4.95417]\n",
      "485: Train Loss: [2.3500264, 0.3380208, 4.362032] | Test Loss: [2.4573164, 0.41777226, 4.4968605]\n",
      "486: Train Loss: [2.3674517, 0.35830843, 4.376595] | Test Loss: [2.630134, 0.36187732, 4.898391]\n",
      "487: Train Loss: [2.350681, 0.35595492, 4.345407] | Test Loss: [2.6246035, 0.30918738, 4.9400196]\n",
      "488: Train Loss: [2.3051918, 0.31177843, 4.298605] | Test Loss: [2.4394124, 0.3840581, 4.4947667]\n",
      "489: Train Loss: [2.407697, 0.31898007, 4.4964137] | Test Loss: [2.5048897, 0.35948277, 4.6502967]\n",
      "490: Train Loss: [2.3898664, 0.38119912, 4.3985333] | Test Loss: [2.6992335, 0.3436505, 5.0548167]\n",
      "491: Train Loss: [2.5203228, 0.37271687, 4.6679287] | Test Loss: [2.7578948, 0.42056948, 5.09522]\n",
      "492: Train Loss: [2.653852, 0.4992527, 4.808451] | Test Loss: [2.8129282, 0.41328847, 5.212568]\n",
      "493: Train Loss: [2.6406183, 0.43518618, 4.8460503] | Test Loss: [2.7098973, 0.38994026, 5.0298543]\n",
      "494: Train Loss: [2.3347692, 0.34068212, 4.3288565] | Test Loss: [2.6770937, 0.35895258, 4.995235]\n",
      "495: Train Loss: [2.5738397, 0.4106176, 4.7370615] | Test Loss: [2.8096695, 0.42107224, 5.1982665]\n",
      "496: Train Loss: [2.4778543, 0.45583117, 4.4998775] | Test Loss: [2.649309, 0.41589215, 4.8827257]\n",
      "497: Train Loss: [2.2451587, 0.3743438, 4.1159735] | Test Loss: [2.8918748, 0.4031778, 5.380572]\n",
      "498: Train Loss: [2.5047913, 0.36082304, 4.6487594] | Test Loss: [2.5226815, 0.3691856, 4.6761775]\n",
      "499: Train Loss: [2.1733232, 0.33770707, 4.0089393] | Test Loss: [2.5474153, 0.32654682, 4.768284]\n",
      "500: Train Loss: [2.3447933, 0.34495068, 4.344636] | Test Loss: [2.6853194, 0.39365596, 4.976983]\n",
      "501: Train Loss: [2.3735023, 0.45873967, 4.2882648] | Test Loss: [2.2306259, 0.3828949, 4.0783567]\n",
      "502: Train Loss: [2.409267, 0.34351492, 4.475019] | Test Loss: [2.7714515, 0.40033275, 5.14257]\n",
      "503: Train Loss: [2.2866688, 0.3364603, 4.2368774] | Test Loss: [2.508928, 0.3252092, 4.692647]\n",
      "504: Train Loss: [2.281265, 0.3332545, 4.2292757] | Test Loss: [2.696909, 0.4009606, 4.9928575]\n",
      "505: Train Loss: [2.307712, 0.33342022, 4.282004] | Test Loss: [2.8776193, 0.4826964, 5.272542]\n",
      "506: Train Loss: [2.4027038, 0.3981664, 4.4072413] | Test Loss: [2.5673134, 0.46369988, 4.670927]\n",
      "507: Train Loss: [2.456068, 0.34456652, 4.5675697] | Test Loss: [2.5980778, 0.38261095, 4.8135448]\n",
      "508: Train Loss: [2.3745682, 0.2958413, 4.453295] | Test Loss: [2.5103848, 0.29750213, 4.7232676]\n",
      "509: Train Loss: [2.4126542, 0.33413586, 4.4911723] | Test Loss: [2.7077107, 0.32879204, 5.0866294]\n",
      "510: Train Loss: [2.4370487, 0.34086803, 4.5332294] | Test Loss: [2.565347, 0.34075743, 4.7899365]\n",
      "511: Train Loss: [2.5163355, 0.413725, 4.618946] | Test Loss: [2.6055458, 0.34964025, 4.861451]\n",
      "512: Train Loss: [2.5092578, 0.35234332, 4.666172] | Test Loss: [2.7101152, 0.4489276, 4.971303]\n",
      "513: Train Loss: [2.243985, 0.39693016, 4.0910397] | Test Loss: [2.7073882, 0.43297482, 4.9818015]\n",
      "514: Train Loss: [2.2258465, 0.3715974, 4.080096] | Test Loss: [2.6967413, 0.32444182, 5.069041]\n",
      "515: Train Loss: [2.3463354, 0.35107258, 4.341598] | Test Loss: [2.9945025, 0.43776786, 5.551237]\n",
      "516: Train Loss: [2.468921, 0.40118998, 4.536652] | Test Loss: [2.5607858, 0.3816787, 4.739893]\n",
      "517: Train Loss: [2.355696, 0.374538, 4.336854] | Test Loss: [2.7128115, 0.41491073, 5.010712]\n",
      "518: Train Loss: [2.3647192, 0.36301067, 4.3664274] | Test Loss: [2.5553539, 0.3364005, 4.7743073]\n",
      "519: Train Loss: [2.2742982, 0.3450599, 4.2035365] | Test Loss: [2.6690664, 0.3475428, 4.99059]\n",
      "520: Train Loss: [2.3026352, 0.3144397, 4.2908306] | Test Loss: [2.6820385, 0.44440258, 4.9196744]\n",
      "521: Train Loss: [2.5740483, 0.3011469, 4.8469496] | Test Loss: [2.6223736, 0.35065714, 4.89409]\n",
      "522: Train Loss: [2.4179714, 0.48772267, 4.34822] | Test Loss: [2.7166383, 0.3309573, 5.1023192]\n",
      "523: Train Loss: [2.697043, 0.402311, 4.991775] | Test Loss: [2.660059, 0.29279944, 5.0273185]\n",
      "Epoch 17\n",
      "0: Train Loss: [2.1755044, 0.3619992, 3.9890096] | Test Loss: [2.6659436, 0.36691386, 4.9649734]\n",
      "1: Train Loss: [2.14728, 0.34995887, 3.9446013] | Test Loss: [2.7986376, 0.3731198, 5.2241554]\n",
      "2: Train Loss: [2.230396, 0.39118585, 4.0696063] | Test Loss: [2.6538887, 0.31587356, 4.991904]\n",
      "3: Train Loss: [2.2970698, 0.34587443, 4.2482653] | Test Loss: [2.739893, 0.3526566, 5.1271296]\n",
      "4: Train Loss: [2.4284124, 0.38779145, 4.4690332] | Test Loss: [2.3893166, 0.33782208, 4.440811]\n",
      "5: Train Loss: [2.3358693, 0.36834428, 4.3033943] | Test Loss: [2.7314467, 0.35050255, 5.112391]\n",
      "6: Train Loss: [2.2488053, 0.3375154, 4.160095] | Test Loss: [2.6794696, 0.42312342, 4.935816]\n",
      "7: Train Loss: [2.1937752, 0.29568052, 4.09187] | Test Loss: [2.7937996, 0.47840303, 5.109196]\n",
      "8: Train Loss: [2.3526611, 0.37414333, 4.331179] | Test Loss: [2.5180633, 0.35286793, 4.6832585]\n",
      "9: Train Loss: [2.29177, 0.29378784, 4.289752] | Test Loss: [2.70099, 0.35708767, 5.0448923]\n",
      "10: Train Loss: [2.2777038, 0.31088948, 4.244518] | Test Loss: [2.5925076, 0.3884004, 4.7966146]\n",
      "11: Train Loss: [2.2565265, 0.3401546, 4.1728983] | Test Loss: [2.6480503, 0.4576459, 4.8384547]\n",
      "12: Train Loss: [2.3634841, 0.30458084, 4.4223876] | Test Loss: [2.9121313, 0.5042045, 5.320058]\n",
      "13: Train Loss: [2.3695655, 0.33077827, 4.408353] | Test Loss: [2.8891175, 0.52277243, 5.2554626]\n",
      "14: Train Loss: [2.414418, 0.34176987, 4.4870663] | Test Loss: [2.5984623, 0.42378464, 4.77314]\n",
      "15: Train Loss: [2.2586408, 0.3424402, 4.1748414] | Test Loss: [2.7451432, 0.3835482, 5.106738]\n",
      "16: Train Loss: [2.3221257, 0.36947677, 4.2747746] | Test Loss: [2.6525085, 0.39147723, 4.91354]\n",
      "17: Train Loss: [2.1310139, 0.3398237, 3.9222043] | Test Loss: [2.788301, 0.3442343, 5.2323675]\n",
      "18: Train Loss: [2.2475803, 0.42481124, 4.070349] | Test Loss: [2.4647806, 0.38958204, 4.539979]\n",
      "19: Train Loss: [2.2769299, 0.3761999, 4.17766] | Test Loss: [2.7469532, 0.3647825, 5.129124]\n",
      "20: Train Loss: [2.2339013, 0.37006152, 4.097741] | Test Loss: [2.7775147, 0.36360037, 5.191429]\n",
      "21: Train Loss: [2.4704838, 0.35441136, 4.5865564] | Test Loss: [2.8105137, 0.38496652, 5.236061]\n",
      "22: Train Loss: [2.3393986, 0.39166507, 4.2871323] | Test Loss: [2.4924028, 0.3574154, 4.6273904]\n",
      "23: Train Loss: [2.2815511, 0.36947903, 4.193623] | Test Loss: [2.4977384, 0.3663957, 4.629081]\n",
      "24: Train Loss: [2.3841147, 0.3339237, 4.4343057] | Test Loss: [2.6575654, 0.33789122, 4.9772396]\n",
      "25: Train Loss: [2.230191, 0.30049756, 4.1598845] | Test Loss: [2.7185423, 0.3216709, 5.1154137]\n",
      "26: Train Loss: [2.3019047, 0.37408245, 4.229727] | Test Loss: [2.6754498, 0.34452754, 5.006372]\n",
      "27: Train Loss: [2.2109995, 0.34727874, 4.0747204] | Test Loss: [2.56465, 0.2894707, 4.8398294]\n",
      "28: Train Loss: [2.2695036, 0.2924899, 4.246517] | Test Loss: [2.5464504, 0.3677087, 4.725192]\n",
      "29: Train Loss: [2.4392507, 0.5987676, 4.2797337] | Test Loss: [2.7280304, 0.30724612, 5.1488147]\n",
      "30: Train Loss: [2.3775048, 0.35889885, 4.396111] | Test Loss: [2.7624285, 0.3973941, 5.127463]\n",
      "31: Train Loss: [2.3773203, 0.5259722, 4.228668] | Test Loss: [2.902257, 0.37182945, 5.4326844]\n",
      "32: Train Loss: [2.253591, 0.38103974, 4.1261425] | Test Loss: [2.541983, 0.39556783, 4.688398]\n",
      "33: Train Loss: [2.3526254, 0.3830588, 4.322192] | Test Loss: [2.696867, 0.38834804, 5.005386]\n",
      "34: Train Loss: [2.445742, 0.38588333, 4.5056005] | Test Loss: [2.490596, 0.3958682, 4.585324]\n",
      "35: Train Loss: [2.280139, 0.37963566, 4.180642] | Test Loss: [2.578477, 0.33254662, 4.824407]\n",
      "36: Train Loss: [2.3456793, 0.38444048, 4.306918] | Test Loss: [2.7665174, 0.38324243, 5.149792]\n",
      "37: Train Loss: [2.3136706, 0.35836098, 4.2689805] | Test Loss: [2.8451142, 0.18685047, 5.503378]\n",
      "38: Train Loss: [2.4040272, 0.37824145, 4.429813] | Test Loss: [2.8083844, 0.4454766, 5.1712923]\n",
      "39: Train Loss: [2.3472548, 0.40603724, 4.288472] | Test Loss: [2.7116258, 0.40866587, 5.014586]\n",
      "40: Train Loss: [2.3758492, 0.3695831, 4.3821154] | Test Loss: [2.681522, 0.35078815, 5.0122557]\n",
      "41: Train Loss: [2.4089339, 0.37945595, 4.4384117] | Test Loss: [2.7083235, 0.3752385, 5.0414085]\n",
      "42: Train Loss: [2.349802, 0.34436455, 4.3552394] | Test Loss: [2.5958738, 0.3374805, 4.854267]\n",
      "43: Train Loss: [2.2715862, 0.3432663, 4.199906] | Test Loss: [2.598888, 0.37594628, 4.82183]\n",
      "44: Train Loss: [2.3366992, 0.36988768, 4.3035107] | Test Loss: [2.6923172, 0.37669662, 5.007938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45: Train Loss: [2.339549, 0.33943042, 4.339668] | Test Loss: [2.9167264, 0.33741096, 5.496042]\n",
      "46: Train Loss: [2.2673998, 0.38258764, 4.152212] | Test Loss: [2.5792012, 0.3926979, 4.7657046]\n",
      "47: Train Loss: [2.31919, 0.3419697, 4.2964106] | Test Loss: [2.6914513, 0.3732666, 5.009636]\n",
      "48: Train Loss: [2.3217416, 0.33053935, 4.312944] | Test Loss: [2.4371345, 0.42819926, 4.4460697]\n",
      "49: Train Loss: [2.2969058, 0.37916803, 4.2146435] | Test Loss: [2.628972, 0.3934769, 4.864467]\n",
      "50: Train Loss: [2.3885105, 0.39218494, 4.384836] | Test Loss: [2.8036852, 0.3379194, 5.269451]\n",
      "51: Train Loss: [2.292891, 0.38052854, 4.2052536] | Test Loss: [2.6581154, 0.39893964, 4.917291]\n",
      "52: Train Loss: [2.3305361, 0.3939037, 4.2671685] | Test Loss: [2.6153982, 0.38252866, 4.8482676]\n",
      "53: Train Loss: [2.2404046, 0.3698469, 4.1109624] | Test Loss: [2.6884437, 0.39339963, 4.9834876]\n",
      "54: Train Loss: [2.41031, 0.44126952, 4.3793507] | Test Loss: [2.7705932, 0.37711096, 5.1640754]\n",
      "55: Train Loss: [2.3072734, 0.3887754, 4.2257714] | Test Loss: [2.6155417, 0.39950284, 4.8315806]\n",
      "56: Train Loss: [2.3135312, 0.36669576, 4.2603664] | Test Loss: [2.7757978, 0.35669518, 5.1949005]\n",
      "57: Train Loss: [2.3396811, 0.39239636, 4.286966] | Test Loss: [2.8425055, 0.41745275, 5.267558]\n",
      "58: Train Loss: [2.470919, 0.36384648, 4.5779915] | Test Loss: [2.6349916, 0.31112212, 4.9588614]\n",
      "59: Train Loss: [2.1788087, 0.35031468, 4.0073028] | Test Loss: [2.6584618, 0.37493697, 4.9419866]\n",
      "60: Train Loss: [2.3073394, 0.35476306, 4.259916] | Test Loss: [2.7016976, 0.31404674, 5.0893483]\n",
      "61: Train Loss: [2.2799106, 0.3048942, 4.254927] | Test Loss: [2.649128, 0.40136182, 4.896894]\n",
      "62: Train Loss: [2.316577, 0.3193321, 4.313822] | Test Loss: [2.6412797, 0.4361451, 4.846414]\n",
      "63: Train Loss: [2.2523966, 0.3556488, 4.149144] | Test Loss: [2.766754, 0.4283548, 5.105153]\n",
      "64: Train Loss: [2.286227, 0.32777116, 4.244683] | Test Loss: [2.5936625, 0.40515384, 4.7821712]\n",
      "65: Train Loss: [2.3721373, 0.29930544, 4.444969] | Test Loss: [2.7132356, 0.364611, 5.06186]\n",
      "66: Train Loss: [2.3350482, 0.39533827, 4.2747583] | Test Loss: [2.5133042, 0.40072417, 4.625884]\n",
      "67: Train Loss: [2.3401847, 0.33730605, 4.3430634] | Test Loss: [2.5113888, 0.35208687, 4.6706905]\n",
      "68: Train Loss: [2.3836422, 0.3133757, 4.4539084] | Test Loss: [2.5744681, 0.43016592, 4.7187705]\n",
      "69: Train Loss: [2.2665455, 0.3525876, 4.1805034] | Test Loss: [2.4545958, 0.40899038, 4.500201]\n",
      "70: Train Loss: [2.3122885, 0.3282994, 4.2962775] | Test Loss: [2.7818036, 0.39649355, 5.167114]\n",
      "71: Train Loss: [2.4043455, 0.35349503, 4.455196] | Test Loss: [2.6834276, 0.33805826, 5.0287967]\n",
      "72: Train Loss: [2.3201315, 0.29584318, 4.34442] | Test Loss: [2.7714596, 0.357638, 5.1852813]\n",
      "73: Train Loss: [2.3438094, 0.32386854, 4.36375] | Test Loss: [2.5079129, 0.33217627, 4.6836495]\n",
      "74: Train Loss: [2.3299766, 0.32277036, 4.3371825] | Test Loss: [2.8277078, 0.36474738, 5.290668]\n",
      "75: Train Loss: [2.3643181, 0.44897065, 4.2796655] | Test Loss: [2.8182952, 0.43069687, 5.2058935]\n",
      "76: Train Loss: [2.2619755, 0.30901584, 4.2149353] | Test Loss: [2.6250837, 0.35855561, 4.8916116]\n",
      "77: Train Loss: [2.382653, 0.4114291, 4.353877] | Test Loss: [2.7505322, 0.3842386, 5.1168256]\n",
      "78: Train Loss: [2.2467952, 0.39978713, 4.0938034] | Test Loss: [2.3351493, 0.33970487, 4.3305936]\n",
      "79: Train Loss: [2.2988808, 0.38707587, 4.2106857] | Test Loss: [2.4421587, 0.44994444, 4.434373]\n",
      "80: Train Loss: [2.318781, 0.31788775, 4.319674] | Test Loss: [2.5998328, 0.37801206, 4.8216534]\n",
      "81: Train Loss: [2.2761426, 0.34324008, 4.209045] | Test Loss: [2.777526, 0.3677974, 5.1872544]\n",
      "82: Train Loss: [2.232927, 0.3342251, 4.131629] | Test Loss: [2.6658318, 0.34116495, 4.9904985]\n",
      "83: Train Loss: [2.3474135, 0.40620875, 4.288618] | Test Loss: [2.6866167, 0.38167042, 4.991563]\n",
      "84: Train Loss: [2.4116745, 0.347403, 4.475946] | Test Loss: [2.7261643, 0.34184796, 5.110481]\n",
      "85: Train Loss: [2.2813435, 0.39893967, 4.1637473] | Test Loss: [2.9146423, 0.41243792, 5.4168468]\n",
      "86: Train Loss: [2.4293492, 0.33457714, 4.5241213] | Test Loss: [2.7354684, 0.3463711, 5.1245656]\n",
      "87: Train Loss: [2.3078387, 0.36545423, 4.250223] | Test Loss: [2.7395685, 0.3891664, 5.0899706]\n",
      "88: Train Loss: [2.3125525, 0.36367464, 4.2614303] | Test Loss: [2.8113432, 0.35868573, 5.2640004]\n",
      "89: Train Loss: [2.278935, 0.3077538, 4.2501163] | Test Loss: [2.8152215, 0.38471133, 5.245732]\n",
      "90: Train Loss: [2.4527724, 0.4828395, 4.422705] | Test Loss: [2.6211348, 0.31389058, 4.928379]\n",
      "91: Train Loss: [2.380958, 0.3549359, 4.40698] | Test Loss: [2.8530424, 0.3215436, 5.384541]\n",
      "92: Train Loss: [2.2756827, 0.34767145, 4.203694] | Test Loss: [2.5974169, 0.35856417, 4.8362694]\n",
      "93: Train Loss: [2.3534267, 0.36320364, 4.34365] | Test Loss: [2.7255416, 0.3845845, 5.0664988]\n",
      "94: Train Loss: [2.2230089, 0.37576082, 4.0702567] | Test Loss: [2.6006794, 0.3692648, 4.832094]\n",
      "95: Train Loss: [2.3321075, 0.37900516, 4.28521] | Test Loss: [2.7518568, 0.37307987, 5.130634]\n",
      "96: Train Loss: [2.4651642, 0.3507294, 4.579599] | Test Loss: [2.4939444, 0.378833, 4.609056]\n",
      "97: Train Loss: [2.319449, 0.32615688, 4.312741] | Test Loss: [1.5703794, 0.2909886, 2.84977]\n",
      "98: Train Loss: [2.5121653, 0.5012878, 4.5230427] | Test Loss: [2.6785, 0.3511407, 5.0058594]\n",
      "99: Train Loss: [2.238801, 0.32116953, 4.1564326] | Test Loss: [2.63217, 0.3487207, 4.9156194]\n",
      "100: Train Loss: [2.407545, 0.404083, 4.411007] | Test Loss: [2.7004812, 0.35810634, 5.042856]\n",
      "101: Train Loss: [2.3535855, 0.37965477, 4.327516] | Test Loss: [2.6612706, 0.36282286, 4.959718]\n",
      "102: Train Loss: [2.3313458, 0.39910033, 4.2635913] | Test Loss: [2.6618807, 0.4798521, 4.8439093]\n",
      "103: Train Loss: [2.326247, 0.31375852, 4.3387356] | Test Loss: [2.6223164, 0.35988307, 4.8847494]\n",
      "104: Train Loss: [2.3382125, 0.33824602, 4.338179] | Test Loss: [2.7301354, 0.38428894, 5.075982]\n",
      "105: Train Loss: [2.1352239, 0.35212144, 3.9183264] | Test Loss: [2.5298696, 0.393691, 4.666048]\n",
      "106: Train Loss: [2.333578, 0.30820358, 4.3589525] | Test Loss: [2.6084757, 0.36831173, 4.8486395]\n",
      "107: Train Loss: [2.373462, 0.35069638, 4.3962274] | Test Loss: [2.9057965, 0.32677191, 5.4848213]\n",
      "108: Train Loss: [2.4251127, 0.38124427, 4.4689813] | Test Loss: [2.6612778, 0.38482484, 4.937731]\n",
      "109: Train Loss: [2.2942066, 0.35556847, 4.232845] | Test Loss: [2.6303723, 0.33709913, 4.9236455]\n",
      "110: Train Loss: [2.2732155, 0.27887553, 4.2675557] | Test Loss: [2.7219467, 0.3621671, 5.0817266]\n",
      "111: Train Loss: [2.4258373, 0.38627625, 4.4653983] | Test Loss: [2.7686286, 0.36246762, 5.1747894]\n",
      "112: Train Loss: [2.2608602, 0.46156004, 4.06016] | Test Loss: [2.7044609, 0.5055688, 4.9033527]\n",
      "113: Train Loss: [2.377026, 0.36049306, 4.393559] | Test Loss: [2.6024942, 0.35596052, 4.849028]\n",
      "114: Train Loss: [2.3466527, 0.33638087, 4.3569245] | Test Loss: [2.6001496, 0.35024625, 4.850053]\n",
      "115: Train Loss: [2.268882, 0.33690614, 4.200858] | Test Loss: [2.8249297, 0.3389074, 5.310952]\n",
      "116: Train Loss: [2.567272, 0.42564023, 4.708904] | Test Loss: [2.73286, 0.39356348, 5.072157]\n",
      "117: Train Loss: [2.5637605, 0.39285868, 4.7346625] | Test Loss: [2.6393802, 0.3842793, 4.894481]\n",
      "118: Train Loss: [2.4409394, 0.33697543, 4.5449033] | Test Loss: [2.524793, 0.3685042, 4.681082]\n",
      "119: Train Loss: [2.2631958, 0.3190356, 4.207356] | Test Loss: [2.6013305, 0.3872379, 4.815423]\n",
      "120: Train Loss: [2.4594495, 0.30121475, 4.6176844] | Test Loss: [2.6934526, 0.38917556, 4.99773]\n",
      "121: Train Loss: [2.466242, 0.4036764, 4.5288076] | Test Loss: [2.6473806, 0.42460537, 4.870156]\n",
      "122: Train Loss: [2.4664958, 0.3672253, 4.5657663] | Test Loss: [2.5405734, 0.3272288, 4.7539177]\n",
      "123: Train Loss: [2.3853745, 0.4383984, 4.3323507] | Test Loss: [2.7162309, 0.32118398, 5.1112776]\n",
      "124: Train Loss: [2.3193948, 0.32911533, 4.3096743] | Test Loss: [2.5674562, 0.33408207, 4.8008304]\n",
      "125: Train Loss: [2.2489123, 0.36323008, 4.1345944] | Test Loss: [2.7590442, 0.3794972, 5.1385913]\n",
      "126: Train Loss: [2.302915, 0.30201593, 4.3038144] | Test Loss: [2.763972, 0.35128546, 5.1766586]\n",
      "127: Train Loss: [2.512991, 0.45764193, 4.56834] | Test Loss: [2.8218346, 0.36656192, 5.2771072]\n",
      "128: Train Loss: [2.3454764, 0.34957716, 4.341376] | Test Loss: [2.5168962, 0.36924, 4.6645527]\n",
      "129: Train Loss: [2.2803457, 0.33222005, 4.2284713] | Test Loss: [2.6702144, 0.39172286, 4.948706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130: Train Loss: [2.409248, 0.32549897, 4.492997] | Test Loss: [2.734527, 0.43684822, 5.032206]\n",
      "131: Train Loss: [2.3599596, 0.3615664, 4.3583527] | Test Loss: [2.6322844, 0.43870082, 4.825868]\n",
      "132: Train Loss: [2.2609253, 0.3179729, 4.2038774] | Test Loss: [2.6448214, 0.3155942, 4.9740486]\n",
      "133: Train Loss: [2.3607457, 0.47129244, 4.250199] | Test Loss: [2.7131183, 0.31065243, 5.1155844]\n",
      "134: Train Loss: [2.439029, 0.3363565, 4.5417013] | Test Loss: [2.825685, 0.34288776, 5.308482]\n",
      "135: Train Loss: [2.228107, 0.38331562, 4.0728984] | Test Loss: [2.5491667, 0.34601608, 4.7523174]\n",
      "136: Train Loss: [2.3788671, 0.3268675, 4.4308667] | Test Loss: [2.6407468, 0.3805376, 4.900956]\n",
      "137: Train Loss: [2.3770857, 0.3961034, 4.358068] | Test Loss: [2.6567101, 0.34746093, 4.9659595]\n",
      "138: Train Loss: [2.349868, 0.39557806, 4.304158] | Test Loss: [2.6349573, 0.38701752, 4.882897]\n",
      "139: Train Loss: [2.438331, 0.3432395, 4.5334225] | Test Loss: [2.8048487, 0.4487116, 5.160986]\n",
      "140: Train Loss: [2.407881, 0.3666225, 4.4491396] | Test Loss: [2.738782, 0.35402283, 5.123541]\n",
      "141: Train Loss: [2.348689, 0.3434829, 4.353895] | Test Loss: [2.798558, 0.39793834, 5.1991777]\n",
      "142: Train Loss: [2.4963243, 0.32933596, 4.6633124] | Test Loss: [2.7875197, 0.31436968, 5.2606697]\n",
      "143: Train Loss: [2.328991, 0.3356168, 4.3223653] | Test Loss: [2.6581197, 0.36603898, 4.9502006]\n",
      "144: Train Loss: [2.3211596, 0.3721144, 4.270205] | Test Loss: [2.6953378, 0.32763267, 5.0630426]\n",
      "145: Train Loss: [2.3380818, 0.2713829, 4.404781] | Test Loss: [2.5258036, 0.37941536, 4.6721916]\n",
      "146: Train Loss: [2.3918066, 0.4396131, 4.3440003] | Test Loss: [2.6683023, 0.37879023, 4.957814]\n",
      "147: Train Loss: [2.3666513, 0.3530977, 4.3802047] | Test Loss: [2.8258264, 0.4025359, 5.249117]\n",
      "148: Train Loss: [2.4080927, 0.3754368, 4.4407487] | Test Loss: [2.6132395, 0.33933413, 4.887145]\n",
      "149: Train Loss: [2.3118386, 0.31010213, 4.3135753] | Test Loss: [2.5655894, 0.3301332, 4.801046]\n",
      "150: Train Loss: [2.4289613, 0.35911563, 4.498807] | Test Loss: [2.806265, 0.36955687, 5.2429733]\n",
      "151: Train Loss: [2.3963664, 0.3734809, 4.419252] | Test Loss: [2.7852144, 0.38950995, 5.1809187]\n",
      "152: Train Loss: [2.3066065, 0.38018247, 4.233031] | Test Loss: [2.5712123, 0.37514475, 4.7672796]\n",
      "153: Train Loss: [2.3458204, 0.35999864, 4.331642] | Test Loss: [2.5001702, 0.35746625, 4.6428742]\n",
      "154: Train Loss: [2.5335429, 0.33968198, 4.7274036] | Test Loss: [2.5557287, 0.3363027, 4.7751546]\n",
      "155: Train Loss: [2.3618407, 0.33213234, 4.391549] | Test Loss: [2.532984, 0.46866125, 4.5973067]\n",
      "156: Train Loss: [2.3685808, 0.36769086, 4.3694706] | Test Loss: [2.9007945, 0.43813443, 5.363455]\n",
      "157: Train Loss: [2.3859289, 0.33244035, 4.4394174] | Test Loss: [2.5894463, 0.37279254, 4.8061]\n",
      "158: Train Loss: [2.418852, 0.32577497, 4.511929] | Test Loss: [2.6063235, 0.38347575, 4.829171]\n",
      "159: Train Loss: [2.4225094, 0.34409064, 4.5009284] | Test Loss: [2.7091513, 0.34336075, 5.0749416]\n",
      "160: Train Loss: [2.2872384, 0.27495778, 4.299519] | Test Loss: [2.665264, 0.3331228, 4.997405]\n",
      "161: Train Loss: [2.287755, 0.35347164, 4.2220383] | Test Loss: [2.8106878, 0.39186695, 5.2295084]\n",
      "162: Train Loss: [2.3752043, 0.43636513, 4.3140435] | Test Loss: [2.6073616, 0.29568896, 4.919034]\n",
      "163: Train Loss: [2.4651132, 0.42144415, 4.5087824] | Test Loss: [2.5456133, 0.41721573, 4.6740108]\n",
      "164: Train Loss: [2.5442255, 0.36822012, 4.7202306] | Test Loss: [2.784432, 0.3839188, 5.184945]\n",
      "165: Train Loss: [2.3687623, 0.34502873, 4.3924956] | Test Loss: [2.6174884, 0.38778418, 4.847193]\n",
      "166: Train Loss: [2.362491, 0.36540726, 4.3595743] | Test Loss: [2.674112, 0.3764498, 4.9717746]\n",
      "167: Train Loss: [2.4173717, 0.34132734, 4.4934163] | Test Loss: [2.7414613, 0.38272318, 5.100199]\n",
      "168: Train Loss: [2.2378006, 0.3438947, 4.1317067] | Test Loss: [2.6126213, 0.38061494, 4.844628]\n",
      "169: Train Loss: [2.4950197, 0.44869402, 4.541345] | Test Loss: [2.580774, 0.30271518, 4.858833]\n",
      "170: Train Loss: [2.3997216, 0.36815846, 4.431285] | Test Loss: [2.6886885, 0.3670526, 5.0103245]\n",
      "171: Train Loss: [2.379953, 0.34031558, 4.4195905] | Test Loss: [2.6154468, 0.32172635, 4.9091673]\n",
      "172: Train Loss: [2.453125, 0.3199465, 4.5863037] | Test Loss: [2.715981, 0.3146383, 5.117324]\n",
      "173: Train Loss: [2.3359969, 0.3615535, 4.31044] | Test Loss: [2.6793094, 0.34691823, 5.0117006]\n",
      "174: Train Loss: [2.3649774, 0.36084542, 4.369109] | Test Loss: [2.6500928, 0.36746114, 4.9327245]\n",
      "175: Train Loss: [2.3393128, 0.29556647, 4.383059] | Test Loss: [2.800068, 0.34575573, 5.25438]\n",
      "176: Train Loss: [2.392038, 0.34665287, 4.437423] | Test Loss: [2.619755, 0.3937072, 4.845803]\n",
      "177: Train Loss: [2.3941054, 0.3362806, 4.45193] | Test Loss: [2.7307951, 0.3707924, 5.090798]\n",
      "178: Train Loss: [2.393887, 0.35621297, 4.431561] | Test Loss: [2.6890755, 0.38199162, 4.996159]\n",
      "179: Train Loss: [2.5067089, 0.38943103, 4.6239867] | Test Loss: [2.5555017, 0.3521096, 4.758894]\n",
      "180: Train Loss: [2.3711839, 0.31288362, 4.4294844] | Test Loss: [2.853297, 0.44089368, 5.2657003]\n",
      "181: Train Loss: [2.3300085, 0.35721782, 4.302799] | Test Loss: [2.5432076, 0.38778454, 4.698631]\n",
      "182: Train Loss: [2.3690686, 0.35507965, 4.3830576] | Test Loss: [2.498251, 0.36859575, 4.6279063]\n",
      "183: Train Loss: [2.294245, 0.31597045, 4.2725196] | Test Loss: [2.6443937, 0.34482434, 4.943963]\n",
      "184: Train Loss: [2.4236646, 0.30349416, 4.543835] | Test Loss: [2.61195, 0.45389, 4.77001]\n",
      "185: Train Loss: [2.3687186, 0.3649981, 4.3724394] | Test Loss: [2.7767773, 0.42877075, 5.124784]\n",
      "186: Train Loss: [2.4646735, 0.38869694, 4.54065] | Test Loss: [2.7883484, 0.4576915, 5.119005]\n",
      "187: Train Loss: [2.33669, 0.36204904, 4.311331] | Test Loss: [2.4400945, 0.27980557, 4.6003833]\n",
      "188: Train Loss: [2.4257812, 0.37032896, 4.4812336] | Test Loss: [2.7189848, 0.3534993, 5.0844703]\n",
      "189: Train Loss: [2.289028, 0.3133987, 4.264657] | Test Loss: [2.7304103, 0.44394705, 5.016874]\n",
      "190: Train Loss: [2.2090826, 0.3765267, 4.0416384] | Test Loss: [2.6900775, 0.32260138, 5.057554]\n",
      "191: Train Loss: [2.4907978, 0.33530656, 4.646289] | Test Loss: [2.6106234, 0.43107295, 4.790174]\n",
      "192: Train Loss: [2.3149326, 0.3679625, 4.261903] | Test Loss: [2.4664936, 0.39049065, 4.5424967]\n",
      "193: Train Loss: [2.3606012, 0.33437794, 4.3868246] | Test Loss: [2.622635, 0.32394725, 4.9213223]\n",
      "194: Train Loss: [2.2965267, 0.38032532, 4.212728] | Test Loss: [2.8095145, 0.36150753, 5.2575216]\n",
      "195: Train Loss: [2.3885813, 0.38879576, 4.3883667] | Test Loss: [2.648158, 0.3991622, 4.897154]\n",
      "196: Train Loss: [2.347993, 0.36127257, 4.3347135] | Test Loss: [2.698907, 0.38109756, 5.016716]\n",
      "197: Train Loss: [2.0958066, 0.3284292, 3.863184] | Test Loss: [2.468194, 0.33948815, 4.5969]\n",
      "198: Train Loss: [2.49777, 0.36752787, 4.628012] | Test Loss: [2.6887653, 0.3448035, 5.0327272]\n",
      "199: Train Loss: [2.4152405, 0.30207416, 4.528407] | Test Loss: [2.621804, 0.34614724, 4.897461]\n",
      "200: Train Loss: [2.2576094, 0.3410069, 4.174212] | Test Loss: [2.6844597, 0.43147287, 4.9374466]\n",
      "201: Train Loss: [2.413583, 0.42186382, 4.405302] | Test Loss: [2.623876, 0.39276147, 4.8549905]\n",
      "202: Train Loss: [2.2807689, 0.37073904, 4.1907988] | Test Loss: [2.7639413, 0.4004612, 5.1274214]\n",
      "203: Train Loss: [2.3594065, 0.2854464, 4.4333663] | Test Loss: [2.8157723, 0.3197462, 5.3117986]\n",
      "204: Train Loss: [2.3274245, 0.31650802, 4.338341] | Test Loss: [2.7848322, 0.36336768, 5.206297]\n",
      "205: Train Loss: [2.4497666, 0.36123988, 4.5382934] | Test Loss: [2.7224793, 0.3697884, 5.07517]\n",
      "206: Train Loss: [2.3219016, 0.30707842, 4.3367248] | Test Loss: [2.7752833, 0.38766974, 5.162897]\n",
      "207: Train Loss: [2.2880423, 0.30167934, 4.2744055] | Test Loss: [2.8808112, 0.40874833, 5.3528743]\n",
      "208: Train Loss: [2.287834, 0.3601778, 4.21549] | Test Loss: [2.6459324, 0.33132866, 4.960536]\n",
      "209: Train Loss: [2.489764, 0.40161884, 4.577909] | Test Loss: [2.5922408, 0.30521366, 4.879268]\n",
      "210: Train Loss: [2.371811, 0.31116164, 4.4324603] | Test Loss: [2.668028, 0.40536365, 4.9306927]\n",
      "211: Train Loss: [2.3293755, 0.31094873, 4.347802] | Test Loss: [2.5400605, 0.3630321, 4.7170887]\n",
      "212: Train Loss: [2.2056227, 0.29306394, 4.118181] | Test Loss: [2.648517, 0.4019321, 4.8951015]\n",
      "213: Train Loss: [2.4471345, 0.43435088, 4.459918] | Test Loss: [2.5912883, 0.31118023, 4.8713965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214: Train Loss: [2.2135026, 0.32720885, 4.0997963] | Test Loss: [2.7053244, 0.48857948, 4.9220695]\n",
      "215: Train Loss: [2.3457742, 0.33589825, 4.35565] | Test Loss: [2.5332248, 0.36732188, 4.6991277]\n",
      "216: Train Loss: [2.384117, 0.39875117, 4.3694825] | Test Loss: [2.4490297, 0.29475892, 4.6033006]\n",
      "217: Train Loss: [2.493834, 0.377875, 4.609793] | Test Loss: [2.6712494, 0.3420841, 5.000415]\n",
      "218: Train Loss: [2.2472095, 0.32302392, 4.1713953] | Test Loss: [2.5647273, 0.38210654, 4.747348]\n",
      "219: Train Loss: [2.442772, 0.31806093, 4.567483] | Test Loss: [2.8239331, 0.5072082, 5.140658]\n",
      "220: Train Loss: [2.3412986, 0.3272062, 4.355391] | Test Loss: [2.6293416, 0.4049848, 4.8536983]\n",
      "221: Train Loss: [2.3296099, 0.36251807, 4.296702] | Test Loss: [2.8740695, 0.30712467, 5.4410143]\n",
      "222: Train Loss: [2.3683178, 0.33901343, 4.397622] | Test Loss: [2.7748907, 0.45108023, 5.098701]\n",
      "223: Train Loss: [2.33001, 0.33466524, 4.3253546] | Test Loss: [2.6631, 0.3136652, 5.0125346]\n",
      "224: Train Loss: [2.386329, 0.39806226, 4.3745956] | Test Loss: [2.7423654, 0.39135754, 5.0933733]\n",
      "225: Train Loss: [2.4501088, 0.3498389, 4.550379] | Test Loss: [2.6645548, 0.34889132, 4.9802184]\n",
      "226: Train Loss: [2.3463178, 0.4073622, 4.2852736] | Test Loss: [2.7817407, 0.39253733, 5.170944]\n",
      "227: Train Loss: [2.2813723, 0.39515883, 4.167586] | Test Loss: [2.5736985, 0.32152915, 4.825868]\n",
      "228: Train Loss: [2.4141846, 0.37320334, 4.455166] | Test Loss: [2.6582515, 0.3781204, 4.9383826]\n",
      "229: Train Loss: [2.284436, 0.3435027, 4.2253695] | Test Loss: [2.570732, 0.38064668, 4.7608175]\n",
      "230: Train Loss: [2.326093, 0.37091917, 4.2812667] | Test Loss: [2.6682572, 0.37249798, 4.9640164]\n",
      "231: Train Loss: [2.409921, 0.4118463, 4.4079957] | Test Loss: [2.4662585, 0.38713002, 4.545387]\n",
      "232: Train Loss: [2.5298624, 0.4233096, 4.636415] | Test Loss: [2.675996, 0.35488257, 4.9971094]\n",
      "233: Train Loss: [2.470774, 0.34957826, 4.5919695] | Test Loss: [2.6212182, 0.34831253, 4.894124]\n",
      "234: Train Loss: [2.2503774, 0.36146733, 4.1392875] | Test Loss: [2.7020292, 0.33091712, 5.0731416]\n",
      "235: Train Loss: [2.4661853, 0.37662685, 4.5557437] | Test Loss: [2.7509758, 0.3455053, 5.1564465]\n",
      "236: Train Loss: [2.2842226, 0.3220873, 4.246358] | Test Loss: [2.7199922, 0.40118152, 5.0388026]\n",
      "237: Train Loss: [2.451569, 0.39071572, 4.5124226] | Test Loss: [2.621571, 0.3074807, 4.9356613]\n",
      "238: Train Loss: [2.3246133, 0.38476977, 4.2644567] | Test Loss: [2.5471458, 0.34753147, 4.7467604]\n",
      "239: Train Loss: [2.3825316, 0.30568135, 4.459382] | Test Loss: [2.8471994, 0.37455115, 5.3198476]\n",
      "240: Train Loss: [2.3480444, 0.33297384, 4.363115] | Test Loss: [2.7273295, 0.38512164, 5.069537]\n",
      "241: Train Loss: [2.4105985, 0.36871618, 4.452481] | Test Loss: [2.5991383, 0.36521134, 4.833065]\n",
      "242: Train Loss: [2.3919206, 0.34787554, 4.4359655] | Test Loss: [2.6397185, 0.44218746, 4.8372498]\n",
      "243: Train Loss: [2.4310582, 0.37361658, 4.4884996] | Test Loss: [2.4344983, 0.3254939, 4.543503]\n",
      "244: Train Loss: [2.308919, 0.34544638, 4.2723913] | Test Loss: [2.62886, 0.32253373, 4.9351864]\n",
      "245: Train Loss: [2.4130797, 0.34249267, 4.483667] | Test Loss: [2.6183746, 0.34895334, 4.887796]\n",
      "246: Train Loss: [2.4467497, 0.41286862, 4.480631] | Test Loss: [2.7010567, 0.38491017, 5.0172033]\n",
      "247: Train Loss: [2.441795, 0.325997, 4.5575933] | Test Loss: [2.9431853, 0.6314795, 5.2548914]\n",
      "248: Train Loss: [2.2810295, 0.31686452, 4.2451944] | Test Loss: [2.5593982, 0.3366133, 4.782183]\n",
      "249: Train Loss: [2.4286184, 0.3299791, 4.527258] | Test Loss: [2.818791, 0.5385875, 5.0989943]\n",
      "250: Train Loss: [2.505575, 0.3344107, 4.676739] | Test Loss: [2.7249596, 0.3028737, 5.1470456]\n",
      "251: Train Loss: [2.3787286, 0.3774743, 4.379983] | Test Loss: [2.6440895, 0.29268008, 4.9954987]\n",
      "252: Train Loss: [2.4212985, 0.3428723, 4.499725] | Test Loss: [2.8807402, 0.35024932, 5.411231]\n",
      "253: Train Loss: [2.3636258, 0.29730844, 4.429943] | Test Loss: [2.7060857, 0.38063693, 5.0315347]\n",
      "254: Train Loss: [2.2233849, 0.38038725, 4.0663824] | Test Loss: [2.4613469, 0.32976726, 4.5929265]\n",
      "255: Train Loss: [2.5268576, 0.42116216, 4.632553] | Test Loss: [2.7088907, 0.366296, 5.0514855]\n",
      "256: Train Loss: [2.257607, 0.39764085, 4.1175733] | Test Loss: [2.7918658, 0.37924033, 5.204491]\n",
      "257: Train Loss: [2.377745, 0.34953162, 4.405958] | Test Loss: [2.667502, 0.35191268, 4.9830914]\n",
      "258: Train Loss: [2.4645364, 0.37714702, 4.5519257] | Test Loss: [2.659258, 0.36929, 4.949226]\n",
      "259: Train Loss: [2.4390354, 0.36210024, 4.5159707] | Test Loss: [2.6770127, 0.36542135, 4.988604]\n",
      "260: Train Loss: [2.156358, 0.34859422, 3.9641216] | Test Loss: [2.593976, 0.36503753, 4.8229146]\n",
      "261: Train Loss: [2.3383088, 0.34268576, 4.333932] | Test Loss: [2.6073446, 0.41326743, 4.8014216]\n",
      "262: Train Loss: [2.3995473, 0.37112087, 4.4279737] | Test Loss: [2.5350833, 0.42701948, 4.643147]\n",
      "263: Train Loss: [2.4013326, 0.3659461, 4.436719] | Test Loss: [2.7809093, 0.34612903, 5.2156897]\n",
      "264: Train Loss: [2.3751178, 0.413575, 4.3366604] | Test Loss: [2.717631, 0.36611754, 5.0691447]\n",
      "265: Train Loss: [2.386845, 0.3785292, 4.395161] | Test Loss: [2.8202996, 0.34262875, 5.2979703]\n",
      "266: Train Loss: [2.4305441, 0.35817093, 4.5029173] | Test Loss: [2.6508877, 0.42894858, 4.872827]\n",
      "267: Train Loss: [2.444208, 0.35445404, 4.533962] | Test Loss: [2.636421, 0.33309412, 4.939748]\n",
      "268: Train Loss: [2.485086, 0.40014842, 4.5700235] | Test Loss: [2.6581635, 0.35423446, 4.9620924]\n",
      "269: Train Loss: [2.4428883, 0.4916299, 4.3941464] | Test Loss: [2.7686062, 0.35216957, 5.185043]\n",
      "270: Train Loss: [2.3793628, 0.42734554, 4.33138] | Test Loss: [2.5725214, 0.44016677, 4.704876]\n",
      "271: Train Loss: [2.3787673, 0.36178684, 4.3957477] | Test Loss: [2.5889533, 0.4088036, 4.769103]\n",
      "272: Train Loss: [2.3318105, 0.4004114, 4.2632093] | Test Loss: [2.5111144, 0.39185363, 4.630375]\n",
      "273: Train Loss: [2.4848175, 0.41681919, 4.552816] | Test Loss: [2.7368267, 0.35204828, 5.121605]\n",
      "274: Train Loss: [2.4007769, 0.33991387, 4.46164] | Test Loss: [2.5546846, 0.42839354, 4.680976]\n",
      "275: Train Loss: [2.3519185, 0.34844595, 4.355391] | Test Loss: [2.5810423, 0.391288, 4.770797]\n",
      "276: Train Loss: [2.4418232, 0.35898215, 4.5246644] | Test Loss: [2.6513603, 0.3340031, 4.9687176]\n",
      "277: Train Loss: [2.3381925, 0.33619997, 4.340185] | Test Loss: [2.6394958, 0.43385664, 4.845135]\n",
      "278: Train Loss: [2.246692, 0.3042095, 4.189174] | Test Loss: [2.6902463, 0.4482079, 4.932285]\n",
      "279: Train Loss: [2.48356, 0.3964814, 4.5706387] | Test Loss: [2.608365, 0.33662587, 4.880104]\n",
      "280: Train Loss: [2.3975308, 0.39819098, 4.3968706] | Test Loss: [2.5605757, 0.44973052, 4.671421]\n",
      "281: Train Loss: [2.391728, 0.34419525, 4.4392605] | Test Loss: [2.7154663, 0.398474, 5.0324583]\n",
      "282: Train Loss: [2.505549, 0.4302218, 4.5808764] | Test Loss: [2.5448756, 0.3467724, 4.742979]\n",
      "283: Train Loss: [2.5869527, 0.37347516, 4.8004303] | Test Loss: [2.5401611, 0.36320734, 4.717115]\n",
      "284: Train Loss: [2.4745684, 0.4015319, 4.547605] | Test Loss: [2.5725095, 0.41456935, 4.7304497]\n",
      "285: Train Loss: [2.2834172, 0.38580444, 4.18103] | Test Loss: [2.6195123, 0.36568353, 4.873341]\n",
      "286: Train Loss: [2.4280155, 0.35477072, 4.5012603] | Test Loss: [2.5340838, 0.3544624, 4.713705]\n",
      "287: Train Loss: [2.5110886, 0.36220378, 4.6599736] | Test Loss: [2.785902, 0.40297997, 5.168824]\n",
      "288: Train Loss: [2.4501383, 0.3659447, 4.534332] | Test Loss: [2.7099001, 0.41669962, 5.003101]\n",
      "289: Train Loss: [2.3531926, 0.33506697, 4.3713183] | Test Loss: [2.7092605, 0.35417274, 5.064348]\n",
      "290: Train Loss: [2.377958, 0.307016, 4.4489] | Test Loss: [2.6333873, 0.3476323, 4.9191422]\n",
      "291: Train Loss: [2.487281, 0.35877198, 4.6157904] | Test Loss: [2.687135, 0.36318347, 5.0110865]\n",
      "292: Train Loss: [2.4845083, 0.47856846, 4.490448] | Test Loss: [2.6563106, 0.39699244, 4.915629]\n",
      "293: Train Loss: [2.6994553, 0.4241316, 4.974779] | Test Loss: [2.861122, 0.3444607, 5.3777833]\n",
      "294: Train Loss: [2.3870833, 0.35853797, 4.4156284] | Test Loss: [2.8499186, 0.3448842, 5.354953]\n",
      "295: Train Loss: [2.3407996, 0.34564814, 4.335951] | Test Loss: [2.6566396, 0.34521937, 4.96806]\n",
      "296: Train Loss: [2.3932757, 0.3608854, 4.425666] | Test Loss: [2.6387217, 0.35728157, 4.9201617]\n",
      "297: Train Loss: [2.5027483, 0.38008088, 4.625416] | Test Loss: [2.6930108, 0.37259004, 5.0134315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298: Train Loss: [2.447168, 0.38943246, 4.504904] | Test Loss: [2.6307082, 0.38727224, 4.874144]\n",
      "299: Train Loss: [2.3956506, 0.33643597, 4.4548655] | Test Loss: [2.6037502, 0.42066675, 4.786834]\n",
      "300: Train Loss: [2.3274374, 0.3559844, 4.2988906] | Test Loss: [2.653366, 0.34564906, 4.961083]\n",
      "301: Train Loss: [2.3726616, 0.3545158, 4.3908076] | Test Loss: [2.8168957, 0.39976948, 5.234022]\n",
      "302: Train Loss: [2.4095092, 0.28238097, 4.5366373] | Test Loss: [2.723553, 0.33812279, 5.108983]\n",
      "303: Train Loss: [2.5229483, 0.42309517, 4.6228013] | Test Loss: [2.7264128, 0.45518693, 4.9976387]\n",
      "304: Train Loss: [2.3401055, 0.35832804, 4.321883] | Test Loss: [2.5222065, 0.34980887, 4.6946044]\n",
      "305: Train Loss: [2.4133475, 0.42378998, 4.402905] | Test Loss: [2.785158, 0.36780736, 5.2025084]\n",
      "306: Train Loss: [2.5761085, 0.43168044, 4.720536] | Test Loss: [2.5863268, 0.38006613, 4.7925878]\n",
      "307: Train Loss: [2.4651387, 0.33528197, 4.5949955] | Test Loss: [2.8455496, 0.41393816, 5.277161]\n",
      "308: Train Loss: [2.3057811, 0.3422456, 4.2693167] | Test Loss: [2.5950058, 0.4838416, 4.70617]\n",
      "309: Train Loss: [2.2986584, 0.34578118, 4.2515354] | Test Loss: [2.7484953, 0.38050073, 5.11649]\n",
      "310: Train Loss: [2.3102434, 0.32826677, 4.29222] | Test Loss: [2.6468852, 0.3834806, 4.91029]\n",
      "311: Train Loss: [2.3971364, 0.46527246, 4.3290005] | Test Loss: [2.7984884, 0.3738906, 5.2230864]\n",
      "312: Train Loss: [2.1706696, 0.32783705, 4.013502] | Test Loss: [2.727717, 0.3443837, 5.11105]\n",
      "313: Train Loss: [2.307781, 0.30465564, 4.3109064] | Test Loss: [2.61356, 0.41313013, 4.8139896]\n",
      "314: Train Loss: [2.318156, 0.30627364, 4.3300385] | Test Loss: [2.4456382, 0.34702003, 4.544256]\n",
      "315: Train Loss: [2.4497292, 0.39976153, 4.4996967] | Test Loss: [2.712715, 0.33273324, 5.0926967]\n",
      "316: Train Loss: [2.323465, 0.3988539, 4.2480764] | Test Loss: [2.6071248, 0.33646962, 4.87778]\n",
      "317: Train Loss: [2.350242, 0.33378074, 4.366703] | Test Loss: [2.6801543, 0.35208744, 5.008221]\n",
      "318: Train Loss: [2.3955746, 0.3278464, 4.4633026] | Test Loss: [2.6031723, 0.3726685, 4.8336763]\n",
      "319: Train Loss: [2.4576836, 0.3536929, 4.561674] | Test Loss: [2.6331275, 0.3714016, 4.894853]\n",
      "320: Train Loss: [2.4202724, 0.39630777, 4.4442368] | Test Loss: [2.6571875, 0.48734903, 4.827026]\n",
      "321: Train Loss: [2.3476665, 0.3615448, 4.3337884] | Test Loss: [2.5436811, 0.31791845, 4.769444]\n",
      "322: Train Loss: [2.552583, 0.39071193, 4.714454] | Test Loss: [2.7601638, 0.37504956, 5.145278]\n",
      "323: Train Loss: [2.2027912, 0.3707788, 4.034804] | Test Loss: [2.4774938, 0.37188193, 4.5831056]\n",
      "324: Train Loss: [2.5012088, 0.49138737, 4.51103] | Test Loss: [2.552639, 0.39223486, 4.713043]\n",
      "325: Train Loss: [2.4497874, 0.4218599, 4.477715] | Test Loss: [2.6940527, 0.3800158, 5.0080895]\n",
      "326: Train Loss: [2.3525198, 0.40961206, 4.2954273] | Test Loss: [2.749199, 0.41004127, 5.0883565]\n",
      "327: Train Loss: [2.3659313, 0.3746357, 4.357227] | Test Loss: [2.6826422, 0.35512468, 5.01016]\n",
      "328: Train Loss: [2.5181708, 0.37525716, 4.6610847] | Test Loss: [2.8267784, 0.41896433, 5.2345924]\n",
      "329: Train Loss: [2.519442, 0.35983092, 4.6790533] | Test Loss: [2.5557468, 0.29391086, 4.8175826]\n",
      "330: Train Loss: [2.6299913, 0.36041918, 4.8995633] | Test Loss: [2.5626397, 0.34336513, 4.781914]\n",
      "331: Train Loss: [2.2634299, 0.44913858, 4.077721] | Test Loss: [2.488255, 0.3191641, 4.657346]\n",
      "332: Train Loss: [2.4181182, 0.3784528, 4.4577837] | Test Loss: [2.5819707, 0.30760968, 4.856332]\n",
      "333: Train Loss: [2.4488888, 0.3047408, 4.5930367] | Test Loss: [2.738975, 0.39315692, 5.084793]\n",
      "334: Train Loss: [2.3502052, 0.3552118, 4.3451986] | Test Loss: [2.9319985, 0.42927086, 5.434726]\n",
      "335: Train Loss: [2.5410168, 0.41208965, 4.669944] | Test Loss: [2.792254, 0.34377107, 5.240737]\n",
      "336: Train Loss: [2.4596446, 0.38942334, 4.5298657] | Test Loss: [2.5800505, 0.29071525, 4.8693857]\n",
      "337: Train Loss: [2.5482028, 0.36436242, 4.7320433] | Test Loss: [2.846857, 0.4029583, 5.2907557]\n",
      "338: Train Loss: [2.5773315, 0.43290183, 4.721761] | Test Loss: [2.619154, 0.42119545, 4.8171124]\n",
      "339: Train Loss: [2.4785933, 0.38038135, 4.5768056] | Test Loss: [2.766409, 0.40585706, 5.1269608]\n",
      "340: Train Loss: [2.3240213, 0.40018427, 4.2478585] | Test Loss: [2.5830524, 0.36663806, 4.7994666]\n",
      "341: Train Loss: [2.4317584, 0.3120164, 4.5515003] | Test Loss: [2.565307, 0.32499588, 4.805618]\n",
      "342: Train Loss: [2.54056, 0.38160548, 4.6995144] | Test Loss: [2.7732038, 0.3904789, 5.1559286]\n",
      "343: Train Loss: [2.2967162, 0.34110072, 4.2523317] | Test Loss: [2.8288388, 0.4165237, 5.241154]\n",
      "344: Train Loss: [2.4665027, 0.34595025, 4.587055] | Test Loss: [2.6173527, 0.3989075, 4.835798]\n",
      "345: Train Loss: [2.5019252, 0.3532701, 4.6505804] | Test Loss: [2.7631178, 0.48107922, 5.0451565]\n",
      "346: Train Loss: [2.522267, 0.30974543, 4.734789] | Test Loss: [2.6953938, 0.41523328, 4.9755545]\n",
      "347: Train Loss: [2.6048238, 0.46323326, 4.746414] | Test Loss: [2.5390077, 0.36403087, 4.7139845]\n",
      "348: Train Loss: [2.4289932, 0.35792994, 4.5000563] | Test Loss: [2.6906304, 0.32375056, 5.0575104]\n",
      "349: Train Loss: [2.461727, 0.3281487, 4.595305] | Test Loss: [2.6863556, 0.3219049, 5.050806]\n",
      "350: Train Loss: [2.5119934, 0.327518, 4.696469] | Test Loss: [2.779718, 0.3848808, 5.174555]\n",
      "351: Train Loss: [2.593866, 0.3975105, 4.7902217] | Test Loss: [2.7134297, 0.3488031, 5.0780563]\n",
      "352: Train Loss: [2.5039816, 0.386842, 4.6211214] | Test Loss: [2.4661, 0.35239837, 4.5798016]\n",
      "353: Train Loss: [2.3282669, 0.31809577, 4.338438] | Test Loss: [2.7760239, 0.34567007, 5.2063775]\n",
      "354: Train Loss: [2.5273743, 0.4095371, 4.645211] | Test Loss: [2.847023, 0.42397445, 5.2700715]\n",
      "355: Train Loss: [2.4765732, 0.33974993, 4.6133966] | Test Loss: [2.7402768, 0.33425546, 5.146298]\n",
      "356: Train Loss: [2.473053, 0.30474895, 4.641357] | Test Loss: [2.568286, 0.30088043, 4.8356915]\n",
      "357: Train Loss: [2.3954787, 0.3030616, 4.487896] | Test Loss: [2.742252, 0.4080358, 5.0764685]\n",
      "358: Train Loss: [2.3389006, 0.37007177, 4.3077292] | Test Loss: [2.801035, 0.35899878, 5.243071]\n",
      "359: Train Loss: [2.44571, 0.4230495, 4.4683704] | Test Loss: [2.6077445, 0.3924914, 4.8229976]\n",
      "360: Train Loss: [2.5363965, 0.4026843, 4.670109] | Test Loss: [2.7429533, 0.33202684, 5.1538796]\n",
      "361: Train Loss: [2.4964375, 0.3724149, 4.62046] | Test Loss: [2.6881614, 0.43877113, 4.9375515]\n",
      "362: Train Loss: [2.5130665, 0.36157456, 4.6645584] | Test Loss: [2.5950658, 0.42009127, 4.7700405]\n",
      "363: Train Loss: [2.355897, 0.320701, 4.391093] | Test Loss: [2.76465, 0.36372882, 5.165571]\n",
      "364: Train Loss: [2.4908175, 0.33898506, 4.64265] | Test Loss: [2.9493537, 0.3522389, 5.5464687]\n",
      "365: Train Loss: [2.5189862, 0.39743304, 4.640539] | Test Loss: [2.6995497, 0.34065107, 5.0584483]\n",
      "366: Train Loss: [2.519092, 0.33078393, 4.7074003] | Test Loss: [2.3558977, 0.2607713, 4.451024]\n",
      "367: Train Loss: [2.3913736, 0.31923655, 4.4635105] | Test Loss: [2.6303585, 0.33307466, 4.9276423]\n",
      "368: Train Loss: [2.563208, 0.2966333, 4.829783] | Test Loss: [2.8083427, 0.30144003, 5.315245]\n",
      "369: Train Loss: [2.5410557, 0.3280167, 4.7540946] | Test Loss: [2.7085881, 0.46997568, 4.947201]\n",
      "370: Train Loss: [2.5490582, 0.34748584, 4.7506304] | Test Loss: [2.6580133, 0.3619457, 4.954081]\n",
      "371: Train Loss: [2.367682, 0.28611258, 4.449251] | Test Loss: [2.6860387, 0.49546188, 4.8766155]\n",
      "372: Train Loss: [2.5305104, 0.35265538, 4.7083654] | Test Loss: [2.9406688, 0.4458324, 5.4355054]\n",
      "373: Train Loss: [2.3169806, 0.44184223, 4.192119] | Test Loss: [2.6040335, 0.3302972, 4.87777]\n",
      "374: Train Loss: [2.471981, 0.36510545, 4.5788565] | Test Loss: [2.5745463, 0.31949764, 4.829595]\n",
      "375: Train Loss: [2.3866117, 0.3795642, 4.393659] | Test Loss: [2.5052953, 0.37225923, 4.6383314]\n",
      "376: Train Loss: [2.605836, 0.36812463, 4.8435473] | Test Loss: [2.7293634, 0.36536676, 5.09336]\n",
      "377: Train Loss: [2.3639688, 0.34155953, 4.3863783] | Test Loss: [2.5567167, 0.4179165, 4.695517]\n",
      "378: Train Loss: [2.5220404, 0.43212008, 4.611961] | Test Loss: [2.6256235, 0.39671272, 4.854534]\n",
      "379: Train Loss: [2.5390184, 0.4111871, 4.6668496] | Test Loss: [2.604523, 0.32862014, 4.880426]\n",
      "380: Train Loss: [2.556063, 0.33390343, 4.7782226] | Test Loss: [2.5735989, 0.31784078, 4.829357]\n",
      "381: Train Loss: [2.3935854, 0.31381816, 4.473353] | Test Loss: [2.7421725, 0.46869323, 5.0156517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382: Train Loss: [2.4934115, 0.28152943, 4.7052937] | Test Loss: [2.768329, 0.37321994, 5.163438]\n",
      "383: Train Loss: [2.397619, 0.3804553, 4.4147825] | Test Loss: [2.7700026, 0.39909303, 5.140912]\n",
      "384: Train Loss: [2.2751732, 0.3391518, 4.2111945] | Test Loss: [2.7534914, 0.43426293, 5.07272]\n",
      "385: Train Loss: [2.671176, 0.3963745, 4.945977] | Test Loss: [2.5245786, 0.3328789, 4.716278]\n",
      "386: Train Loss: [2.4422848, 0.3631404, 4.521429] | Test Loss: [2.607272, 0.4354855, 4.7790585]\n",
      "387: Train Loss: [2.5785263, 0.43732166, 4.719731] | Test Loss: [2.64526, 0.33879822, 4.951722]\n",
      "388: Train Loss: [2.5368164, 0.29643917, 4.7771935] | Test Loss: [2.6434457, 0.35267848, 4.934213]\n",
      "389: Train Loss: [2.4537098, 0.3353222, 4.5720973] | Test Loss: [2.7242832, 0.37122923, 5.0773373]\n",
      "390: Train Loss: [2.3998072, 0.40151852, 4.398096] | Test Loss: [2.901179, 0.4022341, 5.400124]\n",
      "391: Train Loss: [2.5368743, 0.38066635, 4.6930823] | Test Loss: [2.6657698, 0.36126173, 4.970278]\n",
      "392: Train Loss: [2.4918365, 0.35860696, 4.6250663] | Test Loss: [2.6692574, 0.33999476, 4.99852]\n",
      "393: Train Loss: [2.5271442, 0.3654495, 4.688839] | Test Loss: [2.6162114, 0.32195303, 4.91047]\n",
      "394: Train Loss: [2.4453251, 0.31202987, 4.5786204] | Test Loss: [2.8388493, 0.4636398, 5.214059]\n",
      "395: Train Loss: [2.5914724, 0.39859405, 4.784351] | Test Loss: [2.7526917, 0.3552154, 5.150168]\n",
      "396: Train Loss: [2.579848, 0.31238905, 4.847307] | Test Loss: [2.5364997, 0.3678962, 4.7051034]\n",
      "397: Train Loss: [2.440651, 0.39016664, 4.491135] | Test Loss: [2.700605, 0.41100854, 4.9902015]\n",
      "398: Train Loss: [2.5975413, 0.40227392, 4.7928085] | Test Loss: [2.8460858, 0.37600094, 5.3161707]\n",
      "399: Train Loss: [2.4330597, 0.39286405, 4.473255] | Test Loss: [2.7566812, 0.39714345, 5.116219]\n",
      "400: Train Loss: [2.4621978, 0.41419, 4.5102057] | Test Loss: [2.843637, 0.3319864, 5.3552876]\n",
      "401: Train Loss: [2.4204965, 0.3279969, 4.512996] | Test Loss: [2.4770327, 0.33617505, 4.6178904]\n",
      "402: Train Loss: [2.3575644, 0.40243647, 4.3126926] | Test Loss: [2.7465286, 0.37659097, 5.1164665]\n",
      "403: Train Loss: [2.4343283, 0.3838425, 4.484814] | Test Loss: [2.5221868, 0.3976598, 4.6467137]\n",
      "404: Train Loss: [2.382664, 0.33866265, 4.4266653] | Test Loss: [2.663436, 0.33869937, 4.9881725]\n",
      "405: Train Loss: [2.3853068, 0.30753535, 4.4630785] | Test Loss: [2.5905533, 0.3691608, 4.811946]\n",
      "406: Train Loss: [2.5742319, 0.41476014, 4.7337036] | Test Loss: [2.702386, 0.31083483, 5.093937]\n",
      "407: Train Loss: [2.3214502, 0.35644957, 4.286451] | Test Loss: [2.5381076, 0.37841213, 4.697803]\n",
      "408: Train Loss: [2.415474, 0.31024694, 4.520701] | Test Loss: [2.8252707, 0.37682027, 5.273721]\n",
      "409: Train Loss: [2.311418, 0.30938616, 4.31345] | Test Loss: [2.4562137, 0.4260368, 4.4863906]\n",
      "410: Train Loss: [2.6246488, 0.35026366, 4.899034] | Test Loss: [2.7861044, 0.48145816, 5.0907507]\n",
      "411: Train Loss: [2.5052562, 0.3646715, 4.6458406] | Test Loss: [2.648157, 0.38270584, 4.913608]\n",
      "412: Train Loss: [2.5222106, 0.35557157, 4.6888494] | Test Loss: [2.562753, 0.3611879, 4.764318]\n",
      "413: Train Loss: [2.4428115, 0.34949937, 4.5361238] | Test Loss: [2.7236464, 0.3587799, 5.088513]\n",
      "414: Train Loss: [2.5805535, 0.3918354, 4.769272] | Test Loss: [2.7950523, 0.36692926, 5.2231755]\n",
      "415: Train Loss: [2.329175, 0.3659652, 4.2923846] | Test Loss: [2.6240945, 0.34796605, 4.900223]\n",
      "416: Train Loss: [2.5386345, 0.31436843, 4.762901] | Test Loss: [2.7874267, 0.35659397, 5.2182593]\n",
      "417: Train Loss: [2.5469635, 0.3501972, 4.7437296] | Test Loss: [2.5953283, 0.34303272, 4.847624]\n",
      "418: Train Loss: [2.4466856, 0.39380142, 4.49957] | Test Loss: [2.8182123, 0.3968045, 5.23962]\n",
      "419: Train Loss: [2.498633, 0.33464295, 4.662623] | Test Loss: [2.66074, 0.3265013, 4.9949784]\n",
      "420: Train Loss: [2.5485764, 0.37714636, 4.7200065] | Test Loss: [2.5504005, 0.35793793, 4.742863]\n",
      "421: Train Loss: [2.4148211, 0.34723315, 4.482409] | Test Loss: [2.9105904, 0.48150963, 5.339671]\n",
      "422: Train Loss: [2.4377172, 0.39455572, 4.480879] | Test Loss: [2.6670015, 0.3154967, 5.018506]\n",
      "423: Train Loss: [2.4778218, 0.33336404, 4.6222796] | Test Loss: [2.5023727, 0.29813653, 4.706609]\n",
      "424: Train Loss: [2.4975474, 0.40038124, 4.5947137] | Test Loss: [2.7358956, 0.4550276, 5.0167637]\n",
      "425: Train Loss: [2.6001654, 0.38122344, 4.8191075] | Test Loss: [2.5748227, 0.37863073, 4.7710147]\n",
      "426: Train Loss: [2.3496535, 0.34934932, 4.3499575] | Test Loss: [2.20028, 0.46725765, 3.9333024]\n",
      "427: Train Loss: [2.5089483, 0.3744161, 4.6434803] | Test Loss: [2.5687335, 0.41530603, 4.722161]\n",
      "428: Train Loss: [2.4141445, 0.36318654, 4.4651027] | Test Loss: [2.7933135, 0.37707248, 5.2095547]\n",
      "429: Train Loss: [2.2920897, 0.3307281, 4.2534513] | Test Loss: [2.696049, 0.34350118, 5.048597]\n",
      "430: Train Loss: [2.495489, 0.37528622, 4.6156917] | Test Loss: [2.7853796, 0.34009418, 5.230665]\n",
      "431: Train Loss: [2.5697505, 0.35318553, 4.7863154] | Test Loss: [2.7249956, 0.4328775, 5.0171137]\n",
      "432: Train Loss: [2.3436203, 0.34020063, 4.34704] | Test Loss: [2.7051053, 0.30556044, 5.10465]\n",
      "433: Train Loss: [2.4069474, 0.3550064, 4.4588885] | Test Loss: [2.6656075, 0.39475128, 4.936464]\n",
      "434: Train Loss: [2.347366, 0.35864675, 4.3360853] | Test Loss: [2.8009934, 0.3719951, 5.229992]\n",
      "435: Train Loss: [2.4364963, 0.33272022, 4.540272] | Test Loss: [2.8480399, 0.47904575, 5.217034]\n",
      "436: Train Loss: [2.5473526, 0.4083291, 4.686376] | Test Loss: [2.6098337, 0.37151802, 4.8481493]\n",
      "437: Train Loss: [2.5750227, 0.40837657, 4.7416687] | Test Loss: [2.49303, 0.40325952, 4.5828004]\n",
      "438: Train Loss: [2.5212147, 0.33814192, 4.7042875] | Test Loss: [2.757527, 0.33660418, 5.17845]\n",
      "439: Train Loss: [2.275284, 0.34752694, 4.203041] | Test Loss: [2.6207662, 0.40049943, 4.841033]\n",
      "440: Train Loss: [2.5599911, 0.36090925, 4.759073] | Test Loss: [2.7794635, 0.3568394, 5.202088]\n",
      "441: Train Loss: [2.336221, 0.35745463, 4.314987] | Test Loss: [2.5606887, 0.34322873, 4.7781487]\n",
      "442: Train Loss: [2.3080044, 0.340666, 4.275343] | Test Loss: [2.4035816, 0.34405863, 4.4631047]\n",
      "443: Train Loss: [2.6084843, 0.44822872, 4.7687397] | Test Loss: [2.4427938, 0.34088913, 4.5446987]\n",
      "444: Train Loss: [2.4567266, 0.3666182, 4.546835] | Test Loss: [2.6722898, 0.34653327, 4.9980464]\n",
      "445: Train Loss: [2.5550745, 0.34640804, 4.763741] | Test Loss: [2.628715, 0.3860719, 4.8713584]\n",
      "446: Train Loss: [2.5845366, 0.37128365, 4.7977896] | Test Loss: [2.7156262, 0.36593, 5.0653224]\n",
      "447: Train Loss: [2.4170997, 0.30532175, 4.5288777] | Test Loss: [2.6182737, 0.34744114, 4.8891063]\n",
      "448: Train Loss: [2.528596, 0.38660887, 4.670583] | Test Loss: [2.787439, 0.3636081, 5.2112703]\n",
      "449: Train Loss: [2.3383915, 0.26810637, 4.4086766] | Test Loss: [2.6791227, 0.34610593, 5.0121393]\n",
      "450: Train Loss: [2.4362848, 0.3585801, 4.5139894] | Test Loss: [2.6032596, 0.320244, 4.8862753]\n",
      "451: Train Loss: [2.4643054, 0.37241125, 4.5561996] | Test Loss: [2.590825, 0.37798083, 4.8036695]\n",
      "452: Train Loss: [2.33717, 0.33195508, 4.342385] | Test Loss: [2.7137895, 0.37981072, 5.047768]\n",
      "453: Train Loss: [2.5810275, 0.3696395, 4.7924156] | Test Loss: [2.626512, 0.31555155, 4.9374723]\n",
      "454: Train Loss: [2.3712294, 0.3636199, 4.378839] | Test Loss: [2.6797223, 0.39864802, 4.960797]\n",
      "455: Train Loss: [2.4520664, 0.35015377, 4.553979] | Test Loss: [2.715644, 0.38847286, 5.0428147]\n",
      "456: Train Loss: [2.502922, 0.36870253, 4.6371417] | Test Loss: [2.4518192, 0.31575888, 4.5878797]\n",
      "457: Train Loss: [2.6606054, 0.3642011, 4.95701] | Test Loss: [2.7194126, 0.36088273, 5.0779424]\n",
      "458: Train Loss: [2.4406416, 0.35705593, 4.524227] | Test Loss: [2.8167794, 0.46117577, 5.172383]\n",
      "459: Train Loss: [2.4064746, 0.38340312, 4.429546] | Test Loss: [2.6607642, 0.41417915, 4.907349]\n",
      "460: Train Loss: [2.479761, 0.36296353, 4.596558] | Test Loss: [2.5987341, 0.3741482, 4.82332]\n",
      "461: Train Loss: [2.4969268, 0.37875292, 4.615101] | Test Loss: [2.461538, 0.34763208, 4.575444]\n",
      "462: Train Loss: [2.5851016, 0.3115642, 4.858639] | Test Loss: [2.709066, 0.2868996, 5.1312323]\n",
      "463: Train Loss: [2.4145586, 0.42350855, 4.4056087] | Test Loss: [2.5514638, 0.32578233, 4.7771454]\n",
      "464: Train Loss: [2.4224505, 0.35118663, 4.4937143] | Test Loss: [2.6103516, 0.37581503, 4.844888]\n",
      "465: Train Loss: [2.526284, 0.39172176, 4.660846] | Test Loss: [2.6576047, 0.38372248, 4.931487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466: Train Loss: [2.3712103, 0.40524223, 4.337178] | Test Loss: [2.704331, 0.36814, 5.0405216]\n",
      "467: Train Loss: [2.397373, 0.38909787, 4.405648] | Test Loss: [2.700288, 0.38641843, 5.014158]\n",
      "468: Train Loss: [2.5404513, 0.35455197, 4.726351] | Test Loss: [2.5712934, 0.35446963, 4.788117]\n",
      "469: Train Loss: [2.5005534, 0.3997588, 4.601348] | Test Loss: [2.6040735, 0.37476552, 4.8333817]\n",
      "470: Train Loss: [2.5737567, 0.37575886, 4.7717547] | Test Loss: [2.6548274, 0.37718758, 4.932467]\n",
      "471: Train Loss: [2.5372632, 0.37530515, 4.699221] | Test Loss: [2.7912936, 0.2937743, 5.288813]\n",
      "472: Train Loss: [2.5766165, 0.33555752, 4.8176756] | Test Loss: [2.707062, 0.31551325, 5.098611]\n",
      "473: Train Loss: [2.4367142, 0.30454257, 4.568886] | Test Loss: [2.5275955, 0.32795107, 4.72724]\n",
      "474: Train Loss: [2.4792042, 0.37724233, 4.5811663] | Test Loss: [2.7886398, 0.32914072, 5.248139]\n",
      "475: Train Loss: [2.35591, 0.3417269, 4.3700933] | Test Loss: [2.7146418, 0.4917571, 4.9375267]\n",
      "476: Train Loss: [2.421879, 0.31580475, 4.527953] | Test Loss: [2.6750035, 0.41219774, 4.9378095]\n",
      "477: Train Loss: [2.5676055, 0.41876173, 4.7164493] | Test Loss: [2.6623354, 0.3719499, 4.9527206]\n",
      "478: Train Loss: [2.3580105, 0.30776697, 4.408254] | Test Loss: [2.8603253, 0.3295089, 5.391142]\n",
      "479: Train Loss: [2.5376523, 0.39143085, 4.6838737] | Test Loss: [2.6356006, 0.34818083, 4.9230204]\n",
      "480: Train Loss: [2.5579019, 0.38183907, 4.7339644] | Test Loss: [2.6344934, 0.36338577, 4.905601]\n",
      "481: Train Loss: [2.4871352, 0.33304927, 4.641221] | Test Loss: [2.6967087, 0.41867045, 4.9747467]\n",
      "482: Train Loss: [2.4473145, 0.38299817, 4.511631] | Test Loss: [2.6934938, 0.38319743, 5.0037904]\n",
      "483: Train Loss: [2.438942, 0.3259195, 4.5519643] | Test Loss: [2.829205, 0.40029413, 5.258116]\n",
      "484: Train Loss: [2.4740088, 0.3220201, 4.6259975] | Test Loss: [2.7043054, 0.33991992, 5.068691]\n",
      "485: Train Loss: [2.3667867, 0.34890682, 4.3846664] | Test Loss: [2.6856825, 0.3420772, 5.029288]\n",
      "486: Train Loss: [2.461586, 0.35858655, 4.564585] | Test Loss: [2.3130608, 0.61093736, 4.015184]\n",
      "487: Train Loss: [2.538321, 0.3907771, 4.685865] | Test Loss: [2.6516306, 0.41829717, 4.884964]\n",
      "488: Train Loss: [2.4000015, 0.2739287, 4.5260744] | Test Loss: [2.529813, 0.33009824, 4.729528]\n",
      "489: Train Loss: [2.6115735, 0.3568856, 4.8662615] | Test Loss: [2.6254096, 0.3748305, 4.8759885]\n",
      "490: Train Loss: [2.604991, 0.3168729, 4.893109] | Test Loss: [2.6897573, 0.3298476, 5.049667]\n",
      "491: Train Loss: [2.4568324, 0.44405305, 4.4696116] | Test Loss: [2.7994733, 0.5310114, 5.067935]\n",
      "492: Train Loss: [2.2755427, 0.31932008, 4.2317653] | Test Loss: [2.7325041, 0.44214857, 5.0228596]\n",
      "493: Train Loss: [2.4454718, 0.4058355, 4.485108] | Test Loss: [2.4895868, 0.38357583, 4.5955977]\n",
      "494: Train Loss: [2.5192008, 0.4384148, 4.5999866] | Test Loss: [2.7602875, 0.38752472, 5.1330504]\n",
      "495: Train Loss: [2.4340744, 0.35886502, 4.509284] | Test Loss: [2.6666129, 0.3901453, 4.9430804]\n",
      "496: Train Loss: [2.3838115, 0.39716348, 4.3704596] | Test Loss: [2.418019, 0.39440665, 4.4416313]\n",
      "497: Train Loss: [2.416924, 0.36732012, 4.466528] | Test Loss: [2.6128979, 0.29168063, 4.934115]\n",
      "498: Train Loss: [2.470914, 0.3499244, 4.591903] | Test Loss: [2.7360587, 0.30331925, 5.168798]\n",
      "499: Train Loss: [2.433556, 0.3166018, 4.5505104] | Test Loss: [2.729682, 0.40518796, 5.054176]\n",
      "500: Train Loss: [2.4319024, 0.32995826, 4.5338464] | Test Loss: [2.724951, 0.41597816, 5.033924]\n",
      "501: Train Loss: [2.5478418, 0.3736346, 4.7220488] | Test Loss: [2.7402847, 0.3811761, 5.0993934]\n",
      "502: Train Loss: [2.4809809, 0.3751233, 4.5868382] | Test Loss: [2.824727, 0.5013899, 5.148064]\n",
      "503: Train Loss: [2.4852076, 0.35365888, 4.6167564] | Test Loss: [2.6383014, 0.363527, 4.913076]\n",
      "504: Train Loss: [2.410474, 0.55970794, 4.26124] | Test Loss: [2.6555657, 0.33966377, 4.9714675]\n",
      "505: Train Loss: [2.441475, 0.39479005, 4.4881597] | Test Loss: [2.6741867, 0.34170204, 5.0066714]\n",
      "506: Train Loss: [2.590656, 0.34860918, 4.832703] | Test Loss: [2.5435941, 0.36864603, 4.718542]\n",
      "507: Train Loss: [2.5849292, 0.37101296, 4.7988453] | Test Loss: [2.7877295, 0.33861437, 5.2368445]\n",
      "508: Train Loss: [2.401268, 0.3601686, 4.4423676] | Test Loss: [2.6362934, 0.35611606, 4.9164705]\n",
      "509: Train Loss: [2.4628391, 0.3579259, 4.5677524] | Test Loss: [2.7606528, 0.372372, 5.1489334]\n",
      "510: Train Loss: [2.4590786, 0.36799344, 4.5501637] | Test Loss: [2.587193, 0.3559646, 4.8184214]\n",
      "511: Train Loss: [2.3517537, 0.37431496, 4.3291926] | Test Loss: [2.7324016, 0.34783715, 5.1169662]\n",
      "512: Train Loss: [2.3966694, 0.32374352, 4.4695954] | Test Loss: [2.7160616, 0.388524, 5.043599]\n",
      "513: Train Loss: [2.3973339, 0.32155195, 4.473116] | Test Loss: [2.6791193, 0.34370142, 5.0145373]\n",
      "514: Train Loss: [2.6935053, 0.46510792, 4.9219027] | Test Loss: [2.6414747, 0.40826312, 4.8746862]\n",
      "515: Train Loss: [2.4072053, 0.35463127, 4.4597793] | Test Loss: [2.586128, 0.36518186, 4.807074]\n",
      "516: Train Loss: [2.4710543, 0.31660512, 4.6255035] | Test Loss: [2.8785253, 0.49519366, 5.261857]\n",
      "517: Train Loss: [2.6346564, 0.4587804, 4.8105326] | Test Loss: [2.7529907, 0.34885487, 5.1571264]\n",
      "518: Train Loss: [2.4122148, 0.32723898, 4.4971905] | Test Loss: [2.768441, 0.36585099, 5.171031]\n",
      "519: Train Loss: [2.6594138, 0.36237055, 4.956457] | Test Loss: [2.7207747, 0.3385406, 5.1030087]\n",
      "520: Train Loss: [2.6401634, 0.37261683, 4.90771] | Test Loss: [2.4628844, 0.33542815, 4.5903406]\n",
      "521: Train Loss: [2.5060678, 0.38160855, 4.630527] | Test Loss: [2.6190822, 0.3460068, 4.8921576]\n",
      "522: Train Loss: [2.3606153, 0.4135189, 4.3077116] | Test Loss: [2.789969, 0.3028444, 5.2770934]\n",
      "523: Train Loss: [2.5441496, 0.3526034, 4.735696] | Test Loss: [2.6466467, 0.34801495, 4.9452786]\n",
      "Epoch 18\n",
      "0: Train Loss: [2.433116, 0.34711576, 4.5191164] | Test Loss: [2.8010209, 0.34365284, 5.258389]\n",
      "1: Train Loss: [2.2314749, 0.34312367, 4.119826] | Test Loss: [2.7802782, 0.40336984, 5.1571865]\n",
      "2: Train Loss: [2.288942, 0.34193468, 4.2359495] | Test Loss: [2.7144947, 0.3812308, 5.0477586]\n",
      "3: Train Loss: [2.431612, 0.31639227, 4.5468316] | Test Loss: [2.7312386, 0.35857078, 5.1039066]\n",
      "4: Train Loss: [2.3194742, 0.40003037, 4.238918] | Test Loss: [2.7019484, 0.39449462, 5.0094023]\n",
      "5: Train Loss: [2.2043335, 0.33139676, 4.0772705] | Test Loss: [2.661258, 0.45584714, 4.8666687]\n",
      "6: Train Loss: [2.4014912, 0.31435478, 4.4886274] | Test Loss: [2.4276276, 0.3139751, 4.5412803]\n",
      "7: Train Loss: [2.312937, 0.39359495, 4.2322793] | Test Loss: [2.678407, 0.3527352, 5.004079]\n",
      "8: Train Loss: [2.2891924, 0.32541993, 4.252965] | Test Loss: [2.7086897, 0.3761979, 5.0411816]\n",
      "9: Train Loss: [2.3554666, 0.35373396, 4.357199] | Test Loss: [2.6408322, 0.43057647, 4.851088]\n",
      "10: Train Loss: [2.2723913, 0.2861145, 4.258668] | Test Loss: [2.6964593, 0.41328454, 4.979634]\n",
      "11: Train Loss: [2.5262494, 0.3827229, 4.669776] | Test Loss: [2.6667368, 0.39130333, 4.94217]\n",
      "12: Train Loss: [2.4271193, 0.42425838, 4.4299803] | Test Loss: [2.7603452, 0.4730597, 5.047631]\n",
      "13: Train Loss: [2.352724, 0.39268818, 4.31276] | Test Loss: [2.6992114, 0.38190126, 5.0165215]\n",
      "14: Train Loss: [2.3270235, 0.28942007, 4.364627] | Test Loss: [2.567686, 0.35227233, 4.7830997]\n",
      "15: Train Loss: [2.2954614, 0.37104163, 4.219881] | Test Loss: [2.586593, 0.36377004, 4.809416]\n",
      "16: Train Loss: [2.351234, 0.28772402, 4.414744] | Test Loss: [2.5881658, 0.36785108, 4.8084803]\n",
      "17: Train Loss: [2.2975636, 0.4001705, 4.194957] | Test Loss: [2.6254878, 0.38619214, 4.8647833]\n",
      "18: Train Loss: [2.4983714, 0.45333773, 4.543405] | Test Loss: [2.6580582, 0.32993048, 4.986186]\n",
      "19: Train Loss: [2.4082606, 0.3910137, 4.4255075] | Test Loss: [2.7281425, 0.41830158, 5.0379834]\n",
      "20: Train Loss: [2.2186425, 0.40711612, 4.030169] | Test Loss: [2.572714, 0.32666627, 4.818762]\n",
      "21: Train Loss: [2.2075005, 0.32192314, 4.0930777] | Test Loss: [2.4906762, 0.4020229, 4.5793295]\n",
      "22: Train Loss: [2.3150668, 0.384189, 4.2459445] | Test Loss: [2.709719, 0.39120224, 5.0282354]\n",
      "23: Train Loss: [2.330058, 0.35257137, 4.3075447] | Test Loss: [2.5630093, 0.406188, 4.7198305]\n",
      "24: Train Loss: [2.467115, 0.39258924, 4.5416408] | Test Loss: [2.6366134, 0.33741793, 4.9358087]\n",
      "25: Train Loss: [2.4931622, 0.33697742, 4.649347] | Test Loss: [2.659341, 0.31437296, 5.004309]\n",
      "26: Train Loss: [2.4115427, 0.39334095, 4.4297442] | Test Loss: [2.4991517, 0.3680129, 4.6302905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27: Train Loss: [2.4835176, 0.36062294, 4.6064124] | Test Loss: [2.8329782, 0.40779766, 5.2581587]\n",
      "28: Train Loss: [2.2831295, 0.34632123, 4.219938] | Test Loss: [2.6867893, 0.45070785, 4.9228706]\n",
      "29: Train Loss: [2.2667909, 0.30947518, 4.224107] | Test Loss: [2.6115613, 0.32262394, 4.900499]\n",
      "30: Train Loss: [2.4053445, 0.33775976, 4.472929] | Test Loss: [2.8918803, 0.44091117, 5.3428493]\n",
      "31: Train Loss: [2.283985, 0.36579546, 4.202174] | Test Loss: [2.695882, 0.3801829, 5.0115814]\n",
      "32: Train Loss: [2.3564816, 0.37635744, 4.3366055] | Test Loss: [2.6494303, 0.3103788, 4.9884815]\n",
      "33: Train Loss: [2.3724394, 0.3811895, 4.3636894] | Test Loss: [2.5860004, 0.30874538, 4.8632555]\n",
      "34: Train Loss: [2.4729083, 0.46762982, 4.4781866] | Test Loss: [2.7629135, 0.45541257, 5.0704145]\n",
      "35: Train Loss: [2.342636, 0.46133816, 4.223934] | Test Loss: [2.5137148, 0.38664782, 4.640782]\n",
      "36: Train Loss: [2.311327, 0.41239128, 4.210263] | Test Loss: [2.5401313, 0.40939105, 4.6708717]\n",
      "37: Train Loss: [2.3311872, 0.36252806, 4.2998466] | Test Loss: [2.6652434, 0.36245048, 4.968036]\n",
      "38: Train Loss: [2.3167489, 0.37038216, 4.2631154] | Test Loss: [2.879187, 0.35811257, 5.4002614]\n",
      "39: Train Loss: [2.532752, 0.36700225, 4.698502] | Test Loss: [2.5653944, 0.44426638, 4.6865225]\n",
      "40: Train Loss: [2.3522387, 0.33429092, 4.3701863] | Test Loss: [2.6570675, 0.36142966, 4.9527054]\n",
      "41: Train Loss: [2.3300142, 0.42357054, 4.236458] | Test Loss: [2.5765145, 0.39059356, 4.7624354]\n",
      "42: Train Loss: [2.2129667, 0.3577531, 4.06818] | Test Loss: [2.5912821, 0.3337323, 4.848832]\n",
      "43: Train Loss: [2.262584, 0.32849228, 4.196676] | Test Loss: [2.735416, 0.35001746, 5.1208143]\n",
      "44: Train Loss: [2.3021264, 0.40372053, 4.2005324] | Test Loss: [2.8136418, 0.4361544, 5.191129]\n",
      "45: Train Loss: [2.3062768, 0.31310302, 4.2994504] | Test Loss: [2.7334719, 0.35970166, 5.107242]\n",
      "46: Train Loss: [2.358221, 0.31636745, 4.4000745] | Test Loss: [2.8265355, 0.33211848, 5.3209524]\n",
      "47: Train Loss: [2.1283045, 0.29718477, 3.959424] | Test Loss: [2.6219988, 0.3290152, 4.9149823]\n",
      "48: Train Loss: [2.295826, 0.33601934, 4.2556324] | Test Loss: [2.6608875, 0.34198436, 4.9797907]\n",
      "49: Train Loss: [2.292284, 0.39966607, 4.184902] | Test Loss: [2.5640466, 0.35672396, 4.7713695]\n",
      "50: Train Loss: [2.3455665, 0.36602214, 4.325111] | Test Loss: [2.7186477, 0.41716224, 5.020133]\n",
      "51: Train Loss: [2.2360244, 0.31536233, 4.1566863] | Test Loss: [2.5957801, 0.36154997, 4.8300104]\n",
      "52: Train Loss: [2.40682, 0.49850315, 4.315137] | Test Loss: [2.6672852, 0.4039499, 4.9306207]\n",
      "53: Train Loss: [2.2641838, 0.3551298, 4.173238] | Test Loss: [2.6772952, 0.34993026, 5.00466]\n",
      "54: Train Loss: [2.4583979, 0.32020175, 4.596594] | Test Loss: [2.303884, 0.30765787, 4.3001103]\n",
      "55: Train Loss: [2.5316, 0.43452826, 4.6286716] | Test Loss: [2.8354008, 0.4189256, 5.251876]\n",
      "56: Train Loss: [2.3911932, 0.44195634, 4.34043] | Test Loss: [2.6640809, 0.38814166, 4.94002]\n",
      "57: Train Loss: [2.4084342, 0.3702286, 4.4466395] | Test Loss: [2.849666, 0.42187086, 5.2774615]\n",
      "58: Train Loss: [2.293915, 0.30251724, 4.2853127] | Test Loss: [2.6246872, 0.3409842, 4.90839]\n",
      "59: Train Loss: [2.49846, 0.34318176, 4.6537385] | Test Loss: [2.6167066, 0.3772163, 4.856197]\n",
      "60: Train Loss: [2.3633015, 0.35536376, 4.371239] | Test Loss: [2.7602792, 0.3541093, 5.166449]\n",
      "61: Train Loss: [2.274889, 0.34546024, 4.2043176] | Test Loss: [2.681059, 0.29411468, 5.068003]\n",
      "62: Train Loss: [2.139031, 0.31651, 3.9615517] | Test Loss: [2.5313501, 0.39453873, 4.6681614]\n",
      "63: Train Loss: [2.2444775, 0.38244903, 4.106506] | Test Loss: [2.751193, 0.40228006, 5.1001062]\n",
      "64: Train Loss: [2.2229862, 0.36395282, 4.08202] | Test Loss: [2.7281914, 0.36260244, 5.0937805]\n",
      "65: Train Loss: [2.5108137, 0.4284088, 4.593219] | Test Loss: [2.6977997, 0.336817, 5.0587826]\n",
      "66: Train Loss: [2.3838837, 0.41659257, 4.351175] | Test Loss: [2.8731637, 0.3663055, 5.380022]\n",
      "67: Train Loss: [2.345419, 0.40971884, 4.281119] | Test Loss: [2.7637687, 0.496952, 5.0305853]\n",
      "68: Train Loss: [2.2698727, 0.3592175, 4.1805277] | Test Loss: [2.5737827, 0.30253598, 4.8450294]\n",
      "69: Train Loss: [2.4399316, 0.39004734, 4.4898157] | Test Loss: [2.7049937, 0.37815225, 5.031835]\n",
      "70: Train Loss: [2.3738823, 0.34655935, 4.401205] | Test Loss: [2.599356, 0.3399068, 4.858805]\n",
      "71: Train Loss: [2.3498008, 0.34607553, 4.353526] | Test Loss: [2.6905704, 0.34921533, 5.031925]\n",
      "72: Train Loss: [2.4678278, 0.3891173, 4.5465384] | Test Loss: [2.8241296, 0.4165665, 5.231693]\n",
      "73: Train Loss: [2.3513908, 0.29076076, 4.4120207] | Test Loss: [2.938213, 0.37758508, 5.4988413]\n",
      "74: Train Loss: [2.2380714, 0.42541522, 4.050728] | Test Loss: [2.6179757, 0.3629122, 4.8730392]\n",
      "75: Train Loss: [2.3903487, 0.32750928, 4.453188] | Test Loss: [2.5500324, 0.38469878, 4.715366]\n",
      "76: Train Loss: [2.1976628, 0.34533083, 4.049995] | Test Loss: [2.7486148, 0.5047109, 4.9925184]\n",
      "77: Train Loss: [2.4742055, 0.4096199, 4.538791] | Test Loss: [2.529694, 0.34234783, 4.7170405]\n",
      "78: Train Loss: [2.3578012, 0.36240003, 4.3532023] | Test Loss: [2.6433496, 0.32544068, 4.9612584]\n",
      "79: Train Loss: [2.2941182, 0.4489338, 4.1393027] | Test Loss: [2.452978, 0.3585423, 4.5474133]\n",
      "80: Train Loss: [2.2932038, 0.30170402, 4.2847037] | Test Loss: [2.6618865, 0.33858237, 4.9851904]\n",
      "81: Train Loss: [2.504546, 0.46170968, 4.5473824] | Test Loss: [2.504084, 0.42123008, 4.5869384]\n",
      "82: Train Loss: [2.2234154, 0.37688416, 4.069947] | Test Loss: [2.6280124, 0.37175387, 4.884271]\n",
      "83: Train Loss: [2.416529, 0.39562052, 4.4374375] | Test Loss: [2.573341, 0.3469431, 4.799739]\n",
      "84: Train Loss: [2.516541, 0.40315783, 4.6299243] | Test Loss: [2.7345858, 0.38815033, 5.0810213]\n",
      "85: Train Loss: [2.3488896, 0.39180034, 4.305979] | Test Loss: [2.6514604, 0.351394, 4.9515266]\n",
      "86: Train Loss: [2.3142745, 0.4198318, 4.2087173] | Test Loss: [2.6734214, 0.38560376, 4.961239]\n",
      "87: Train Loss: [2.3533232, 0.3460311, 4.3606153] | Test Loss: [2.8403013, 0.31635413, 5.3642483]\n",
      "88: Train Loss: [2.109457, 0.27340722, 3.9455066] | Test Loss: [2.7638502, 0.41403663, 5.1136637]\n",
      "89: Train Loss: [2.4936152, 0.34389904, 4.643331] | Test Loss: [2.5564644, 0.3971427, 4.715786]\n",
      "90: Train Loss: [2.3920598, 0.29538342, 4.488736] | Test Loss: [2.5886936, 0.43228945, 4.7450976]\n",
      "91: Train Loss: [2.3716917, 0.3441478, 4.3992357] | Test Loss: [2.4498606, 0.3365071, 4.563214]\n",
      "92: Train Loss: [2.4357188, 0.37000772, 4.50143] | Test Loss: [2.5555594, 0.32205585, 4.789063]\n",
      "93: Train Loss: [2.3355534, 0.58056414, 4.090543] | Test Loss: [2.73748, 0.3534204, 5.1215396]\n",
      "94: Train Loss: [2.3367946, 0.33880958, 4.3347797] | Test Loss: [2.6193273, 0.4047143, 4.8339405]\n",
      "95: Train Loss: [2.4261055, 0.3523631, 4.499848] | Test Loss: [2.7608361, 0.3781674, 5.143505]\n",
      "96: Train Loss: [2.4002626, 0.32775256, 4.4727726] | Test Loss: [2.9145806, 0.39826107, 5.4309]\n",
      "97: Train Loss: [2.3234582, 0.38786358, 4.2590528] | Test Loss: [2.669347, 0.3407218, 4.9979725]\n",
      "98: Train Loss: [2.3494253, 0.36731368, 4.331537] | Test Loss: [2.4955034, 0.340899, 4.650108]\n",
      "99: Train Loss: [2.2691107, 0.37275422, 4.1654673] | Test Loss: [2.4532714, 0.35627756, 4.5502653]\n",
      "100: Train Loss: [2.282793, 0.40569627, 4.1598897] | Test Loss: [2.6848464, 0.38072056, 4.988972]\n",
      "101: Train Loss: [2.4111726, 0.37157917, 4.450766] | Test Loss: [2.6192183, 0.41983667, 4.8186]\n",
      "102: Train Loss: [2.3143482, 0.30700475, 4.3216915] | Test Loss: [2.6279774, 0.37883848, 4.877116]\n",
      "103: Train Loss: [2.3771443, 0.35182738, 4.4024615] | Test Loss: [2.659089, 0.3655376, 4.9526405]\n",
      "104: Train Loss: [2.3878102, 0.3102743, 4.4653463] | Test Loss: [2.6148, 0.41719133, 4.8124084]\n",
      "105: Train Loss: [2.362545, 0.4197918, 4.3052983] | Test Loss: [2.790935, 0.33029163, 5.2515783]\n",
      "106: Train Loss: [2.3992786, 0.37270617, 4.425851] | Test Loss: [2.5736847, 0.4313109, 4.7160583]\n",
      "107: Train Loss: [2.2919931, 0.3553473, 4.228639] | Test Loss: [2.670567, 0.36411557, 4.9770184]\n",
      "108: Train Loss: [2.3735075, 0.3512018, 4.395813] | Test Loss: [2.899722, 0.36150765, 5.437937]\n",
      "109: Train Loss: [2.4006205, 0.36467323, 4.436568] | Test Loss: [2.841421, 0.46160138, 5.2212405]\n",
      "110: Train Loss: [2.2788384, 0.3627434, 4.1949334] | Test Loss: [2.5609355, 0.3364638, 4.785407]\n",
      "111: Train Loss: [2.38811, 0.3602556, 4.415964] | Test Loss: [2.6288152, 0.3886893, 4.868941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112: Train Loss: [2.393003, 0.3688324, 4.4171734] | Test Loss: [2.8354173, 0.27569798, 5.3951364]\n",
      "113: Train Loss: [2.500583, 0.42117292, 4.579993] | Test Loss: [2.7122626, 0.32028076, 5.1042447]\n",
      "114: Train Loss: [2.4018726, 0.36593977, 4.4378057] | Test Loss: [2.8828266, 0.30762148, 5.4580317]\n",
      "115: Train Loss: [2.4606106, 0.34299752, 4.5782237] | Test Loss: [2.5401304, 0.3040094, 4.7762513]\n",
      "116: Train Loss: [2.3604164, 0.39222574, 4.328607] | Test Loss: [2.5902596, 0.39100006, 4.789519]\n",
      "117: Train Loss: [2.4419253, 0.3325339, 4.5513167] | Test Loss: [2.7734203, 0.4391608, 5.10768]\n",
      "118: Train Loss: [2.4364996, 0.37485594, 4.498143] | Test Loss: [2.9328322, 0.53101933, 5.3346453]\n",
      "119: Train Loss: [2.3774867, 0.44766638, 4.3073072] | Test Loss: [2.6708465, 0.34079856, 5.0008945]\n",
      "120: Train Loss: [2.3905876, 0.34392002, 4.437255] | Test Loss: [2.778634, 0.35670903, 5.200559]\n",
      "121: Train Loss: [2.4315588, 0.34657136, 4.5165462] | Test Loss: [2.694585, 0.42155588, 4.967614]\n",
      "122: Train Loss: [2.3236437, 0.34080425, 4.3064833] | Test Loss: [2.5137663, 0.36380216, 4.6637306]\n",
      "123: Train Loss: [2.4054854, 0.34357798, 4.467393] | Test Loss: [2.7076364, 0.30351365, 5.111759]\n",
      "124: Train Loss: [2.2187083, 0.34383094, 4.0935855] | Test Loss: [2.67412, 0.39167, 4.95657]\n",
      "125: Train Loss: [2.1915793, 0.34058332, 4.0425754] | Test Loss: [2.6659431, 0.3569224, 4.974964]\n",
      "126: Train Loss: [2.3466005, 0.33307794, 4.360123] | Test Loss: [2.538635, 0.28886077, 4.788409]\n",
      "127: Train Loss: [2.412587, 0.43415537, 4.3910184] | Test Loss: [2.5622118, 0.42415413, 4.700269]\n",
      "128: Train Loss: [2.318233, 0.30902982, 4.327436] | Test Loss: [2.6476364, 0.3912226, 4.9040504]\n",
      "129: Train Loss: [2.3221278, 0.305897, 4.3383584] | Test Loss: [2.6790242, 0.35549682, 5.0025516]\n",
      "130: Train Loss: [2.4328575, 0.3152722, 4.5504427] | Test Loss: [2.6030893, 0.3689407, 4.837238]\n",
      "131: Train Loss: [2.2418895, 0.3628799, 4.120899] | Test Loss: [2.833943, 0.34318635, 5.3246994]\n",
      "132: Train Loss: [2.475593, 0.32023677, 4.6309495] | Test Loss: [2.622372, 0.3373014, 4.9074426]\n",
      "133: Train Loss: [2.470611, 0.33285975, 4.6083627] | Test Loss: [2.698136, 0.39205045, 5.004222]\n",
      "134: Train Loss: [2.2988493, 0.38618073, 4.211518] | Test Loss: [2.465854, 0.34565488, 4.586053]\n",
      "135: Train Loss: [2.448372, 0.3537099, 4.543034] | Test Loss: [2.675128, 0.37609485, 4.974161]\n",
      "136: Train Loss: [2.1846163, 0.3595379, 4.0096946] | Test Loss: [2.5940433, 0.42172605, 4.7663603]\n",
      "137: Train Loss: [2.351922, 0.35985702, 4.343987] | Test Loss: [2.543932, 0.3587102, 4.7291536]\n",
      "138: Train Loss: [2.2757387, 0.39169148, 4.1597857] | Test Loss: [2.5694363, 0.36826572, 4.770607]\n",
      "139: Train Loss: [2.247975, 0.32815295, 4.167797] | Test Loss: [2.7734156, 0.3380826, 5.2087483]\n",
      "140: Train Loss: [2.4241273, 0.33078614, 4.5174685] | Test Loss: [2.6432207, 0.41386014, 4.872581]\n",
      "141: Train Loss: [2.3668163, 0.43377167, 4.299861] | Test Loss: [2.5916784, 0.38029164, 4.8030653]\n",
      "142: Train Loss: [2.3677595, 0.3482875, 4.3872313] | Test Loss: [2.7330577, 0.2739148, 5.1922007]\n",
      "143: Train Loss: [2.370377, 0.3782627, 4.3624916] | Test Loss: [2.4839714, 0.31309283, 4.65485]\n",
      "144: Train Loss: [2.3775368, 0.37810355, 4.37697] | Test Loss: [2.6981542, 0.32765275, 5.0686555]\n",
      "145: Train Loss: [2.270601, 0.2819043, 4.259298] | Test Loss: [2.510319, 0.36465055, 4.6559873]\n",
      "146: Train Loss: [2.1986277, 0.35362476, 4.0436306] | Test Loss: [2.590669, 0.32738003, 4.8539577]\n",
      "147: Train Loss: [2.3300028, 0.3752439, 4.284762] | Test Loss: [2.763688, 0.3438893, 5.183487]\n",
      "148: Train Loss: [2.5409734, 0.38671967, 4.695227] | Test Loss: [2.6671119, 0.34896088, 4.985263]\n",
      "149: Train Loss: [2.4819787, 0.3221789, 4.6417785] | Test Loss: [2.7727778, 0.34436458, 5.201191]\n",
      "150: Train Loss: [2.4267807, 0.35093006, 4.502631] | Test Loss: [2.4368422, 0.42850316, 4.4451814]\n",
      "151: Train Loss: [2.417933, 0.34585908, 4.490007] | Test Loss: [2.3264523, 0.36916605, 4.2837386]\n",
      "152: Train Loss: [2.4232812, 0.33828345, 4.508279] | Test Loss: [2.700038, 0.38104448, 5.0190315]\n",
      "153: Train Loss: [2.254321, 0.37045372, 4.1381884] | Test Loss: [2.5846198, 0.38868842, 4.780551]\n",
      "154: Train Loss: [2.2504268, 0.34746924, 4.153384] | Test Loss: [2.5932438, 0.37423414, 4.8122535]\n",
      "155: Train Loss: [2.2596056, 0.34599316, 4.1732183] | Test Loss: [2.6507986, 0.3984366, 4.9031606]\n",
      "156: Train Loss: [2.413118, 0.34483504, 4.4814005] | Test Loss: [2.7867231, 0.42824933, 5.145197]\n",
      "157: Train Loss: [2.3628128, 0.32312143, 4.402504] | Test Loss: [2.806378, 0.43543428, 5.1773214]\n",
      "158: Train Loss: [2.3536973, 0.36210176, 4.345293] | Test Loss: [2.6169667, 0.33659822, 4.897335]\n",
      "159: Train Loss: [2.326831, 0.3962907, 4.2573714] | Test Loss: [2.7708125, 0.30415848, 5.2374663]\n",
      "160: Train Loss: [2.3135035, 0.3632856, 4.2637215] | Test Loss: [2.5263724, 0.35799786, 4.694747]\n",
      "161: Train Loss: [2.383076, 0.35204446, 4.4141073] | Test Loss: [2.786322, 0.3614386, 5.2112055]\n",
      "162: Train Loss: [2.3784206, 0.32371837, 4.4331226] | Test Loss: [2.6510181, 0.37079477, 4.9312415]\n",
      "163: Train Loss: [2.37348, 0.35508993, 4.39187] | Test Loss: [2.733729, 0.36671036, 5.1007476]\n",
      "164: Train Loss: [2.4124603, 0.3421145, 4.482806] | Test Loss: [2.620298, 0.43038484, 4.810211]\n",
      "165: Train Loss: [2.4609365, 0.5392511, 4.382622] | Test Loss: [2.7746496, 0.4080718, 5.1412272]\n",
      "166: Train Loss: [2.3866465, 0.35699853, 4.4162946] | Test Loss: [2.8446023, 0.35772365, 5.331481]\n",
      "167: Train Loss: [2.2482462, 0.38292974, 4.1135626] | Test Loss: [2.7203617, 0.42191336, 5.0188103]\n",
      "168: Train Loss: [2.4072518, 0.35141778, 4.4630857] | Test Loss: [2.4867444, 0.36143506, 4.612054]\n",
      "169: Train Loss: [2.5056517, 0.34164065, 4.669663] | Test Loss: [2.715426, 0.35919636, 5.0716558]\n",
      "170: Train Loss: [2.2701056, 0.32522187, 4.214989] | Test Loss: [2.5595634, 0.31474194, 4.8043847]\n",
      "171: Train Loss: [2.4390943, 0.32564357, 4.552545] | Test Loss: [2.720855, 0.42261583, 5.019094]\n",
      "172: Train Loss: [2.454547, 0.5451824, 4.3639116] | Test Loss: [2.9236128, 0.30476207, 5.542464]\n",
      "173: Train Loss: [2.3118863, 0.34375057, 4.280022] | Test Loss: [2.824672, 0.44214875, 5.2071953]\n",
      "174: Train Loss: [2.2937937, 0.2919063, 4.295681] | Test Loss: [2.6663048, 0.35654673, 4.976063]\n",
      "175: Train Loss: [2.3269653, 0.35985586, 4.294075] | Test Loss: [2.749758, 0.32152495, 5.177991]\n",
      "176: Train Loss: [2.4347115, 0.37743068, 4.491992] | Test Loss: [2.4936085, 0.36046454, 4.6267524]\n",
      "177: Train Loss: [2.320992, 0.37188873, 4.2700953] | Test Loss: [2.7203767, 0.2913461, 5.1494074]\n",
      "178: Train Loss: [2.204765, 0.33265093, 4.076879] | Test Loss: [2.6311276, 0.3494669, 4.9127884]\n",
      "179: Train Loss: [2.341011, 0.36547643, 4.3165455] | Test Loss: [2.6470785, 0.36044037, 4.933717]\n",
      "180: Train Loss: [2.2175295, 0.32133788, 4.1137214] | Test Loss: [2.7572045, 0.39593613, 5.118473]\n",
      "181: Train Loss: [2.374396, 0.33465007, 4.414142] | Test Loss: [2.578249, 0.30943274, 4.8470654]\n",
      "182: Train Loss: [2.2923589, 0.3282789, 4.2564387] | Test Loss: [2.5359707, 0.35860324, 4.7133384]\n",
      "183: Train Loss: [2.3986065, 0.35067374, 4.4465394] | Test Loss: [2.6162524, 0.3507527, 4.881752]\n",
      "184: Train Loss: [2.4147382, 0.31894392, 4.5105324] | Test Loss: [2.614658, 0.32082713, 4.908489]\n",
      "185: Train Loss: [2.389644, 0.33344322, 4.4458447] | Test Loss: [2.5970945, 0.40389946, 4.7902894]\n",
      "186: Train Loss: [2.3985739, 0.36847988, 4.428668] | Test Loss: [2.7201486, 0.37728202, 5.063015]\n",
      "187: Train Loss: [2.3340082, 0.35840893, 4.3096075] | Test Loss: [2.6628807, 0.39893577, 4.9268255]\n",
      "188: Train Loss: [2.4419494, 0.40632606, 4.477573] | Test Loss: [2.5895863, 0.39320046, 4.785972]\n",
      "189: Train Loss: [2.4842236, 0.36585143, 4.602596] | Test Loss: [2.6383533, 0.3184907, 4.958216]\n",
      "190: Train Loss: [2.357208, 0.39251623, 4.3219] | Test Loss: [2.5305016, 0.37160873, 4.6893945]\n",
      "191: Train Loss: [2.3051045, 0.36890632, 4.2413025] | Test Loss: [2.6806817, 0.41104952, 4.950314]\n",
      "192: Train Loss: [2.3038354, 0.3087766, 4.2988944] | Test Loss: [2.7710705, 0.4012291, 5.140912]\n",
      "193: Train Loss: [2.4099662, 0.3796688, 4.4402637] | Test Loss: [2.6463468, 0.33160162, 4.961092]\n",
      "194: Train Loss: [2.3304806, 0.364396, 4.296565] | Test Loss: [2.7434852, 0.41435307, 5.0726175]\n",
      "195: Train Loss: [2.3271556, 0.35436982, 4.2999415] | Test Loss: [2.6588953, 0.3880771, 4.9297132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196: Train Loss: [2.322933, 0.3420881, 4.3037777] | Test Loss: [2.6880794, 0.32657, 5.0495887]\n",
      "197: Train Loss: [2.385151, 0.44736847, 4.322933] | Test Loss: [2.4943125, 0.4554404, 4.5331845]\n",
      "198: Train Loss: [2.4650195, 0.36696765, 4.5630713] | Test Loss: [2.6664784, 0.3841407, 4.9488163]\n",
      "199: Train Loss: [2.3587434, 0.3237157, 4.393771] | Test Loss: [2.7288675, 0.43641186, 5.021323]\n",
      "200: Train Loss: [2.3224165, 0.33949623, 4.305337] | Test Loss: [2.658248, 0.36347082, 4.953025]\n",
      "201: Train Loss: [2.3041415, 0.3533546, 4.2549286] | Test Loss: [2.6925983, 0.41442126, 4.9707756]\n",
      "202: Train Loss: [2.3122852, 0.37473643, 4.249834] | Test Loss: [2.4316633, 0.46516666, 4.39816]\n",
      "203: Train Loss: [2.4058955, 0.4049565, 4.4068346] | Test Loss: [2.6382482, 0.36441705, 4.9120793]\n",
      "204: Train Loss: [2.4489837, 0.37057617, 4.527391] | Test Loss: [2.6561558, 0.38322714, 4.9290843]\n",
      "205: Train Loss: [2.4396787, 0.39720267, 4.482155] | Test Loss: [2.952864, 0.37089327, 5.5348344]\n",
      "206: Train Loss: [2.3443768, 0.3342546, 4.354499] | Test Loss: [2.7735813, 0.4390687, 5.1080937]\n",
      "207: Train Loss: [2.4286509, 0.32931522, 4.5279865] | Test Loss: [2.5845191, 0.33790132, 4.831137]\n",
      "208: Train Loss: [2.5472689, 0.33755833, 4.7569795] | Test Loss: [2.5582752, 0.38510785, 4.7314425]\n",
      "209: Train Loss: [2.518054, 0.4229498, 4.613158] | Test Loss: [2.8163357, 0.33005497, 5.3026166]\n",
      "210: Train Loss: [2.3394408, 0.4266126, 4.2522693] | Test Loss: [2.4617572, 0.33928445, 4.58423]\n",
      "211: Train Loss: [2.445531, 0.28911737, 4.6019444] | Test Loss: [2.581528, 0.33801013, 4.8250456]\n",
      "212: Train Loss: [2.3608341, 0.3508636, 4.370805] | Test Loss: [2.682741, 0.4000315, 4.9654503]\n",
      "213: Train Loss: [2.3212717, 0.38127077, 4.2612724] | Test Loss: [2.5931506, 0.3709796, 4.8153214]\n",
      "214: Train Loss: [2.2171102, 0.36921895, 4.0650015] | Test Loss: [2.7529233, 0.36594546, 5.139901]\n",
      "215: Train Loss: [2.2221513, 0.32232338, 4.121979] | Test Loss: [2.6701665, 0.3663002, 4.974033]\n",
      "216: Train Loss: [2.3535762, 0.31969112, 4.387461] | Test Loss: [2.703711, 0.32594794, 5.0814743]\n",
      "217: Train Loss: [2.3863544, 0.32657725, 4.4461317] | Test Loss: [2.7882953, 0.45015585, 5.126435]\n",
      "218: Train Loss: [2.5117471, 0.38671806, 4.636776] | Test Loss: [2.5518582, 0.35816318, 4.745553]\n",
      "219: Train Loss: [2.4277484, 0.34737536, 4.5081215] | Test Loss: [2.7425625, 0.33308193, 5.1520433]\n",
      "220: Train Loss: [2.410748, 0.39627814, 4.4252176] | Test Loss: [2.4337919, 0.47450927, 4.3930745]\n",
      "221: Train Loss: [2.3191643, 0.3980157, 4.240313] | Test Loss: [2.7868636, 0.35334614, 5.220381]\n",
      "222: Train Loss: [2.4037232, 0.32558945, 4.481857] | Test Loss: [2.644487, 0.3547925, 4.934181]\n",
      "223: Train Loss: [2.4359903, 0.37430403, 4.497677] | Test Loss: [2.7196307, 0.41380882, 5.0254526]\n",
      "224: Train Loss: [2.444396, 0.38410264, 4.504689] | Test Loss: [2.5620744, 0.35320124, 4.7709475]\n",
      "225: Train Loss: [2.38619, 0.34908745, 4.4232926] | Test Loss: [2.6612606, 0.40170112, 4.92082]\n",
      "226: Train Loss: [2.3795593, 0.34256265, 4.416556] | Test Loss: [2.6347446, 0.35333207, 4.9161572]\n",
      "227: Train Loss: [2.3142252, 0.3576658, 4.2707844] | Test Loss: [2.592667, 0.39160156, 4.7937326]\n",
      "228: Train Loss: [2.3326857, 0.2987835, 4.366588] | Test Loss: [2.43154, 0.33875751, 4.5243225]\n",
      "229: Train Loss: [2.3151124, 0.3560908, 4.2741337] | Test Loss: [2.4968898, 0.3223593, 4.6714206]\n",
      "230: Train Loss: [2.537744, 0.3464329, 4.7290554] | Test Loss: [2.6745515, 0.34319103, 5.005912]\n",
      "231: Train Loss: [2.5130596, 0.3299659, 4.696153] | Test Loss: [2.6098447, 0.39470345, 4.824986]\n",
      "232: Train Loss: [2.3316255, 0.37817758, 4.2850733] | Test Loss: [2.8822637, 0.3064324, 5.458095]\n",
      "233: Train Loss: [2.417759, 0.3103027, 4.525215] | Test Loss: [2.5994327, 0.40945876, 4.789407]\n",
      "234: Train Loss: [2.5312178, 0.3661989, 4.6962366] | Test Loss: [2.5218937, 0.35397747, 4.68981]\n",
      "235: Train Loss: [2.3633697, 0.3937648, 4.3329744] | Test Loss: [2.6392984, 0.3237267, 4.95487]\n",
      "236: Train Loss: [2.3428392, 0.3045038, 4.3811746] | Test Loss: [2.6960628, 0.34654644, 5.045579]\n",
      "237: Train Loss: [2.3177724, 0.31570196, 4.319843] | Test Loss: [2.5673993, 0.41802174, 4.716777]\n",
      "238: Train Loss: [2.3722744, 0.31671578, 4.427833] | Test Loss: [2.700306, 0.34101823, 5.0595937]\n",
      "239: Train Loss: [2.329051, 0.36727446, 4.2908278] | Test Loss: [2.4920483, 0.30855483, 4.675542]\n",
      "240: Train Loss: [2.322842, 0.31473887, 4.330945] | Test Loss: [2.6317601, 0.34120446, 4.9223156]\n",
      "241: Train Loss: [2.3240852, 0.391878, 4.2562923] | Test Loss: [2.504803, 0.3158154, 4.6937904]\n",
      "242: Train Loss: [2.3251584, 0.29802576, 4.352291] | Test Loss: [2.6577265, 0.32144567, 4.9940076]\n",
      "243: Train Loss: [2.422334, 0.35413492, 4.490533] | Test Loss: [2.770346, 0.3664245, 5.1742673]\n",
      "244: Train Loss: [2.4215403, 0.38810834, 4.4549723] | Test Loss: [2.6865509, 0.31857455, 5.0545273]\n",
      "245: Train Loss: [2.299535, 0.40343878, 4.1956315] | Test Loss: [2.6715307, 0.38360867, 4.9594526]\n",
      "246: Train Loss: [2.5022, 0.36809874, 4.636301] | Test Loss: [2.6596394, 0.42140782, 4.897871]\n",
      "247: Train Loss: [2.4555943, 0.41595343, 4.495235] | Test Loss: [2.5732343, 0.3317436, 4.814725]\n",
      "248: Train Loss: [2.5235631, 0.33530286, 4.7118235] | Test Loss: [2.60376, 0.35263175, 4.8548884]\n",
      "249: Train Loss: [2.4138293, 0.39296937, 4.4346895] | Test Loss: [2.7457612, 0.31938177, 5.1721406]\n",
      "250: Train Loss: [2.4687867, 0.38505813, 4.5525155] | Test Loss: [2.611487, 0.43726027, 4.7857137]\n",
      "251: Train Loss: [2.3626661, 0.3757316, 4.349601] | Test Loss: [2.631967, 0.4138085, 4.850126]\n",
      "252: Train Loss: [2.3528218, 0.37120363, 4.33444] | Test Loss: [2.664654, 0.39986265, 4.9294453]\n",
      "253: Train Loss: [2.4800465, 0.35294375, 4.607149] | Test Loss: [2.6638696, 0.37506494, 4.9526744]\n",
      "254: Train Loss: [2.3864915, 0.39313212, 4.379851] | Test Loss: [2.7869246, 0.36790845, 5.2059407]\n",
      "255: Train Loss: [2.5482872, 0.3662349, 4.7303395] | Test Loss: [2.6855998, 0.38130948, 4.98989]\n",
      "256: Train Loss: [2.287085, 0.40220344, 4.1719666] | Test Loss: [2.7975326, 0.34252036, 5.252545]\n",
      "257: Train Loss: [2.3396866, 0.36904487, 4.3103285] | Test Loss: [2.54459, 0.35705176, 4.732128]\n",
      "258: Train Loss: [2.5293896, 0.4205878, 4.638191] | Test Loss: [2.6227927, 0.42570087, 4.819885]\n",
      "259: Train Loss: [2.3807373, 0.37617663, 4.385298] | Test Loss: [2.7678702, 0.39736512, 5.1383753]\n",
      "260: Train Loss: [2.4694204, 0.3370184, 4.6018224] | Test Loss: [2.6703415, 0.39263853, 4.9480443]\n",
      "261: Train Loss: [2.4027948, 0.43411732, 4.3714724] | Test Loss: [2.8401666, 0.5047694, 5.175564]\n",
      "262: Train Loss: [2.5962431, 0.35417047, 4.838316] | Test Loss: [2.5548434, 0.25553405, 4.8541527]\n",
      "263: Train Loss: [2.3969185, 0.3857484, 4.4080887] | Test Loss: [2.4254904, 0.30838758, 4.542593]\n",
      "264: Train Loss: [2.4219396, 0.34364325, 4.500236] | Test Loss: [2.6038225, 0.40876225, 4.7988825]\n",
      "265: Train Loss: [2.2426941, 0.30416647, 4.181222] | Test Loss: [2.8227594, 0.3863315, 5.259187]\n",
      "266: Train Loss: [2.4558058, 0.31459904, 4.5970125] | Test Loss: [2.6555328, 0.38495335, 4.926112]\n",
      "267: Train Loss: [2.5364084, 0.330168, 4.742649] | Test Loss: [2.5464869, 0.38427937, 4.7086945]\n",
      "268: Train Loss: [2.3218806, 0.43080634, 4.212955] | Test Loss: [2.730721, 0.35565165, 5.10579]\n",
      "269: Train Loss: [2.187333, 0.32058036, 4.0540857] | Test Loss: [2.5084715, 0.42860907, 4.588334]\n",
      "270: Train Loss: [2.290045, 0.34729597, 4.2327943] | Test Loss: [2.6909704, 0.2973087, 5.084632]\n",
      "271: Train Loss: [2.203328, 0.306931, 4.099725] | Test Loss: [2.6541002, 0.36042768, 4.9477725]\n",
      "272: Train Loss: [2.5048325, 0.423899, 4.585766] | Test Loss: [2.7369716, 0.39645657, 5.0774865]\n",
      "273: Train Loss: [2.3449628, 0.35518157, 4.334744] | Test Loss: [2.4109502, 0.40079814, 4.421102]\n",
      "274: Train Loss: [2.3664777, 0.38850462, 4.344451] | Test Loss: [2.6005456, 0.38252336, 4.8185678]\n",
      "275: Train Loss: [2.4609737, 0.39670825, 4.525239] | Test Loss: [2.5786548, 0.34209248, 4.815217]\n",
      "276: Train Loss: [2.4302697, 0.37363818, 4.4869013] | Test Loss: [2.7417095, 0.3331279, 5.150291]\n",
      "277: Train Loss: [2.3526144, 0.31202057, 4.393208] | Test Loss: [2.6433675, 0.31409484, 4.97264]\n",
      "278: Train Loss: [2.4173326, 0.3871101, 4.447555] | Test Loss: [2.6466641, 0.33000615, 4.963322]\n",
      "279: Train Loss: [2.3998713, 0.3375786, 4.462164] | Test Loss: [2.6152265, 0.31382954, 4.9166236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280: Train Loss: [2.3800666, 0.34789756, 4.4122357] | Test Loss: [2.7475703, 0.3745275, 5.120613]\n",
      "281: Train Loss: [2.3957572, 0.44917512, 4.3423395] | Test Loss: [2.7295485, 0.4295166, 5.02958]\n",
      "282: Train Loss: [2.523425, 0.3617336, 4.685117] | Test Loss: [2.8091006, 0.4615994, 5.156602]\n",
      "283: Train Loss: [2.4726152, 0.36179197, 4.5834384] | Test Loss: [2.7033951, 0.36975068, 5.0370398]\n",
      "284: Train Loss: [2.4077802, 0.36297148, 4.452589] | Test Loss: [2.944266, 0.39964035, 5.4888916]\n",
      "285: Train Loss: [2.6163294, 0.37952557, 4.853133] | Test Loss: [2.6516423, 0.39430574, 4.908979]\n",
      "286: Train Loss: [2.2918658, 0.32826442, 4.2554674] | Test Loss: [2.6279185, 0.3611934, 4.894644]\n",
      "287: Train Loss: [2.31768, 0.40092573, 4.234434] | Test Loss: [2.614195, 0.3783139, 4.850076]\n",
      "288: Train Loss: [2.2770555, 0.31423166, 4.239879] | Test Loss: [2.7850528, 0.41575766, 5.154348]\n",
      "289: Train Loss: [2.3622317, 0.36512142, 4.359342] | Test Loss: [2.5549629, 0.33695078, 4.772975]\n",
      "290: Train Loss: [2.4230142, 0.35063592, 4.4953923] | Test Loss: [2.6486313, 0.35027465, 4.946988]\n",
      "291: Train Loss: [2.4452991, 0.36193806, 4.5286603] | Test Loss: [2.5669708, 0.39262834, 4.7413135]\n",
      "292: Train Loss: [2.537366, 0.3565168, 4.718215] | Test Loss: [2.419545, 0.48287296, 4.356217]\n",
      "293: Train Loss: [2.2985656, 0.36558288, 4.2315483] | Test Loss: [2.5964248, 0.33855617, 4.8542933]\n",
      "294: Train Loss: [2.2940743, 0.34628612, 4.2418623] | Test Loss: [2.5765443, 0.3216656, 4.831423]\n",
      "295: Train Loss: [2.4449885, 0.3619065, 4.5280704] | Test Loss: [2.620791, 0.3425777, 4.8990045]\n",
      "296: Train Loss: [2.3328938, 0.3486032, 4.3171844] | Test Loss: [2.844709, 0.4135856, 5.275832]\n",
      "297: Train Loss: [2.2946415, 0.34610516, 4.243178] | Test Loss: [2.689708, 0.38201973, 4.9973965]\n",
      "298: Train Loss: [2.3071723, 0.28639865, 4.3279457] | Test Loss: [2.8321502, 0.3596526, 5.304648]\n",
      "299: Train Loss: [2.3703928, 0.34348834, 4.3972974] | Test Loss: [2.6464689, 0.34507814, 4.94786]\n",
      "300: Train Loss: [2.4970913, 0.4204408, 4.573742] | Test Loss: [2.5992146, 0.32657906, 4.87185]\n",
      "301: Train Loss: [2.3728287, 0.3478689, 4.3977885] | Test Loss: [2.6904821, 0.43947518, 4.941489]\n",
      "302: Train Loss: [2.330031, 0.36193207, 4.2981296] | Test Loss: [2.8332772, 0.39035073, 5.2762036]\n",
      "303: Train Loss: [2.3928227, 0.32107243, 4.464573] | Test Loss: [2.4796634, 0.30827212, 4.6510544]\n",
      "304: Train Loss: [2.3538141, 0.37101203, 4.336616] | Test Loss: [2.691492, 0.30746683, 5.075517]\n",
      "305: Train Loss: [2.3694885, 0.3402511, 4.398726] | Test Loss: [2.4850852, 0.31512085, 4.65505]\n",
      "306: Train Loss: [2.4512203, 0.3109801, 4.59146] | Test Loss: [2.8302667, 0.42113706, 5.2393966]\n",
      "307: Train Loss: [2.307333, 0.33162013, 4.283046] | Test Loss: [2.747559, 0.33318627, 5.161932]\n",
      "308: Train Loss: [2.39924, 0.3544406, 4.4440393] | Test Loss: [2.6713648, 0.38454592, 4.958184]\n",
      "309: Train Loss: [2.3701246, 0.40696272, 4.3332863] | Test Loss: [2.519818, 0.31700328, 4.722633]\n",
      "310: Train Loss: [2.379677, 0.3387632, 4.420591] | Test Loss: [2.575284, 0.35301992, 4.7975483]\n",
      "311: Train Loss: [2.344191, 0.28371003, 4.404672] | Test Loss: [2.6795285, 0.34596077, 5.0130963]\n",
      "312: Train Loss: [2.463088, 0.35520434, 4.5709715] | Test Loss: [2.697849, 0.3894967, 5.0062013]\n",
      "313: Train Loss: [2.3606484, 0.3536196, 4.367677] | Test Loss: [2.5451941, 0.44213378, 4.6482544]\n",
      "314: Train Loss: [2.289372, 0.3624833, 4.2162604] | Test Loss: [2.6302605, 0.4048371, 4.855684]\n",
      "315: Train Loss: [2.3734825, 0.3870817, 4.3598833] | Test Loss: [2.5887012, 0.3395637, 4.8378386]\n",
      "316: Train Loss: [2.5140831, 0.34972888, 4.678437] | Test Loss: [2.6017706, 0.4399822, 4.763559]\n",
      "317: Train Loss: [2.3445003, 0.35575777, 4.333243] | Test Loss: [2.903003, 0.37500712, 5.430999]\n",
      "318: Train Loss: [2.3405716, 0.34788916, 4.3332543] | Test Loss: [2.7536986, 0.49649984, 5.010897]\n",
      "319: Train Loss: [2.432362, 0.4343878, 4.4303365] | Test Loss: [2.5873451, 0.37890744, 4.7957826]\n",
      "320: Train Loss: [2.3819366, 0.3234795, 4.4403934] | Test Loss: [2.611351, 0.33116078, 4.891541]\n",
      "321: Train Loss: [2.49726, 0.37094858, 4.6235714] | Test Loss: [2.4441643, 0.36660174, 4.5217266]\n",
      "322: Train Loss: [2.4608102, 0.34146038, 4.58016] | Test Loss: [2.8277664, 0.4012357, 5.2542973]\n",
      "323: Train Loss: [2.3741558, 0.35288152, 4.39543] | Test Loss: [2.5911965, 0.39913166, 4.7832613]\n",
      "324: Train Loss: [2.1747751, 0.34151864, 4.008032] | Test Loss: [2.746569, 0.33541003, 5.1577277]\n",
      "325: Train Loss: [2.5011973, 0.43804336, 4.5643516] | Test Loss: [2.5948884, 0.4225336, 4.7672434]\n",
      "326: Train Loss: [2.3658674, 0.28284276, 4.448892] | Test Loss: [2.6148028, 0.32656908, 4.9030366]\n",
      "327: Train Loss: [2.3921795, 0.32793102, 4.456428] | Test Loss: [2.6559076, 0.4011724, 4.9106426]\n",
      "328: Train Loss: [2.244019, 0.28138995, 4.206648] | Test Loss: [2.641818, 0.34766227, 4.9359736]\n",
      "329: Train Loss: [2.3189855, 0.37744755, 4.2605233] | Test Loss: [2.724492, 0.4026179, 5.046366]\n",
      "330: Train Loss: [2.474136, 0.32743594, 4.6208363] | Test Loss: [2.7471704, 0.31889564, 5.175445]\n",
      "331: Train Loss: [2.329345, 0.36014923, 4.2985406] | Test Loss: [2.7510607, 0.36600426, 5.136117]\n",
      "332: Train Loss: [2.359129, 0.33107856, 4.3871794] | Test Loss: [2.6237268, 0.36637136, 4.8810825]\n",
      "333: Train Loss: [2.301465, 0.4807253, 4.122205] | Test Loss: [2.7714605, 0.38609228, 5.156829]\n",
      "334: Train Loss: [2.4574625, 0.3907228, 4.5242023] | Test Loss: [2.6328323, 0.32449955, 4.941165]\n",
      "335: Train Loss: [2.4750884, 0.3703405, 4.5798364] | Test Loss: [2.738424, 0.38220328, 5.094645]\n",
      "336: Train Loss: [2.302599, 0.46093372, 4.144264] | Test Loss: [2.6299326, 0.379244, 4.8806214]\n",
      "337: Train Loss: [2.3284163, 0.31981885, 4.3370137] | Test Loss: [2.5703666, 0.30015984, 4.8405733]\n",
      "338: Train Loss: [2.5330298, 0.36095285, 4.7051067] | Test Loss: [2.6947048, 0.377608, 5.0118017]\n",
      "339: Train Loss: [2.4419844, 0.39834073, 4.485628] | Test Loss: [2.7906654, 0.35271257, 5.228618]\n",
      "340: Train Loss: [2.439117, 0.35880277, 4.519431] | Test Loss: [2.4522967, 0.3740487, 4.5305448]\n",
      "341: Train Loss: [2.3956118, 0.3225938, 4.46863] | Test Loss: [2.6207325, 0.44845024, 4.793015]\n",
      "342: Train Loss: [2.4799573, 0.32849017, 4.6314244] | Test Loss: [2.814847, 0.40712443, 5.2225695]\n",
      "343: Train Loss: [2.322059, 0.31440005, 4.3297176] | Test Loss: [2.686248, 0.4972494, 4.8752465]\n",
      "344: Train Loss: [2.4561403, 0.503672, 4.4086084] | Test Loss: [2.8261735, 0.41459417, 5.237753]\n",
      "345: Train Loss: [2.405019, 0.46490166, 4.3451366] | Test Loss: [2.677233, 0.35199228, 5.002474]\n",
      "346: Train Loss: [2.3590467, 0.3302864, 4.387807] | Test Loss: [2.6927505, 0.3783985, 5.0071025]\n",
      "347: Train Loss: [2.4199216, 0.38097158, 4.458872] | Test Loss: [2.5750425, 0.3822203, 4.7678647]\n",
      "348: Train Loss: [2.5539496, 0.34214792, 4.7657514] | Test Loss: [2.6058922, 0.3356113, 4.876173]\n",
      "349: Train Loss: [2.3301415, 0.35493314, 4.30535] | Test Loss: [2.7081547, 0.4034862, 5.012823]\n",
      "350: Train Loss: [2.2500215, 0.31013313, 4.18991] | Test Loss: [2.628693, 0.3593537, 4.8980327]\n",
      "351: Train Loss: [2.3007143, 0.4025559, 4.1988726] | Test Loss: [2.7457228, 0.32131472, 5.1701307]\n",
      "352: Train Loss: [2.393194, 0.3656643, 4.4207234] | Test Loss: [2.587845, 0.39024353, 4.7854466]\n",
      "353: Train Loss: [2.323922, 0.3692905, 4.2785535] | Test Loss: [2.509768, 0.36682013, 4.6527157]\n",
      "354: Train Loss: [2.368587, 0.3301543, 4.4070196] | Test Loss: [2.5570946, 0.32291192, 4.7912774]\n",
      "355: Train Loss: [2.4643092, 0.32099107, 4.6076274] | Test Loss: [2.8373601, 0.37415835, 5.300562]\n",
      "356: Train Loss: [2.396944, 0.34679312, 4.447095] | Test Loss: [2.6337366, 0.33270258, 4.9347706]\n",
      "357: Train Loss: [2.5026317, 0.35464656, 4.6506166] | Test Loss: [2.682936, 0.44150868, 4.924363]\n",
      "358: Train Loss: [2.3658152, 0.35235494, 4.3792753] | Test Loss: [2.5683796, 0.34778866, 4.7889705]\n",
      "359: Train Loss: [2.4811502, 0.3231581, 4.639142] | Test Loss: [2.6467595, 0.34992597, 4.943593]\n",
      "360: Train Loss: [2.4107957, 0.3294408, 4.492151] | Test Loss: [2.7360086, 0.37000823, 5.102009]\n",
      "361: Train Loss: [2.510717, 0.3778805, 4.6435533] | Test Loss: [2.6410584, 0.3463058, 4.935811]\n",
      "362: Train Loss: [2.4622898, 0.31947505, 4.6051044] | Test Loss: [2.654902, 0.3262925, 4.9835114]\n",
      "363: Train Loss: [2.4525921, 0.37578022, 4.529404] | Test Loss: [2.4403124, 0.3426504, 4.5379744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364: Train Loss: [2.1526406, 0.30880618, 3.996475] | Test Loss: [2.6973379, 0.35232627, 5.0423493]\n",
      "365: Train Loss: [2.4511042, 0.32023242, 4.581976] | Test Loss: [2.548866, 0.40692362, 4.6908083]\n",
      "366: Train Loss: [2.4711344, 0.31655866, 4.62571] | Test Loss: [2.6236386, 0.35635266, 4.8909245]\n",
      "367: Train Loss: [2.4499557, 0.3381872, 4.561724] | Test Loss: [2.773018, 0.39348334, 5.1525526]\n",
      "368: Train Loss: [2.4740865, 0.37988365, 4.5682893] | Test Loss: [2.5577064, 0.31411237, 4.8013005]\n",
      "369: Train Loss: [2.4044428, 0.3396016, 4.469284] | Test Loss: [2.6293356, 0.35481787, 4.9038534]\n",
      "370: Train Loss: [2.4008932, 0.37566614, 4.4261203] | Test Loss: [2.8264852, 0.43988532, 5.213085]\n",
      "371: Train Loss: [2.3157427, 0.3200448, 4.3114405] | Test Loss: [2.7398493, 0.35753855, 5.12216]\n",
      "372: Train Loss: [2.3785489, 0.36021292, 4.396885] | Test Loss: [2.6249826, 0.34799686, 4.9019685]\n",
      "373: Train Loss: [2.3640697, 0.36572102, 4.362418] | Test Loss: [2.7456927, 0.36259156, 5.1287937]\n",
      "374: Train Loss: [2.4060864, 0.34193927, 4.4702334] | Test Loss: [2.4771762, 0.3669726, 4.58738]\n",
      "375: Train Loss: [2.4871395, 0.31975502, 4.654524] | Test Loss: [2.638402, 0.39582166, 4.8809824]\n",
      "376: Train Loss: [2.5753934, 0.3405182, 4.810269] | Test Loss: [2.6760788, 0.3621991, 4.9899583]\n",
      "377: Train Loss: [2.3200228, 0.40428826, 4.2357574] | Test Loss: [2.5361018, 0.4337457, 4.638458]\n",
      "378: Train Loss: [2.3495421, 0.31606406, 4.3830204] | Test Loss: [2.7203116, 0.3311095, 5.1095138]\n",
      "379: Train Loss: [2.6181958, 0.36452857, 4.871863] | Test Loss: [3.0301251, 0.49633425, 5.563916]\n",
      "380: Train Loss: [2.3752716, 0.42000386, 4.330539] | Test Loss: [2.5829952, 0.37773564, 4.7882547]\n",
      "381: Train Loss: [2.3336177, 0.35705322, 4.310182] | Test Loss: [2.5870984, 0.4179621, 4.7562346]\n",
      "382: Train Loss: [2.4136071, 0.34547326, 4.481741] | Test Loss: [2.7060902, 0.32532528, 5.086855]\n",
      "383: Train Loss: [2.4888897, 0.37702006, 4.6007595] | Test Loss: [2.64704, 0.36058056, 4.9334993]\n",
      "384: Train Loss: [2.2408166, 0.37283114, 4.108802] | Test Loss: [2.7690885, 0.46459466, 5.073582]\n",
      "385: Train Loss: [2.4081302, 0.3307798, 4.485481] | Test Loss: [2.501867, 0.34594324, 4.6577907]\n",
      "386: Train Loss: [2.4123576, 0.35235977, 4.4723554] | Test Loss: [2.625277, 0.3522003, 4.8983536]\n",
      "387: Train Loss: [2.3283362, 0.3503586, 4.306314] | Test Loss: [2.4741886, 0.37316948, 4.5752077]\n",
      "388: Train Loss: [2.399673, 0.30548105, 4.493865] | Test Loss: [2.7294846, 0.42526457, 5.0337048]\n",
      "389: Train Loss: [2.2851562, 0.30809057, 4.262222] | Test Loss: [2.6508043, 0.38139173, 4.920217]\n",
      "390: Train Loss: [2.268749, 0.3120519, 4.225446] | Test Loss: [2.7634408, 0.40476486, 5.122117]\n",
      "391: Train Loss: [2.6649556, 0.33827764, 4.9916334] | Test Loss: [2.6315944, 0.38501817, 4.8781705]\n",
      "392: Train Loss: [2.3646362, 0.27778232, 4.45149] | Test Loss: [2.5958009, 0.4101795, 4.781422]\n",
      "393: Train Loss: [2.4909258, 0.33067173, 4.65118] | Test Loss: [2.6021235, 0.30266318, 4.9015837]\n",
      "394: Train Loss: [2.2828376, 0.34156448, 4.2241106] | Test Loss: [2.6408117, 0.44668186, 4.8349414]\n",
      "395: Train Loss: [2.504684, 0.31533188, 4.694036] | Test Loss: [2.5738251, 0.4171686, 4.7304816]\n",
      "396: Train Loss: [2.3197465, 0.34612578, 4.2933674] | Test Loss: [2.4323015, 0.2859105, 4.5786924]\n",
      "397: Train Loss: [2.349314, 0.3217377, 4.37689] | Test Loss: [2.7504122, 0.33798647, 5.162838]\n",
      "398: Train Loss: [2.5184884, 0.37936854, 4.657608] | Test Loss: [2.7610505, 0.37179223, 5.1503086]\n",
      "399: Train Loss: [2.4430733, 0.36160254, 4.5245442] | Test Loss: [2.8823361, 0.43508384, 5.3295884]\n",
      "400: Train Loss: [2.3743522, 0.2990579, 4.4496465] | Test Loss: [2.8126864, 0.3894611, 5.235912]\n",
      "401: Train Loss: [2.2328866, 0.4203906, 4.0453825] | Test Loss: [2.6173303, 0.3902958, 4.8443646]\n",
      "402: Train Loss: [2.4516509, 0.31745994, 4.5858417] | Test Loss: [2.6086636, 0.3329629, 4.884364]\n",
      "403: Train Loss: [2.3446057, 0.3560052, 4.333206] | Test Loss: [2.6610487, 0.40543252, 4.9166646]\n",
      "404: Train Loss: [2.4466972, 0.36636606, 4.5270286] | Test Loss: [2.694268, 0.3336927, 5.0548434]\n",
      "405: Train Loss: [2.4513469, 0.34928873, 4.553405] | Test Loss: [2.6127214, 0.35082853, 4.8746142]\n",
      "406: Train Loss: [2.5101156, 0.365876, 4.654355] | Test Loss: [2.6155117, 0.34658608, 4.884437]\n",
      "407: Train Loss: [2.4325607, 0.3540462, 4.511075] | Test Loss: [2.4491901, 0.31735194, 4.5810285]\n",
      "408: Train Loss: [2.5075994, 0.40055695, 4.6146417] | Test Loss: [2.6274192, 0.29309094, 4.9617476]\n",
      "409: Train Loss: [2.4337912, 0.29969457, 4.567888] | Test Loss: [2.683329, 0.37336034, 4.993298]\n",
      "410: Train Loss: [2.4370449, 0.3259282, 4.5481615] | Test Loss: [2.644269, 0.31862754, 4.9699106]\n",
      "411: Train Loss: [2.2879417, 0.3123269, 4.2635565] | Test Loss: [2.7073019, 0.4299279, 4.984676]\n",
      "412: Train Loss: [2.4824684, 0.36204654, 4.60289] | Test Loss: [2.6636386, 0.32685083, 5.0004263]\n",
      "413: Train Loss: [2.4128296, 0.4069621, 4.4186974] | Test Loss: [2.6335862, 0.32662112, 4.9405513]\n",
      "414: Train Loss: [2.2592773, 0.3392143, 4.1793404] | Test Loss: [2.75916, 0.42302975, 5.09529]\n",
      "415: Train Loss: [2.368963, 0.38463572, 4.35329] | Test Loss: [2.580222, 0.3171563, 4.8432875]\n",
      "416: Train Loss: [2.4722652, 0.3857184, 4.558812] | Test Loss: [2.661969, 0.39265665, 4.931281]\n",
      "417: Train Loss: [2.5610738, 0.3728171, 4.7493305] | Test Loss: [2.637218, 0.4228776, 4.851558]\n",
      "418: Train Loss: [2.4469736, 0.41909355, 4.4748535] | Test Loss: [2.7267172, 0.4572852, 4.996149]\n",
      "419: Train Loss: [2.5781555, 0.35650426, 4.7998066] | Test Loss: [2.8271685, 0.35730347, 5.2970333]\n",
      "420: Train Loss: [2.468821, 0.36603886, 4.5716033] | Test Loss: [2.5120692, 0.39041555, 4.633723]\n",
      "421: Train Loss: [2.2323742, 0.33861846, 4.12613] | Test Loss: [2.6407545, 0.3173942, 4.9641147]\n",
      "422: Train Loss: [2.3597894, 0.35108548, 4.368493] | Test Loss: [2.6768932, 0.35856038, 4.995226]\n",
      "423: Train Loss: [2.3773284, 0.3923313, 4.3623257] | Test Loss: [2.6832697, 0.3316732, 5.0348663]\n",
      "424: Train Loss: [2.4474998, 0.3424981, 4.552501] | Test Loss: [2.706586, 0.37050334, 5.0426683]\n",
      "425: Train Loss: [2.6142325, 0.50354886, 4.7249165] | Test Loss: [2.8568113, 0.34786838, 5.365754]\n",
      "426: Train Loss: [2.5794966, 0.32433063, 4.8346624] | Test Loss: [2.5774546, 0.36703575, 4.7878733]\n",
      "427: Train Loss: [2.3831306, 0.30200747, 4.4642534] | Test Loss: [2.5927644, 0.36247972, 4.823049]\n",
      "428: Train Loss: [2.394837, 0.35728717, 4.4323864] | Test Loss: [2.44795, 0.34593895, 4.5499606]\n",
      "429: Train Loss: [2.6031244, 0.32455695, 4.881692] | Test Loss: [2.6847088, 0.35923663, 5.010181]\n",
      "430: Train Loss: [2.5696967, 0.37032545, 4.769068] | Test Loss: [2.5851958, 0.3758878, 4.7945037]\n",
      "431: Train Loss: [2.5519953, 0.33594298, 4.768048] | Test Loss: [2.5624733, 0.37316108, 4.7517858]\n",
      "432: Train Loss: [2.399633, 0.34231004, 4.456956] | Test Loss: [2.6918688, 0.3286945, 5.055043]\n",
      "433: Train Loss: [2.490002, 0.36673683, 4.613267] | Test Loss: [2.858668, 0.3821544, 5.3351817]\n",
      "434: Train Loss: [2.3712847, 0.38121408, 4.3613553] | Test Loss: [2.5281944, 0.31685346, 4.7395353]\n",
      "435: Train Loss: [2.3888786, 0.40059644, 4.3771605] | Test Loss: [2.6912892, 0.3458268, 5.0367517]\n",
      "436: Train Loss: [2.410078, 0.33739257, 4.4827633] | Test Loss: [2.8191307, 0.30693895, 5.331322]\n",
      "437: Train Loss: [2.4446604, 0.4136984, 4.4756227] | Test Loss: [2.5477257, 0.31849682, 4.7769547]\n",
      "438: Train Loss: [2.5131383, 0.41878265, 4.607494] | Test Loss: [2.639455, 0.39610964, 4.8828006]\n",
      "439: Train Loss: [2.3810766, 0.40065047, 4.3615026] | Test Loss: [2.673441, 0.4176777, 4.929204]\n",
      "440: Train Loss: [2.3957698, 0.3841354, 4.4074044] | Test Loss: [2.5940144, 0.46358827, 4.7244406]\n",
      "441: Train Loss: [2.5156152, 0.3818772, 4.649353] | Test Loss: [2.608834, 0.41040993, 4.807258]\n",
      "442: Train Loss: [2.3731804, 0.3411567, 4.4052043] | Test Loss: [2.5711048, 0.3922959, 4.7499137]\n",
      "443: Train Loss: [2.5894134, 0.31638888, 4.8624377] | Test Loss: [2.6213925, 0.3858809, 4.856904]\n",
      "444: Train Loss: [2.5789337, 0.35216555, 4.8057017] | Test Loss: [2.5123882, 0.32946765, 4.6953087]\n",
      "445: Train Loss: [2.535491, 0.3886337, 4.6823483] | Test Loss: [2.6037333, 0.32667196, 4.8807945]\n",
      "446: Train Loss: [2.5394013, 0.57083523, 4.5079675] | Test Loss: [2.6248875, 0.3904303, 4.8593445]\n",
      "447: Train Loss: [2.3211198, 0.38164246, 4.260597] | Test Loss: [2.589457, 0.335737, 4.843177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448: Train Loss: [2.4511738, 0.3695009, 4.5328465] | Test Loss: [2.6483088, 0.39764228, 4.8989754]\n",
      "449: Train Loss: [2.3783424, 0.39579093, 4.3608937] | Test Loss: [2.5964952, 0.33178675, 4.8612037]\n",
      "450: Train Loss: [2.2419982, 0.3259282, 4.158068] | Test Loss: [2.6499712, 0.3735163, 4.9264264]\n",
      "451: Train Loss: [2.4796345, 0.3469053, 4.612364] | Test Loss: [2.7789524, 0.33623266, 5.221672]\n",
      "452: Train Loss: [2.4192793, 0.34080818, 4.4977503] | Test Loss: [2.6190844, 0.3608032, 4.8773656]\n",
      "453: Train Loss: [2.500082, 0.33565107, 4.664513] | Test Loss: [2.5896215, 0.39595088, 4.7832923]\n",
      "454: Train Loss: [2.5220323, 0.39787307, 4.6461916] | Test Loss: [2.5587494, 0.34104854, 4.77645]\n",
      "455: Train Loss: [2.3789706, 0.33053088, 4.42741] | Test Loss: [2.6749353, 0.3242786, 5.0255923]\n",
      "456: Train Loss: [2.3947325, 0.4671744, 4.3222904] | Test Loss: [2.9903343, 0.43304121, 5.5476274]\n",
      "457: Train Loss: [2.4463396, 0.38422355, 4.5084558] | Test Loss: [2.6599994, 0.44665602, 4.8733425]\n",
      "458: Train Loss: [2.4397976, 0.36011818, 4.519477] | Test Loss: [2.8480635, 0.43437648, 5.26175]\n",
      "459: Train Loss: [2.4731934, 0.32981375, 4.616573] | Test Loss: [2.6923513, 0.35126582, 5.033437]\n",
      "460: Train Loss: [2.4793587, 0.38858506, 4.5701323] | Test Loss: [2.6602366, 0.35584396, 4.964629]\n",
      "461: Train Loss: [2.4465473, 0.35363424, 4.53946] | Test Loss: [2.9407792, 0.42071533, 5.460843]\n",
      "462: Train Loss: [2.3216991, 0.3527068, 4.2906914] | Test Loss: [2.563584, 0.3378253, 4.789343]\n",
      "463: Train Loss: [2.484137, 0.3631399, 4.605134] | Test Loss: [2.7603896, 0.32860744, 5.1921716]\n",
      "464: Train Loss: [2.4989467, 0.35745972, 4.640434] | Test Loss: [2.5462577, 0.40487418, 4.687641]\n",
      "465: Train Loss: [2.5362837, 0.36192068, 4.7106466] | Test Loss: [2.663006, 0.356005, 4.970007]\n",
      "466: Train Loss: [2.537625, 0.33793974, 4.7373104] | Test Loss: [2.514659, 0.39403403, 4.635284]\n",
      "467: Train Loss: [2.5557714, 0.36311492, 4.748428] | Test Loss: [2.6293368, 0.36806655, 4.8906074]\n",
      "468: Train Loss: [2.5571547, 0.39221662, 4.7220926] | Test Loss: [2.6100645, 0.38998115, 4.8301477]\n",
      "469: Train Loss: [2.420904, 0.4266713, 4.4151363] | Test Loss: [2.51686, 0.35359788, 4.680122]\n",
      "470: Train Loss: [2.509617, 0.36050323, 4.658731] | Test Loss: [2.6804245, 0.3673611, 4.993488]\n",
      "471: Train Loss: [2.447299, 0.35987794, 4.53472] | Test Loss: [2.7254646, 0.43399078, 5.016938]\n",
      "472: Train Loss: [2.448434, 0.36274496, 4.5341234] | Test Loss: [3.4463916, 0.29648063, 6.5963025]\n",
      "473: Train Loss: [2.4795923, 0.38350952, 4.575675] | Test Loss: [2.7685747, 0.34122512, 5.1959243]\n",
      "474: Train Loss: [2.3395472, 0.3289486, 4.350146] | Test Loss: [2.6391258, 0.34385324, 4.934398]\n",
      "475: Train Loss: [2.5024338, 0.394501, 4.6103663] | Test Loss: [2.7461998, 0.3368093, 5.1555905]\n",
      "476: Train Loss: [2.2763526, 0.37669003, 4.1760154] | Test Loss: [2.7043777, 0.390535, 5.0182204]\n",
      "477: Train Loss: [2.4171095, 0.37981343, 4.4544053] | Test Loss: [2.5797741, 0.38826743, 4.771281]\n",
      "478: Train Loss: [2.356045, 0.3320131, 4.380077] | Test Loss: [2.6669557, 0.4691303, 4.864781]\n",
      "479: Train Loss: [2.465615, 0.31858236, 4.6126475] | Test Loss: [2.6556394, 0.34215266, 4.969126]\n",
      "480: Train Loss: [2.4451098, 0.39782378, 4.492396] | Test Loss: [2.6691287, 0.36604437, 4.972213]\n",
      "481: Train Loss: [2.455458, 0.37032652, 4.5405893] | Test Loss: [2.7439003, 0.45063478, 5.0371656]\n",
      "482: Train Loss: [2.34877, 0.42308262, 4.274457] | Test Loss: [2.6520348, 0.3239103, 4.9801593]\n",
      "483: Train Loss: [2.3134637, 0.30688927, 4.3200383] | Test Loss: [2.884643, 0.3432963, 5.42599]\n",
      "484: Train Loss: [2.400547, 0.36506212, 4.436032] | Test Loss: [2.6891909, 0.324952, 5.0534296]\n",
      "485: Train Loss: [2.5584629, 0.4045519, 4.7123737] | Test Loss: [2.3319814, 0.31853884, 4.345424]\n",
      "486: Train Loss: [2.40875, 0.30015513, 4.517345] | Test Loss: [2.3780124, 0.3676801, 4.388345]\n",
      "487: Train Loss: [2.4048092, 0.37257317, 4.437045] | Test Loss: [2.7026231, 0.3366354, 5.0686107]\n",
      "488: Train Loss: [2.365591, 0.3771459, 4.3540363] | Test Loss: [2.6278684, 0.4660086, 4.789728]\n",
      "489: Train Loss: [2.556667, 0.3448219, 4.7685122] | Test Loss: [2.620206, 0.36204928, 4.878363]\n",
      "490: Train Loss: [2.5230968, 0.35562316, 4.6905704] | Test Loss: [2.6690717, 0.42316636, 4.914977]\n",
      "491: Train Loss: [2.3294513, 0.365873, 4.29303] | Test Loss: [2.6293657, 0.40578753, 4.852944]\n",
      "492: Train Loss: [2.3858173, 0.32540432, 4.4462304] | Test Loss: [2.6332538, 0.3988705, 4.867637]\n",
      "493: Train Loss: [2.444127, 0.3446066, 4.543648] | Test Loss: [2.5684412, 0.31975418, 4.817128]\n",
      "494: Train Loss: [2.432097, 0.35734388, 4.5068502] | Test Loss: [2.6796913, 0.42455524, 4.9348273]\n",
      "495: Train Loss: [2.6219227, 0.36482903, 4.8790164] | Test Loss: [2.5484686, 0.41044322, 4.686494]\n",
      "496: Train Loss: [2.2775247, 0.359851, 4.1951985] | Test Loss: [2.7601337, 0.40334144, 5.116926]\n",
      "497: Train Loss: [2.4030185, 0.34212524, 4.4639115] | Test Loss: [2.5617373, 0.38576603, 4.7377086]\n",
      "498: Train Loss: [2.3230507, 0.35531962, 4.290782] | Test Loss: [2.643864, 0.33049744, 4.9572306]\n",
      "499: Train Loss: [2.5153043, 0.34453124, 4.6860776] | Test Loss: [2.6250014, 0.36113906, 4.8888636]\n",
      "500: Train Loss: [2.4450793, 0.301306, 4.5888524] | Test Loss: [2.6980448, 0.30491212, 5.0911775]\n",
      "501: Train Loss: [2.5431085, 0.35975468, 4.7264624] | Test Loss: [2.7330317, 0.3508816, 5.115182]\n",
      "502: Train Loss: [2.4957592, 0.33574992, 4.6557684] | Test Loss: [2.4666793, 0.36081314, 4.5725455]\n",
      "503: Train Loss: [2.460983, 0.28899938, 4.6329665] | Test Loss: [2.6815124, 0.40604603, 4.956979]\n",
      "504: Train Loss: [2.5120354, 0.42613143, 4.5979395] | Test Loss: [2.5305338, 0.38657126, 4.674496]\n",
      "505: Train Loss: [2.4654715, 0.40477297, 4.5261703] | Test Loss: [2.5179965, 0.36883727, 4.6671557]\n",
      "506: Train Loss: [2.4130304, 0.40420806, 4.4218526] | Test Loss: [2.581952, 0.3208706, 4.843034]\n",
      "507: Train Loss: [2.4138517, 0.35218993, 4.4755135] | Test Loss: [2.7624145, 0.35411796, 5.170711]\n",
      "508: Train Loss: [2.4679387, 0.34982473, 4.5860524] | Test Loss: [2.5726929, 0.37030768, 4.775078]\n",
      "509: Train Loss: [2.3501167, 0.36232582, 4.337908] | Test Loss: [2.6648624, 0.31601828, 5.0137067]\n",
      "510: Train Loss: [2.344424, 0.3253966, 4.3634515] | Test Loss: [2.7266016, 0.44198668, 5.0112166]\n",
      "511: Train Loss: [2.5269482, 0.33721665, 4.7166796] | Test Loss: [2.6276448, 0.37713334, 4.878156]\n",
      "512: Train Loss: [2.4897377, 0.35617262, 4.623303] | Test Loss: [2.5767422, 0.3248976, 4.8285866]\n",
      "513: Train Loss: [2.3439612, 0.30912155, 4.378801] | Test Loss: [2.6964002, 0.38729572, 5.0055046]\n",
      "514: Train Loss: [2.3894608, 0.3515186, 4.427403] | Test Loss: [2.5478036, 0.34537503, 4.750232]\n",
      "515: Train Loss: [2.5850308, 0.37834084, 4.791721] | Test Loss: [2.6915317, 0.38680038, 4.996263]\n",
      "516: Train Loss: [2.3469322, 0.33192286, 4.3619413] | Test Loss: [2.6469414, 0.4055604, 4.8883224]\n",
      "517: Train Loss: [2.3911543, 0.26227155, 4.520037] | Test Loss: [2.7218459, 0.40633628, 5.0373554]\n",
      "518: Train Loss: [2.457631, 0.29264146, 4.6226206] | Test Loss: [2.648008, 0.33905184, 4.9569645]\n",
      "519: Train Loss: [2.422207, 0.39600477, 4.4484096] | Test Loss: [2.666772, 0.35491568, 4.978628]\n",
      "520: Train Loss: [2.3567343, 0.299336, 4.4141326] | Test Loss: [2.6461883, 0.37281376, 4.919563]\n",
      "521: Train Loss: [2.4384522, 0.370787, 4.5061173] | Test Loss: [2.5406303, 0.39053562, 4.690725]\n",
      "522: Train Loss: [2.588872, 0.44048572, 4.737258] | Test Loss: [2.6575894, 0.432897, 4.882282]\n",
      "523: Train Loss: [2.3970385, 0.34608254, 4.447994] | Test Loss: [2.7004662, 0.3579989, 5.0429335]\n",
      "Epoch 19\n",
      "0: Train Loss: [2.3768337, 0.3453073, 4.40836] | Test Loss: [2.5027196, 0.3813712, 4.6240683]\n",
      "1: Train Loss: [2.2330787, 0.36049297, 4.1056643] | Test Loss: [2.625505, 0.3495084, 4.9015017]\n",
      "2: Train Loss: [2.353808, 0.37427634, 4.3333397] | Test Loss: [2.6621616, 0.37079722, 4.953526]\n",
      "3: Train Loss: [2.2517831, 0.29329747, 4.210269] | Test Loss: [2.591687, 0.36322355, 4.8201504]\n",
      "4: Train Loss: [2.1814742, 0.34688625, 4.0160623] | Test Loss: [2.8323238, 0.36654234, 5.2981052]\n",
      "5: Train Loss: [2.2758346, 0.35089976, 4.2007694] | Test Loss: [2.7925153, 0.3923718, 5.192659]\n",
      "6: Train Loss: [2.3096077, 0.30978346, 4.309432] | Test Loss: [2.635956, 0.4231201, 4.848792]\n",
      "7: Train Loss: [2.2683601, 0.46637505, 4.0703454] | Test Loss: [2.7008727, 0.31845978, 5.0832853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: Train Loss: [2.2559774, 0.3321927, 4.179762] | Test Loss: [2.4750361, 0.38905147, 4.561021]\n",
      "9: Train Loss: [2.27275, 0.38414624, 4.1613536] | Test Loss: [2.7302701, 0.39683682, 5.0637035]\n",
      "10: Train Loss: [2.367605, 0.31981793, 4.415392] | Test Loss: [2.7071118, 0.39013448, 5.0240893]\n",
      "11: Train Loss: [2.1467223, 0.30803898, 3.9854057] | Test Loss: [2.685217, 0.32975316, 5.040681]\n",
      "12: Train Loss: [2.204846, 0.32606065, 4.083631] | Test Loss: [2.60478, 0.37290743, 4.8366523]\n",
      "13: Train Loss: [2.3158102, 0.38284975, 4.2487707] | Test Loss: [2.64976, 0.38606125, 4.913459]\n",
      "14: Train Loss: [2.249877, 0.45120984, 4.048544] | Test Loss: [2.5501533, 0.3613501, 4.7389565]\n",
      "15: Train Loss: [2.1246529, 0.34445512, 3.9048505] | Test Loss: [2.800327, 0.36869723, 5.231957]\n",
      "16: Train Loss: [2.4100828, 0.4183986, 4.4017673] | Test Loss: [2.5924456, 0.35955483, 4.8253365]\n",
      "17: Train Loss: [2.1948657, 0.288618, 4.1011133] | Test Loss: [2.4949331, 0.3544978, 4.6353683]\n",
      "18: Train Loss: [2.26711, 0.47771135, 4.056509] | Test Loss: [2.6331732, 0.3679109, 4.8984356]\n",
      "19: Train Loss: [2.3088744, 0.38062036, 4.2371283] | Test Loss: [2.7475123, 0.36411232, 5.1309123]\n",
      "20: Train Loss: [2.1807563, 0.36077744, 4.0007353] | Test Loss: [2.6153946, 0.32735872, 4.9034305]\n",
      "21: Train Loss: [2.3733535, 0.33117977, 4.4155273] | Test Loss: [2.923026, 0.35792354, 5.4881287]\n",
      "22: Train Loss: [2.245097, 0.36217612, 4.128018] | Test Loss: [2.7056353, 0.3271388, 5.0841317]\n",
      "23: Train Loss: [2.3497784, 0.34940308, 4.350154] | Test Loss: [2.670616, 0.42132854, 4.9199033]\n",
      "24: Train Loss: [2.2813487, 0.37024632, 4.192451] | Test Loss: [2.486958, 0.33690724, 4.6370087]\n",
      "25: Train Loss: [2.3409996, 0.371351, 4.310648] | Test Loss: [2.7027938, 0.36820683, 5.0373807]\n",
      "26: Train Loss: [2.4474607, 0.45639756, 4.438524] | Test Loss: [2.635706, 0.36734205, 4.90407]\n",
      "27: Train Loss: [2.3106565, 0.33817115, 4.283142] | Test Loss: [2.603251, 0.39827248, 4.8082294]\n",
      "28: Train Loss: [2.3189447, 0.39790612, 4.239983] | Test Loss: [2.5372317, 0.3540122, 4.7204514]\n",
      "29: Train Loss: [2.522, 0.44811708, 4.595883] | Test Loss: [2.7360249, 0.384198, 5.0878515]\n",
      "30: Train Loss: [2.122576, 0.32247454, 3.9226773] | Test Loss: [2.463919, 0.37243193, 4.555406]\n",
      "31: Train Loss: [2.2150886, 0.34081438, 4.0893626] | Test Loss: [2.6694052, 0.35802308, 4.9807873]\n",
      "32: Train Loss: [2.3236337, 0.35750595, 4.2897615] | Test Loss: [2.786213, 0.45125002, 5.121176]\n",
      "33: Train Loss: [2.4265678, 0.41689485, 4.4362407] | Test Loss: [2.6350813, 0.36265758, 4.907505]\n",
      "34: Train Loss: [2.2744138, 0.2998223, 4.2490053] | Test Loss: [2.5478246, 0.3563223, 4.739327]\n",
      "35: Train Loss: [2.248786, 0.3295579, 4.168014] | Test Loss: [2.582971, 0.40146858, 4.7644734]\n",
      "36: Train Loss: [2.286669, 0.337742, 4.235596] | Test Loss: [2.5830865, 0.42074764, 4.745425]\n",
      "37: Train Loss: [2.1434052, 0.39682925, 3.8899813] | Test Loss: [2.5965955, 0.3650092, 4.8281817]\n",
      "38: Train Loss: [2.3435345, 0.4774035, 4.2096653] | Test Loss: [2.4240627, 0.8847217, 3.963404]\n",
      "39: Train Loss: [2.178513, 0.32944953, 4.0275764] | Test Loss: [2.5125425, 0.33936235, 4.685723]\n",
      "40: Train Loss: [2.2049665, 0.3631803, 4.046753] | Test Loss: [2.6205213, 0.38723123, 4.8538113]\n",
      "41: Train Loss: [2.3891788, 0.36608672, 4.412271] | Test Loss: [2.723069, 0.39535815, 5.05078]\n",
      "42: Train Loss: [2.3080752, 0.36786637, 4.248284] | Test Loss: [2.749274, 0.36246315, 5.136085]\n",
      "43: Train Loss: [2.2891726, 0.385044, 4.193301] | Test Loss: [2.461954, 0.40035075, 4.5235577]\n",
      "44: Train Loss: [2.3502371, 0.34477377, 4.3557005] | Test Loss: [2.6151774, 0.3665489, 4.863806]\n",
      "45: Train Loss: [2.3129637, 0.34824228, 4.277685] | Test Loss: [2.8075914, 0.46017343, 5.1550093]\n",
      "46: Train Loss: [2.191948, 0.37706718, 4.006829] | Test Loss: [2.621449, 0.36122823, 4.8816695]\n",
      "47: Train Loss: [2.1935325, 0.29895028, 4.0881147] | Test Loss: [2.5069666, 0.31274, 4.7011933]\n",
      "48: Train Loss: [2.209794, 0.34374368, 4.0758443] | Test Loss: [2.6952803, 0.35108155, 5.0394793]\n",
      "49: Train Loss: [2.3213568, 0.27287915, 4.3698344] | Test Loss: [2.7020092, 0.36081478, 5.043204]\n",
      "50: Train Loss: [2.3357632, 0.35380644, 4.31772] | Test Loss: [2.611881, 0.42651826, 4.7972436]\n",
      "51: Train Loss: [2.1422486, 0.25735503, 4.027142] | Test Loss: [2.7751086, 0.36812955, 5.1820874]\n",
      "52: Train Loss: [2.360031, 0.31040874, 4.409653] | Test Loss: [2.6440525, 0.34098908, 4.947116]\n",
      "53: Train Loss: [2.3426633, 0.35018927, 4.3351374] | Test Loss: [2.6470973, 0.3569221, 4.9372725]\n",
      "54: Train Loss: [2.18773, 0.40300524, 3.9724548] | Test Loss: [2.4979646, 0.39994675, 4.5959826]\n",
      "55: Train Loss: [2.293198, 0.37350225, 4.212894] | Test Loss: [2.6421032, 0.33469546, 4.949511]\n",
      "56: Train Loss: [2.2441096, 0.30936196, 4.1788573] | Test Loss: [2.492363, 0.40613198, 4.5785937]\n",
      "57: Train Loss: [2.1831405, 0.30257973, 4.063701] | Test Loss: [2.6835825, 0.39317644, 4.9739885]\n",
      "58: Train Loss: [2.38912, 0.34706417, 4.431176] | Test Loss: [2.535156, 0.39326715, 4.677045]\n",
      "59: Train Loss: [2.3132927, 0.38026857, 4.246317] | Test Loss: [2.7349062, 0.40210253, 5.06771]\n",
      "60: Train Loss: [2.3213058, 0.35414562, 4.288466] | Test Loss: [2.5712752, 0.39790595, 4.7446446]\n",
      "61: Train Loss: [2.2578478, 0.3423937, 4.1733017] | Test Loss: [2.6120174, 0.40117458, 4.8228602]\n",
      "62: Train Loss: [2.219937, 0.34856227, 4.091312] | Test Loss: [2.7668445, 0.28488845, 5.2488008]\n",
      "63: Train Loss: [2.3212974, 0.31026572, 4.3323293] | Test Loss: [2.570244, 0.36789897, 4.772589]\n",
      "64: Train Loss: [2.3095775, 0.43239474, 4.1867604] | Test Loss: [2.537942, 0.33492532, 4.7409587]\n",
      "65: Train Loss: [2.3752494, 0.35316268, 4.397336] | Test Loss: [2.771787, 0.40562916, 5.1379447]\n",
      "66: Train Loss: [2.3308918, 0.45667067, 4.205113] | Test Loss: [2.675603, 0.36140302, 4.989803]\n",
      "67: Train Loss: [2.3163676, 0.36121058, 4.2715244] | Test Loss: [2.7330852, 0.33945066, 5.1267195]\n",
      "68: Train Loss: [2.3563406, 0.40654188, 4.3061395] | Test Loss: [2.4570916, 0.30493408, 4.609249]\n",
      "69: Train Loss: [2.3634255, 0.40249985, 4.3243513] | Test Loss: [2.7052484, 0.3957018, 5.014795]\n",
      "70: Train Loss: [2.336154, 0.35626128, 4.3160467] | Test Loss: [2.484316, 0.4188182, 4.549814]\n",
      "71: Train Loss: [2.3644242, 0.39520365, 4.333645] | Test Loss: [2.6041079, 0.32613036, 4.8820853]\n",
      "72: Train Loss: [2.3869638, 0.39248666, 4.381441] | Test Loss: [2.7214355, 0.337197, 5.1056743]\n",
      "73: Train Loss: [2.2201884, 0.31456473, 4.125812] | Test Loss: [2.5927281, 0.34407735, 4.8413787]\n",
      "74: Train Loss: [2.244432, 0.3732865, 4.115577] | Test Loss: [2.6405146, 0.34803355, 4.932996]\n",
      "75: Train Loss: [2.3737946, 0.45030025, 4.297289] | Test Loss: [2.5833073, 0.3603002, 4.8063145]\n",
      "76: Train Loss: [2.3787217, 0.34496617, 4.412477] | Test Loss: [2.6482835, 0.3556259, 4.940941]\n",
      "77: Train Loss: [2.326556, 0.35090017, 4.3022118] | Test Loss: [2.548405, 0.40219226, 4.6946177]\n",
      "78: Train Loss: [2.3234162, 0.31856835, 4.328264] | Test Loss: [2.6446705, 0.31981802, 4.969523]\n",
      "79: Train Loss: [2.3085175, 0.29145563, 4.325579] | Test Loss: [2.4707196, 0.43305308, 4.508386]\n",
      "80: Train Loss: [2.1680925, 0.31002778, 4.0261574] | Test Loss: [2.818085, 0.39411393, 5.242056]\n",
      "81: Train Loss: [2.3235471, 0.27464706, 4.372447] | Test Loss: [2.362354, 0.39422208, 4.330486]\n",
      "82: Train Loss: [2.2822, 0.30725983, 4.25714] | Test Loss: [2.6280851, 0.32201642, 4.934154]\n",
      "83: Train Loss: [2.1955624, 0.5209969, 3.870128] | Test Loss: [2.6903813, 0.37436578, 5.006397]\n",
      "84: Train Loss: [2.3678596, 0.3406167, 4.3951025] | Test Loss: [2.6170478, 0.39589703, 4.8381987]\n",
      "85: Train Loss: [2.154166, 0.3158687, 3.992463] | Test Loss: [2.6102803, 0.37947243, 4.8410883]\n",
      "86: Train Loss: [2.4766548, 0.35401118, 4.5992985] | Test Loss: [2.6577117, 0.37312517, 4.9422984]\n",
      "87: Train Loss: [2.297056, 0.33698484, 4.2571273] | Test Loss: [2.6600392, 0.41075727, 4.9093213]\n",
      "88: Train Loss: [2.275476, 0.33664954, 4.2143025] | Test Loss: [2.63102, 0.31776354, 4.944277]\n",
      "89: Train Loss: [2.2588477, 0.35617912, 4.161516] | Test Loss: [2.6324265, 0.31529716, 4.949556]\n",
      "90: Train Loss: [2.2783136, 0.33724824, 4.219379] | Test Loss: [2.7974067, 0.47605088, 5.1187625]\n",
      "91: Train Loss: [2.3264484, 0.373596, 4.2793007] | Test Loss: [2.63657, 0.34898746, 4.9241524]\n",
      "92: Train Loss: [2.283165, 0.34463632, 4.2216935] | Test Loss: [2.8389766, 0.32711568, 5.3508377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93: Train Loss: [2.216792, 0.30218998, 4.1313944] | Test Loss: [2.6287506, 0.40808526, 4.849416]\n",
      "94: Train Loss: [2.2492814, 0.29975486, 4.1988077] | Test Loss: [2.811907, 0.36860153, 5.255213]\n",
      "95: Train Loss: [2.2942019, 0.37416625, 4.214237] | Test Loss: [2.5602968, 0.33455098, 4.7860427]\n",
      "96: Train Loss: [2.2652032, 0.28380755, 4.2465987] | Test Loss: [2.6081011, 0.3473825, 4.8688197]\n",
      "97: Train Loss: [2.1819572, 0.3032698, 4.0606446] | Test Loss: [2.85575, 0.48233885, 5.2291613]\n",
      "98: Train Loss: [2.3359613, 0.35564563, 4.316277] | Test Loss: [2.4062426, 0.26989958, 4.542586]\n",
      "99: Train Loss: [2.252114, 0.3484687, 4.1557593] | Test Loss: [2.8413436, 0.51013315, 5.172554]\n",
      "100: Train Loss: [2.501655, 0.42047715, 4.582833] | Test Loss: [2.7565467, 0.4601053, 5.052988]\n",
      "101: Train Loss: [2.5029895, 0.3394236, 4.6665554] | Test Loss: [2.7868488, 0.35280168, 5.220896]\n",
      "102: Train Loss: [2.2810361, 0.3733517, 4.1887207] | Test Loss: [2.5651736, 0.3900622, 4.740285]\n",
      "103: Train Loss: [2.3916836, 0.3593859, 4.423981] | Test Loss: [2.6550546, 0.3929083, 4.917201]\n",
      "104: Train Loss: [2.3077986, 0.33416417, 4.281433] | Test Loss: [2.5839813, 0.32274565, 4.8452168]\n",
      "105: Train Loss: [2.260256, 0.36110052, 4.1594114] | Test Loss: [2.7301247, 0.37205055, 5.0881987]\n",
      "106: Train Loss: [2.3039892, 0.34924427, 4.258734] | Test Loss: [2.3604631, 0.39376956, 4.3271565]\n",
      "107: Train Loss: [2.3016453, 0.33597255, 4.267318] | Test Loss: [2.6162994, 0.36000308, 4.872596]\n",
      "108: Train Loss: [2.3320863, 0.34635162, 4.317821] | Test Loss: [2.654196, 0.43137926, 4.8770127]\n",
      "109: Train Loss: [2.4513447, 0.3755649, 4.5271244] | Test Loss: [2.51802, 0.3873839, 4.648656]\n",
      "110: Train Loss: [2.2811809, 0.2928115, 4.2695503] | Test Loss: [2.5722227, 0.36813253, 4.776313]\n",
      "111: Train Loss: [2.3493934, 0.4096682, 4.289119] | Test Loss: [2.5366387, 0.31500039, 4.758277]\n",
      "112: Train Loss: [2.348898, 0.3441361, 4.3536596] | Test Loss: [2.555337, 0.33671853, 4.7739553]\n",
      "113: Train Loss: [2.2921522, 0.38096336, 4.203341] | Test Loss: [2.5571418, 0.3318021, 4.7824817]\n",
      "114: Train Loss: [2.246558, 0.3204012, 4.1727147] | Test Loss: [2.5231268, 0.357948, 4.688306]\n",
      "115: Train Loss: [2.2183359, 0.36927664, 4.067395] | Test Loss: [2.5896726, 0.37124804, 4.808097]\n",
      "116: Train Loss: [2.3526816, 0.3821242, 4.323239] | Test Loss: [2.7423234, 0.43951467, 5.045132]\n",
      "117: Train Loss: [2.1343863, 0.3815087, 3.8872638] | Test Loss: [2.8746214, 0.38812464, 5.3611183]\n",
      "118: Train Loss: [2.2948794, 0.38231128, 4.2074475] | Test Loss: [2.6132271, 0.33085415, 4.8956003]\n",
      "119: Train Loss: [2.3830018, 0.3434258, 4.422578] | Test Loss: [2.833516, 0.35245147, 5.3145804]\n",
      "120: Train Loss: [2.2501884, 0.4012279, 4.0991488] | Test Loss: [2.5396469, 0.34854698, 4.7307467]\n",
      "121: Train Loss: [2.3154004, 0.31022644, 4.3205743] | Test Loss: [2.7026663, 0.38452816, 5.0208044]\n",
      "122: Train Loss: [2.3955607, 0.34898576, 4.442136] | Test Loss: [2.5714836, 0.34459358, 4.7983737]\n",
      "123: Train Loss: [2.2601004, 0.37589765, 4.1443033] | Test Loss: [2.725274, 0.4375584, 5.01299]\n",
      "124: Train Loss: [2.1540017, 0.359326, 3.9486775] | Test Loss: [2.622434, 0.33120093, 4.9136667]\n",
      "125: Train Loss: [2.3109107, 0.31075835, 4.3110633] | Test Loss: [2.6383047, 0.3747123, 4.901897]\n",
      "126: Train Loss: [2.418134, 0.38962248, 4.4466453] | Test Loss: [2.6804192, 0.31524572, 5.045593]\n",
      "127: Train Loss: [2.4118898, 0.39275396, 4.4310255] | Test Loss: [2.6051798, 0.2923051, 4.9180546]\n",
      "128: Train Loss: [2.3463206, 0.34133732, 4.351304] | Test Loss: [3.097113, 0.25910896, 5.935117]\n",
      "129: Train Loss: [2.5104446, 0.34559292, 4.6752963] | Test Loss: [2.6834292, 0.4344156, 4.9324427]\n",
      "130: Train Loss: [2.2827938, 0.41543776, 4.15015] | Test Loss: [2.7233944, 0.36849064, 5.078298]\n",
      "131: Train Loss: [2.193614, 0.3251502, 4.062078] | Test Loss: [2.536362, 0.31924218, 4.753482]\n",
      "132: Train Loss: [2.382905, 0.43157813, 4.334232] | Test Loss: [2.763074, 0.4086478, 5.1175]\n",
      "133: Train Loss: [2.4646692, 0.34452906, 4.5848093] | Test Loss: [2.5757308, 0.32512826, 4.8263335]\n",
      "134: Train Loss: [2.274888, 0.33208156, 4.2176948] | Test Loss: [2.5678341, 0.38331872, 4.7523494]\n",
      "135: Train Loss: [2.3025355, 0.3871516, 4.2179193] | Test Loss: [2.540315, 0.38780737, 4.6928225]\n",
      "136: Train Loss: [2.2394576, 0.3113449, 4.16757] | Test Loss: [2.586388, 0.34897414, 4.823802]\n",
      "137: Train Loss: [2.3383908, 0.3095136, 4.367268] | Test Loss: [2.5666556, 0.44097745, 4.6923337]\n",
      "138: Train Loss: [2.3728855, 0.3631258, 4.382645] | Test Loss: [2.7278936, 0.40267852, 5.0531087]\n",
      "139: Train Loss: [2.214932, 0.3319301, 4.097934] | Test Loss: [2.566681, 0.3386426, 4.794719]\n",
      "140: Train Loss: [2.5023682, 0.35286868, 4.651868] | Test Loss: [2.7296062, 0.38080835, 5.078404]\n",
      "141: Train Loss: [2.280826, 0.3490459, 4.2126064] | Test Loss: [2.5320768, 0.3095787, 4.754575]\n",
      "142: Train Loss: [2.4111962, 0.39368188, 4.4287105] | Test Loss: [2.6745212, 0.40026346, 4.948779]\n",
      "143: Train Loss: [2.243013, 0.39646506, 4.0895605] | Test Loss: [2.666193, 0.36392435, 4.9684615]\n",
      "144: Train Loss: [2.216492, 0.35288456, 4.080099] | Test Loss: [2.8000128, 0.42869774, 5.171328]\n",
      "145: Train Loss: [2.3000684, 0.3268331, 4.2733035] | Test Loss: [2.6298864, 0.36495218, 4.8948207]\n",
      "146: Train Loss: [2.235108, 0.3957035, 4.0745125] | Test Loss: [2.6499956, 0.3486952, 4.951296]\n",
      "147: Train Loss: [2.374667, 0.35787097, 4.391463] | Test Loss: [2.7278051, 0.37752265, 5.078088]\n",
      "148: Train Loss: [2.3389747, 0.3597941, 4.3181553] | Test Loss: [2.6480777, 0.3317088, 4.9644465]\n",
      "149: Train Loss: [2.3697724, 0.41529763, 4.3242474] | Test Loss: [2.8506265, 0.45083487, 5.250418]\n",
      "150: Train Loss: [2.3193326, 0.40173134, 4.2369337] | Test Loss: [2.6540515, 0.34792262, 4.9601803]\n",
      "151: Train Loss: [2.3813417, 0.3997385, 4.362945] | Test Loss: [2.475796, 0.33006513, 4.6215267]\n",
      "152: Train Loss: [2.233339, 0.3190423, 4.147636] | Test Loss: [2.6268005, 0.34598905, 4.907612]\n",
      "153: Train Loss: [2.4288104, 0.34030533, 4.5173154] | Test Loss: [2.4965348, 0.3283608, 4.6647086]\n",
      "154: Train Loss: [2.2241726, 0.34654176, 4.1018033] | Test Loss: [2.7142909, 0.37889397, 5.049688]\n",
      "155: Train Loss: [2.5013041, 0.34668946, 4.6559186] | Test Loss: [2.5600762, 0.4130924, 4.70706]\n",
      "156: Train Loss: [2.281169, 0.32359922, 4.2387385] | Test Loss: [2.6705766, 0.3103894, 5.0307636]\n",
      "157: Train Loss: [2.3174245, 0.34833983, 4.286509] | Test Loss: [2.731798, 0.38018215, 5.0834136]\n",
      "158: Train Loss: [2.3845189, 0.36440614, 4.4046316] | Test Loss: [2.517865, 0.3889577, 4.6467724]\n",
      "159: Train Loss: [2.4651976, 0.29988196, 4.630513] | Test Loss: [2.7158535, 0.36863652, 5.0630703]\n",
      "160: Train Loss: [2.3091192, 0.42880186, 4.1894364] | Test Loss: [2.78158, 0.39668489, 5.1664753]\n",
      "161: Train Loss: [2.1944377, 0.3139833, 4.074892] | Test Loss: [2.5046573, 0.43991494, 4.5693994]\n",
      "162: Train Loss: [2.4986563, 0.3825684, 4.614744] | Test Loss: [2.7643514, 0.41216522, 5.1165376]\n",
      "163: Train Loss: [2.3190892, 0.37485525, 4.2633233] | Test Loss: [2.616407, 0.32919753, 4.9036164]\n",
      "164: Train Loss: [2.53841, 0.3702078, 4.706612] | Test Loss: [2.5396433, 0.43637562, 4.642911]\n",
      "165: Train Loss: [2.3029315, 0.41152003, 4.194343] | Test Loss: [2.5885367, 0.3489686, 4.828105]\n",
      "166: Train Loss: [2.2438724, 0.33483756, 4.1529074] | Test Loss: [2.7648265, 0.38479206, 5.144861]\n",
      "167: Train Loss: [2.2922578, 0.3695533, 4.2149625] | Test Loss: [2.8210635, 0.44802862, 5.1940985]\n",
      "168: Train Loss: [2.2136376, 0.33387282, 4.0934024] | Test Loss: [2.571323, 0.3553036, 4.787342]\n",
      "169: Train Loss: [2.3625162, 0.3465344, 4.378498] | Test Loss: [2.6627276, 0.41970026, 4.905755]\n",
      "170: Train Loss: [2.2841172, 0.3408787, 4.227356] | Test Loss: [2.8490734, 0.32176352, 5.3763833]\n",
      "171: Train Loss: [2.2487268, 0.31868738, 4.1787663] | Test Loss: [2.8173342, 0.509011, 5.1256576]\n",
      "172: Train Loss: [2.3319044, 0.35220557, 4.311603] | Test Loss: [2.625387, 0.3286763, 4.9220977]\n",
      "173: Train Loss: [2.400336, 0.3274445, 4.4732275] | Test Loss: [2.7081025, 0.3461469, 5.070058]\n",
      "174: Train Loss: [2.3064864, 0.37464413, 4.2383285] | Test Loss: [2.4938936, 0.32582772, 4.6619596]\n",
      "175: Train Loss: [2.369005, 0.30375928, 4.434251] | Test Loss: [2.4690533, 0.3806579, 4.557449]\n",
      "176: Train Loss: [2.3668299, 0.36827472, 4.365385] | Test Loss: [2.7432897, 0.3210846, 5.165495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177: Train Loss: [2.3670135, 0.35999265, 4.3740344] | Test Loss: [2.4255257, 0.31560102, 4.5354505]\n",
      "178: Train Loss: [2.2939465, 0.3898547, 4.198038] | Test Loss: [2.6698396, 0.41133407, 4.928345]\n",
      "179: Train Loss: [2.3901055, 0.34087172, 4.439339] | Test Loss: [2.6101441, 0.35296398, 4.8673244]\n",
      "180: Train Loss: [2.3830492, 0.37303555, 4.393063] | Test Loss: [2.682217, 0.36907828, 4.9953556]\n",
      "181: Train Loss: [2.4678361, 0.3956496, 4.540023] | Test Loss: [2.5840828, 0.37181434, 4.7963514]\n",
      "182: Train Loss: [2.3667374, 0.34167016, 4.3918047] | Test Loss: [2.5720463, 0.30856618, 4.8355265]\n",
      "183: Train Loss: [2.3880196, 0.42073432, 4.3553047] | Test Loss: [2.7432897, 0.3362465, 5.150333]\n",
      "184: Train Loss: [2.4359105, 0.43531805, 4.436503] | Test Loss: [2.5851665, 0.32334903, 4.846984]\n",
      "185: Train Loss: [2.3539822, 0.34832075, 4.3596435] | Test Loss: [2.6687803, 0.46084544, 4.876715]\n",
      "186: Train Loss: [2.2961903, 0.415436, 4.1769447] | Test Loss: [2.5243924, 0.35335863, 4.695426]\n",
      "187: Train Loss: [2.4237583, 0.32638416, 4.5211325] | Test Loss: [2.5858643, 0.30489215, 4.8668365]\n",
      "188: Train Loss: [2.1919088, 0.38435042, 3.9994671] | Test Loss: [2.6899395, 0.37403753, 5.0058413]\n",
      "189: Train Loss: [2.3430178, 0.38122883, 4.3048067] | Test Loss: [2.454951, 0.3428446, 4.5670576]\n",
      "190: Train Loss: [2.2902143, 0.35462978, 4.2257986] | Test Loss: [2.8701897, 0.49155024, 5.248829]\n",
      "191: Train Loss: [2.3393285, 0.33201644, 4.3466406] | Test Loss: [2.6920931, 0.4323711, 4.951815]\n",
      "192: Train Loss: [2.5102234, 0.33760425, 4.6828427] | Test Loss: [2.5480387, 0.36977062, 4.726307]\n",
      "193: Train Loss: [2.3542004, 0.34167665, 4.366724] | Test Loss: [2.672622, 0.35757816, 4.9876657]\n",
      "194: Train Loss: [2.315577, 0.3418984, 4.2892556] | Test Loss: [2.785375, 0.40359902, 5.167151]\n",
      "195: Train Loss: [2.3018928, 0.41736686, 4.1864185] | Test Loss: [2.6951816, 0.44240993, 4.947953]\n",
      "196: Train Loss: [2.2286873, 0.34035218, 4.1170225] | Test Loss: [2.681027, 0.34150645, 5.0205474]\n",
      "197: Train Loss: [2.4136605, 0.36338717, 4.463934] | Test Loss: [2.721214, 0.34370115, 5.0987267]\n",
      "198: Train Loss: [2.3778722, 0.3734225, 4.382322] | Test Loss: [2.5039082, 0.41959357, 4.5882225]\n",
      "199: Train Loss: [2.433285, 0.4220138, 4.444556] | Test Loss: [2.6041303, 0.3705608, 4.8377]\n",
      "200: Train Loss: [2.2772808, 0.39377895, 4.160783] | Test Loss: [2.6139555, 0.3941623, 4.833749]\n",
      "201: Train Loss: [2.3534646, 0.35678205, 4.3501472] | Test Loss: [2.4938147, 0.38032904, 4.6073003]\n",
      "202: Train Loss: [2.3624785, 0.3463824, 4.3785744] | Test Loss: [2.630558, 0.34090808, 4.920208]\n",
      "203: Train Loss: [2.2450879, 0.3380291, 4.152147] | Test Loss: [2.523592, 0.43777815, 4.609406]\n",
      "204: Train Loss: [2.348637, 0.34466708, 4.3526073] | Test Loss: [2.6625593, 0.37709087, 4.9480276]\n",
      "205: Train Loss: [2.3628337, 0.2895259, 4.4361415] | Test Loss: [2.6625843, 0.37319788, 4.9519706]\n",
      "206: Train Loss: [2.4124389, 0.36353517, 4.461343] | Test Loss: [2.8172317, 0.379452, 5.2550116]\n",
      "207: Train Loss: [2.5112243, 0.34426844, 4.67818] | Test Loss: [2.6135912, 0.32414886, 4.9030337]\n",
      "208: Train Loss: [2.352337, 0.4081523, 4.2965217] | Test Loss: [2.6496525, 0.28691813, 5.012387]\n",
      "209: Train Loss: [2.4143393, 0.42125, 4.4074287] | Test Loss: [2.6391263, 0.28763527, 4.9906173]\n",
      "210: Train Loss: [2.331024, 0.4424493, 4.219599] | Test Loss: [2.6380053, 0.39180872, 4.884202]\n",
      "211: Train Loss: [2.4174984, 0.41518193, 4.4198146] | Test Loss: [2.6140387, 0.36971983, 4.8583574]\n",
      "212: Train Loss: [2.4265952, 0.4120853, 4.441105] | Test Loss: [2.6956508, 0.43152478, 4.959777]\n",
      "213: Train Loss: [2.4290204, 0.42421946, 4.433821] | Test Loss: [2.7438133, 0.37277833, 5.114848]\n",
      "214: Train Loss: [2.2738228, 0.3524762, 4.1951694] | Test Loss: [2.6165018, 0.37965807, 4.8533454]\n",
      "215: Train Loss: [2.426955, 0.40444386, 4.449466] | Test Loss: [2.462146, 0.34960285, 4.5746894]\n",
      "216: Train Loss: [2.3443038, 0.40920728, 4.2794003] | Test Loss: [2.7693038, 0.31874368, 5.219864]\n",
      "217: Train Loss: [2.3086183, 0.29415652, 4.32308] | Test Loss: [2.5947332, 0.37983057, 4.809636]\n",
      "218: Train Loss: [2.3844557, 0.38216284, 4.3867483] | Test Loss: [2.5685205, 0.30172524, 4.8353157]\n",
      "219: Train Loss: [2.2494493, 0.30488607, 4.1940126] | Test Loss: [2.725293, 0.39392483, 5.056661]\n",
      "220: Train Loss: [2.3310404, 0.4153868, 4.246694] | Test Loss: [2.6605203, 0.39894545, 4.9220953]\n",
      "221: Train Loss: [2.2388275, 0.32106963, 4.156585] | Test Loss: [2.573615, 0.39757383, 4.749656]\n",
      "222: Train Loss: [2.3039513, 0.4272293, 4.180673] | Test Loss: [2.5620086, 0.3389591, 4.785058]\n",
      "223: Train Loss: [2.3208504, 0.31637186, 4.325329] | Test Loss: [2.6753085, 0.37327263, 4.9773445]\n",
      "224: Train Loss: [2.3591995, 0.31649816, 4.401901] | Test Loss: [2.5679457, 0.37113842, 4.764753]\n",
      "225: Train Loss: [2.456836, 0.37542593, 4.538246] | Test Loss: [2.6609478, 0.33474517, 4.98715]\n",
      "226: Train Loss: [2.3364356, 0.36344633, 4.309425] | Test Loss: [2.7737327, 0.35059962, 5.1968656]\n",
      "227: Train Loss: [2.392667, 0.37932825, 4.406006] | Test Loss: [2.5267627, 0.4114667, 4.642059]\n",
      "228: Train Loss: [2.3122158, 0.3027423, 4.321689] | Test Loss: [2.669077, 0.484784, 4.8533697]\n",
      "229: Train Loss: [2.5031981, 0.341994, 4.6644025] | Test Loss: [2.6912866, 0.32064527, 5.061928]\n",
      "230: Train Loss: [2.3735332, 0.4065266, 4.34054] | Test Loss: [2.617699, 0.41861925, 4.8167787]\n",
      "231: Train Loss: [2.33601, 0.3162902, 4.3557296] | Test Loss: [2.4629824, 0.3327571, 4.593208]\n",
      "232: Train Loss: [2.4457533, 0.4276994, 4.463807] | Test Loss: [2.6639342, 0.4386796, 4.889189]\n",
      "233: Train Loss: [2.3646963, 0.50184166, 4.227551] | Test Loss: [2.7153773, 0.33469597, 5.096059]\n",
      "234: Train Loss: [2.405629, 0.31504244, 4.4962153] | Test Loss: [2.7319872, 0.42292374, 5.041051]\n",
      "235: Train Loss: [2.4233575, 0.35284021, 4.4938745] | Test Loss: [2.6942117, 0.38255626, 5.005867]\n",
      "236: Train Loss: [2.3099117, 0.37359983, 4.2462234] | Test Loss: [2.5877273, 0.40353534, 4.7719193]\n",
      "237: Train Loss: [2.5225928, 0.3650092, 4.6801763] | Test Loss: [2.7268395, 0.3707952, 5.082884]\n",
      "238: Train Loss: [2.505366, 0.3817521, 4.62898] | Test Loss: [2.5975711, 0.3688696, 4.8262725]\n",
      "239: Train Loss: [2.4336898, 0.33357546, 4.5338044] | Test Loss: [2.5675316, 0.34504467, 4.7900186]\n",
      "240: Train Loss: [2.2461538, 0.33163267, 4.160675] | Test Loss: [2.6975198, 0.37084797, 5.0241914]\n",
      "241: Train Loss: [2.3455093, 0.32086262, 4.370156] | Test Loss: [2.6284769, 0.35368904, 4.9032645]\n",
      "242: Train Loss: [2.3397398, 0.39890394, 4.2805758] | Test Loss: [2.4889488, 0.31596795, 4.6619296]\n",
      "243: Train Loss: [2.2098389, 0.32630378, 4.093374] | Test Loss: [2.6940467, 0.3070354, 5.081058]\n",
      "244: Train Loss: [2.3253863, 0.3506202, 4.3001523] | Test Loss: [2.6774597, 0.40991682, 4.9450026]\n",
      "245: Train Loss: [2.2075124, 0.38125995, 4.033765] | Test Loss: [2.6862788, 0.3438559, 5.028702]\n",
      "246: Train Loss: [2.3365104, 0.3632493, 4.3097715] | Test Loss: [2.6922867, 0.4184892, 4.9660845]\n",
      "247: Train Loss: [2.4260006, 0.38972038, 4.4622808] | Test Loss: [2.6510262, 0.34834352, 4.953709]\n",
      "248: Train Loss: [2.2511635, 0.3210149, 4.181312] | Test Loss: [2.6898246, 0.3379611, 5.041688]\n",
      "249: Train Loss: [2.4706397, 0.3524559, 4.5888233] | Test Loss: [2.627994, 0.3236053, 4.9323826]\n",
      "250: Train Loss: [2.362372, 0.34839222, 4.376352] | Test Loss: [2.5746164, 0.34793726, 4.8012958]\n",
      "251: Train Loss: [2.332767, 0.37663755, 4.2888966] | Test Loss: [2.7915668, 0.42276293, 5.160371]\n",
      "252: Train Loss: [2.1136408, 0.30543834, 3.921843] | Test Loss: [2.554585, 0.47092262, 4.6382475]\n",
      "253: Train Loss: [2.1581962, 0.33487448, 3.981518] | Test Loss: [2.6264007, 0.42144573, 4.8313556]\n",
      "254: Train Loss: [2.4166415, 0.33453146, 4.4987516] | Test Loss: [2.7251594, 0.33305982, 5.117259]\n",
      "255: Train Loss: [2.2844496, 0.44699416, 4.121905] | Test Loss: [2.506367, 0.41657814, 4.5961556]\n",
      "256: Train Loss: [2.3019657, 0.3228579, 4.2810736] | Test Loss: [2.8652964, 0.3416531, 5.38894]\n",
      "257: Train Loss: [2.4782739, 0.3933197, 4.563228] | Test Loss: [2.508316, 0.3400007, 4.6766315]\n",
      "258: Train Loss: [2.4679823, 0.32334465, 4.61262] | Test Loss: [2.4667926, 0.34496805, 4.5886173]\n",
      "259: Train Loss: [2.3972697, 0.38346663, 4.4110727] | Test Loss: [2.6582205, 0.37790915, 4.938532]\n",
      "260: Train Loss: [2.4151752, 0.30325997, 4.5270905] | Test Loss: [2.6638896, 0.4054477, 4.922332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261: Train Loss: [2.3025773, 0.33807474, 4.26708] | Test Loss: [2.645297, 0.379781, 4.9108133]\n",
      "262: Train Loss: [2.279661, 0.386407, 4.172915] | Test Loss: [2.4064379, 0.35363275, 4.459243]\n",
      "263: Train Loss: [2.1778514, 0.37033015, 3.9853725] | Test Loss: [2.831424, 0.38441178, 5.278436]\n",
      "264: Train Loss: [2.3182073, 0.36635232, 4.270062] | Test Loss: [2.644667, 0.2952032, 4.9941306]\n",
      "265: Train Loss: [2.4746678, 0.3445854, 4.60475] | Test Loss: [2.601583, 0.3725054, 4.830661]\n",
      "266: Train Loss: [2.5180287, 0.3388668, 4.6971908] | Test Loss: [2.6086397, 0.3806168, 4.836663]\n",
      "267: Train Loss: [2.4597588, 0.43189538, 4.4876223] | Test Loss: [2.802719, 0.33350697, 5.271931]\n",
      "268: Train Loss: [2.2950327, 0.3794335, 4.210632] | Test Loss: [2.7016454, 0.39902022, 5.0042706]\n",
      "269: Train Loss: [2.388671, 0.3867613, 4.3905807] | Test Loss: [2.6036906, 0.37446955, 4.8329115]\n",
      "270: Train Loss: [2.3908582, 0.34556314, 4.4361534] | Test Loss: [2.7648864, 0.35786664, 5.171906]\n",
      "271: Train Loss: [2.3686724, 0.3505455, 4.3867993] | Test Loss: [2.7004836, 0.37962168, 5.0213456]\n",
      "272: Train Loss: [2.4219925, 0.35133708, 4.492648] | Test Loss: [2.5769877, 0.35558584, 4.7983894]\n",
      "273: Train Loss: [2.3526716, 0.37227267, 4.3330708] | Test Loss: [2.510397, 0.44918534, 4.5716085]\n",
      "274: Train Loss: [2.4563005, 0.35754174, 4.5550594] | Test Loss: [2.6439726, 0.3575868, 4.9303584]\n",
      "275: Train Loss: [2.4723914, 0.31349453, 4.631288] | Test Loss: [2.5713127, 0.31502554, 4.8276]\n",
      "276: Train Loss: [2.5704243, 0.42624205, 4.714607] | Test Loss: [2.8040252, 0.4080575, 5.1999927]\n",
      "277: Train Loss: [2.3294969, 0.37821296, 4.280781] | Test Loss: [2.655651, 0.3455913, 4.965711]\n",
      "278: Train Loss: [2.19698, 0.4004873, 3.9934728] | Test Loss: [3.0085201, 0.37695357, 5.6400867]\n",
      "279: Train Loss: [2.316862, 0.35920224, 4.274522] | Test Loss: [2.5567684, 0.35176492, 4.7617717]\n",
      "280: Train Loss: [2.5096407, 0.37008387, 4.6491976] | Test Loss: [2.6826365, 0.3848577, 4.9804153]\n",
      "281: Train Loss: [2.3513622, 0.35750267, 4.345222] | Test Loss: [2.707248, 0.3448994, 5.069597]\n",
      "282: Train Loss: [2.4025555, 0.3157242, 4.4893866] | Test Loss: [2.6354272, 0.38652474, 4.88433]\n",
      "283: Train Loss: [2.3064394, 0.2941951, 4.3186836] | Test Loss: [2.7859302, 0.39291072, 5.1789494]\n",
      "284: Train Loss: [2.3697643, 0.3758787, 4.36365] | Test Loss: [2.6793206, 0.4798998, 4.8787413]\n",
      "285: Train Loss: [2.3990211, 0.3244252, 4.473617] | Test Loss: [2.5726204, 0.3320739, 4.813167]\n",
      "286: Train Loss: [2.3968258, 0.3086385, 4.485013] | Test Loss: [2.797609, 0.31946674, 5.2757516]\n",
      "287: Train Loss: [2.4632478, 0.37518072, 4.551315] | Test Loss: [2.6137693, 0.408722, 4.8188167]\n",
      "288: Train Loss: [2.3206537, 0.34012908, 4.3011785] | Test Loss: [2.5280812, 0.31702155, 4.739141]\n",
      "289: Train Loss: [2.4349618, 0.3407329, 4.5291905] | Test Loss: [2.7158701, 0.4549676, 4.976773]\n",
      "290: Train Loss: [2.281869, 0.28579152, 4.2779465] | Test Loss: [2.5711641, 0.38171083, 4.7606173]\n",
      "291: Train Loss: [2.3549376, 0.32733986, 4.3825355] | Test Loss: [2.5730042, 0.3836224, 4.7623863]\n",
      "292: Train Loss: [2.3255172, 0.3382331, 4.3128014] | Test Loss: [2.7117774, 0.39110687, 5.032448]\n",
      "293: Train Loss: [2.3152215, 0.29219404, 4.338249] | Test Loss: [2.7450676, 0.32214308, 5.167992]\n",
      "294: Train Loss: [2.4621367, 0.3115553, 4.612718] | Test Loss: [2.680748, 0.32902843, 5.0324674]\n",
      "295: Train Loss: [2.3462338, 0.3564367, 4.336031] | Test Loss: [2.634512, 0.3486283, 4.9203954]\n",
      "296: Train Loss: [2.2913368, 0.369176, 4.2134976] | Test Loss: [2.7367904, 0.34240612, 5.1311746]\n",
      "297: Train Loss: [2.4767113, 0.34570768, 4.6077147] | Test Loss: [2.4421186, 0.36402428, 4.520213]\n",
      "298: Train Loss: [2.398983, 0.29648536, 4.5014806] | Test Loss: [2.7056675, 0.31698653, 5.0943484]\n",
      "299: Train Loss: [2.3916457, 0.32905778, 4.4542336] | Test Loss: [2.4455633, 0.35847533, 4.5326514]\n",
      "300: Train Loss: [2.3297732, 0.38258037, 4.276966] | Test Loss: [2.6914768, 0.3324737, 5.05048]\n",
      "301: Train Loss: [2.3990881, 0.3025367, 4.49564] | Test Loss: [2.5081525, 0.4191025, 4.5972023]\n",
      "302: Train Loss: [2.3868103, 0.29722393, 4.4763966] | Test Loss: [2.5990522, 0.3428605, 4.8552437]\n",
      "303: Train Loss: [2.3959284, 0.33795884, 4.453898] | Test Loss: [2.568036, 0.3516237, 4.7844486]\n",
      "304: Train Loss: [2.4381297, 0.36507177, 4.5111876] | Test Loss: [2.6845608, 0.36868468, 5.000437]\n",
      "305: Train Loss: [2.3363218, 0.3462419, 4.3264017] | Test Loss: [2.692289, 0.3893521, 4.995226]\n",
      "306: Train Loss: [2.4769008, 0.35972103, 4.5940804] | Test Loss: [2.6943727, 0.49336314, 4.895382]\n",
      "307: Train Loss: [2.4392219, 0.39144823, 4.4869957] | Test Loss: [2.5195947, 0.37730768, 4.6618814]\n",
      "308: Train Loss: [2.255158, 0.37274086, 4.137575] | Test Loss: [2.3946893, 0.33324102, 4.4561377]\n",
      "309: Train Loss: [2.3188155, 0.33328912, 4.304342] | Test Loss: [2.7942405, 0.39111257, 5.1973686]\n",
      "310: Train Loss: [2.3924916, 0.32682618, 4.458157] | Test Loss: [2.3967962, 0.302234, 4.4913583]\n",
      "311: Train Loss: [2.4292731, 0.3616667, 4.4968796] | Test Loss: [2.6906512, 0.384071, 4.9972315]\n",
      "312: Train Loss: [2.4284377, 0.36652875, 4.490347] | Test Loss: [2.5349956, 0.39439732, 4.675594]\n",
      "313: Train Loss: [2.3704207, 0.41257083, 4.3282704] | Test Loss: [2.7158358, 0.34530035, 5.0863714]\n",
      "314: Train Loss: [2.5140715, 0.32348174, 4.7046614] | Test Loss: [2.6323674, 0.34668344, 4.9180512]\n",
      "315: Train Loss: [2.2662451, 0.3576443, 4.174846] | Test Loss: [2.5067945, 0.2800956, 4.7334933]\n",
      "316: Train Loss: [2.4332333, 0.33920193, 4.5272646] | Test Loss: [2.670344, 0.36652792, 4.97416]\n",
      "317: Train Loss: [2.3400896, 0.34026337, 4.3399158] | Test Loss: [2.6848683, 0.4064112, 4.9633255]\n",
      "318: Train Loss: [2.4597645, 0.3388536, 4.5806756] | Test Loss: [2.6081066, 0.28007218, 4.936141]\n",
      "319: Train Loss: [2.2385755, 0.31054252, 4.1666083] | Test Loss: [2.569822, 0.45340088, 4.686243]\n",
      "320: Train Loss: [2.3699684, 0.35916328, 4.3807735] | Test Loss: [2.5665808, 0.2993815, 4.8337803]\n",
      "321: Train Loss: [2.353196, 0.41943416, 4.2869577] | Test Loss: [2.6389127, 0.41754392, 4.8602815]\n",
      "322: Train Loss: [2.4987998, 0.36380506, 4.6337943] | Test Loss: [2.9694376, 0.34748483, 5.5913906]\n",
      "323: Train Loss: [2.3387911, 0.38536516, 4.2922173] | Test Loss: [2.60421, 0.36215344, 4.8462663]\n",
      "324: Train Loss: [2.4374, 0.31500715, 4.559793] | Test Loss: [2.8027456, 0.4614704, 5.1440206]\n",
      "325: Train Loss: [2.2469819, 0.38945252, 4.1045113] | Test Loss: [2.731423, 0.4078541, 5.0549917]\n",
      "326: Train Loss: [2.4082897, 0.37947354, 4.4371057] | Test Loss: [2.5991032, 0.34168646, 4.85652]\n",
      "327: Train Loss: [2.376911, 0.3839491, 4.3698726] | Test Loss: [2.588631, 0.33479282, 4.842469]\n",
      "328: Train Loss: [2.4408586, 0.43040034, 4.451317] | Test Loss: [2.8563914, 0.41206574, 5.3007174]\n",
      "329: Train Loss: [2.513004, 0.34850854, 4.6775] | Test Loss: [2.559205, 0.38822094, 4.7301893]\n",
      "330: Train Loss: [2.1046855, 0.37357002, 3.8358011] | Test Loss: [2.6389997, 0.41737536, 4.860624]\n",
      "331: Train Loss: [2.430719, 0.31511074, 4.546327] | Test Loss: [2.5613399, 0.36695457, 4.7557254]\n",
      "332: Train Loss: [2.4149966, 0.3419398, 4.4880533] | Test Loss: [2.871345, 0.32041174, 5.4222784]\n",
      "333: Train Loss: [2.2891498, 0.32340112, 4.2548985] | Test Loss: [2.7859905, 0.37050787, 5.201473]\n",
      "334: Train Loss: [2.3220665, 0.3519296, 4.2922034] | Test Loss: [2.409426, 0.42671543, 4.3921366]\n",
      "335: Train Loss: [2.447769, 0.31262153, 4.5829163] | Test Loss: [2.6549683, 0.40147644, 4.90846]\n",
      "336: Train Loss: [2.4143116, 0.4056646, 4.422959] | Test Loss: [2.54934, 0.34675458, 4.7519255]\n",
      "337: Train Loss: [2.3684893, 0.310415, 4.4265637] | Test Loss: [2.5590029, 0.36582354, 4.752182]\n",
      "338: Train Loss: [2.4032784, 0.36637047, 4.440186] | Test Loss: [2.2790477, 0.37563002, 4.1824656]\n",
      "339: Train Loss: [2.430079, 0.43642032, 4.4237375] | Test Loss: [2.5675056, 0.3426856, 4.7923255]\n",
      "340: Train Loss: [2.4371767, 0.32800624, 4.546347] | Test Loss: [2.6665168, 0.33215606, 5.0008774]\n",
      "341: Train Loss: [2.4960732, 0.44724563, 4.544901] | Test Loss: [2.673655, 0.37686056, 4.9704494]\n",
      "342: Train Loss: [2.4116168, 0.33599862, 4.487235] | Test Loss: [2.5985074, 0.34046233, 4.8565526]\n",
      "343: Train Loss: [2.4366856, 0.351466, 4.521905] | Test Loss: [2.5734704, 0.36884898, 4.778092]\n",
      "344: Train Loss: [2.3364427, 0.32596165, 4.346924] | Test Loss: [2.5112689, 0.43781987, 4.5847178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345: Train Loss: [2.4911528, 0.36851928, 4.613786] | Test Loss: [2.7639768, 0.3906171, 5.1373367]\n",
      "346: Train Loss: [2.4927685, 0.42244712, 4.56309] | Test Loss: [2.4163063, 0.34016234, 4.49245]\n",
      "347: Train Loss: [2.488277, 0.50367105, 4.4728827] | Test Loss: [2.6497133, 0.40812853, 4.891298]\n",
      "348: Train Loss: [2.397144, 0.3965257, 4.3977623] | Test Loss: [2.7869349, 0.3818881, 5.191982]\n",
      "349: Train Loss: [2.4486694, 0.35836905, 4.53897] | Test Loss: [2.5907183, 0.38163888, 4.7997975]\n",
      "350: Train Loss: [2.4860299, 0.39028594, 4.5817738] | Test Loss: [2.726948, 0.38533026, 5.068566]\n",
      "351: Train Loss: [2.3764286, 0.40226173, 4.3505955] | Test Loss: [2.609871, 0.34966174, 4.87008]\n",
      "352: Train Loss: [2.44605, 0.318861, 4.573239] | Test Loss: [2.6738553, 0.44682208, 4.9008884]\n",
      "353: Train Loss: [2.3796678, 0.40982455, 4.349511] | Test Loss: [2.6564937, 0.34753722, 4.9654503]\n",
      "354: Train Loss: [2.372347, 0.30914265, 4.4355516] | Test Loss: [2.7003777, 0.3692989, 5.0314565]\n",
      "355: Train Loss: [2.5570748, 0.340622, 4.7735276] | Test Loss: [2.8236346, 0.4055325, 5.241737]\n",
      "356: Train Loss: [2.5137906, 0.33167502, 4.695906] | Test Loss: [2.6714506, 0.3585899, 4.984311]\n",
      "357: Train Loss: [2.4993117, 0.42674476, 4.5718784] | Test Loss: [2.625543, 0.38560843, 4.8654776]\n",
      "358: Train Loss: [2.5082443, 0.33705142, 4.679437] | Test Loss: [2.6423764, 0.37984338, 4.9049096]\n",
      "359: Train Loss: [2.375285, 0.26169932, 4.4888706] | Test Loss: [2.7070174, 0.41487348, 4.9991612]\n",
      "360: Train Loss: [2.5061193, 0.33646205, 4.6757765] | Test Loss: [2.6723304, 0.3646959, 4.9799647]\n",
      "361: Train Loss: [2.3889923, 0.30863467, 4.46935] | Test Loss: [2.7726328, 0.37194163, 5.173324]\n",
      "362: Train Loss: [2.4752958, 0.3425146, 4.608077] | Test Loss: [2.6942394, 0.32317826, 5.0653005]\n",
      "363: Train Loss: [2.5160224, 0.399817, 4.632228] | Test Loss: [2.6279743, 0.3474518, 4.908497]\n",
      "364: Train Loss: [2.356164, 0.39305496, 4.319273] | Test Loss: [2.7807634, 0.42390543, 5.1376214]\n",
      "365: Train Loss: [2.4474514, 0.39992025, 4.4949822] | Test Loss: [2.637717, 0.34111553, 4.9343185]\n",
      "366: Train Loss: [2.3874757, 0.352161, 4.4227905] | Test Loss: [2.739364, 0.34281796, 5.13591]\n",
      "367: Train Loss: [2.460298, 0.329968, 4.590628] | Test Loss: [2.7017817, 0.35585642, 5.047707]\n",
      "368: Train Loss: [2.5751498, 0.40442464, 4.745875] | Test Loss: [2.0203984, 0.32899472, 3.711802]\n",
      "369: Train Loss: [2.4117057, 0.32826594, 4.4951453] | Test Loss: [2.4814894, 0.43218815, 4.530791]\n",
      "370: Train Loss: [2.4313285, 0.35043395, 4.5122232] | Test Loss: [2.5997624, 0.3842287, 4.815296]\n",
      "371: Train Loss: [2.40191, 0.34572205, 4.458098] | Test Loss: [2.6337426, 0.36327046, 4.904215]\n",
      "372: Train Loss: [2.5955384, 0.34670106, 4.8443756] | Test Loss: [2.6308799, 0.41419664, 4.8475633]\n",
      "373: Train Loss: [2.3296375, 0.31998494, 4.33929] | Test Loss: [2.573695, 0.37323987, 4.77415]\n",
      "374: Train Loss: [2.4421883, 0.3383136, 4.546063] | Test Loss: [2.5292156, 0.37595662, 4.6824746]\n",
      "375: Train Loss: [2.5850825, 0.3981322, 4.7720327] | Test Loss: [2.6592512, 0.39577028, 4.9227324]\n",
      "376: Train Loss: [2.488214, 0.364224, 4.612204] | Test Loss: [2.6197715, 0.40019506, 4.839348]\n",
      "377: Train Loss: [2.3391874, 0.46680233, 4.2115726] | Test Loss: [2.7457688, 0.3250138, 5.166524]\n",
      "378: Train Loss: [2.5627196, 0.36940044, 4.7560387] | Test Loss: [2.694229, 0.34600228, 5.0424557]\n",
      "379: Train Loss: [2.5161717, 0.3977595, 4.634584] | Test Loss: [2.899174, 0.3769944, 5.4213533]\n",
      "380: Train Loss: [2.5606313, 0.36214533, 4.759117] | Test Loss: [2.4853573, 0.37269425, 4.5980206]\n",
      "381: Train Loss: [2.577788, 0.3584617, 4.7971144] | Test Loss: [2.6853027, 0.38262063, 4.9879847]\n",
      "382: Train Loss: [2.567459, 0.3533966, 4.781522] | Test Loss: [2.5194664, 0.32739466, 4.7115383]\n",
      "383: Train Loss: [2.478684, 0.38896412, 4.5684037] | Test Loss: [2.7233875, 0.45681676, 4.9899583]\n",
      "384: Train Loss: [2.3575935, 0.34480882, 4.370378] | Test Loss: [2.715709, 0.33233535, 5.0990825]\n",
      "385: Train Loss: [2.4464076, 0.36458597, 4.528229] | Test Loss: [2.6857967, 0.41146383, 4.9601297]\n",
      "386: Train Loss: [2.5234673, 0.4016877, 4.645247] | Test Loss: [2.837735, 0.3420355, 5.3334346]\n",
      "387: Train Loss: [2.5897636, 0.40861568, 4.7709117] | Test Loss: [2.766303, 0.3647529, 5.1678534]\n",
      "388: Train Loss: [2.5340023, 0.3847258, 4.6832786] | Test Loss: [2.8726883, 0.44906586, 5.296311]\n",
      "389: Train Loss: [2.6294034, 0.38145462, 4.877352] | Test Loss: [2.7282615, 0.36504486, 5.091478]\n",
      "390: Train Loss: [2.442208, 0.33613616, 4.54828] | Test Loss: [2.73881, 0.30331898, 5.174301]\n",
      "391: Train Loss: [2.4197285, 0.3638055, 4.4756517] | Test Loss: [2.5986013, 0.409936, 4.7872667]\n",
      "392: Train Loss: [2.3895478, 0.35526595, 4.4238296] | Test Loss: [2.7380784, 0.3345474, 5.141609]\n",
      "393: Train Loss: [2.4744015, 0.31452703, 4.634276] | Test Loss: [2.6484034, 0.3081774, 4.9886293]\n",
      "394: Train Loss: [2.416269, 0.34668773, 4.4858503] | Test Loss: [2.7072854, 0.3252424, 5.0893283]\n",
      "395: Train Loss: [2.4421113, 0.3188826, 4.56534] | Test Loss: [2.716704, 0.4243959, 5.0090117]\n",
      "396: Train Loss: [2.733136, 0.5315855, 4.934686] | Test Loss: [2.659854, 0.42815307, 4.891555]\n",
      "397: Train Loss: [2.4458942, 0.37364236, 4.518146] | Test Loss: [2.6099238, 0.36247745, 4.8573704]\n",
      "398: Train Loss: [2.519514, 0.39126813, 4.64776] | Test Loss: [2.4880707, 0.3310429, 4.6450987]\n",
      "399: Train Loss: [2.4274356, 0.33528292, 4.5195885] | Test Loss: [2.6434648, 0.33015597, 4.9567738]\n",
      "400: Train Loss: [2.3704739, 0.36944693, 4.371501] | Test Loss: [2.6336727, 0.36068222, 4.9066634]\n",
      "401: Train Loss: [2.41312, 0.34278324, 4.4834566] | Test Loss: [2.6296947, 0.38142887, 4.8779607]\n",
      "402: Train Loss: [2.404475, 0.38033727, 4.4286127] | Test Loss: [2.9062457, 0.45493564, 5.357556]\n",
      "403: Train Loss: [2.5170636, 0.36737454, 4.666753] | Test Loss: [2.744243, 0.32385218, 5.1646338]\n",
      "404: Train Loss: [2.4911315, 0.33785847, 4.6444044] | Test Loss: [2.7406104, 0.3723922, 5.1088285]\n",
      "405: Train Loss: [2.4432888, 0.28497884, 4.6015987] | Test Loss: [2.722776, 0.43292046, 5.0126314]\n",
      "406: Train Loss: [2.44316, 0.30016622, 4.586154] | Test Loss: [2.6306133, 0.37596482, 4.885262]\n",
      "407: Train Loss: [2.5056312, 0.31811306, 4.6931496] | Test Loss: [2.542594, 0.49444547, 4.5907426]\n",
      "408: Train Loss: [2.3147075, 0.30193254, 4.3274827] | Test Loss: [2.7152867, 0.390589, 5.0399847]\n",
      "409: Train Loss: [2.4638932, 0.3935475, 4.534239] | Test Loss: [2.6256702, 0.37794808, 4.873392]\n",
      "410: Train Loss: [2.4476957, 0.35732755, 4.538064] | Test Loss: [2.7247474, 0.32854542, 5.1209493]\n",
      "411: Train Loss: [2.4560645, 0.3462066, 4.5659223] | Test Loss: [2.732379, 0.30932784, 5.15543]\n",
      "412: Train Loss: [2.6230168, 0.3938496, 4.8521843] | Test Loss: [2.6302452, 0.33392835, 4.926562]\n",
      "413: Train Loss: [2.4960477, 0.3329225, 4.659173] | Test Loss: [2.8432257, 0.34454387, 5.3419075]\n",
      "414: Train Loss: [2.4903648, 0.4062836, 4.574446] | Test Loss: [2.6160417, 0.35247782, 4.8796053]\n",
      "415: Train Loss: [2.5320232, 0.40033904, 4.6637073] | Test Loss: [2.5578966, 0.39883223, 4.716961]\n",
      "416: Train Loss: [2.5206382, 0.4860727, 4.555204] | Test Loss: [2.4609928, 0.32178864, 4.600197]\n",
      "417: Train Loss: [2.428598, 0.35738158, 4.499814] | Test Loss: [2.6032743, 0.41466463, 4.791884]\n",
      "418: Train Loss: [2.3381577, 0.32973802, 4.346577] | Test Loss: [2.7031765, 0.37696904, 5.029384]\n",
      "419: Train Loss: [2.4783967, 0.3692484, 4.587545] | Test Loss: [2.7189963, 0.42925704, 5.0087357]\n",
      "420: Train Loss: [2.495289, 0.38395053, 4.6066275] | Test Loss: [2.843875, 0.4509583, 5.2367916]\n",
      "421: Train Loss: [2.4155083, 0.3120138, 4.519003] | Test Loss: [2.6452844, 0.43737411, 4.8531947]\n",
      "422: Train Loss: [2.380926, 0.30675536, 4.4550962] | Test Loss: [2.7559423, 0.429217, 5.082668]\n",
      "423: Train Loss: [2.2513745, 0.343617, 4.159132] | Test Loss: [2.7766497, 0.4792794, 5.07402]\n",
      "424: Train Loss: [2.3694162, 0.33273292, 4.4060993] | Test Loss: [2.4904914, 0.35782886, 4.623154]\n",
      "425: Train Loss: [2.3514311, 0.38061076, 4.3222513] | Test Loss: [2.5037334, 0.34243944, 4.665027]\n",
      "426: Train Loss: [2.4075422, 0.36751738, 4.447567] | Test Loss: [2.6787157, 0.34921628, 5.008215]\n",
      "427: Train Loss: [2.452774, 0.33609602, 4.5694523] | Test Loss: [2.8683867, 0.3318263, 5.4049473]\n",
      "428: Train Loss: [2.643307, 0.30986348, 4.9767504] | Test Loss: [2.6678536, 0.37943015, 4.956277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429: Train Loss: [2.5364394, 0.38228396, 4.6905947] | Test Loss: [2.5102122, 0.39702332, 4.623401]\n",
      "430: Train Loss: [2.288033, 0.346759, 4.229307] | Test Loss: [2.706495, 0.40209687, 5.0108933]\n",
      "431: Train Loss: [2.5914128, 0.34438303, 4.8384423] | Test Loss: [2.6040552, 0.39613777, 4.8119726]\n",
      "432: Train Loss: [2.2982082, 0.36430678, 4.2321095] | Test Loss: [2.6505625, 0.33862504, 4.9625]\n",
      "433: Train Loss: [2.5533519, 0.34399873, 4.762705] | Test Loss: [2.731352, 0.39278767, 5.0699167]\n",
      "434: Train Loss: [2.4686925, 0.406999, 4.530386] | Test Loss: [2.801827, 0.3839014, 5.2197523]\n",
      "435: Train Loss: [2.531302, 0.3720935, 4.6905103] | Test Loss: [2.4765718, 0.33958247, 4.613561]\n",
      "436: Train Loss: [2.3456373, 0.38408032, 4.307194] | Test Loss: [2.689478, 0.41666374, 4.962292]\n",
      "437: Train Loss: [2.4270208, 0.36903563, 4.485006] | Test Loss: [2.7276716, 0.3652626, 5.0900807]\n",
      "438: Train Loss: [2.5259063, 0.4207648, 4.6310477] | Test Loss: [2.8388753, 0.3692711, 5.3084793]\n",
      "439: Train Loss: [2.3983583, 0.3703084, 4.4264083] | Test Loss: [2.6409001, 0.363095, 4.9187055]\n",
      "440: Train Loss: [2.5313094, 0.3444027, 4.718216] | Test Loss: [2.7258055, 0.35915503, 5.092456]\n",
      "441: Train Loss: [2.4681973, 0.35746205, 4.578933] | Test Loss: [2.3679266, 0.29050627, 4.445347]\n",
      "442: Train Loss: [2.480253, 0.35898986, 4.6015162] | Test Loss: [2.6410613, 0.28941137, 4.992711]\n",
      "443: Train Loss: [2.381303, 0.3661144, 4.3964915] | Test Loss: [2.6026883, 0.35416597, 4.8512106]\n",
      "444: Train Loss: [2.6261377, 0.37852508, 4.87375] | Test Loss: [2.708487, 0.32461458, 5.0923595]\n",
      "445: Train Loss: [2.5571582, 0.3188916, 4.795425] | Test Loss: [2.6414704, 0.42421865, 4.858722]\n",
      "446: Train Loss: [2.6312408, 0.37382433, 4.8886576] | Test Loss: [2.6844926, 0.34442005, 5.024565]\n",
      "447: Train Loss: [2.5710664, 0.3438226, 4.7983103] | Test Loss: [2.7984805, 0.43855873, 5.1584024]\n",
      "448: Train Loss: [2.415218, 0.40725428, 4.423182] | Test Loss: [2.641013, 0.30595607, 4.97607]\n",
      "449: Train Loss: [2.5075393, 0.46087655, 4.554202] | Test Loss: [2.5543203, 0.38201502, 4.7266254]\n",
      "450: Train Loss: [2.3363197, 0.34468892, 4.3279505] | Test Loss: [2.8206944, 0.3439254, 5.2974634]\n",
      "451: Train Loss: [2.4796312, 0.32672116, 4.632541] | Test Loss: [2.5158234, 0.37834615, 4.653301]\n",
      "452: Train Loss: [2.3786142, 0.3708617, 4.386367] | Test Loss: [2.577579, 0.4253084, 4.72985]\n",
      "453: Train Loss: [2.5461504, 0.44241685, 4.649884] | Test Loss: [2.6729293, 0.365721, 4.980138]\n",
      "454: Train Loss: [2.4414058, 0.340173, 4.542639] | Test Loss: [2.4670553, 0.33571327, 4.5983973]\n",
      "455: Train Loss: [2.3959737, 0.39757356, 4.394374] | Test Loss: [2.623561, 0.43899754, 4.808124]\n",
      "456: Train Loss: [2.213244, 0.42362627, 4.0028615] | Test Loss: [2.820755, 0.4521379, 5.189372]\n",
      "457: Train Loss: [2.5935204, 0.41783044, 4.7692103] | Test Loss: [2.7680933, 0.35206488, 5.1841216]\n",
      "458: Train Loss: [2.4315453, 0.3209233, 4.542167] | Test Loss: [2.698133, 0.37132394, 5.024942]\n",
      "459: Train Loss: [2.4959505, 0.3716341, 4.620267] | Test Loss: [2.6316676, 0.4007978, 4.8625374]\n",
      "460: Train Loss: [2.3123715, 0.36217564, 4.2625675] | Test Loss: [2.755132, 0.4597706, 5.0504932]\n",
      "461: Train Loss: [2.371527, 0.35777268, 4.385281] | Test Loss: [2.6094687, 0.37204212, 4.846895]\n",
      "462: Train Loss: [2.3650858, 0.35545814, 4.3747134] | Test Loss: [2.7095954, 0.37384233, 5.0453486]\n",
      "463: Train Loss: [2.5607567, 0.37244254, 4.7490706] | Test Loss: [2.764039, 0.39287135, 5.1352067]\n",
      "464: Train Loss: [2.507879, 0.39256415, 4.6231937] | Test Loss: [2.4627194, 0.2709372, 4.654502]\n",
      "465: Train Loss: [2.40209, 0.31054705, 4.4936333] | Test Loss: [2.7097673, 0.3772811, 5.0422535]\n",
      "466: Train Loss: [2.3572285, 0.3891951, 4.325262] | Test Loss: [2.631649, 0.4229062, 4.8403916]\n",
      "467: Train Loss: [2.5100052, 0.50045687, 4.5195537] | Test Loss: [2.8096116, 0.37901527, 5.2402077]\n",
      "468: Train Loss: [2.5016053, 0.40946236, 4.593748] | Test Loss: [2.6546614, 0.33392063, 4.9754024]\n",
      "469: Train Loss: [2.4960637, 0.32210544, 4.670022] | Test Loss: [2.7173412, 0.38808787, 5.0465946]\n",
      "470: Train Loss: [2.546122, 0.40490052, 4.6873436] | Test Loss: [2.7266474, 0.35435936, 5.0989356]\n",
      "471: Train Loss: [2.5165262, 0.39365512, 4.639397] | Test Loss: [2.7825167, 0.33891666, 5.2261167]\n",
      "472: Train Loss: [2.4397178, 0.30401036, 4.575425] | Test Loss: [2.5094957, 0.36808795, 4.6509037]\n",
      "473: Train Loss: [2.4306452, 0.36375597, 4.4975343] | Test Loss: [2.6113033, 0.31908494, 4.9035215]\n",
      "474: Train Loss: [2.4477582, 0.36960384, 4.525913] | Test Loss: [2.648965, 0.4095416, 4.888388]\n",
      "475: Train Loss: [2.3467112, 0.31523496, 4.378187] | Test Loss: [2.6027358, 0.3579313, 4.8475404]\n",
      "476: Train Loss: [2.413132, 0.44895098, 4.377313] | Test Loss: [2.646082, 0.36215222, 4.9300117]\n",
      "477: Train Loss: [2.4525332, 0.36895302, 4.5361133] | Test Loss: [2.6782582, 0.3715317, 4.984985]\n",
      "478: Train Loss: [2.290824, 0.32377368, 4.257874] | Test Loss: [2.585737, 0.3532531, 4.818221]\n",
      "479: Train Loss: [2.545634, 0.39836827, 4.6928997] | Test Loss: [2.6764543, 0.36772427, 4.985184]\n",
      "480: Train Loss: [2.6035695, 0.3344274, 4.8727117] | Test Loss: [2.5815566, 0.37718433, 4.7859287]\n",
      "481: Train Loss: [2.3702288, 0.3667127, 4.373745] | Test Loss: [2.5199955, 0.41762158, 4.6223693]\n",
      "482: Train Loss: [2.4069269, 0.3513778, 4.462476] | Test Loss: [2.706912, 0.3807587, 5.0330653]\n",
      "483: Train Loss: [2.4968429, 0.3744775, 4.6192083] | Test Loss: [2.6484337, 0.44063002, 4.8562374]\n",
      "484: Train Loss: [2.438849, 0.3469846, 4.5307136] | Test Loss: [2.621108, 0.4039126, 4.8383036]\n",
      "485: Train Loss: [2.5418267, 0.36705688, 4.7165966] | Test Loss: [2.5431583, 0.32663184, 4.7596846]\n",
      "486: Train Loss: [2.6147888, 0.37204033, 4.8575373] | Test Loss: [2.5539696, 0.32296342, 4.784976]\n",
      "487: Train Loss: [2.370596, 0.31552622, 4.425666] | Test Loss: [2.9721332, 0.4742909, 5.4699755]\n",
      "488: Train Loss: [2.5407286, 0.37633204, 4.7051253] | Test Loss: [2.7194653, 0.34830174, 5.0906286]\n",
      "489: Train Loss: [2.4080498, 0.35560992, 4.4604897] | Test Loss: [2.798088, 0.48203063, 5.1141458]\n",
      "490: Train Loss: [2.490683, 0.37770241, 4.603664] | Test Loss: [2.6396513, 0.42977065, 4.849532]\n",
      "491: Train Loss: [2.4323812, 0.423392, 4.4413705] | Test Loss: [2.5997443, 0.33756804, 4.861921]\n",
      "492: Train Loss: [2.3850164, 0.3635523, 4.406481] | Test Loss: [2.6820998, 0.3736387, 4.990561]\n",
      "493: Train Loss: [2.3487651, 0.37873715, 4.3187933] | Test Loss: [2.7668068, 0.3729663, 5.1606474]\n",
      "494: Train Loss: [2.439737, 0.33317506, 4.546299] | Test Loss: [2.67405, 0.38300413, 4.965096]\n",
      "495: Train Loss: [2.5375884, 0.319054, 4.7561226] | Test Loss: [2.542505, 0.40506688, 4.679943]\n",
      "496: Train Loss: [2.3251255, 0.32764253, 4.3226085] | Test Loss: [2.7904508, 0.42266515, 5.1582365]\n",
      "497: Train Loss: [2.4438732, 0.3907601, 4.4969864] | Test Loss: [2.6287396, 0.35815609, 4.899323]\n",
      "498: Train Loss: [2.5439372, 0.40361017, 4.684264] | Test Loss: [2.6273289, 0.3759515, 4.8787065]\n",
      "499: Train Loss: [2.5980027, 0.35009056, 4.845915] | Test Loss: [2.7166069, 0.34492296, 5.0882907]\n",
      "500: Train Loss: [2.4378815, 0.35552508, 4.520238] | Test Loss: [2.6371853, 0.3086506, 4.96572]\n",
      "501: Train Loss: [2.4859145, 0.37387493, 4.597954] | Test Loss: [2.7361655, 0.37360212, 5.098729]\n",
      "502: Train Loss: [2.4541502, 0.33514902, 4.5731516] | Test Loss: [2.6935308, 0.3481129, 5.0389485]\n",
      "503: Train Loss: [2.4042249, 0.38100913, 4.4274406] | Test Loss: [2.352769, 0.39560482, 4.309933]\n",
      "504: Train Loss: [2.602812, 0.3738969, 4.831727] | Test Loss: [2.5629218, 0.3419634, 4.78388]\n",
      "505: Train Loss: [2.4110465, 0.3318024, 4.4902906] | Test Loss: [2.5721886, 0.3480992, 4.796278]\n",
      "506: Train Loss: [2.5616567, 0.35790458, 4.765409] | Test Loss: [2.9510772, 0.38627914, 5.5158753]\n",
      "507: Train Loss: [2.5757227, 0.35567954, 4.795766] | Test Loss: [2.6362596, 0.3322271, 4.940292]\n",
      "508: Train Loss: [2.5301816, 0.35885096, 4.7015123] | Test Loss: [2.5973678, 0.30297482, 4.891761]\n",
      "509: Train Loss: [2.5477293, 0.39406344, 4.701395] | Test Loss: [2.4942212, 0.3466322, 4.6418104]\n",
      "510: Train Loss: [2.5315847, 0.38589787, 4.677272] | Test Loss: [2.6332846, 0.36205575, 4.9045134]\n",
      "511: Train Loss: [2.6232834, 0.381279, 4.865288] | Test Loss: [2.7992764, 0.38360474, 5.214948]\n",
      "512: Train Loss: [2.5822153, 0.36891532, 4.7955155] | Test Loss: [2.7060711, 0.33692598, 5.0752163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513: Train Loss: [2.4138772, 0.3511518, 4.4766026] | Test Loss: [2.725643, 0.35115427, 5.1001315]\n",
      "514: Train Loss: [2.3987818, 0.36147124, 4.4360924] | Test Loss: [2.4716737, 0.38856208, 4.5547853]\n",
      "515: Train Loss: [2.498111, 0.50269604, 4.493526] | Test Loss: [2.680606, 0.44095206, 4.92026]\n",
      "516: Train Loss: [2.4515185, 0.3264609, 4.576576] | Test Loss: [2.645974, 0.39245403, 4.8994937]\n",
      "517: Train Loss: [2.484456, 0.3268864, 4.642026] | Test Loss: [3.1000433, 0.28213325, 5.9179535]\n",
      "518: Train Loss: [2.4512718, 0.35342348, 4.54912] | Test Loss: [2.727371, 0.34302977, 5.111712]\n",
      "519: Train Loss: [2.4359004, 0.37687895, 4.494922] | Test Loss: [2.8241134, 0.34983325, 5.2983932]\n",
      "520: Train Loss: [2.4597607, 0.30675435, 4.612767] | Test Loss: [2.6504343, 0.34127575, 4.959593]\n",
      "521: Train Loss: [2.5822473, 0.4136942, 4.7508] | Test Loss: [2.6413784, 0.35324535, 4.9295115]\n",
      "522: Train Loss: [2.3339412, 0.30953488, 4.3583474] | Test Loss: [2.5602381, 0.43696344, 4.6835127]\n",
      "523: Train Loss: [2.4485881, 0.4110111, 4.486165] | Test Loss: [2.6897893, 0.43134516, 4.9482336]\n",
      "Epoch 20\n",
      "0: Train Loss: [2.2733083, 0.35268164, 4.193935] | Test Loss: [2.4899292, 0.3551659, 4.6246924]\n",
      "1: Train Loss: [2.3508513, 0.3368286, 4.364874] | Test Loss: [2.6695912, 0.34546015, 4.9937224]\n",
      "2: Train Loss: [2.3823903, 0.3066573, 4.458123] | Test Loss: [2.6046224, 0.4170034, 4.792241]\n",
      "3: Train Loss: [2.3741765, 0.34704506, 4.401308] | Test Loss: [2.5448654, 0.30345458, 4.7862763]\n",
      "4: Train Loss: [2.2987485, 0.34691957, 4.2505774] | Test Loss: [2.6508086, 0.4021241, 4.899493]\n",
      "5: Train Loss: [2.3056023, 0.38553676, 4.225668] | Test Loss: [2.794118, 0.36662415, 5.2216115]\n",
      "6: Train Loss: [2.33645, 0.3026994, 4.3702006] | Test Loss: [2.6972635, 0.38233238, 5.0121946]\n",
      "7: Train Loss: [2.247028, 0.32628834, 4.167768] | Test Loss: [2.5521681, 0.3302149, 4.7741213]\n",
      "8: Train Loss: [2.2201896, 0.34075108, 4.099628] | Test Loss: [2.5959263, 0.29878658, 4.893066]\n",
      "9: Train Loss: [2.2610667, 0.30990043, 4.212233] | Test Loss: [2.6192732, 0.4148112, 4.823735]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f42782bcfc82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         training_loss = total_model.train_on_batch(\n\u001b[1;32m     19\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_input_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_output_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### TRAIN MODEL ####\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS + 1):\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    \n",
    "    for i, batch in enumerate(training_data()):\n",
    "        \n",
    "        val_batch = next(test_data_gen, None)\n",
    "        \n",
    "        if val_batch is None:\n",
    "            test_data_gen = test_data()\n",
    "            val_batch = next(test_data_gen, None)\n",
    "            \n",
    "        training_loss = total_model.train_on_batch(\n",
    "            [batch['document_tokens'], batch['question_input_tokens'], batch['answer_masks']]\n",
    "            , [np.expand_dims(batch['answer_labels'], axis = -1), np.expand_dims(batch['question_output_tokens'], axis = -1)]\n",
    "        )\n",
    "        \n",
    "        test_loss = total_model.test_on_batch(\n",
    "            [val_batch['document_tokens'], val_batch['question_input_tokens'], val_batch['answer_masks']]\n",
    "            , [np.expand_dims(val_batch['answer_labels'], axis = -1), np.expand_dims(val_batch['question_output_tokens'], axis = -1)]\n",
    "        )\n",
    "        \n",
    "        training_loss_history.append(training_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "        \n",
    "        print(\"{}: Train Loss: {} | Test Loss: {}\".format(i, training_loss, test_loss))\n",
    "        \n",
    "    total_model.save_weights('./saved_models/keras_{}.h5'.format(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####LOAD OLD MODELS ####\n",
    "\n",
    "# total_model.load_weights('./saved_models/keras_1.h5', by_name = True)\n",
    "# question_model.load_weights('./saved_models/keras_1.h5', by_name = True)\n",
    "# answer_model.load_weights('./saved_models/keras_1.h5', by_name = True)\n",
    "# decoder_initial_state_model.load_weights('./saved_models/keras_1.h5', by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### SHOW LOSSES ####\n",
    "\n",
    "utils.show_losses(np.array(training_loss_history)[:,0], np.array(test_loss_history)[:,0])\n",
    "pkl.dump([training_loss_history, test_loss_history], open('./saved_models/histories.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GET TEST BATCH\n",
    "\n",
    "batch = next(test_data_gen)\n",
    "batch = collapse_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      " TEXT\n",
      "NEW YORK -LRB- CNN -RRB- -- Yet another scale Il Divo has mastered : scaling the heights of success . The quartet was formed in 2004 after a global search . The operatic quartet 's new album , `` The Promise , '' debuted atop the UK charts , their third collection to hit the No. 1 spot . The album -LRB- which was released in the U.S. last Tuesday -RRB- features Leonard Cohen 's `` Hallelujah , '' Frankie Goes to Hollywood 's `` The Power of Love , '' and the traditional `` Amazing Grace . '' Il Divo also takes a chance on the ABBA hit `` The Winner Takes it All . '' The group is the brainchild of `` American Idol '' judge Simon Cowell , who saw a potential market for an international , `` popera '' - style act after the soaring success of the Andrea Bocelli-Sarah Brightman duet `` Time to Say Goodbye -LRB- Con te Partiro -RRB- . '' Watch Il Divo in action '' Formed in 2004 after a global search , Il Divo -LRB- Italian for `` divine male performer '' -RRB- consists of Spanish baritone Carlos Marin , American tenor David Miller , French pop singer Sebastien Izambard , and Swiss tenor Urs Buhler . According to Syco Music , Il Divo 's UK-based label , the quartet has sold more than 22 million albums worldwide . It seems Cowell knew what he was doing . CNN caught up with Il Divo to find out how close they are to Cowell , and which American pop diva they dream of collaborating with . CNN : Who do you most get compared to , the Three Tenors or the Backstreet Boys ? Carlos Marin : -L\n",
      "\n",
      " LAST WORDS\n",
      "['spanish', 'baritone', 'carlos', 'marin', ',']\n",
      "\n",
      " QUESTION TEXT\n",
      "Who created this group ?\n",
      "\n",
      " ANSWER TEXT\n",
      "simon cowell\n",
      "\n",
      " ANSWER INDICES\n",
      "[128, 129]\n",
      "\n",
      " ANSWER TOKENS\n",
      "[3311 9691]\n",
      "\n",
      " MODEL ANSWER TEXT\n",
      "\n",
      "\n",
      " MODEL ANSWER INDICES\n",
      "[]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tried to read from index -1 but array size is: 0\n\t [[{{node encoder_cell/TensorArrayReadV3}} = TensorArrayReadV3[_class=[\"loc:@train...rayWriteV3\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](encoder_cell/TensorArray, encoder_cell/sub_1, encoder_cell/while/Exit_1)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5aa9a90e3057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0manswer_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#### PLOT ANSWER LIKELIHOOD ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/Personal/GDL/qgen-workshop/qgen/utils.py\u001b[0m in \u001b[0;36mshow_test\u001b[0;34m(batch, answer_batch, idx, answer_model, decoder_initial_state_model, question_model)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mnext_decoder_init_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_initial_state_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSTART_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tried to read from index -1 but array size is: 0\n\t [[{{node encoder_cell/TensorArrayReadV3}} = TensorArrayReadV3[_class=[\"loc:@train...rayWriteV3\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](encoder_cell/TensorArray, encoder_cell/sub_1, encoder_cell/while/Exit_1)]]"
     ]
    }
   ],
   "source": [
    "#### TESTING ####\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "idx = 0\n",
    "idx = random.randint(0,10)\n",
    "print(idx)\n",
    "\n",
    "answer_preds = answer_model.predict(batch[\"document_tokens\"])\n",
    "\n",
    "answers = np.squeeze(np.round(answer_preds))\n",
    "answer_batch = expand_answers(batch, answers)\n",
    "\n",
    "utils.show_test(batch, answer_batch, idx, answer_model, decoder_initial_state_model, question_model)\n",
    "\n",
    "#### PLOT ANSWER LIKELIHOOD ####\n",
    "plt.plot(batch['answer_labels'][idx])\n",
    "plt.plot(answer_preds[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
